{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb086908",
   "metadata": {
    "papermill": {
     "duration": 0.011894,
     "end_time": "2025-06-08T19:09:18.737650",
     "exception": false,
     "start_time": "2025-06-08T19:09:18.725756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0f1df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:18.760001Z",
     "iopub.status.busy": "2025-06-08T19:09:18.759742Z",
     "iopub.status.idle": "2025-06-08T19:09:40.443622Z",
     "shell.execute_reply": "2025-06-08T19:09:40.442961Z"
    },
    "papermill": {
     "duration": 21.69674,
     "end_time": "2025-06-08T19:09:40.445223",
     "exception": false,
     "start_time": "2025-06-08T19:09:18.748483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefa45a",
   "metadata": {
    "papermill": {
     "duration": 0.01067,
     "end_time": "2025-06-08T19:09:40.467409",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.456739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598fedec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.489748Z",
     "iopub.status.busy": "2025-06-08T19:09:40.489297Z",
     "iopub.status.idle": "2025-06-08T19:09:40.492642Z",
     "shell.execute_reply": "2025-06-08T19:09:40.492030Z"
    },
    "papermill": {
     "duration": 0.015683,
     "end_time": "2025-06-08T19:09:40.493758",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.478075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b2448c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.516232Z",
     "iopub.status.busy": "2025-06-08T19:09:40.516022Z",
     "iopub.status.idle": "2025-06-08T19:09:40.519524Z",
     "shell.execute_reply": "2025-06-08T19:09:40.518915Z"
    },
    "papermill": {
     "duration": 0.016065,
     "end_time": "2025-06-08T19:09:40.520701",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.504636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0880abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.543012Z",
     "iopub.status.busy": "2025-06-08T19:09:40.542779Z",
     "iopub.status.idle": "2025-06-08T19:09:40.551474Z",
     "shell.execute_reply": "2025-06-08T19:09:40.550922Z"
    },
    "papermill": {
     "duration": 0.021004,
     "end_time": "2025-06-08T19:09:40.552618",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.531614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaefa71",
   "metadata": {
    "papermill": {
     "duration": 0.010716,
     "end_time": "2025-06-08T19:09:40.574261",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.563545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3c23f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.596303Z",
     "iopub.status.busy": "2025-06-08T19:09:40.596075Z",
     "iopub.status.idle": "2025-06-08T19:09:40.651537Z",
     "shell.execute_reply": "2025-06-08T19:09:40.650191Z"
    },
    "papermill": {
     "duration": 0.068265,
     "end_time": "2025-06-08T19:09:40.653280",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.585015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "aspect_accuracies = manager.list()\n",
    "aspect_f1_micros = manager.list()\n",
    "aspect_f1_macros = manager.list()\n",
    "sentiment_accuracies = manager.list()\n",
    "sentiment_f1_micros = manager.list()\n",
    "sentiment_f1_macros = manager.list()\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'casa-besra'\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "sequence_length = 48\n",
    "\n",
    "aspect_list = ['fuel', 'machine', 'others', 'part', 'price', 'service']\n",
    "aspect_mapping = {'fuel': 0, 'machine': 1, 'others': 2, 'part': 3, 'price': 4, 'service': 5 }\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, 'positive': 2}\n",
    "ignored_keys = ['labels', 'ori_text', 'ori_label', 'ori_indices', 'aspect']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53376b8",
   "metadata": {
    "papermill": {
     "duration": 0.011031,
     "end_time": "2025-06-08T19:09:40.675337",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.664306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56535199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.697970Z",
     "iopub.status.busy": "2025-06-08T19:09:40.697676Z",
     "iopub.status.idle": "2025-06-08T19:09:40.771433Z",
     "shell.execute_reply": "2025-06-08T19:09:40.770543Z"
    },
    "papermill": {
     "duration": 0.086677,
     "end_time": "2025-06-08T19:09:40.772737",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.686060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/casa-dataset/train_preprocess.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/casa-dataset/valid_preprocess.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('/kaggle/input/casa-dataset/test_preprocess.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b4fa8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.795875Z",
     "iopub.status.busy": "2025-06-08T19:09:40.795628Z",
     "iopub.status.idle": "2025-06-08T19:09:40.804022Z",
     "shell.execute_reply": "2025-06-08T19:09:40.803239Z"
    },
    "papermill": {
     "duration": 0.021269,
     "end_time": "2025-06-08T19:09:40.805200",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.783931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2281b40f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.828296Z",
     "iopub.status.busy": "2025-06-08T19:09:40.828038Z",
     "iopub.status.idle": "2025-06-08T19:09:40.835377Z",
     "shell.execute_reply": "2025-06-08T19:09:40.834531Z"
    },
    "papermill": {
     "duration": 0.020333,
     "end_time": "2025-06-08T19:09:40.836618",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.816285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecab43e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.860269Z",
     "iopub.status.busy": "2025-06-08T19:09:40.860038Z",
     "iopub.status.idle": "2025-06-08T19:09:40.869966Z",
     "shell.execute_reply": "2025-06-08T19:09:40.869220Z"
    },
    "papermill": {
     "duration": 0.022885,
     "end_time": "2025-06-08T19:09:40.871194",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.848309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864,) (864, 6)\n",
      "(216,) (216, 6)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data.columns[1:]\n",
    "val_labels = val_data.columns[1:]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['sentence'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['sentence'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907c73b",
   "metadata": {
    "papermill": {
     "duration": 0.010906,
     "end_time": "2025-06-08T19:09:40.893695",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.882789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526d14af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.918099Z",
     "iopub.status.busy": "2025-06-08T19:09:40.917826Z",
     "iopub.status.idle": "2025-06-08T19:09:40.926669Z",
     "shell.execute_reply": "2025-06-08T19:09:40.925850Z"
    },
    "papermill": {
     "duration": 0.022858,
     "end_time": "2025-06-08T19:09:40.927804",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.904946",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AspectDetectionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, label_mapping, tokenizer, max_length=sequence_length, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        original_labels = [self.label_mapping[label] for label in self.labels[idx]]\n",
    "        encoded_labels = [1 if label == 1 else 0 for label in original_labels]\n",
    "        \n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['ori_indices'] = idx\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(original_labels, dtype=torch.float)\n",
    "        item['labels'] = torch.tensor(encoded_labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd3a9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.952203Z",
     "iopub.status.busy": "2025-06-08T19:09:40.951974Z",
     "iopub.status.idle": "2025-06-08T19:09:40.961203Z",
     "shell.execute_reply": "2025-06-08T19:09:40.960390Z"
    },
    "papermill": {
     "duration": 0.022791,
     "end_time": "2025-06-08T19:09:40.962370",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.939579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, texts, labels, aspects, indices, label_mapping, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.aspects = aspects\n",
    "        self.indices = indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = aspect_token + ' ' + self.aspects[idx] + ' ' + review_token + ' ' + self.texts[idx] \n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        if isinstance(self.labels[idx], str):\n",
    "            self.labels[idx] = self.label_mapping[self.labels[idx]]\n",
    "        elif torch.is_tensor(self.labels[idx]):\n",
    "            self.labels[idx] = int(self.labels[idx].item())\n",
    "\n",
    "        encoded_label = 1 if self.labels[idx] == 2 else self.labels[idx]\n",
    "        one_hot_label = F.one_hot(torch.tensor(encoded_label, dtype=torch.long), num_classes=2).float()\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['aspect'] = self.aspects[idx]\n",
    "        item['labels'] = one_hot_label\n",
    "        item['ori_indices'] = self.indices[idx]\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdce1794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:40.985763Z",
     "iopub.status.busy": "2025-06-08T19:09:40.985565Z",
     "iopub.status.idle": "2025-06-08T19:09:41.749700Z",
     "shell.execute_reply": "2025-06-08T19:09:41.748831Z"
    },
    "papermill": {
     "duration": 0.777592,
     "end_time": "2025-06-08T19:09:41.751377",
     "exception": false,
     "start_time": "2025-06-08T19:09:40.973785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f4b1a77005431d90f296e45b907836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3839c9740a1345389bfbbeecea20c12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47db88806b1404e9aaaa97ed0188005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5287a55a8d5242a7a1980d366574a44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "review_token = '[REVIEW]'\n",
    "aspect_token = '[ASPECT]'\n",
    "special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00125e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:41.777490Z",
     "iopub.status.busy": "2025-06-08T19:09:41.777242Z",
     "iopub.status.idle": "2025-06-08T19:09:41.781607Z",
     "shell.execute_reply": "2025-06-08T19:09:41.780783Z"
    },
    "papermill": {
     "duration": 0.018157,
     "end_time": "2025-06-08T19:09:41.782994",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.764837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_aspect_dataset(X_train, y_train, X_val, y_val, sequence_length, num_workers=4):\n",
    "    train_dataset = AspectDetectionDataset(X_train, y_train, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = AspectDetectionDataset(X_val, y_val, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "546e0ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:41.807652Z",
     "iopub.status.busy": "2025-06-08T19:09:41.807432Z",
     "iopub.status.idle": "2025-06-08T19:09:41.817340Z",
     "shell.execute_reply": "2025-06-08T19:09:41.816685Z"
    },
    "papermill": {
     "duration": 0.023678,
     "end_time": "2025-06-08T19:09:41.818452",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.794774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_sentiment_dataset(device, train_dataset, val_dataset, aspect_detection_model, tokenizer, max_length=sequence_length):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    aspect_detection_model.to(device)\n",
    "    aspect_detection_model.eval()\n",
    "\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_aspects = []\n",
    "    train_indices = []\n",
    "\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    val_aspects = []\n",
    "    val_indices = []\n",
    "\n",
    "    # Transform train set\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        train_aspects.append(aspect_list[j])\n",
    "                        train_data.append(batch['ori_text'][i])\n",
    "                        train_labels.append(batch['ori_label'][i][j])\n",
    "                        train_indices.append(batch['ori_indices'][i])\n",
    "            \n",
    "        # Transform validation set\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        val_aspects.append(aspect_list[j])\n",
    "                        val_data.append(batch['ori_text'][i])\n",
    "                        val_labels.append(batch['ori_label'][i][j])\n",
    "                        val_indices.append(batch['ori_indices'][i])\n",
    "\n",
    "    # if len(train_data) > 0:\n",
    "    train_dataset = SentimentAnalysisDataset(train_data, train_labels, train_aspects, train_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "    val_dataset = SentimentAnalysisDataset(val_data, val_labels, val_aspects, val_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "    # return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb692ee",
   "metadata": {
    "papermill": {
     "duration": 0.016247,
     "end_time": "2025-06-08T19:09:41.847723",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.831476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab7ee0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:41.880401Z",
     "iopub.status.busy": "2025-06-08T19:09:41.880116Z",
     "iopub.status.idle": "2025-06-08T19:09:41.884728Z",
     "shell.execute_reply": "2025-06-08T19:09:41.883675Z"
    },
    "papermill": {
     "duration": 0.020334,
     "end_time": "2025-06-08T19:09:41.886004",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.865670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    # int(0.1 * total_data),\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c0541f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:41.911966Z",
     "iopub.status.busy": "2025-06-08T19:09:41.911721Z",
     "iopub.status.idle": "2025-06-08T19:09:41.916291Z",
     "shell.execute_reply": "2025-06-08T19:09:41.915678Z"
    },
    "papermill": {
     "duration": 0.01919,
     "end_time": "2025-06-08T19:09:41.917407",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.898217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p, label, classes):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=label,\n",
    "        target_names=classes,\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3981efc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:41.942311Z",
     "iopub.status.busy": "2025-06-08T19:09:41.942072Z",
     "iopub.status.idle": "2025-06-08T19:09:41.948211Z",
     "shell.execute_reply": "2025-06-08T19:09:41.947424Z"
    },
    "papermill": {
     "duration": 0.020143,
     "end_time": "2025-06-08T19:09:41.949465",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.929322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_overall(p, classes):\n",
    "    preds = torch.tensor(p.predictions)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Ensure it's in the correct shape\n",
    "    if preds.shape != labels.shape:\n",
    "        raise ValueError(\"Shape mismatch: predictions and labels must have the same shape.\")\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Compute per-label (column-wise) precision, recall, F1\n",
    "    precision_list, recall_list, f1_micro_list, f1_macro_list = [], [], [], []\n",
    "    \n",
    "    for i in range(labels.shape[1]):  # Loop through each column (multi-output)\n",
    "        prec, rec, f1_micro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='micro', zero_division=0\n",
    "        )\n",
    "        _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        precision_list.append(prec)\n",
    "        recall_list.append(rec)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        f1_macro_list.append(f1_macro)\n",
    "\n",
    "    # Compute average metrics across all outputs\n",
    "    precision = sum(precision_list) / len(precision_list)\n",
    "    recall = sum(recall_list) / len(recall_list)\n",
    "    f1_micro = sum(f1_micro_list) / len(f1_micro_list)\n",
    "    f1_macro = sum(f1_macro_list) / len(f1_macro_list)\n",
    "\n",
    "    # Generate classification report per output\n",
    "    reports = [classification_report(labels[:, i], preds[:, i], target_names=classes, zero_division=0) for i in range(labels.shape[1])]\n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'reports': reports  # Returns list of reports, one for each output label\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3d47f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:41.975259Z",
     "iopub.status.busy": "2025-06-08T19:09:41.974923Z",
     "iopub.status.idle": "2025-06-08T19:09:42.012840Z",
     "shell.execute_reply": "2025-06-08T19:09:42.011948Z"
    },
    "papermill": {
     "duration": 0.052994,
     "end_time": "2025-06-08T19:09:42.014573",
     "exception": false,
     "start_time": "2025-06-08T19:09:41.961579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, aspect_metrics, sentiment_metrics, metrics, trials, model_num):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Setup Aspect Model\n",
    "    aspect_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=len(train_labels),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    ) \n",
    "    aspect_optimizer = torch.optim.AdamW(aspect_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    for name, param in aspect_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Setup Sentiment Model\n",
    "    sentiment_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=2,\n",
    "    )\n",
    "    sentiment_optimizer = torch.optim.AdamW(sentiment_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    for name, param in sentiment_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare models\n",
    "    aspect_model, aspect_optimizer = accelerator.prepare(aspect_model, aspect_optimizer)\n",
    "    sentiment_model, sentiment_optimizer = accelerator.prepare(sentiment_model, sentiment_optimizer)\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    aspect_train_loader, aspect_val_loader, aspect_train_dataset, aspect_val_dataset = build_aspect_dataset(current_X_train, current_y_train, X_val, y_val, sequence_length)\n",
    "\n",
    "    # Prepare train loaders\n",
    "    aspect_train_loader, aspect_val_loader = accelerator.prepare(\n",
    "        aspect_train_loader, aspect_val_loader\n",
    "    )\n",
    "\n",
    "    nearest_cp = current_train_size\n",
    "    if nearest_cp not in checkpoints:\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "    percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "\n",
    "    aspect_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # ASPECT DETECTION\n",
    "    accelerator.print(\"ASPECT DETECTION\")\n",
    "    for epoch in range(epochs):\n",
    "        aspect_model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in aspect_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            aspect_optimizer.zero_grad()\n",
    "            outputs = aspect_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            aspect_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        aspect_model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in aspect_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = aspect_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}),\n",
    "            None,\n",
    "            aspect_list,\n",
    "        )\n",
    "\n",
    "        if aspect_result is None or result['f1_micro'] >= aspect_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(aspect_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-aspect-{trials + 1}-model-{model_num+1}-{percentage}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            aspect_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(aspect_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"\\nModel {model_num+1} of aspect detection, Accuracy: {round(aspect_result['accuracy'], 4)}, F1 Micro: {round(aspect_result['f1_micro'], 4)}, F1 Macro: {round(aspect_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(aspect_result['report'])\n",
    "\n",
    "    best_aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{trials + 1}-model-{model_num+1}-{percentage}')\n",
    "    best_aspect_model = accelerator.prepare(best_aspect_model)\n",
    "\n",
    "    # SENTIMENT ANALYSIS ON NON NEUTRAL ASPECTS\n",
    "    accelerator.print(\"--------------------------------------------------\")\n",
    "    accelerator.print(\"SENTIMENT ANALYSIS\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    sentiment_train_loader, sentiment_val_loader, sentiment_train_dataset, sentiment_val_dataset = build_sentiment_dataset(\n",
    "        device, aspect_train_dataset, aspect_val_dataset, best_aspect_model, tokenizer, max_length=sequence_length\n",
    "    )\n",
    "    sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader = accelerator.prepare(\n",
    "        sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader\n",
    "    )\n",
    "    sentiment_result = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sentiment_model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in sentiment_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            sentiment_optimizer.zero_grad()\n",
    "            outputs = sentiment_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            sentiment_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        sentiment_model.eval()\n",
    "        sentiment_val_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in sentiment_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                \n",
    "                outputs = sentiment_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                for i in range(len(preds)):\n",
    "                    val_output = {\n",
    "                        'label': batch['labels'][i],\n",
    "                        'aspect': batch['aspect'][i],\n",
    "                        'ori_indices': batch['ori_indices'][i],\n",
    "                        'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                    }\n",
    "                    sentiment_val_outputs.append(val_output)\n",
    "\n",
    "        sentiment_val_outputs = accelerator.gather_for_metrics(sentiment_val_outputs)\n",
    "        unique_val_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_val_outputs}\n",
    "        sentiment_val_outputs = list(unique_val_outputs.values())\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': [item['pred'] for item in sentiment_val_outputs], 'label_ids': [np.argmax(item['label'].cpu().numpy()) for item in sentiment_val_outputs]}),\n",
    "            [0, 1],\n",
    "            ['negative', 'positive']\n",
    "        )\n",
    "\n",
    "        if sentiment_result is None or result['f1_micro'] >= sentiment_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            sentiment_result = result\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(sentiment_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                 f'{filename}-sentiment-{trials + 1}-model-{model_num+1}-{percentage}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(sentiment_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    accelerator.print(f\"\\nModel {model_num+1} of sentiment analysis, accuracy: {round(sentiment_result['accuracy'], 4)}, F1 Micro: {round(sentiment_result['f1_micro'], 4)}, F1 Macro: {round(sentiment_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(sentiment_result['report'])\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    best_sentiment_model = BertForSequenceClassification.from_pretrained( f'{filename}-sentiment-{trials + 1}-model-{model_num+1}-{percentage}')\n",
    "    best_sentiment_model = accelerator.prepare(best_sentiment_model)\n",
    "\n",
    "    # Compute overall metrics\n",
    "    aspect_labels = []\n",
    "    aspect_indices = []\n",
    "    aspect_preds = []\n",
    "\n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = []\n",
    "    \n",
    "    best_aspect_model.eval()\n",
    "    best_sentiment_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in aspect_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = best_aspect_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            aspect_indices.append(accelerator.gather(batch['ori_indices']))\n",
    "            aspect_labels.append(accelerator.gather(batch['ori_label']))\n",
    "            aspect_preds.append(accelerator.gather(preds))\n",
    "\n",
    "        aspect_indices = torch.cat(aspect_indices).cpu().numpy()\n",
    "        aspect_labels = torch.cat(aspect_labels).cpu().numpy()\n",
    "        aspect_preds = torch.cat(aspect_preds).cpu().numpy()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        aspect_outputs = [\n",
    "            {'ori_indices': aspect_indices[i], \n",
    "             'ori_labels': aspect_labels[i], \n",
    "             'pred': aspect_preds[i]}\n",
    "            for i in range(len(aspect_preds))\n",
    "        ]\n",
    "        aspect_outputs = {x['ori_indices'].item(): x for x in aspect_outputs}\n",
    "    \n",
    "        for batch in sentiment_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = best_sentiment_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "    \n",
    "            for i in range(len(preds)):\n",
    "                output = {\n",
    "                    'aspect': batch['aspect'][i],\n",
    "                    'ori_indices': batch['ori_indices'][i],\n",
    "                    'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                }\n",
    "                sentiment_outputs.append(output)\n",
    "\n",
    "        sentiment_outputs = accelerator.gather_for_metrics(sentiment_outputs)\n",
    "        sentiment_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_outputs}\n",
    "\n",
    "    # Replcae non neutral aspect to its predicted sentiment\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        i = -1\n",
    "        for (ori_index, aspect), value in sentiment_outputs.items():\n",
    "            aspect = aspect_mapping[aspect]\n",
    "            aspect_outputs[ori_index]['pred'][aspect] = 2 if value['pred'] == 1.0 else value['pred']\n",
    "\n",
    "        result = compute_metrics_overall(\n",
    "            type('EvalOutput', (object,), {'predictions': [output['pred'] for output in aspect_outputs.values()], 'label_ids': [output['ori_labels'] for output in aspect_outputs.values()]}),\n",
    "            ['negative', 'neutral', 'positive'],\n",
    "        )\n",
    "\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        accelerator.print(f\"Model {model_num+1} - Iteration {current_train_size}: Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        for i in range(len(train_labels)):\n",
    "            accelerator.print(f\"Aspect {aspect_list[i]} report:\")\n",
    "            accelerator.print(result['reports'][i])\n",
    "       \n",
    "        aspect_metrics[0].append(aspect_result['accuracy'])\n",
    "        aspect_metrics[1].append(aspect_result['f1_micro'])\n",
    "        aspect_metrics[2].append(aspect_result['f1_macro'])\n",
    "        sentiment_metrics[0].append(sentiment_result['accuracy'])\n",
    "        sentiment_metrics[1].append(sentiment_result['f1_micro'])\n",
    "        sentiment_metrics[2].append(sentiment_result['f1_macro'])\n",
    "        metrics[0].append(result['accuracy'])\n",
    "        metrics[1].append(result['f1_micro'])\n",
    "        metrics[2].append(result['f1_macro'])\n",
    "        \n",
    "    accelerator.print(f\"Total train time: {duration} s\")\n",
    "    accelerator.end_training()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21147f06",
   "metadata": {
    "papermill": {
     "duration": 0.015095,
     "end_time": "2025-06-08T19:09:42.046644",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.031549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d7592a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:42.074219Z",
     "iopub.status.busy": "2025-06-08T19:09:42.073988Z",
     "iopub.status.idle": "2025-06-08T19:09:42.079212Z",
     "shell.execute_reply": "2025-06-08T19:09:42.078583Z"
    },
    "papermill": {
     "duration": 0.019168,
     "end_time": "2025-06-08T19:09:42.080377",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.061209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82a4bc",
   "metadata": {
    "papermill": {
     "duration": 0.011688,
     "end_time": "2025-06-08T19:09:42.104158",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.092470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564852c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:42.128780Z",
     "iopub.status.busy": "2025-06-08T19:09:42.128578Z",
     "iopub.status.idle": "2025-06-08T19:09:42.139906Z",
     "shell.execute_reply": "2025-06-08T19:09:42.139125Z"
    },
    "papermill": {
     "duration": 0.025043,
     "end_time": "2025-06-08T19:09:42.141094",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.116051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p_pred_one, y_outcome, alpha=0.1, beta_p=3.0):\n",
    "    \"\"\"Calculates Beta score for a given probability p_pred_one and label y_outcome.\"\"\"\n",
    "    epsilon = 1e-9\n",
    "    p_safe = np.clip(p_pred_one, epsilon, 1.0 - epsilon)\n",
    "\n",
    "    if y_outcome == 1:\n",
    "        arg1_a = alpha\n",
    "        arg2_b_p1 = beta_p + 1\n",
    "        arg3_a_p = alpha + p_safe\n",
    "        arg4_b_p1_m_p = beta_p + 1 - p_safe\n",
    "        if not (arg1_a > 0 and arg2_b_p1 > 0 and arg3_a_p > 0 and arg4_b_p1_m_p > 0):\n",
    "            return -1e9\n",
    "        return -betaln(arg1_a, arg2_b_p1) + betaln(arg3_a_p, arg4_b_p1_m_p)\n",
    "    elif y_outcome == 0:\n",
    "        arg1_a_p1 = alpha + 1\n",
    "        arg2_b = beta_p\n",
    "        arg3_a_p1_m_p = alpha + 1 - p_safe\n",
    "        arg4_b_p = beta_p + p_safe\n",
    "        if not (arg1_a_p1 > 0 and arg2_b > 0 and arg3_a_p1_m_p > 0 and arg4_b_p > 0):\n",
    "            return -1e9\n",
    "        return -betaln(arg1_a_p1, arg2_b) + betaln(arg3_a_p1_m_p, arg4_b_p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y_outcome must be 0 or 1.\")\n",
    "\n",
    "def calculate_expected_beta_score(p_one, alpha, beta_p):\n",
    "    \"\"\"\n",
    "    Calculates E[S(P,Y)] = P(Y=1|P)S(P,1) + P(Y=0|P)S(P,0) [cite: 54, 65]\n",
    "    This is related to the 'gp' (green line) in Figure 1 of the paper.\n",
    "    \"\"\"\n",
    "    score_if_one = beta_score(p_one, 1, alpha, beta_p)\n",
    "    score_if_zero = beta_score(p_one, 0, alpha, beta_p)\n",
    "    expected_score = p_one * score_if_one + (1.0 - p_one) * score_if_zero\n",
    "    return expected_score\n",
    "\n",
    "def get_ensemble_predictions_for_batch(models_ensemble, input_ids, attn_mask, device):\n",
    "    \"\"\"\n",
    "    Gets predictions from the ensemble for a given batch.\n",
    "    Returns:\n",
    "        avg_probs (Tensor): Average probabilities [batch_size, num_classes].\n",
    "        indiv_probs_list (List[Tensor]): List of probabilities for each model.\n",
    "    \"\"\"\n",
    "    indiv_probs_gpu_list = []\n",
    "    with torch.no_grad():\n",
    "        for model_instance in models_ensemble: \n",
    "            model_instance.eval()\n",
    "            model_instance.to(device)\n",
    "            outputs = model_instance(input_ids.to(device), attention_mask=attn_mask.to(device))\n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            indiv_probs_gpu_list.append(probs)\n",
    "\n",
    "        if not indiv_probs_gpu_list:\n",
    "            return torch.empty(0, device=device), []\n",
    "\n",
    "        indiv_probs_stack = torch.stack(indiv_probs_gpu_list)\n",
    "        avg_probs = torch.mean(indiv_probs_stack, dim=0)\n",
    "    \n",
    "    return avg_probs, indiv_probs_gpu_list\n",
    "\n",
    "def calculate_score_change(cand_avg_probs, est_avg_probs, cand_indiv_probs_list_concatenated, est_indiv_probs_list_concatenated, num_classes, alpha_param=0.1, beta_param=3.0):\n",
    "    cand_prob_L_all_cls = cand_avg_probs\n",
    "    cand_prob_models_all_cls = [m_probs for m_probs in cand_indiv_probs_list_concatenated]\n",
    "\n",
    "    current_cand_score = 0.0 \n",
    "\n",
    "    # Iterate through estimation samples x' from Estimation Pool X\n",
    "    for i_est in range(len(est_avg_probs)): \n",
    "        est_prob_L_all_cls = est_avg_probs[i_est]\n",
    "        est_prob_models_all_cls = [m_probs[i_est] for m_probs in est_indiv_probs_list_concatenated]\n",
    "\n",
    "        # Calculate estimated beta score\n",
    "        est_score_L_sum_k = 0.0\n",
    "        for k_est_cls_idx in range(num_classes):\n",
    "            prob_one_L_est_k = est_prob_L_all_cls[k_est_cls_idx].item()\n",
    "            est_score_L_sum_k += calculate_expected_beta_score(prob_one_L_est_k, alpha_param, beta_param)\n",
    "        \n",
    "        # Ensemble reweighting\n",
    "        delta_Q_est = 0.0 \n",
    "        for j_cand_cls_idx in range(num_classes): # For each class j of candidate x\n",
    "            avg_est_score_Lplus_for_j = 0.0\n",
    "            for cand_hypo_label_y_j in [0, 1]: \n",
    "                prob_one_cand_j_L = cand_prob_L_all_cls[j_cand_cls_idx].item()\n",
    "                prob_cand_j_hypo_L = prob_one_cand_j_L if cand_hypo_label_y_j == 1 else (1.0 - prob_one_cand_j_L)\n",
    "\n",
    "                # Calculate weights for each model based on the candidate's hypothetical label\n",
    "                model_weights = [] \n",
    "                for model_idx_loop in range(3):\n",
    "                    prob_one_cand_j_model_e = cand_prob_models_all_cls[model_idx_loop][j_cand_cls_idx].item()\n",
    "                    prob_cand_j_hypo_model_e = prob_one_cand_j_model_e if cand_hypo_label_y_j == 1 else (1.0 - prob_one_cand_j_model_e)\n",
    "                    model_weights.append(prob_cand_j_hypo_model_e)\n",
    "                \n",
    "                # Normalize model weights\n",
    "                sum_model_weights = sum(model_weights)\n",
    "                if sum_model_weights < 1e-9:\n",
    "                    norm_model_weights = [1.0 / 3] * 3\n",
    "                else:\n",
    "                    norm_model_weights = [w / sum_model_weights for w in model_weights]\n",
    "\n",
    "                # Calculate average expected score for L+ given x' and class j\n",
    "                current_est_score_Lplus_sum_k = 0.0\n",
    "                prob_one_Lplus_est_k = 0.0\n",
    "                for model_idx_loop_2 in range(3): \n",
    "                    prob_one_model_e_est_k = est_prob_models_all_cls[model_idx_loop_2][j_cand_cls_idx].item()\n",
    "                    prob_one_Lplus_est_k += norm_model_weights[model_idx_loop_2] * prob_one_model_e_est_k\n",
    "                    \n",
    "                current_est_score_Lplus_sum_k += calculate_expected_beta_score(prob_one_Lplus_est_k, alpha_param, beta_param)\n",
    "                \n",
    "                avg_est_score_Lplus_for_j += prob_cand_j_hypo_L * current_est_score_Lplus_sum_k\n",
    "        \n",
    "            delta_Q_est += (avg_est_score_Lplus_for_j - est_score_L_sum_k)\n",
    "        current_cand_score += delta_Q_est\n",
    "\n",
    "    return current_cand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ebef245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:42.165984Z",
     "iopub.status.busy": "2025-06-08T19:09:42.165745Z",
     "iopub.status.idle": "2025-06-08T19:09:42.196781Z",
     "shell.execute_reply": "2025-06-08T19:09:42.195879Z"
    },
    "papermill": {
     "duration": 0.045164,
     "end_time": "2025-06-08T19:09:42.198125",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.152961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def besra_sampling(\n",
    "    aspect_models, \n",
    "    sentiment_models,    \n",
    "    unlabeled_pool_data, \n",
    "    train_indices,\n",
    "    remaining_indices,\n",
    "    estimation_pool_data,\n",
    "    tokenizer,\n",
    "    num_classes,\n",
    "    sampling_dur,   \n",
    "    new_samples,\n",
    "    trials, \n",
    "    alpha_param=0.1,\n",
    "    beta_param=3.0,\n",
    "    n_clusters=min_increment,\n",
    "):\n",
    "    # Set n_clusters to min_increment if not provided\n",
    "    if n_clusters is None:\n",
    "        n_clusters = min_increment\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    ul_aspect_dataset = AspectDetectionDataset(\n",
    "        unlabeled_pool_data, \n",
    "        [['neutral' for i in range(num_classes[0])] for x in range(len(unlabeled_pool_data))], \n",
    "        label_mapping,\n",
    "        tokenizer, \n",
    "        max_length=sequence_length\n",
    "    ) \n",
    "    ul_aspect_loader = DataLoader(ul_aspect_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "\n",
    "    est_aspect_dataset = AspectDetectionDataset(\n",
    "        estimation_pool_data, \n",
    "        [['neutral' for i in range(num_classes[0])] for x in range(len(estimation_pool_data))], \n",
    "        label_mapping,\n",
    "        tokenizer, \n",
    "        max_length=sequence_length\n",
    "    ) \n",
    "    est_aspect_loader = DataLoader(est_aspect_dataset, batch_size=len(est_aspect_dataset), num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "    for aspect_model in aspect_models:\n",
    "        aspect_model.to(device)\n",
    "        aspect_model.eval()\n",
    "\n",
    "    for sentiment_model in sentiment_models:\n",
    "        sentiment_model.to(device)\n",
    "        sentiment_model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_aspect_est_avg_probs = []\n",
    "    all_aspect_est_indiv_probs = [] \n",
    "\n",
    "    all_aspect_cand_avg_probs = []\n",
    "    all_aspect_cand_indiv_probs = []\n",
    "\n",
    "    all_sentiment_est_avg_probs = []\n",
    "    all_sentiment_est_indiv_probs = []\n",
    "\n",
    "    all_sentiment_ul_avg_probs = []\n",
    "    all_sentiment_ul_indiv_probs = []\n",
    "\n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = {}\n",
    "\n",
    "    ul_aspects = []\n",
    "    ul_data = []\n",
    "    ul_labels = []\n",
    "    ul_indices = []\n",
    "\n",
    "    est_aspects = []\n",
    "    est_data = []\n",
    "    est_labels = []\n",
    "    est_indices = []\n",
    "\n",
    "    # inference on estimation pool\n",
    "    for est_b_idx, est_batch in enumerate(est_aspect_loader):\n",
    "        est_ids = est_batch['input_ids']\n",
    "        est_mask = est_batch['attention_mask']\n",
    "\n",
    "        est_avg_probs, est_indiv_prob_list = get_ensemble_predictions_for_batch(aspect_models, est_ids, est_mask, device)\n",
    "        all_aspect_est_avg_probs.append(est_avg_probs)\n",
    "        \n",
    "        # Extend the list with the tensors for each model in the current batch\n",
    "        if not all_aspect_est_indiv_probs: # First batch, initialize list of lists\n",
    "            for _ in range(len(est_indiv_prob_list)):\n",
    "                all_aspect_est_indiv_probs.append([])\n",
    "        for model_idx, probs in enumerate(est_indiv_prob_list):\n",
    "            all_aspect_est_indiv_probs[model_idx].append(probs)\n",
    "\n",
    "        for i in range(len(est_avg_probs)):\n",
    "            for class_idx in range(est_avg_probs.shape[1]):\n",
    "                if int(est_avg_probs[i][class_idx].round()) != 1:\n",
    "                    est_aspects.append(aspect_list[class_idx])\n",
    "                    est_data.append(est_batch['ori_text'][i])\n",
    "                    est_labels.append(est_batch['ori_label'][i][class_idx])\n",
    "                    est_indices.append(est_batch['ori_indices'][i])        \n",
    "\n",
    "    # Concatenate all collected probabilities\n",
    "    all_aspect_est_avg_probs = torch.cat(all_aspect_est_avg_probs, dim=0)\n",
    "    all_aspect_est_indiv_probs = [torch.cat(model_probs, dim=0) for model_probs in all_aspect_est_indiv_probs] \n",
    "    \n",
    "    # Iterate through unlabeled aspect dataset\n",
    "    for cand_b_idx, cand_batch in enumerate(ul_aspect_loader):\n",
    "        cand_ids = cand_batch['input_ids']\n",
    "        cand_mask = cand_batch['attention_mask']\n",
    "\n",
    "        # P(y_k|L,x) and P(y_k|theta_e,x) for current batch of candidates\n",
    "        cand_avg_prob, cand_indiv_prob_list = get_ensemble_predictions_for_batch(aspect_models, cand_ids, cand_mask, device)\n",
    "        all_aspect_cand_avg_probs.append(cand_avg_prob)\n",
    "        \n",
    "        # Extend the list with the tensors for each model in the current batch\n",
    "        if not all_aspect_cand_indiv_probs: # First batch, initialize list of lists\n",
    "            for _ in range(len(cand_indiv_prob_list)):\n",
    "                all_aspect_cand_indiv_probs.append([])\n",
    "        for model_idx, probs in enumerate(cand_indiv_prob_list):\n",
    "            all_aspect_cand_indiv_probs[model_idx].append(probs)\n",
    "\n",
    "        for i in range(len(cand_avg_prob)):\n",
    "            score_diff = calculate_score_change(\n",
    "                cand_avg_prob[i], \n",
    "                all_aspect_est_avg_probs, \n",
    "                [model_probs[i] for model_probs in cand_indiv_prob_list], \n",
    "                all_aspect_est_indiv_probs, \n",
    "                num_classes[0], \n",
    "                alpha_param, \n",
    "                beta_param\n",
    "            )\n",
    "\n",
    "            for class_idx in range(cand_avg_prob.shape[1]):\n",
    "                if int(cand_avg_prob[i][class_idx].round()) != 1:\n",
    "                    ul_aspects.append(aspect_list[class_idx])\n",
    "                    ul_data.append(cand_batch['ori_text'][i])\n",
    "                    ul_labels.append(cand_batch['ori_label'][i][class_idx])\n",
    "                    ul_indices.append(cand_batch['ori_indices'][i])\n",
    "\n",
    "            aspect_outputs[cand_batch['ori_indices'][i].item()] = score_diff\n",
    "        # accelerator.print(f\"Batch {cand_b_idx + 1}/{len(ul_aspect_loader)} processed.\")\n",
    "            \n",
    "    all_aspect_cand_avg_probs = torch.cat(all_aspect_cand_avg_probs, dim=0)\n",
    "    all_aspect_cand_indiv_probs = [torch.cat(model_probs, dim=0) for model_probs in all_aspect_cand_indiv_probs]\n",
    "\n",
    "    est_sentiment_dataset = SentimentAnalysisDataset(est_data, est_labels, est_aspects, est_indices, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    est_sentiment_loader = torch.utils.data.DataLoader(\n",
    "        est_sentiment_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4,\n",
    "    )\n",
    "\n",
    "    ul_sentiment_dataset = SentimentAnalysisDataset(ul_data, ul_labels, ul_aspects, ul_indices, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    ul_sentiment_loader = torch.utils.data.DataLoader(\n",
    "        ul_sentiment_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4,\n",
    "    )\n",
    "\n",
    "    # inference on estimation pool for sentiment analysis\n",
    "    if len(est_sentiment_dataset) > 0:\n",
    "        for est_b_idx, est_batch in enumerate(est_sentiment_loader):\n",
    "            est_ids = est_batch['input_ids']\n",
    "            est_mask = est_batch['attention_mask']\n",
    "    \n",
    "            est_avg_probs, est_indiv_prob_list = get_ensemble_predictions_for_batch(sentiment_models, est_ids, est_mask, device)\n",
    "            all_sentiment_est_avg_probs.append(est_avg_probs)\n",
    "            \n",
    "            # Extend the list with the tensors for each model in the current batch\n",
    "            if not all_sentiment_est_indiv_probs: # First batch, initialize list of lists\n",
    "                for _ in range(len(est_indiv_prob_list)):\n",
    "                    all_sentiment_est_indiv_probs.append([])\n",
    "            for model_idx, probs in enumerate(est_indiv_prob_list):\n",
    "                all_sentiment_est_indiv_probs[model_idx].append(probs)\n",
    "    \n",
    "        all_sentiment_est_avg_probs = torch.cat(all_sentiment_est_avg_probs, dim=0)\n",
    "        all_sentiment_est_indiv_probs = [torch.cat(model_probs, dim=0) for model_probs in all_sentiment_est_indiv_probs] \n",
    "    \n",
    "        # Iterate through unlabeled sentiment dataset\n",
    "        for cand_b_idx, cand_batch in enumerate(ul_sentiment_loader):\n",
    "            cand_ids = cand_batch['input_ids']\n",
    "            cand_mask = cand_batch['attention_mask']\n",
    "    \n",
    "            # P(y_k|L,x) and P(y_k|theta_e,x) for current batch of candidates\n",
    "            cand_avg_prob, cand_indiv_prob_list = get_ensemble_predictions_for_batch(sentiment_models, cand_ids, cand_mask, device)\n",
    "            all_sentiment_ul_avg_probs.append(cand_avg_prob)\n",
    "            \n",
    "            # Extend the list with the tensors for each model in the current batch\n",
    "            if not all_sentiment_ul_indiv_probs: \n",
    "                for _ in range(len(cand_indiv_prob_list)):\n",
    "                    all_sentiment_ul_indiv_probs.append([])\n",
    "            for model_idx, probs in enumerate(cand_indiv_prob_list):\n",
    "                all_sentiment_ul_indiv_probs[model_idx].append(probs)\n",
    "    \n",
    "            for j in range(len(cand_avg_prob)):\n",
    "                score_diff = calculate_score_change(\n",
    "                    cand_avg_prob[j], \n",
    "                    all_sentiment_est_avg_probs, \n",
    "                    [model_probs[j] for model_probs in cand_indiv_prob_list], \n",
    "                    all_sentiment_est_indiv_probs, \n",
    "                    num_classes[1], \n",
    "                    alpha_param, \n",
    "                    beta_param\n",
    "                )\n",
    "    \n",
    "                ori_index = cand_batch['ori_indices'][j].item()\n",
    "                if ori_index in sentiment_outputs.keys():\n",
    "                    sentiment_outputs[ori_index].append(score_diff)\n",
    "                else:\n",
    "                    sentiment_outputs[ori_index] = [score_diff]\n",
    "        # accelerator.print(f\"Batch {cand_b_idx + 1}/{len(ul_sentiment_loader)} processed.\")\n",
    "    \n",
    "    accelerator.wait_for_everyone() \n",
    "                \n",
    "    # --- K-Means Clustering and Selection ---\n",
    "    # Ensure all processes sync before main process continues with selection\n",
    "    if accelerator.is_main_process:\n",
    "        aspect_outputs = dict(sorted(aspect_outputs.items()))\n",
    "        if len(ul_sentiment_dataset) > 0:\n",
    "            for key, val in sentiment_outputs.items():\n",
    "                aspect_outputs[key] = (np.mean(np.array(val)) + aspect_outputs[key]) / 2\n",
    "        candidate_scores = np.array(list(aspect_outputs.values())).reshape(-1, 1)\n",
    "    \n",
    "        candidate_scores = np.array(candidate_scores)\n",
    "        candidate_scores = candidate_scores.reshape(-1, 1)\n",
    "    \n",
    "        accelerator.print(f\"BESRA Uncertainty Score Threshold {np.percentile(candidate_scores, 90)}\")\n",
    "    \n",
    "        target_samples = math.ceil(0.1 * len(unlabeled_pool_data))\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "    \n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "    \n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'fuel': [y_train[i][0] for i in temp],\n",
    "                'machine': [y_train[i][1] for i in temp],\n",
    "                'others': [y_train[i][2] for i in temp],\n",
    "                'part': [y_train[i][3] for i in temp],\n",
    "                'price': [y_train[i][4] for i in temp],\n",
    "                'service': [y_train[i][5] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "    \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "    \n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(candidate_scores)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(candidate_scores[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "    \n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'fuel': [y_train[i][0] for i in temp],\n",
    "                    'machine': [y_train[i][1] for i in temp],\n",
    "                    'others': [y_train[i][2] for i in temp],\n",
    "                    'part': [y_train[i][3] for i in temp],\n",
    "                    'price': [y_train[i][4] for i in temp],\n",
    "                    'service': [y_train[i][5] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "    \n",
    "            # threshold_data = pd.DataFrame({\n",
    "            #     'Threshold': thresholds\n",
    "            # })\n",
    "            # threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528d8e1",
   "metadata": {
    "papermill": {
     "duration": 0.011916,
     "end_time": "2025-06-08T19:09:42.222319",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.210403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "494925ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:42.247151Z",
     "iopub.status.busy": "2025-06-08T19:09:42.246917Z",
     "iopub.status.idle": "2025-06-08T19:09:42.261452Z",
     "shell.execute_reply": "2025-06-08T19:09:42.260809Z"
    },
    "papermill": {
     "duration": 0.028381,
     "end_time": "2025-06-08T19:09:42.262660",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.234279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    aspect_accuracies = manager.list()\n",
    "    aspect_f1_micros = manager.list()\n",
    "    aspect_f1_macros = manager.list()\n",
    "    sentiment_accuracies = manager.list()\n",
    "    sentiment_f1_micros = manager.list()\n",
    "    sentiment_f1_macros = manager.list()\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"===============================================\")\n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_aspect_accuracies = manager.list()\n",
    "        model_aspect_f1_micros = manager.list()\n",
    "        model_aspect_f1_macros = manager.list()\n",
    "        model_sentiment_accuracies = manager.list()\n",
    "        model_sentiment_f1_micros = manager.list()\n",
    "        model_sentiment_f1_macros = manager.list()\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "\n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (\n",
    "                current_train_size, \n",
    "                train_indices, \n",
    "                (model_aspect_accuracies, model_aspect_f1_micros, model_aspect_f1_macros), \n",
    "                (model_sentiment_accuracies, model_sentiment_f1_micros, model_sentiment_f1_macros),\n",
    "                (model_accuracies, model_f1_micros, model_f1_macros), \n",
    "                i,\n",
    "                j\n",
    "            )\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        aspect_accuracies.append(np.mean(model_aspect_accuracies))\n",
    "        aspect_f1_micros.append(np.mean(model_aspect_f1_micros))\n",
    "        aspect_f1_macros.append(np.mean(model_aspect_f1_macros))\n",
    "        sentiment_accuracies.append(np.mean(model_sentiment_accuracies))\n",
    "        sentiment_f1_micros.append(np.mean(model_sentiment_f1_micros))\n",
    "        sentiment_f1_macros.append(np.mean(model_sentiment_f1_macros))\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "\n",
    "        nearest_cp = current_train_size\n",
    "        if nearest_cp not in checkpoints:\n",
    "            for cp in checkpoints:\n",
    "                if cp > current_train_size:\n",
    "                    nearest_cp = cp\n",
    "                    break\n",
    "        percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "        \n",
    "        aspect_models = []\n",
    "        sentiment_models = []\n",
    "        for j in range(3):\n",
    "            # aspect_model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=6, problem_type='multi_label_classification')\n",
    "            # sentiment_model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=2)\n",
    "            aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{i+1}-model-{j+1}-{percentage}')\n",
    "            sentiment_model = BertForSequenceClassification.from_pretrained(f'{filename}-sentiment-{i+1}-model-{j+1}-{percentage}')\n",
    "            \n",
    "            aspect_models.append(aspect_model)\n",
    "            sentiment_models.append(sentiment_model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        estimation_pool_indices = np.random.choice(remaining_indices, size=min(10, math.ceil(0.1 * len(remaining_indices))), replace=False).tolist()\n",
    "        estimation_pool_data = [X_train[i] for i in estimation_pool_indices]\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (\n",
    "            aspect_models, \n",
    "            sentiment_models, \n",
    "            [X_train[i] for i in remaining_indices], \n",
    "            train_indices, \n",
    "            remaining_indices, \n",
    "            estimation_pool_data,\n",
    "            tokenizer,\n",
    "            (6, 2),\n",
    "            sampling_dur, \n",
    "            new_samples, \n",
    "            i\n",
    "        )\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (\n",
    "            current_train_size, \n",
    "            train_indices, \n",
    "            (model_aspect_accuracies, model_aspect_f1_micros, model_aspect_f1_macros), \n",
    "            (model_sentiment_accuracies, model_sentiment_f1_micros, model_sentiment_f1_macros),\n",
    "            (model_accuracies, model_f1_micros, model_f1_macros), \n",
    "            i,\n",
    "            j\n",
    "        )\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "    data_used.append(current_train_size)\n",
    "    aspect_accuracies.append(np.mean(model_aspect_accuracies))\n",
    "    aspect_f1_micros.append(np.mean(model_aspect_f1_micros))\n",
    "    aspect_f1_macros.append(np.mean(model_aspect_f1_macros))\n",
    "    sentiment_accuracies.append(np.mean(model_sentiment_accuracies))\n",
    "    sentiment_f1_micros.append(np.mean(model_sentiment_f1_micros))\n",
    "    sentiment_f1_macros.append(np.mean(model_sentiment_f1_macros))\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "\n",
    "    aspect_accuracies, aspect_f1_micros, aspect_f1_macros = list(aspect_accuracies), list(aspect_f1_micros), list(aspect_f1_macros)\n",
    "    sentiment_accuracies, sentiment_f1_micros, sentiment_f1_macros = list(sentiment_accuracies), list(sentiment_f1_micros), list(sentiment_f1_macros)\n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Aspect Accuracy': aspect_accuracies,\n",
    "        'Aspect F1 Micro': aspect_f1_micros,\n",
    "        'Aspect F1 Macro': aspect_f1_macros,\n",
    "        'Sentiment Accuracy': sentiment_accuracies,\n",
    "        'Sentiment F1 Micro': sentiment_f1_micros,\n",
    "        'Sentiment F1 Macro': sentiment_f1_macros,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c694043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:42.287995Z",
     "iopub.status.busy": "2025-06-08T19:09:42.287759Z",
     "iopub.status.idle": "2025-06-08T19:09:42.291088Z",
     "shell.execute_reply": "2025-06-08T19:09:42.290312Z"
    },
    "papermill": {
     "duration": 0.017246,
     "end_time": "2025-06-08T19:09:42.292350",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.275104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918452e",
   "metadata": {
    "papermill": {
     "duration": 0.012223,
     "end_time": "2025-06-08T19:09:42.317039",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.304816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6756, Accuracy: 0.7701, F1 Micro: 0.8685, F1 Macro: 0.8664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5949, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5663, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5413, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.485, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 6/10, Train Loss: 0.4934, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 7/10, Train Loss: 0.4861, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4478, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "Epoch 9/10, Train Loss: 0.4182, Accuracy: 0.7924, F1 Micro: 0.8826, F1 Macro: 0.8805\n",
      "Epoch 10/10, Train Loss: 0.3944, Accuracy: 0.7939, F1 Micro: 0.8833, F1 Macro: 0.881\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7484, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5901, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4859, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.421, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3657, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2973, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2815, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2474, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2277, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2016, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "    positive       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.303\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.50      0.10      0.16        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.41      0.36      0.33       216\n",
      "weighted avg       0.60      0.71      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 58.46482276916504 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6474, Accuracy: 0.7091, F1 Micro: 0.8196, F1 Macro: 0.7917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.589, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5429, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5279, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 5/10, Train Loss: 0.4875, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 6/10, Train Loss: 0.4868, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 7/10, Train Loss: 0.4839, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4515, Accuracy: 0.7932, F1 Micro: 0.8838, F1 Macro: 0.8822\n",
      "Epoch 9/10, Train Loss: 0.4287, Accuracy: 0.7932, F1 Micro: 0.8834, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.397, Accuracy: 0.7976, F1 Micro: 0.8856, F1 Macro: 0.8839\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7976, F1 Micro: 0.8856, F1 Macro: 0.8839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.98      0.88       175\n",
      "      others       0.74      0.97      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7455, Accuracy: 0.5385, F1 Micro: 0.5385, F1 Macro: 0.35\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6946, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5674, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.579, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.5553, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5072, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4154, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4522, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.5831, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3683, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         5\n",
      "    positive       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.98      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.61      0.76      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.74      0.97      0.84       152\n",
      "    positive       0.76      0.25      0.38        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.50      0.41      0.41       216\n",
      "weighted avg       0.71      0.75      0.68       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.69      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 57.85642123222351 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6607, Accuracy: 0.7649, F1 Micro: 0.864, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5793, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5503, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.517, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4778, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4767, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4783, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4419, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4097, Accuracy: 0.7939, F1 Micro: 0.8842, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3844, Accuracy: 0.7976, F1 Micro: 0.8852, F1 Macro: 0.8831\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7976, F1 Micro: 0.8852, F1 Macro: 0.8831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.72      0.99      0.84       158\n",
      "        part       0.75      0.93      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6786, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.5733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5919, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4489, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4731, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4406, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5754, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.6467, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3345, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.353, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.359, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "    positive       0.82      1.00      0.90        27\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.41      0.50      0.45        33\n",
      "weighted avg       0.67      0.82      0.74        33\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.67      0.08      0.14        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.46      0.36      0.32       216\n",
      "weighted avg       0.67      0.72      0.62       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.76      0.94      0.84       152\n",
      "    positive       0.48      0.32      0.38        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.41      0.42      0.41       216\n",
      "weighted avg       0.62      0.72      0.66       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 69.21425771713257 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7935, F1 Micro: 0.7935, F1 Macro: 0.315\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 934.4164600579751\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 30.513004064559937 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6424, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5478, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Epoch 3/10, Train Loss: 0.5058, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.4713, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4443, Accuracy: 0.8051, F1 Micro: 0.8896, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.417, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3839, Accuracy: 0.84, F1 Micro: 0.9071, F1 Macro: 0.9059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3381, Accuracy: 0.872, F1 Micro: 0.9241, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.302, Accuracy: 0.8862, F1 Micro: 0.931, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2563, Accuracy: 0.9025, F1 Micro: 0.9399, F1 Macro: 0.937\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9025, F1 Micro: 0.9399, F1 Macro: 0.937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.91      0.93      0.92       175\n",
      "      others       0.86      0.89      0.87       158\n",
      "        part       0.88      0.96      0.92       158\n",
      "       price       0.92      1.00      0.96       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.94      1061\n",
      "   macro avg       0.91      0.96      0.94      1061\n",
      "weighted avg       0.92      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5902, Accuracy: 0.7315, F1 Micro: 0.7315, F1 Macro: 0.4225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5794, Accuracy: 0.7315, F1 Micro: 0.7315, F1 Macro: 0.4225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.444, Accuracy: 0.7407, F1 Micro: 0.7407, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4118, Accuracy: 0.838, F1 Micro: 0.838, F1 Macro: 0.7769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2569, Accuracy: 0.838, F1 Micro: 0.838, F1 Macro: 0.8083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1585, Accuracy: 0.8843, F1 Micro: 0.8843, F1 Macro: 0.8565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1175, Accuracy: 0.8843, F1 Micro: 0.8843, F1 Macro: 0.8606\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.8611, F1 Micro: 0.8611, F1 Macro: 0.8319\n",
      "Epoch 9/10, Train Loss: 0.0872, Accuracy: 0.8519, F1 Micro: 0.8519, F1 Macro: 0.7918\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.8657, F1 Micro: 0.8657, F1 Macro: 0.8368\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.8843, F1 Micro: 0.8843, F1 Macro: 0.8606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.80        58\n",
      "    positive       0.95      0.89      0.92       158\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.88      0.86       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8935, F1 Micro: 0.8935, F1 Macro: 0.7691\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       1.00      0.75      0.86        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.98      0.80      0.87       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.75      0.71        16\n",
      "     neutral       0.91      0.93      0.92       167\n",
      "    positive       0.77      0.61      0.68        33\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.78      0.76      0.77       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.33      0.31        12\n",
      "     neutral       0.87      0.89      0.88       152\n",
      "    positive       0.67      0.60      0.63        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.61      0.61      0.61       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.61      0.72        23\n",
      "     neutral       0.88      0.96      0.92       152\n",
      "    positive       0.80      0.68      0.74        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.85      0.75      0.79       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.31      0.47        13\n",
      "     neutral       0.92      1.00      0.96       186\n",
      "    positive       1.00      0.59      0.74        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.97      0.63      0.72       216\n",
      "weighted avg       0.93      0.93      0.91       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.96      0.79      0.86       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 70.91788697242737 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6285, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.528, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 3/10, Train Loss: 0.505, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Epoch 4/10, Train Loss: 0.4685, Accuracy: 0.7909, F1 Micro: 0.8829, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4459, Accuracy: 0.7976, F1 Micro: 0.8851, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4183, Accuracy: 0.814, F1 Micro: 0.8933, F1 Macro: 0.8915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3755, Accuracy: 0.8408, F1 Micro: 0.907, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3281, Accuracy: 0.8743, F1 Micro: 0.9243, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2855, Accuracy: 0.8951, F1 Micro: 0.9362, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.236, Accuracy: 0.9025, F1 Micro: 0.94, F1 Macro: 0.9371\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9025, F1 Micro: 0.94, F1 Macro: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.93      1.00      0.96       187\n",
      "     machine       0.91      0.95      0.93       175\n",
      "      others       0.90      0.85      0.87       158\n",
      "        part       0.87      0.99      0.93       158\n",
      "       price       0.93      1.00      0.96       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.91      0.97      0.94      1061\n",
      "   macro avg       0.91      0.97      0.94      1061\n",
      "weighted avg       0.91      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6408, Accuracy: 0.7156, F1 Micro: 0.7156, F1 Macro: 0.4171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5339, Accuracy: 0.7156, F1 Micro: 0.7156, F1 Macro: 0.4171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4844, Accuracy: 0.7915, F1 Micro: 0.7915, F1 Macro: 0.673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4106, Accuracy: 0.8294, F1 Micro: 0.8294, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2256, Accuracy: 0.8294, F1 Micro: 0.8294, F1 Macro: 0.7999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1612, Accuracy: 0.8863, F1 Micro: 0.8863, F1 Macro: 0.8603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1015, Accuracy: 0.8957, F1 Micro: 0.8957, F1 Macro: 0.8732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1144, Accuracy: 0.8957, F1 Micro: 0.8957, F1 Macro: 0.8663\n",
      "Epoch 9/10, Train Loss: 0.0698, Accuracy: 0.891, F1 Micro: 0.891, F1 Macro: 0.8578\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.8815, F1 Micro: 0.8815, F1 Macro: 0.8506\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8957, F1 Micro: 0.8957, F1 Macro: 0.8663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.75      0.80        60\n",
      "    positive       0.91      0.95      0.93       151\n",
      "\n",
      "    accuracy                           0.90       211\n",
      "   macro avg       0.89      0.85      0.87       211\n",
      "weighted avg       0.89      0.90      0.89       211\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8881, F1 Micro: 0.8881, F1 Macro: 0.7379\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.45      0.62        11\n",
      "     neutral       0.93      1.00      0.96       181\n",
      "    positive       0.88      0.58      0.70        24\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.68      0.76       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.90      0.95      0.93       167\n",
      "    positive       0.73      0.58      0.64        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.76      0.79       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.08      0.12        12\n",
      "     neutral       0.90      0.85      0.87       152\n",
      "    positive       0.57      0.75      0.64        52\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.57      0.56      0.55       216\n",
      "weighted avg       0.78      0.78      0.78       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.52      0.65        23\n",
      "     neutral       0.87      0.99      0.93       152\n",
      "    positive       0.89      0.61      0.72        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.87      0.71      0.77       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.46      0.63        13\n",
      "     neutral       0.93      1.00      0.96       186\n",
      "    positive       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.94      0.66      0.75       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.72      0.81       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Total train time: 72.85045266151428 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6153, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5154, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4957, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.457, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4355, Accuracy: 0.7999, F1 Micro: 0.8872, F1 Macro: 0.8856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3966, Accuracy: 0.8162, F1 Micro: 0.8948, F1 Macro: 0.8932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3663, Accuracy: 0.8445, F1 Micro: 0.9094, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3146, Accuracy: 0.8795, F1 Micro: 0.9277, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2816, Accuracy: 0.9018, F1 Micro: 0.9396, F1 Macro: 0.936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2317, Accuracy: 0.9085, F1 Micro: 0.9434, F1 Macro: 0.9401\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9085, F1 Micro: 0.9434, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.93      0.95      0.94       175\n",
      "      others       0.86      0.84      0.85       158\n",
      "        part       0.89      0.98      0.93       158\n",
      "       price       0.94      1.00      0.97       192\n",
      "     service       0.93      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.94      1061\n",
      "   macro avg       0.92      0.96      0.94      1061\n",
      "weighted avg       0.92      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5702, Accuracy: 0.7207, F1 Micro: 0.7207, F1 Macro: 0.4188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4694, Accuracy: 0.7207, F1 Micro: 0.7207, F1 Macro: 0.4188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4471, Accuracy: 0.7703, F1 Micro: 0.7703, F1 Macro: 0.5993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3251, Accuracy: 0.8198, F1 Micro: 0.8198, F1 Macro: 0.7844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2039, Accuracy: 0.8559, F1 Micro: 0.8559, F1 Macro: 0.829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1852, Accuracy: 0.8694, F1 Micro: 0.8694, F1 Macro: 0.8279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8732\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.8514, F1 Micro: 0.8514, F1 Macro: 0.8272\n",
      "Epoch 9/10, Train Loss: 0.0441, Accuracy: 0.8559, F1 Micro: 0.8559, F1 Macro: 0.8244\n",
      "Epoch 10/10, Train Loss: 0.0156, Accuracy: 0.8559, F1 Micro: 0.8559, F1 Macro: 0.8227\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.84      0.82        62\n",
      "    positive       0.94      0.92      0.93       160\n",
      "\n",
      "    accuracy                           0.90       222\n",
      "   macro avg       0.87      0.88      0.87       222\n",
      "weighted avg       0.90      0.90      0.90       222\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.7779\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.90      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.92      0.96      0.94       167\n",
      "    positive       0.77      0.70      0.73        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.80      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.33      0.32        12\n",
      "     neutral       0.86      0.84      0.85       152\n",
      "    positive       0.56      0.60      0.58        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.58      0.59      0.58       216\n",
      "weighted avg       0.76      0.75      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.57      0.67        23\n",
      "     neutral       0.89      0.98      0.93       152\n",
      "    positive       0.84      0.66      0.74        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.73      0.78       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.38      0.56        13\n",
      "     neutral       0.93      1.00      0.97       186\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.98      0.70      0.78       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.93      1.00      0.96       185\n",
      "    positive       0.86      0.35      0.50        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.67      0.75       216\n",
      "weighted avg       0.92      0.93      0.91       216\n",
      "\n",
      "Total train time: 75.25171732902527 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8925, F1 Micro: 0.8925, F1 Macro: 0.7616\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 962.0874928108024\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 37.88462996482849 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6015, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5176, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Epoch 3/10, Train Loss: 0.4905, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4395, Accuracy: 0.811, F1 Micro: 0.8926, F1 Macro: 0.8913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3843, Accuracy: 0.8475, F1 Micro: 0.9103, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3468, Accuracy: 0.8824, F1 Micro: 0.9297, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2806, Accuracy: 0.9018, F1 Micro: 0.9397, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2519, Accuracy: 0.9152, F1 Micro: 0.9469, F1 Macro: 0.9432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1992, Accuracy: 0.9226, F1 Micro: 0.9518, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1741, Accuracy: 0.9301, F1 Micro: 0.9567, F1 Macro: 0.9542\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9301, F1 Micro: 0.9567, F1 Macro: 0.9542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.91      0.98      0.95       175\n",
      "      others       0.89      0.89      0.89       158\n",
      "        part       0.89      0.99      0.93       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.93      0.98      0.95      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5922, Accuracy: 0.6816, F1 Micro: 0.6816, F1 Macro: 0.4053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5405, Accuracy: 0.7623, F1 Micro: 0.7623, F1 Macro: 0.6341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3447, Accuracy: 0.8744, F1 Micro: 0.8744, F1 Macro: 0.8584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1623, Accuracy: 0.9238, F1 Micro: 0.9238, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1411, Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9297\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9061\n",
      "Epoch 8/10, Train Loss: 0.0396, Accuracy: 0.9193, F1 Micro: 0.9193, F1 Macro: 0.9113\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9066\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9238, F1 Micro: 0.9238, F1 Macro: 0.9149\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        71\n",
      "    positive       0.98      0.93      0.95       152\n",
      "\n",
      "    accuracy                           0.94       223\n",
      "   macro avg       0.92      0.94      0.93       223\n",
      "weighted avg       0.94      0.94      0.94       223\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.8597\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.91      0.99      0.95       167\n",
      "    positive       0.95      0.61      0.74        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.78      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.89      0.89      0.89       152\n",
      "    positive       0.77      0.71      0.74        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.74      0.79      0.76       216\n",
      "weighted avg       0.85      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.70      0.78        23\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.93      0.63      0.75        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.90      0.77      0.82       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.95      1.00      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.96      0.77      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 82.91646385192871 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5964, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 2/10, Train Loss: 0.5097, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4965, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4535, Accuracy: 0.8028, F1 Micro: 0.8869, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.395, Accuracy: 0.8222, F1 Micro: 0.8975, F1 Macro: 0.8955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.36, Accuracy: 0.8735, F1 Micro: 0.9227, F1 Macro: 0.9178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2886, Accuracy: 0.8966, F1 Micro: 0.9371, F1 Macro: 0.9352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2579, Accuracy: 0.9137, F1 Micro: 0.9461, F1 Macro: 0.9422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2044, Accuracy: 0.9234, F1 Micro: 0.952, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1685, Accuracy: 0.933, F1 Micro: 0.9581, F1 Macro: 0.9552\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.933, F1 Micro: 0.9581, F1 Macro: 0.9552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      1.00      0.98       187\n",
      "     machine       0.91      0.97      0.94       175\n",
      "      others       0.91      0.87      0.89       158\n",
      "        part       0.93      0.96      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6018, Accuracy: 0.6855, F1 Micro: 0.6855, F1 Macro: 0.4067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.562, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.6058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4023, Accuracy: 0.8629, F1 Micro: 0.8629, F1 Macro: 0.8432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2043, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9058\n",
      "Epoch 6/10, Train Loss: 0.153, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8968\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8962\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8826\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8808\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8855\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.95      0.88        78\n",
      "    positive       0.97      0.90      0.94       170\n",
      "\n",
      "    accuracy                           0.92       248\n",
      "   macro avg       0.89      0.92      0.91       248\n",
      "weighted avg       0.92      0.92      0.92       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.8637\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.97      1.00      0.98       181\n",
      "    positive       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.88      0.93       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.91      0.98      0.94       167\n",
      "    positive       0.87      0.61      0.71        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.78      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.91      0.87      0.89       152\n",
      "    positive       0.72      0.73      0.72        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.73      0.81      0.76       216\n",
      "weighted avg       0.84      0.83      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.83      0.81        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.85      0.71      0.77        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.85      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 79.26228308677673 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5931, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5006, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 3/10, Train Loss: 0.4872, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4352, Accuracy: 0.7954, F1 Micro: 0.8848, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3821, Accuracy: 0.8497, F1 Micro: 0.9117, F1 Macro: 0.91\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3389, Accuracy: 0.8914, F1 Micro: 0.9341, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2736, Accuracy: 0.9137, F1 Micro: 0.9465, F1 Macro: 0.9429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2448, Accuracy: 0.9271, F1 Micro: 0.9545, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1932, Accuracy: 0.9345, F1 Micro: 0.9589, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1678, Accuracy: 0.9353, F1 Micro: 0.9594, F1 Macro: 0.9562\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9353, F1 Micro: 0.9594, F1 Macro: 0.9562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.91      0.86      0.88       158\n",
      "        part       0.94      0.94      0.94       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6111, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5089, Accuracy: 0.7809, F1 Micro: 0.7809, F1 Macro: 0.716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.395, Accuracy: 0.8486, F1 Micro: 0.8486, F1 Macro: 0.8346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1939, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8968\n",
      "Epoch 5/10, Train Loss: 0.1633, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0759, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8948\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8898\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.8884, F1 Micro: 0.8884, F1 Macro: 0.8769\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8825\n",
      "Epoch 10/10, Train Loss: 0.0999, Accuracy: 0.8845, F1 Micro: 0.8845, F1 Macro: 0.8735\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.88      0.86        78\n",
      "    positive       0.95      0.92      0.93       173\n",
      "\n",
      "    accuracy                           0.91       251\n",
      "   macro avg       0.89      0.90      0.89       251\n",
      "weighted avg       0.91      0.91      0.91       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.8552\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.56      0.69        16\n",
      "     neutral       0.92      0.99      0.95       167\n",
      "    positive       0.78      0.64      0.70        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.73      0.78       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.91      0.86      0.89       152\n",
      "    positive       0.72      0.75      0.74        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.73      0.82      0.76       216\n",
      "weighted avg       0.84      0.83      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.78      0.80        23\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.79      0.76      0.77        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 78.98394846916199 s\n",
      "Averaged - Iteration 208: Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.8595\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 521.742922555092\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 40.49902701377869 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5888, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Epoch 2/10, Train Loss: 0.5041, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4749, Accuracy: 0.808, F1 Micro: 0.8913, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4076, Accuracy: 0.8534, F1 Micro: 0.9143, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3493, Accuracy: 0.9048, F1 Micro: 0.9425, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2909, Accuracy: 0.9315, F1 Micro: 0.9575, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2431, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1811, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1521, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1215, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9648\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.89      0.91       158\n",
      "        part       0.91      1.00      0.95       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5927, Accuracy: 0.6946, F1 Micro: 0.6946, F1 Macro: 0.4099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.483, Accuracy: 0.7824, F1 Micro: 0.7824, F1 Macro: 0.6557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2806, Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9289\n",
      "Epoch 4/10, Train Loss: 0.2172, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9183\n",
      "Epoch 5/10, Train Loss: 0.0952, Accuracy: 0.9247, F1 Micro: 0.9247, F1 Macro: 0.9149\n",
      "Epoch 6/10, Train Loss: 0.145, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9028\n",
      "Epoch 7/10, Train Loss: 0.064, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9066\n",
      "Epoch 8/10, Train Loss: 0.0911, Accuracy: 0.9247, F1 Micro: 0.9247, F1 Macro: 0.9075\n",
      "Epoch 9/10, Train Loss: 0.0937, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9028\n",
      "Epoch 10/10, Train Loss: 0.0869, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9159\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.97      0.90        73\n",
      "    positive       0.99      0.92      0.95       166\n",
      "\n",
      "    accuracy                           0.94       239\n",
      "   macro avg       0.92      0.95      0.93       239\n",
      "weighted avg       0.94      0.94      0.94       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8786\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.93      0.89      0.91       152\n",
      "    positive       0.76      0.79      0.77        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.76      0.81      0.78       216\n",
      "weighted avg       0.87      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.90      1.00      0.95       152\n",
      "    positive       1.00      0.68      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.81      0.85       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 82.9360408782959 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5789, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5115, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.485, Accuracy: 0.8006, F1 Micro: 0.8862, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4209, Accuracy: 0.8177, F1 Micro: 0.8951, F1 Macro: 0.8932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3627, Accuracy: 0.875, F1 Micro: 0.9243, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2988, Accuracy: 0.9137, F1 Micro: 0.9464, F1 Macro: 0.9426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2414, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1885, Accuracy: 0.9412, F1 Micro: 0.9632, F1 Macro: 0.9608\n",
      "Epoch 9/10, Train Loss: 0.1495, Accuracy: 0.9353, F1 Micro: 0.9592, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1221, Accuracy: 0.9479, F1 Micro: 0.9673, F1 Macro: 0.9649\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9479, F1 Micro: 0.9673, F1 Macro: 0.9649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.91      0.87      0.89       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.97      0.96      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5943, Accuracy: 0.6929, F1 Micro: 0.6929, F1 Macro: 0.4093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4544, Accuracy: 0.8661, F1 Micro: 0.8661, F1 Macro: 0.8352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2564, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0935, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0856, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9237\n",
      "Epoch 8/10, Train Loss: 0.0726, Accuracy: 0.9016, F1 Micro: 0.9016, F1 Macro: 0.8916\n",
      "Epoch 9/10, Train Loss: 0.0821, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.92\n",
      "Epoch 10/10, Train Loss: 0.0827, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9242\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.97      0.90        78\n",
      "    positive       0.99      0.92      0.95       176\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.92      0.95      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9406, F1 Micro: 0.9406, F1 Macro: 0.8875\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.82      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.92      0.87      0.89       152\n",
      "    positive       0.71      0.79      0.75        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.76      0.80      0.78       216\n",
      "weighted avg       0.85      0.84      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 86.93999743461609 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5756, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.5021, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4692, Accuracy: 0.7984, F1 Micro: 0.8866, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4062, Accuracy: 0.8445, F1 Micro: 0.9096, F1 Macro: 0.9081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3463, Accuracy: 0.9085, F1 Micro: 0.9442, F1 Macro: 0.9422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2852, Accuracy: 0.9323, F1 Micro: 0.9579, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2342, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1773, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9641\n",
      "Epoch 9/10, Train Loss: 0.1464, Accuracy: 0.9449, F1 Micro: 0.9653, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1193, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9651\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.93      0.86      0.89       158\n",
      "        part       0.94      0.99      0.96       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.97      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5797, Accuracy: 0.6914, F1 Micro: 0.6914, F1 Macro: 0.4088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4542, Accuracy: 0.8633, F1 Micro: 0.8633, F1 Macro: 0.8472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2668, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1668, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1117, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9249\n",
      "Epoch 6/10, Train Loss: 0.0854, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9273\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9171\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        79\n",
      "    positive       0.96      0.95      0.95       177\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.92      0.93      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.882\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.82      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.94      0.86      0.89       152\n",
      "    positive       0.68      0.87      0.76        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.78      0.80      0.78       216\n",
      "weighted avg       0.86      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 87.98464941978455 s\n",
      "Averaged - Iteration 274: Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.8827\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 553.668632986721\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 36.08242416381836 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5776, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4849, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4378, Accuracy: 0.8177, F1 Micro: 0.896, F1 Macro: 0.8945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.381, Accuracy: 0.8899, F1 Micro: 0.9341, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.293, Accuracy: 0.9315, F1 Micro: 0.9572, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2347, Accuracy: 0.9435, F1 Micro: 0.9646, F1 Macro: 0.9617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1787, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1412, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9703\n",
      "Epoch 9/10, Train Loss: 0.112, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9689\n",
      "Epoch 10/10, Train Loss: 0.0928, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9667\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      1.00      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6271, Accuracy: 0.6867, F1 Micro: 0.6867, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4707, Accuracy: 0.8876, F1 Micro: 0.8876, F1 Macro: 0.8735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2784, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.932\n",
      "Epoch 4/10, Train Loss: 0.1474, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9268\n",
      "Epoch 5/10, Train Loss: 0.0931, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9286\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9312\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9187\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9192\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9277\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        78\n",
      "    positive       0.97      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.94       249\n",
      "   macro avg       0.92      0.94      0.93       249\n",
      "weighted avg       0.94      0.94      0.94       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8934\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      1.00      0.97       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.82      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.79      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.83      0.83       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 86.27224731445312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5674, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4851, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4552, Accuracy: 0.8058, F1 Micro: 0.8889, F1 Macro: 0.8871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3977, Accuracy: 0.8862, F1 Micro: 0.9312, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3013, Accuracy: 0.9182, F1 Micro: 0.949, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2377, Accuracy: 0.942, F1 Micro: 0.9635, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1792, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1364, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9662\n",
      "Epoch 9/10, Train Loss: 0.108, Accuracy: 0.9494, F1 Micro: 0.9682, F1 Macro: 0.9656\n",
      "Epoch 10/10, Train Loss: 0.0902, Accuracy: 0.9442, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      1.00      0.97       175\n",
      "      others       0.92      0.87      0.89       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.97      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5949, Accuracy: 0.698, F1 Micro: 0.698, F1 Macro: 0.4111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4475, Accuracy: 0.851, F1 Micro: 0.851, F1 Macro: 0.8095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2441, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9149\n",
      "Epoch 4/10, Train Loss: 0.1621, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9118\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9071\n",
      "Epoch 6/10, Train Loss: 0.1247, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9212\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.8988\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9112\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.90      0.89        77\n",
      "    positive       0.95      0.95      0.95       178\n",
      "\n",
      "    accuracy                           0.93       255\n",
      "   macro avg       0.92      0.92      0.92       255\n",
      "weighted avg       0.93      0.93      0.93       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8796\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.94      1.00      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.79      0.84       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.87      0.89       152\n",
      "    positive       0.73      0.85      0.79        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.78      0.82      0.80       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.83      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 88.98209261894226 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.564, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.4857, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4339, Accuracy: 0.814, F1 Micro: 0.8933, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.378, Accuracy: 0.8966, F1 Micro: 0.9366, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2985, Accuracy: 0.9293, F1 Micro: 0.9562, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2368, Accuracy: 0.9435, F1 Micro: 0.9646, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1786, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9682\n",
      "Epoch 8/10, Train Loss: 0.14, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1134, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9687\n",
      "Epoch 10/10, Train Loss: 0.0936, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9665\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      1.00      0.97       175\n",
      "      others       0.93      0.87      0.90       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5929, Accuracy: 0.7104, F1 Micro: 0.7104, F1 Macro: 0.5166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4078, Accuracy: 0.8378, F1 Micro: 0.8378, F1 Macro: 0.8276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2157, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1113, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9221\n",
      "Epoch 7/10, Train Loss: 0.0847, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0436, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0551, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.97      0.90        80\n",
      "    positive       0.99      0.92      0.95       179\n",
      "\n",
      "    accuracy                           0.93       259\n",
      "   macro avg       0.91      0.95      0.93       259\n",
      "weighted avg       0.94      0.93      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8945\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      1.00      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.83      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.94      0.87      0.90       152\n",
      "    positive       0.75      0.87      0.80        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.76      0.83      0.79       216\n",
      "weighted avg       0.87      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 94.81654953956604 s\n",
      "Averaged - Iteration 333: Accuracy: 0.945, F1 Micro: 0.945, F1 Macro: 0.8892\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 565.5174100258706\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 32.84431004524231 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5698, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4873, Accuracy: 0.8088, F1 Micro: 0.8918, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4328, Accuracy: 0.878, F1 Micro: 0.9276, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3359, Accuracy: 0.933, F1 Micro: 0.9587, F1 Macro: 0.957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.248, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.19, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.147, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.971\n",
      "Epoch 8/10, Train Loss: 0.1135, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0767, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5777, Accuracy: 0.6734, F1 Micro: 0.6734, F1 Macro: 0.4024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4099, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9249\n",
      "Epoch 3/10, Train Loss: 0.1862, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.237, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1036, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9331\n",
      "Epoch 6/10, Train Loss: 0.0814, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9211\n",
      "Epoch 7/10, Train Loss: 0.1218, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.115, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1002, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0551, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.946\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       248\n",
      "   macro avg       0.94      0.95      0.95       248\n",
      "weighted avg       0.95      0.95      0.95       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.91\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 102.16224551200867 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5609, Accuracy: 0.7887, F1 Micro: 0.8817, F1 Macro: 0.88\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4919, Accuracy: 0.7894, F1 Micro: 0.882, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4383, Accuracy: 0.8356, F1 Micro: 0.9045, F1 Macro: 0.9025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3367, Accuracy: 0.9256, F1 Micro: 0.9545, F1 Macro: 0.9525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2496, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1903, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1454, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9684\n",
      "Epoch 8/10, Train Loss: 0.1124, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0931, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0761, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5982, Accuracy: 0.7321, F1 Micro: 0.7321, F1 Macro: 0.552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3544, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9348\n",
      "Epoch 3/10, Train Loss: 0.1865, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.918\n",
      "Epoch 4/10, Train Loss: 0.1437, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9104\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9139\n",
      "Epoch 6/10, Train Loss: 0.1182, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9175\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9129\n",
      "Epoch 8/10, Train Loss: 0.0453, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9424\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        79\n",
      "    positive       0.98      0.95      0.96       186\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9009\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 99.05280494689941 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5558, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.4827, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4259, Accuracy: 0.8586, F1 Micro: 0.9173, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3361, Accuracy: 0.9226, F1 Micro: 0.9529, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2528, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1967, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.968\n",
      "Epoch 7/10, Train Loss: 0.1526, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1168, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1003, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "Epoch 10/10, Train Loss: 0.0831, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.92      0.91       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5672, Accuracy: 0.6811, F1 Micro: 0.6811, F1 Macro: 0.4168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3973, Accuracy: 0.8661, F1 Micro: 0.8661, F1 Macro: 0.8577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1563, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1251, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9335\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8954\n",
      "Epoch 7/10, Train Loss: 0.0673, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1141, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        81\n",
      "    positive       0.98      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.92      0.95      0.93       254\n",
      "weighted avg       0.95      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8986\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.87      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.76      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.79      0.80      0.80       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 98.679283618927 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.9031\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 581.8458245261766\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 29.524015188217163 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.567, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4927, Accuracy: 0.8058, F1 Micro: 0.8905, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4111, Accuracy: 0.8966, F1 Micro: 0.9378, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3182, Accuracy: 0.9338, F1 Micro: 0.9588, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2457, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1806, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Epoch 7/10, Train Loss: 0.1427, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.9718\n",
      "Epoch 8/10, Train Loss: 0.112, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.969\n",
      "Epoch 9/10, Train Loss: 0.0903, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9676\n",
      "Epoch 10/10, Train Loss: 0.0804, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5853, Accuracy: 0.6734, F1 Micro: 0.6734, F1 Macro: 0.4024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3961, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.205, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1577, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "Epoch 7/10, Train Loss: 0.1033, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9257\n",
      "Epoch 8/10, Train Loss: 0.125, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9334\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9241\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9327\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        81\n",
      "    positive       0.97      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       248\n",
      "   macro avg       0.94      0.95      0.94       248\n",
      "weighted avg       0.95      0.95      0.95       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9054\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 98.20888209342957 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5702, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4935, Accuracy: 0.7999, F1 Micro: 0.8851, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4186, Accuracy: 0.8802, F1 Micro: 0.9283, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.321, Accuracy: 0.9263, F1 Micro: 0.9542, F1 Macro: 0.9518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2386, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1758, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1393, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9715\n",
      "Epoch 8/10, Train Loss: 0.1083, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.969\n",
      "Epoch 9/10, Train Loss: 0.0875, Accuracy: 0.9509, F1 Micro: 0.969, F1 Macro: 0.9664\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9688\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6005, Accuracy: 0.753, F1 Micro: 0.753, F1 Macro: 0.65\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3658, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9103\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.8988, F1 Micro: 0.8988, F1 Macro: 0.8899\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.8826, F1 Micro: 0.8826, F1 Macro: 0.8741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9163\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9066\n",
      "Epoch 7/10, Train Loss: 0.0994, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1098, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0858, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.924\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.91      0.90        78\n",
      "    positive       0.96      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.94       247\n",
      "   macro avg       0.93      0.93      0.93       247\n",
      "weighted avg       0.94      0.94      0.94       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8884\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 97.07349705696106 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.4865, Accuracy: 0.7917, F1 Micro: 0.882, F1 Macro: 0.8796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.404, Accuracy: 0.9048, F1 Micro: 0.9424, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3073, Accuracy: 0.9301, F1 Micro: 0.9566, F1 Macro: 0.9541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2398, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.177, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1416, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1118, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9701\n",
      "Epoch 9/10, Train Loss: 0.0885, Accuracy: 0.9501, F1 Micro: 0.9684, F1 Macro: 0.9649\n",
      "Epoch 10/10, Train Loss: 0.0826, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9687\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.87      0.90       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.8213, F1 Micro: 0.8213, F1 Macro: 0.7951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2765, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1697, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1168, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9268\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9041\n",
      "Epoch 6/10, Train Loss: 0.1588, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9239\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9155\n",
      "Epoch 9/10, Train Loss: 0.0855, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9191\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9177\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9239\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.87      0.89        83\n",
      "    positive       0.94      0.97      0.95       180\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.92      0.92       263\n",
      "weighted avg       0.94      0.94      0.93       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8962\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.88      0.90       152\n",
      "    positive       0.70      0.81      0.75        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.81      0.87      0.83       216\n",
      "weighted avg       0.87      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 99.85453820228577 s\n",
      "Averaged - Iteration 435: Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.8967\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 550.4088819094115\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 27.955251693725586 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5551, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4903, Accuracy: 0.8095, F1 Micro: 0.8922, F1 Macro: 0.8911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4075, Accuracy: 0.9062, F1 Micro: 0.9427, F1 Macro: 0.9409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3121, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2297, Accuracy: 0.9449, F1 Micro: 0.9653, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1599, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1256, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Epoch 8/10, Train Loss: 0.1075, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5581, Accuracy: 0.8615, F1 Micro: 0.8615, F1 Macro: 0.8352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3348, Accuracy: 0.8923, F1 Micro: 0.8923, F1 Macro: 0.8844\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2189, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9287\n",
      "Epoch 6/10, Train Loss: 0.111, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1281, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1029, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.935\n",
      "Epoch 9/10, Train Loss: 0.1059, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0924, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.99      0.92        83\n",
      "    positive       0.99      0.92      0.96       177\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9099\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 113.05991649627686 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5494, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4892, Accuracy: 0.7984, F1 Micro: 0.8858, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4146, Accuracy: 0.9018, F1 Micro: 0.9402, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3116, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2191, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9663\n",
      "Epoch 6/10, Train Loss: 0.1543, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.966\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.9487, F1 Micro: 0.9677, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1004, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0833, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.974\n",
      "Epoch 10/10, Train Loss: 0.0707, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9714\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6231, Accuracy: 0.7664, F1 Micro: 0.7664, F1 Macro: 0.6676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.348, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9401\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.914\n",
      "Epoch 4/10, Train Loss: 0.1574, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9102\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9185\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9296\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9251\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.8975, F1 Micro: 0.8975, F1 Macro: 0.8902\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9235\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        79\n",
      "    positive       0.97      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.93      0.95      0.94       244\n",
      "weighted avg       0.95      0.95      0.95       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9035\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 96.65048313140869 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5477, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4776, Accuracy: 0.7961, F1 Micro: 0.8845, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3898, Accuracy: 0.9189, F1 Micro: 0.9503, F1 Macro: 0.9484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2997, Accuracy: 0.9375, F1 Micro: 0.9612, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2212, Accuracy: 0.9427, F1 Micro: 0.9639, F1 Macro: 0.9607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1577, Accuracy: 0.9472, F1 Micro: 0.9668, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1252, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1049, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0846, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9539, F1 Micro: 0.9708, F1 Macro: 0.9684\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5506, Accuracy: 0.7056, F1 Micro: 0.7056, F1 Macro: 0.5088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9453\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9315\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.124, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.9556, F1 Micro: 0.9556, F1 Macro: 0.9494\n",
      "Epoch 7/10, Train Loss: 0.0719, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0905, Accuracy: 0.9597, F1 Micro: 0.9597, F1 Macro: 0.9547\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9246\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9377\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9597, F1 Micro: 0.9597, F1 Macro: 0.9547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        81\n",
      "    positive       0.98      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       248\n",
      "   macro avg       0.95      0.96      0.95       248\n",
      "weighted avg       0.96      0.96      0.96       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9134\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 105.02342319488525 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9089\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 597.6008903090931\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 24.746772050857544 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5628, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4795, Accuracy: 0.8095, F1 Micro: 0.8923, F1 Macro: 0.8911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3833, Accuracy: 0.9241, F1 Micro: 0.9534, F1 Macro: 0.9513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2732, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1905, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5622, Accuracy: 0.689, F1 Micro: 0.689, F1 Macro: 0.4522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3251, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9018\n",
      "Epoch 4/10, Train Loss: 0.2245, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1347, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1273, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9518\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       171\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.94      0.96      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.96      0.76      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 113.38457989692688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5488, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4782, Accuracy: 0.8006, F1 Micro: 0.8872, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.376, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2671, Accuracy: 0.9464, F1 Micro: 0.9663, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1827, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9745\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.973\n",
      "Epoch 8/10, Train Loss: 0.0838, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5734, Accuracy: 0.7132, F1 Micro: 0.7132, F1 Macro: 0.502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3291, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1651, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.922\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9179\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9148\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        82\n",
      "    positive       0.97      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.92      0.94      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9036\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.82      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 111.35484743118286 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5506, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4701, Accuracy: 0.8036, F1 Micro: 0.8893, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.372, Accuracy: 0.9189, F1 Micro: 0.9501, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2702, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1891, Accuracy: 0.9487, F1 Micro: 0.9677, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1075, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0898, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9723\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5715, Accuracy: 0.7439, F1 Micro: 0.7439, F1 Macro: 0.6132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3103, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9504\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9166\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9292\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9414\n",
      "Epoch 6/10, Train Loss: 0.1464, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0994, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9364\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9364\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9335\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9371\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       164\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.94      0.96      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9158\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.96      0.93       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 102.48902535438538 s\n",
      "Averaged - Iteration 517: Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9139\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 583.2970176583794\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 22.735154390335083 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.479, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3778, Accuracy: 0.9256, F1 Micro: 0.9547, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2754, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1986, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1194, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7944, F1 Micro: 0.7944, F1 Macro: 0.7301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3126, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2194, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9374\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.931\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9213\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9338\n",
      "Epoch 9/10, Train Loss: 0.0893, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9377\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        83\n",
      "    positive       0.97      0.94      0.96       165\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.93      0.95      0.94       248\n",
      "weighted avg       0.95      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.915\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 108.92938709259033 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5463, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4812, Accuracy: 0.8051, F1 Micro: 0.8892, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3742, Accuracy: 0.9226, F1 Micro: 0.9523, F1 Macro: 0.9503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2618, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.196, Accuracy: 0.9576, F1 Micro: 0.9737, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1163, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5649, Accuracy: 0.7186, F1 Micro: 0.7186, F1 Macro: 0.5207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3417, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9399\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 5/10, Train Loss: 0.1082, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1144, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        83\n",
      "    positive       0.97      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.94      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9105\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 109.97452926635742 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4686, Accuracy: 0.8043, F1 Micro: 0.8893, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.366, Accuracy: 0.9271, F1 Micro: 0.9555, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2629, Accuracy: 0.9472, F1 Micro: 0.9673, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1952, Accuracy: 0.9546, F1 Micro: 0.972, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1221, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 9/10, Train Loss: 0.0769, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9719\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.8612, F1 Micro: 0.8612, F1 Macro: 0.8477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3417, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2192, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9315\n",
      "Epoch 4/10, Train Loss: 0.1539, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9202\n",
      "Epoch 5/10, Train Loss: 0.1759, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8919\n",
      "Epoch 6/10, Train Loss: 0.2038, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1259, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9413\n",
      "Epoch 8/10, Train Loss: 0.1136, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9376\n",
      "Epoch 9/10, Train Loss: 0.0892, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.933\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9283\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        82\n",
      "    positive       0.97      0.94      0.96       163\n",
      "\n",
      "    accuracy                           0.95       245\n",
      "   macro avg       0.94      0.95      0.94       245\n",
      "weighted avg       0.95      0.95      0.95       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9075\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 106.38971328735352 s\n",
      "Averaged - Iteration 540: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.911\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 597.3014815221183\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 20.476229190826416 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5325, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4522, Accuracy: 0.8251, F1 Micro: 0.9001, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.339, Accuracy: 0.9323, F1 Micro: 0.9581, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2456, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9708\n",
      "Epoch 5/10, Train Loss: 0.1758, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9722\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5507, Accuracy: 0.8086, F1 Micro: 0.8086, F1 Macro: 0.7349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3139, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.9022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1823, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.118, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9254\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1082, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0768, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9387\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        83\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.93      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.91\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.82      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 120.78865075111389 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5443, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4666, Accuracy: 0.8065, F1 Micro: 0.8906, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3602, Accuracy: 0.9286, F1 Micro: 0.9562, F1 Macro: 0.9544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2479, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1755, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9687\n",
      "Epoch 6/10, Train Loss: 0.1312, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0857, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9725\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.88      0.91       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.97      0.97      1061\n",
      "   macro avg       0.98      0.97      0.97      1061\n",
      "weighted avg       0.98      0.97      0.97      1061\n",
      " samples avg       0.98      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5528, Accuracy: 0.8705, F1 Micro: 0.8705, F1 Macro: 0.8396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3181, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.17, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9343\n",
      "Epoch 4/10, Train Loss: 0.1244, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9107\n",
      "Epoch 5/10, Train Loss: 0.1721, Accuracy: 0.9281, F1 Micro: 0.9281, F1 Macro: 0.9147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9327\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1124, Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.9371\n",
      "Epoch 9/10, Train Loss: 0.0893, Accuracy: 0.9281, F1 Micro: 0.9281, F1 Macro: 0.9188\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9335\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.97      0.95      0.96       193\n",
      "\n",
      "    accuracy                           0.95       278\n",
      "   macro avg       0.93      0.94      0.94       278\n",
      "weighted avg       0.95      0.95      0.95       278\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9123\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.96      0.88      0.92       152\n",
      "    positive       0.74      0.88      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.87      0.83       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 114.36402440071106 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4491, Accuracy: 0.843, F1 Micro: 0.9089, F1 Macro: 0.9072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3372, Accuracy: 0.933, F1 Micro: 0.9585, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2453, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9705\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9735\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9715\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.91      0.92       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5494, Accuracy: 0.8774, F1 Micro: 0.8774, F1 Macro: 0.8548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2823, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1571, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 4/10, Train Loss: 0.1945, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9037\n",
      "Epoch 5/10, Train Loss: 0.166, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1486, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1414, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.947\n",
      "Epoch 9/10, Train Loss: 0.0878, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9344\n",
      "Epoch 10/10, Train Loss: 0.0908, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        83\n",
      "    positive       0.97      0.97      0.97       178\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.95      0.95      0.95       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9093\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.77      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.83      0.82       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.94      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 114.8961181640625 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9532, F1 Micro: 0.9532, F1 Macro: 0.9105\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 591.3397750053074\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 19.681666612625122 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5495, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4614, Accuracy: 0.8452, F1 Micro: 0.9104, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3425, Accuracy: 0.9338, F1 Micro: 0.9591, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2365, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1687, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1309, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.522, Accuracy: 0.8962, F1 Micro: 0.8962, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2873, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1891, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9155\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.935\n",
      "Epoch 6/10, Train Loss: 0.132, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Epoch 7/10, Train Loss: 0.1272, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.099, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9334\n",
      "Epoch 9/10, Train Loss: 0.0804, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9155\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.91        83\n",
      "    positive       0.96      0.96      0.96       177\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.93      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9074\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.80      0.94      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 120.67767357826233 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5398, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4686, Accuracy: 0.8132, F1 Micro: 0.8937, F1 Macro: 0.8922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3504, Accuracy: 0.9301, F1 Micro: 0.9565, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2365, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1686, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9546, F1 Micro: 0.9712, F1 Macro: 0.9695\n",
      "Epoch 7/10, Train Loss: 0.0979, Accuracy: 0.9524, F1 Micro: 0.9698, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0668, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.976\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.97      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.8859, F1 Micro: 0.8859, F1 Macro: 0.8601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2951, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1926, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9426\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9433\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.094, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        83\n",
      "    positive       0.98      0.95      0.97       180\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.96      0.95       263\n",
      "weighted avg       0.96      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9139\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.97      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.86      0.88      0.87       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 119.55667328834534 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4578, Accuracy: 0.8363, F1 Micro: 0.9053, F1 Macro: 0.9036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3357, Accuracy: 0.9338, F1 Micro: 0.9591, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2373, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9683\n",
      "Epoch 5/10, Train Loss: 0.1692, Accuracy: 0.9516, F1 Micro: 0.9694, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.976\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5027, Accuracy: 0.8966, F1 Micro: 0.8966, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2761, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1765, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1425, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9378\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9143\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.934\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        82\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9058\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.88      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 122.73847198486328 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9091\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 594.8701954781321\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 18.375719785690308 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5406, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4529, Accuracy: 0.849, F1 Micro: 0.9123, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.333, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2255, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9697\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9554, F1 Micro: 0.9718, F1 Macro: 0.9695\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.8699, F1 Micro: 0.8699, F1 Macro: 0.8416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2922, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9356\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9309, F1 Micro: 0.9309, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1003, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9495\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9407\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9396\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        80\n",
      "    positive       0.98      0.96      0.97       166\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.95      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9132\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 120.1692430973053 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4617, Accuracy: 0.8341, F1 Micro: 0.9034, F1 Macro: 0.9014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3328, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.217, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9719\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9524, F1 Micro: 0.9699, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0736, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9727\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9753\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.523, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2729, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1718, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9252\n",
      "Epoch 5/10, Train Loss: 0.0931, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9285\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9306\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "Epoch 8/10, Train Loss: 0.1024, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0686, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9228\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        82\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.93      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9155\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 120.19370937347412 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5376, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4501, Accuracy: 0.8668, F1 Micro: 0.9215, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3299, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.223, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9684\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9539, F1 Micro: 0.9708, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9737\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.8862, F1 Micro: 0.8862, F1 Macro: 0.8677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2813, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2182, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1575, Accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9548\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9407\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9504\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9448\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9548\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        82\n",
      "    positive       0.98      0.96      0.97       164\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.96      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9156\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 123.03749775886536 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9148\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 578.2194572253887\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 16.477960109710693 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5478, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4484, Accuracy: 0.8512, F1 Micro: 0.9135, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3127, Accuracy: 0.933, F1 Micro: 0.9582, F1 Macro: 0.9559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2165, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1116, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0831, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9793\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.536, Accuracy: 0.8629, F1 Micro: 0.8629, F1 Macro: 0.8308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2736, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9276\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1087, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9466\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9366\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9556, F1 Micro: 0.9556, F1 Macro: 0.9506\n",
      "Epoch 9/10, Train Loss: 0.0769, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9341\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9341\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9556, F1 Micro: 0.9556, F1 Macro: 0.9506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        83\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.96       248\n",
      "   macro avg       0.95      0.95      0.95       248\n",
      "weighted avg       0.96      0.96      0.96       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9197\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.95      0.75      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 121.38555479049683 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5406, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4535, Accuracy: 0.84, F1 Micro: 0.9072, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3193, Accuracy: 0.936, F1 Micro: 0.9604, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2163, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9727\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.97      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5517, Accuracy: 0.8264, F1 Micro: 0.8264, F1 Macro: 0.7725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2905, Accuracy: 0.8792, F1 Micro: 0.8792, F1 Macro: 0.8715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1329, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9211\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0724, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9367\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9303\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       180\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9142\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.97      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.91      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.0877287387848 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5417, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4416, Accuracy: 0.8713, F1 Micro: 0.9242, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3113, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.9578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2124, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "Epoch 7/10, Train Loss: 0.0917, Accuracy: 0.9554, F1 Micro: 0.9717, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.977\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5114, Accuracy: 0.8789, F1 Micro: 0.8789, F1 Macro: 0.8682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2677, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2051, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1127, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.94\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0688, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 10/10, Train Loss: 0.0577, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9397\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9212\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 126.60539436340332 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9594, F1 Micro: 0.9594, F1 Macro: 0.9184\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 632.6869213135692\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 14.600910663604736 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4321, Accuracy: 0.8936, F1 Micro: 0.9358, F1 Macro: 0.9345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2977, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2022, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1449, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0875, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9747\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5261, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9002\n",
      "Epoch 2/10, Train Loss: 0.2898, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8927\n",
      "Epoch 3/10, Train Loss: 0.1834, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1877, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9349\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Epoch 6/10, Train Loss: 0.1251, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9394\n",
      "Epoch 8/10, Train Loss: 0.1013, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9349\n",
      "Epoch 9/10, Train Loss: 0.1033, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.918\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 123.39004945755005 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5395, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4427, Accuracy: 0.869, F1 Micro: 0.9208, F1 Macro: 0.917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3041, Accuracy: 0.9427, F1 Micro: 0.9641, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2051, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.111, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9733\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5199, Accuracy: 0.8821, F1 Micro: 0.8821, F1 Macro: 0.8603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1619, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1777, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1092, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0784, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        86\n",
      "    positive       0.96      0.97      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.95      0.94      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9264\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.80      0.87      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.28686952590942 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5332, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.425, Accuracy: 0.8988, F1 Micro: 0.9386, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2946, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2039, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1128, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5196, Accuracy: 0.9033, F1 Micro: 0.9033, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2812, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 3/10, Train Loss: 0.1829, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9456\n",
      "Epoch 5/10, Train Loss: 0.1047, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1245, Accuracy: 0.9554, F1 Micro: 0.9554, F1 Macro: 0.9493\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 8/10, Train Loss: 0.1036, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9253\n",
      "Epoch 9/10, Train Loss: 0.1225, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9446\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9416\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9554, F1 Micro: 0.9554, F1 Macro: 0.9493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.97       184\n",
      "\n",
      "    accuracy                           0.96       269\n",
      "   macro avg       0.94      0.96      0.95       269\n",
      "weighted avg       0.96      0.96      0.96       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9267\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 126.22688627243042 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9237\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 604.7961188381919\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 13.983600378036499 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5373, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.433, Accuracy: 0.8914, F1 Micro: 0.9348, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2938, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1508, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 8/10, Train Loss: 0.0726, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5289, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2661, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1799, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9324\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9213\n",
      "Epoch 6/10, Train Loss: 0.1213, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 7/10, Train Loss: 0.1142, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9223\n",
      "Epoch 8/10, Train Loss: 0.0838, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9115\n",
      "Epoch 9/10, Train Loss: 0.0785, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9119\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.97      0.91        86\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.95      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.925\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 123.99858283996582 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5359, Accuracy: 0.7894, F1 Micro: 0.8819, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4423, Accuracy: 0.872, F1 Micro: 0.9227, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2944, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2064, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1455, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9792\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5469, Accuracy: 0.7985, F1 Micro: 0.7985, F1 Macro: 0.7249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2758, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1779, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.1182, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9212\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 8/10, Train Loss: 0.1155, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.884\n",
      "Epoch 9/10, Train Loss: 0.0678, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.905\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        87\n",
      "    positive       0.95      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.93       263\n",
      "   macro avg       0.92      0.93      0.92       263\n",
      "weighted avg       0.93      0.93      0.93       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.86      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 126.09712505340576 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5327, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4197, Accuracy: 0.8981, F1 Micro: 0.9384, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2885, Accuracy: 0.9382, F1 Micro: 0.9615, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2104, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.98      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.521, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9477\n",
      "Epoch 3/10, Train Loss: 0.1824, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9438\n",
      "Epoch 4/10, Train Loss: 0.191, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9514\n",
      "Epoch 6/10, Train Loss: 0.1313, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9314\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9426\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9605, F1 Micro: 0.9605, F1 Macro: 0.956\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9605, F1 Micro: 0.9605, F1 Macro: 0.956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        85\n",
      "    positive       0.98      0.96      0.97       168\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9284\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.4039375782013 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9236\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 596.6847223550917\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 12.392969369888306 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.543, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4251, Accuracy: 0.9085, F1 Micro: 0.9437, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2909, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1944, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9725\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0687, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.8774, F1 Micro: 0.8774, F1 Macro: 0.8503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2734, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Epoch 4/10, Train Loss: 0.1683, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.078, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9306\n",
      "Epoch 9/10, Train Loss: 0.087, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0822, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9306\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        85\n",
      "    positive       0.96      0.95      0.95       176\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.93      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9131\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.52457904815674 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.545, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4372, Accuracy: 0.9085, F1 Micro: 0.9437, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2886, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1864, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9734\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9729\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5533, Accuracy: 0.8935, F1 Micro: 0.8935, F1 Macro: 0.8724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1818, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9443\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9164\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9167\n",
      "Epoch 6/10, Train Loss: 0.1374, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9125\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 8/10, Train Loss: 0.0746, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9046\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9256\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.88      0.87        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.93      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.50269913673401 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4248, Accuracy: 0.907, F1 Micro: 0.9429, F1 Macro: 0.9407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2887, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1918, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Epoch 6/10, Train Loss: 0.1089, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9725, F1 Micro: 0.9826, F1 Macro: 0.9816\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9826, F1 Macro: 0.9816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5311, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8942\n",
      "Epoch 2/10, Train Loss: 0.2806, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1944, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1512, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9321\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9209\n",
      "Epoch 7/10, Train Loss: 0.101, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.928\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9081\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9181\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9231\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       181\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9258\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.46135258674622 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9215\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 569.0823806729844\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 10.399487257003784 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4304, Accuracy: 0.9137, F1 Micro: 0.9472, F1 Macro: 0.9457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2686, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1925, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0912, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 8/10, Train Loss: 0.0719, Accuracy: 0.9591, F1 Micro: 0.974, F1 Macro: 0.9716\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4976, Accuracy: 0.8833, F1 Micro: 0.8833, F1 Macro: 0.8758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2433, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1798, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "Epoch 4/10, Train Loss: 0.1553, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9222\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9264\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "Epoch 7/10, Train Loss: 0.0913, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9479\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9259\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9231\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.95       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9269\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.83413457870483 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5378, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4377, Accuracy: 0.9085, F1 Micro: 0.9443, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2696, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9695\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1034, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9749\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5206, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9015\n",
      "Epoch 2/10, Train Loss: 0.2671, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1822, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9172\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9065\n",
      "Epoch 6/10, Train Loss: 0.1204, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9119\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Epoch 8/10, Train Loss: 0.0802, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8966\n",
      "Epoch 9/10, Train Loss: 0.0688, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9096\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.9\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.89      0.89        85\n",
      "    positive       0.95      0.94      0.95       175\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.92      0.92      0.92       260\n",
      "weighted avg       0.93      0.93      0.93       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9098\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.56029224395752 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4197, Accuracy: 0.9085, F1 Micro: 0.9447, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2642, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1938, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9762\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0694, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4912, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9112\n",
      "Epoch 2/10, Train Loss: 0.264, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2141, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1459, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9467\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9222\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9218\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9252\n",
      "Epoch 10/10, Train Loss: 0.0679, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        83\n",
      "    positive       0.98      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.95       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.80367922782898 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9594, F1 Micro: 0.9594, F1 Macro: 0.9179\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 471.68412573288674\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 7.609684467315674 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.522, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4145, Accuracy: 0.8981, F1 Micro: 0.9391, F1 Macro: 0.9385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2805, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1855, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 5/10, Train Loss: 0.1427, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5361, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2446, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9287\n",
      "Epoch 3/10, Train Loss: 0.2043, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9287\n",
      "Epoch 5/10, Train Loss: 0.0969, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9128\n",
      "Epoch 6/10, Train Loss: 0.1345, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9311\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9199\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9124\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.9045\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        85\n",
      "    positive       0.96      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.93      0.93       265\n",
      "weighted avg       0.94      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9172\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 130.23819541931152 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.521, Accuracy: 0.7909, F1 Micro: 0.8829, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4283, Accuracy: 0.9085, F1 Micro: 0.9441, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2799, Accuracy: 0.9501, F1 Micro: 0.9692, F1 Macro: 0.968\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.9479, F1 Micro: 0.967, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5248, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2319, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1953, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "Epoch 7/10, Train Loss: 0.066, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9197\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        85\n",
      "    positive       0.99      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.96      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9279\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.72922158241272 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5208, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.418, Accuracy: 0.8921, F1 Micro: 0.935, F1 Macro: 0.9335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2846, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.496, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9317\n",
      "Epoch 3/10, Train Loss: 0.166, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9176\n",
      "Epoch 4/10, Train Loss: 0.1599, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9258\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9358\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9354\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9215\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.62771010398865 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9222\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 435.66823897537785\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 7.06380033493042 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4162, Accuracy: 0.9226, F1 Micro: 0.9528, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9803\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.513, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.9013\n",
      "Epoch 2/10, Train Loss: 0.2711, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1577, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1447, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Epoch 6/10, Train Loss: 0.0888, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9251\n",
      "Epoch 7/10, Train Loss: 0.0611, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Epoch 8/10, Train Loss: 0.0759, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0538, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9207\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.90        85\n",
      "    positive       0.97      0.94      0.95       180\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.92      0.93      0.93       265\n",
      "weighted avg       0.94      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9132\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.74649930000305 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5406, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4218, Accuracy: 0.9107, F1 Micro: 0.9451, F1 Macro: 0.9429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2747, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1877, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9613, F1 Micro: 0.9754, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5124, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2312, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1394, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1331, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9068\n",
      "Epoch 5/10, Train Loss: 0.1053, Accuracy: 0.9053, F1 Micro: 0.9053, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1013, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "Epoch 7/10, Train Loss: 0.0719, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9038\n",
      "Epoch 8/10, Train Loss: 0.0793, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9152\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9082\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9091\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.93      0.89        85\n",
      "    positive       0.97      0.93      0.95       179\n",
      "\n",
      "    accuracy                           0.93       264\n",
      "   macro avg       0.91      0.93      0.92       264\n",
      "weighted avg       0.93      0.93      0.93       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9133\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.1124804019928 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5328, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4055, Accuracy: 0.9182, F1 Micro: 0.9497, F1 Macro: 0.9476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2724, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1091, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9789\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.95      0.95      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4475, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2183, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1166, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "Epoch 8/10, Train Loss: 0.0974, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9449\n",
      "Epoch 10/10, Train Loss: 0.0683, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9401\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9282\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.83      0.85      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.54836583137512 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9182\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 343.31645222146136\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.087916374206543 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5231, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.399, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2554, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.9695, F1 Micro: 0.981, F1 Macro: 0.9801\n",
      "Epoch 6/10, Train Loss: 0.0997, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4874, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2376, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 3/10, Train Loss: 0.1871, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1226, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 7/10, Train Loss: 0.0894, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.0871, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.91        86\n",
      "    positive       0.98      0.93      0.95       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9191\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.83      0.85      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.4669370651245 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5205, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4114, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2587, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5121, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2335, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1641, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Epoch 4/10, Train Loss: 0.1525, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1078, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0991, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9173\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.85      0.87      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 141.97627878189087 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5159, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3906, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2541, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.52, Accuracy: 0.8797, F1 Micro: 0.8797, F1 Macro: 0.8538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.177, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 4/10, Train Loss: 0.1511, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Epoch 5/10, Train Loss: 0.1134, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 6/10, Train Loss: 0.1123, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9244\n",
      "Epoch 7/10, Train Loss: 0.0963, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0981, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.935\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9086\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9209\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.90      0.91        86\n",
      "    positive       0.95      0.97      0.96       180\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.94      0.93      0.93       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9143\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.79      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.42795968055725 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9596, F1 Micro: 0.9596, F1 Macro: 0.919\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 237.60185499037954\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.2174882888793945 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5306, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4052, Accuracy: 0.9174, F1 Micro: 0.9494, F1 Macro: 0.9477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2711, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1752, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0583, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4896, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1993, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 3/10, Train Loss: 0.1783, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9066\n",
      "Epoch 4/10, Train Loss: 0.1184, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9139\n",
      "Epoch 6/10, Train Loss: 0.0873, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9149\n",
      "Epoch 7/10, Train Loss: 0.0947, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Epoch 8/10, Train Loss: 0.0866, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9217\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9135\n",
      "Epoch 10/10, Train Loss: 0.0895, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9068\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        84\n",
      "    positive       0.99      0.91      0.95       175\n",
      "\n",
      "    accuracy                           0.93       259\n",
      "   macro avg       0.92      0.95      0.93       259\n",
      "weighted avg       0.94      0.93      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9163\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.85465264320374 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5263, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4087, Accuracy: 0.9189, F1 Micro: 0.95, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2679, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4917, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9074\n",
      "Epoch 2/10, Train Loss: 0.2093, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2056, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1234, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1145, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9044\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9109\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.93      0.89        86\n",
      "    positive       0.96      0.93      0.94       176\n",
      "\n",
      "    accuracy                           0.93       262\n",
      "   macro avg       0.91      0.93      0.92       262\n",
      "weighted avg       0.93      0.93      0.93       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9139\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.73985290527344 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5197, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.393, Accuracy: 0.9219, F1 Micro: 0.9516, F1 Macro: 0.9487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2653, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1723, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9728\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4775, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9274\n",
      "Epoch 2/10, Train Loss: 0.2102, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8956\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1301, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9266\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9184\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9225\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        85\n",
      "    positive       0.96      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.94      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.922\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.84463930130005 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9601, F1 Micro: 0.9601, F1 Macro: 0.9174\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 161.17695496629437\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.2051937580108643 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.528, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3965, Accuracy: 0.9174, F1 Micro: 0.9502, F1 Macro: 0.949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2522, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5415, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2172, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Epoch 3/10, Train Loss: 0.178, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8994\n",
      "Epoch 4/10, Train Loss: 0.1173, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 5/10, Train Loss: 0.1234, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9105\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1101, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.0668, Accuracy: 0.9144, F1 Micro: 0.9144, F1 Macro: 0.9064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        84\n",
      "    positive       0.97      0.94      0.95       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9142\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.89282870292664 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5317, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4151, Accuracy: 0.9189, F1 Micro: 0.9509, F1 Macro: 0.9495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2559, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1744, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9673, F1 Micro: 0.9792, F1 Macro: 0.9776\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4605, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9154\n",
      "Epoch 2/10, Train Loss: 0.235, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1519, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "Epoch 4/10, Train Loss: 0.1248, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Epoch 5/10, Train Loss: 0.1138, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0653, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 7/10, Train Loss: 0.0693, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Epoch 9/10, Train Loss: 0.0489, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9199\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9133\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        87\n",
      "    positive       0.97      0.93      0.95       181\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.92      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.86      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.34572911262512 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5207, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3858, Accuracy: 0.9286, F1 Micro: 0.9565, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2481, Accuracy: 0.9516, F1 Micro: 0.9701, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.971, F1 Micro: 0.9816, F1 Macro: 0.9804\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4365, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9129\n",
      "Epoch 2/10, Train Loss: 0.2186, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.9011\n",
      "Epoch 3/10, Train Loss: 0.1561, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1337, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1128, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "Epoch 6/10, Train Loss: 0.13, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.94\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9309\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.36770176887512 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9229\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 40.97309630914209\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.3299460411071777 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5164, Accuracy: 0.8125, F1 Micro: 0.8937, F1 Macro: 0.8928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3892, Accuracy: 0.933, F1 Micro: 0.9585, F1 Macro: 0.9565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1573, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9749\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4552, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9174\n",
      "Epoch 2/10, Train Loss: 0.2416, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.173, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1364, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1068, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Epoch 7/10, Train Loss: 0.0974, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9071, F1 Micro: 0.9071, F1 Macro: 0.898\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        86\n",
      "    positive       0.97      0.93      0.95       183\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.92      0.94      0.93       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 142.85639095306396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5181, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.401, Accuracy: 0.9293, F1 Micro: 0.9564, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2319, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1564, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 5/10, Train Loss: 0.1166, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "Epoch 10/10, Train Loss: 0.0409, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4801, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1945, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9336\n",
      "Epoch 3/10, Train Loss: 0.1713, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 4/10, Train Loss: 0.1128, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Epoch 5/10, Train Loss: 0.1095, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9373\n",
      "Epoch 7/10, Train Loss: 0.0575, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9293\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Epoch 10/10, Train Loss: 0.0783, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9273\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        87\n",
      "    positive       0.98      0.93      0.96       177\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9276\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.6599202156067 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5079, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3818, Accuracy: 0.9286, F1 Micro: 0.9558, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2312, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1623, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9762\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4465, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2357, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 3/10, Train Loss: 0.179, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1379, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Epoch 5/10, Train Loss: 0.1164, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 6/10, Train Loss: 0.0774, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9092\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9054\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        87\n",
      "    positive       0.96      0.96      0.96       175\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9247\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.80      0.85      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 150.85940384864807 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9231\n",
      "Total runtime: 9640.617735147476 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ffd4f",
   "metadata": {
    "papermill": {
     "duration": 0.011918,
     "end_time": "2025-06-08T19:09:42.370825",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.358907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6ba439950641b883e1e619ed96c521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6696, Accuracy: 0.6935, F1 Micro: 0.8002, F1 Macro: 0.707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5881, Accuracy: 0.7775, F1 Micro: 0.8718, F1 Macro: 0.8663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5506, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8808\n",
      "Epoch 4/10, Train Loss: 0.528, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4866, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4714, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.48, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4383, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 9/10, Train Loss: 0.4116, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3836, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.79      0.99      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.80      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6963, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5281, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5145, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4748, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4531, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5539, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.5383, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4717, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3387, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3776, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.4762\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.4762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.20      0.29         5\n",
      "    positive       0.56      0.83      0.67         6\n",
      "\n",
      "    accuracy                           0.55        11\n",
      "   macro avg       0.53      0.52      0.48        11\n",
      "weighted avg       0.53      0.55      0.49        11\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.50      0.06      0.11        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.60      0.37      0.36       216\n",
      "weighted avg       0.72      0.78      0.70       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.50      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.40      0.34      0.29       216\n",
      "weighted avg       0.62      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.33      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.35      0.34      0.29       216\n",
      "weighted avg       0.57      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 62.95226860046387 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6698, Accuracy: 0.7924, F1 Micro: 0.8815, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5861, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Epoch 3/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5332, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.4877, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4812, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4841, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4614, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 9/10, Train Loss: 0.4247, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4075, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8825\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      0.99      0.87       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      1.00      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.812, Accuracy: 0.375, F1 Micro: 0.375, F1 Macro: 0.3651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6454, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3987, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3302, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2917, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2519, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.216, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1941, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0953, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "    positive       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7924, F1 Micro: 0.7924, F1 Macro: 0.3069\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.78      0.99      0.87       167\n",
      "    positive       0.50      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.43      0.34      0.31       216\n",
      "weighted avg       0.68      0.77      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.67      0.10      0.17        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.46      0.37      0.34       216\n",
      "weighted avg       0.64      0.72      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 53.30986738204956 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6693, Accuracy: 0.782, F1 Micro: 0.8767, F1 Macro: 0.875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5726, Accuracy: 0.7894, F1 Micro: 0.8816, F1 Macro: 0.8798\n",
      "Epoch 3/10, Train Loss: 0.5386, Accuracy: 0.7879, F1 Micro: 0.8813, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5272, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4812, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4653, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4676, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Epoch 8/10, Train Loss: 0.4355, Accuracy: 0.7902, F1 Micro: 0.8816, F1 Macro: 0.8795\n",
      "Epoch 9/10, Train Loss: 0.4096, Accuracy: 0.7879, F1 Micro: 0.8795, F1 Macro: 0.8767\n",
      "Epoch 10/10, Train Loss: 0.3739, Accuracy: 0.7879, F1 Micro: 0.8786, F1 Macro: 0.8755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      0.99      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6057, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4393, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4544, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4236, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3367, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Epoch 6/10, Train Loss: 0.3349, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 7/10, Train Loss: 0.2332, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 8/10, Train Loss: 0.2243, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 9/10, Train Loss: 0.1859, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 10/10, Train Loss: 0.1717, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         4\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         4\n",
      "   macro avg       0.50      0.50      0.50         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.2997\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.75      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.49      0.35      0.31       216\n",
      "weighted avg       0.68      0.71      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 50.88021636009216 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3059\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 603.8725983287783\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 23.019099235534668 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6221, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5361, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5299, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4827, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4396, Accuracy: 0.8073, F1 Micro: 0.8911, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3949, Accuracy: 0.8534, F1 Micro: 0.9141, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3513, Accuracy: 0.8735, F1 Micro: 0.9239, F1 Macro: 0.9225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3096, Accuracy: 0.8891, F1 Micro: 0.9325, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2605, Accuracy: 0.9025, F1 Micro: 0.9403, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2288, Accuracy: 0.9159, F1 Micro: 0.948, F1 Macro: 0.9458\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9159, F1 Micro: 0.948, F1 Macro: 0.9458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.93      0.93      0.93       175\n",
      "      others       0.83      0.94      0.88       158\n",
      "        part       0.89      0.97      0.93       158\n",
      "       price       0.97      0.97      0.97       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.93      0.97      0.95      1061\n",
      "   macro avg       0.93      0.97      0.95      1061\n",
      "weighted avg       0.93      0.97      0.95      1061\n",
      " samples avg       0.93      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5385, Accuracy: 0.6964, F1 Micro: 0.6964, F1 Macro: 0.4105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4101, Accuracy: 0.6964, F1 Micro: 0.6964, F1 Macro: 0.4105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4134, Accuracy: 0.817, F1 Micro: 0.817, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2499, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.8649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1431, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.8884, F1 Micro: 0.8884, F1 Macro: 0.8725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1642, Accuracy: 0.9018, F1 Micro: 0.9018, F1 Macro: 0.889\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.8639\n",
      "Epoch 9/10, Train Loss: 0.087, Accuracy: 0.8795, F1 Micro: 0.8795, F1 Macro: 0.8581\n",
      "Epoch 10/10, Train Loss: 0.085, Accuracy: 0.8884, F1 Micro: 0.8884, F1 Macro: 0.8716\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9018, F1 Micro: 0.9018, F1 Macro: 0.889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.93      0.85        68\n",
      "    positive       0.97      0.89      0.93       156\n",
      "\n",
      "    accuracy                           0.90       224\n",
      "   macro avg       0.88      0.91      0.89       224\n",
      "weighted avg       0.91      0.90      0.90       224\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.8086\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.93      0.92      0.92       167\n",
      "    positive       0.67      0.67      0.67        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.80      0.79       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.33      0.35        12\n",
      "     neutral       0.83      0.94      0.88       152\n",
      "    positive       0.78      0.48      0.60        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.66      0.58      0.61       216\n",
      "weighted avg       0.79      0.80      0.78       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.61      0.72        23\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.81      0.61      0.69        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.85      0.73      0.78       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.97      0.97      0.97       186\n",
      "    positive       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.83      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 74.96636915206909 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6282, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 2/10, Train Loss: 0.5389, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.5245, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4841, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4446, Accuracy: 0.8021, F1 Micro: 0.8884, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4072, Accuracy: 0.8259, F1 Micro: 0.8999, F1 Macro: 0.8985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3612, Accuracy: 0.8408, F1 Micro: 0.9074, F1 Macro: 0.9059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3198, Accuracy: 0.875, F1 Micro: 0.9257, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2698, Accuracy: 0.8996, F1 Micro: 0.9388, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2344, Accuracy: 0.9062, F1 Micro: 0.9425, F1 Macro: 0.9401\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9062, F1 Micro: 0.9425, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.90      0.96      0.93       175\n",
      "      others       0.82      0.92      0.87       158\n",
      "        part       0.85      0.97      0.91       158\n",
      "       price       0.97      0.97      0.97       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.91      0.97      0.94      1061\n",
      "   macro avg       0.91      0.97      0.94      1061\n",
      "weighted avg       0.92      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6186, Accuracy: 0.6942, F1 Micro: 0.6942, F1 Macro: 0.4097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3939, Accuracy: 0.6942, F1 Micro: 0.6942, F1 Macro: 0.4097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3582, Accuracy: 0.8204, F1 Micro: 0.8204, F1 Macro: 0.7505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3165, Accuracy: 0.8932, F1 Micro: 0.8932, F1 Macro: 0.8742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1996, Accuracy: 0.8932, F1 Micro: 0.8932, F1 Macro: 0.8742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9126, F1 Micro: 0.9126, F1 Macro: 0.9012\n",
      "Epoch 7/10, Train Loss: 0.0624, Accuracy: 0.9029, F1 Micro: 0.9029, F1 Macro: 0.8787\n",
      "Epoch 8/10, Train Loss: 0.0815, Accuracy: 0.9029, F1 Micro: 0.9029, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1022, Accuracy: 0.9175, F1 Micro: 0.9175, F1 Macro: 0.907\n",
      "Epoch 10/10, Train Loss: 0.0912, Accuracy: 0.8932, F1 Micro: 0.8932, F1 Macro: 0.8826\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9175, F1 Micro: 0.9175, F1 Macro: 0.907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.95      0.88        63\n",
      "    positive       0.98      0.90      0.94       143\n",
      "\n",
      "    accuracy                           0.92       206\n",
      "   macro avg       0.89      0.93      0.91       206\n",
      "weighted avg       0.93      0.92      0.92       206\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8981, F1 Micro: 0.8981, F1 Macro: 0.79\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.90      0.96      0.93       167\n",
      "    positive       0.78      0.55      0.64        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.75      0.79       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.30      0.25      0.27        12\n",
      "     neutral       0.82      0.91      0.86       152\n",
      "    positive       0.69      0.48      0.57        52\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.60      0.55      0.57       216\n",
      "weighted avg       0.76      0.77      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.65      0.77        23\n",
      "     neutral       0.85      0.98      0.91       152\n",
      "    positive       0.88      0.54      0.67        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.89      0.72      0.78       216\n",
      "weighted avg       0.87      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.97      0.97      0.97       186\n",
      "    positive       0.69      0.65      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       1.00      0.53      0.69        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.75      0.82       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Total train time: 71.4807596206665 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6316, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5375, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5374, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5027, Accuracy: 0.7879, F1 Micro: 0.8811, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4683, Accuracy: 0.7969, F1 Micro: 0.8853, F1 Macro: 0.8835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4248, Accuracy: 0.8013, F1 Micro: 0.8867, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4048, Accuracy: 0.8199, F1 Micro: 0.8959, F1 Macro: 0.8935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3413, Accuracy: 0.8534, F1 Micro: 0.9133, F1 Macro: 0.9114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2875, Accuracy: 0.8743, F1 Micro: 0.9242, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2512, Accuracy: 0.8943, F1 Micro: 0.9356, F1 Macro: 0.9332\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8943, F1 Micro: 0.9356, F1 Macro: 0.9332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.93      1.00      0.96       187\n",
      "     machine       0.89      0.93      0.91       175\n",
      "      others       0.84      0.92      0.88       158\n",
      "        part       0.86      0.97      0.91       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.93      0.99      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.97      0.94      1061\n",
      "   macro avg       0.90      0.97      0.93      1061\n",
      "weighted avg       0.90      0.97      0.94      1061\n",
      " samples avg       0.91      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6677, Accuracy: 0.7292, F1 Micro: 0.7292, F1 Macro: 0.4217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.568, Accuracy: 0.7292, F1 Micro: 0.7292, F1 Macro: 0.4217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4444, Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3157, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1667, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.8969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0217, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.8969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.8958\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8798\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.8958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        52\n",
      "    positive       0.95      0.94      0.94       140\n",
      "\n",
      "    accuracy                           0.92       192\n",
      "   macro avg       0.89      0.90      0.90       192\n",
      "weighted avg       0.92      0.92      0.92       192\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8843, F1 Micro: 0.8843, F1 Macro: 0.7461\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.55      0.71        11\n",
      "     neutral       0.93      1.00      0.97       181\n",
      "    positive       0.94      0.62      0.75        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.96      0.72      0.81       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.89      0.93      0.91       167\n",
      "    positive       0.62      0.48      0.54        33\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.79      0.72      0.75       216\n",
      "weighted avg       0.84      0.85      0.85       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.33      0.42        12\n",
      "     neutral       0.84      0.92      0.88       152\n",
      "    positive       0.71      0.58      0.64        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.71      0.61      0.65       216\n",
      "weighted avg       0.79      0.81      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.39      0.53        23\n",
      "     neutral       0.86      0.98      0.91       152\n",
      "    positive       0.77      0.59      0.67        41\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.82      0.65      0.70       216\n",
      "weighted avg       0.84      0.84      0.83       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.73      0.79       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.50      0.64        14\n",
      "     neutral       0.93      0.99      0.96       185\n",
      "    positive       1.00      0.59      0.74        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.69      0.78       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Total train time: 76.1570053100586 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8956, F1 Micro: 0.8956, F1 Macro: 0.7816\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 607.1364671828719\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 30.011548280715942 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5963, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5148, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5014, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4604, Accuracy: 0.8073, F1 Micro: 0.8911, F1 Macro: 0.8898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3848, Accuracy: 0.8854, F1 Micro: 0.9307, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.335, Accuracy: 0.8981, F1 Micro: 0.9372, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2746, Accuracy: 0.9174, F1 Micro: 0.949, F1 Macro: 0.9471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2414, Accuracy: 0.9308, F1 Micro: 0.957, F1 Macro: 0.9546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2071, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1696, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.96\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.86      0.93      0.89       158\n",
      "        part       0.92      0.98      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5594, Accuracy: 0.6781, F1 Micro: 0.6781, F1 Macro: 0.4041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.47, Accuracy: 0.691, F1 Micro: 0.691, F1 Macro: 0.4567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3093, Accuracy: 0.897, F1 Micro: 0.897, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1305, Accuracy: 0.9056, F1 Micro: 0.9056, F1 Macro: 0.8953\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9013, F1 Micro: 0.9013, F1 Macro: 0.8881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0708, Accuracy: 0.9099, F1 Micro: 0.9099, F1 Macro: 0.8998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0543, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9036\n",
      "Epoch 8/10, Train Loss: 0.05, Accuracy: 0.897, F1 Micro: 0.897, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.038, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9054\n",
      "Epoch 10/10, Train Loss: 0.0282, Accuracy: 0.897, F1 Micro: 0.897, F1 Macro: 0.8844\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.95      0.88        75\n",
      "    positive       0.97      0.90      0.93       158\n",
      "\n",
      "    accuracy                           0.91       233\n",
      "   macro avg       0.89      0.92      0.91       233\n",
      "weighted avg       0.92      0.91      0.92       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.929, F1 Micro: 0.929, F1 Macro: 0.8608\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.93      0.97      0.95       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.67      0.59        12\n",
      "     neutral       0.86      0.93      0.90       152\n",
      "    positive       0.78      0.54      0.64        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.72      0.71      0.71       216\n",
      "weighted avg       0.82      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 83.03882598876953 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5855, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.52, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5037, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4691, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3968, Accuracy: 0.8534, F1 Micro: 0.9143, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3428, Accuracy: 0.8936, F1 Micro: 0.9355, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2837, Accuracy: 0.9196, F1 Micro: 0.9505, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2423, Accuracy: 0.9256, F1 Micro: 0.9539, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2013, Accuracy: 0.9375, F1 Micro: 0.961, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1639, Accuracy: 0.9397, F1 Micro: 0.9626, F1 Macro: 0.9605\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9397, F1 Micro: 0.9626, F1 Macro: 0.9605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.90      0.93      0.92       158\n",
      "        part       0.89      0.98      0.93       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6264, Accuracy: 0.6767, F1 Micro: 0.6767, F1 Macro: 0.4036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4994, Accuracy: 0.6983, F1 Micro: 0.6983, F1 Macro: 0.4815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3641, Accuracy: 0.8578, F1 Micro: 0.8578, F1 Macro: 0.8261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2069, Accuracy: 0.8966, F1 Micro: 0.8966, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1327, Accuracy: 0.9138, F1 Micro: 0.9138, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0622, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9147\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8886\n",
      "Epoch 8/10, Train Loss: 0.1166, Accuracy: 0.8879, F1 Micro: 0.8879, F1 Macro: 0.8799\n",
      "Epoch 9/10, Train Loss: 0.0274, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.893\n",
      "Epoch 10/10, Train Loss: 0.1033, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9007\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.85      0.88        75\n",
      "    positive       0.93      0.96      0.95       157\n",
      "\n",
      "    accuracy                           0.93       232\n",
      "   macro avg       0.92      0.91      0.91       232\n",
      "weighted avg       0.93      0.93      0.93       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.929, F1 Micro: 0.929, F1 Macro: 0.8537\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.82      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.90      0.93      0.92       152\n",
      "    positive       0.80      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.79      0.81       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.61      0.72        23\n",
      "     neutral       0.89      0.99      0.93       152\n",
      "    positive       0.81      0.61      0.69        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.74      0.78       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.82      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.81      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 80.87627124786377 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5877, Accuracy: 0.7894, F1 Micro: 0.8819, F1 Macro: 0.8802\n",
      "Epoch 2/10, Train Loss: 0.524, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5102, Accuracy: 0.7924, F1 Micro: 0.8835, F1 Macro: 0.8819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4768, Accuracy: 0.7961, F1 Micro: 0.8846, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4181, Accuracy: 0.8185, F1 Micro: 0.8955, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3601, Accuracy: 0.875, F1 Micro: 0.9244, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2942, Accuracy: 0.9085, F1 Micro: 0.9439, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2497, Accuracy: 0.9182, F1 Micro: 0.9489, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2053, Accuracy: 0.9211, F1 Micro: 0.9508, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1692, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9575\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.94      0.94       175\n",
      "      others       0.87      0.95      0.91       158\n",
      "        part       0.92      0.98      0.95       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6092, Accuracy: 0.6867, F1 Micro: 0.6867, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4723, Accuracy: 0.7425, F1 Micro: 0.7425, F1 Macro: 0.6064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3273, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.153, Accuracy: 0.9056, F1 Micro: 0.9056, F1 Macro: 0.8894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9099, F1 Micro: 0.9099, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0839, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9088\n",
      "Epoch 7/10, Train Loss: 0.0652, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0421, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9184\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.96      0.89        73\n",
      "    positive       0.98      0.91      0.94       160\n",
      "\n",
      "    accuracy                           0.93       233\n",
      "   macro avg       0.91      0.94      0.92       233\n",
      "weighted avg       0.93      0.93      0.93       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.8516\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.93      0.94      0.94       167\n",
      "    positive       0.73      0.73      0.73        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.81      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.67      0.64        12\n",
      "     neutral       0.87      0.95      0.91       152\n",
      "    positive       0.81      0.58      0.67        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.76      0.73      0.74       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.92      0.99      0.95       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.80      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 87.49950742721558 s\n",
      "Averaged - Iteration 208: Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.8554\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 331.7770996529547\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 29.78696298599243 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5821, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5141, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4832, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.393, Accuracy: 0.8713, F1 Micro: 0.9238, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.336, Accuracy: 0.9182, F1 Micro: 0.9494, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2927, Accuracy: 0.9286, F1 Micro: 0.9554, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2288, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9675\n",
      "Epoch 8/10, Train Loss: 0.1879, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1528, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1257, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9699\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9699\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4771, Accuracy: 0.679, F1 Micro: 0.679, F1 Macro: 0.4044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.443, Accuracy: 0.8683, F1 Micro: 0.8683, F1 Macro: 0.8544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2478, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0553, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9156\n",
      "Epoch 6/10, Train Loss: 0.064, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9053\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.909\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9053, F1 Micro: 0.9053, F1 Macro: 0.8968\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8994\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9049\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.90      0.89        78\n",
      "    positive       0.95      0.94      0.95       165\n",
      "\n",
      "    accuracy                           0.93       243\n",
      "   macro avg       0.91      0.92      0.92       243\n",
      "weighted avg       0.93      0.93      0.93       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8829\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.67      0.62        12\n",
      "     neutral       0.91      0.94      0.92       152\n",
      "    positive       0.82      0.69      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.76      0.77      0.76       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 87.82355308532715 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5782, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5161, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4763, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4033, Accuracy: 0.8527, F1 Micro: 0.9143, F1 Macro: 0.9128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3503, Accuracy: 0.9055, F1 Micro: 0.9426, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.296, Accuracy: 0.9278, F1 Micro: 0.9551, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2273, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1815, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1503, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1203, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9713\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.94      0.97      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5789, Accuracy: 0.6853, F1 Micro: 0.6853, F1 Macro: 0.4066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4656, Accuracy: 0.8406, F1 Micro: 0.8406, F1 Macro: 0.8019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2836, Accuracy: 0.8924, F1 Micro: 0.8924, F1 Macro: 0.8766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1703, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.8962\n",
      "Epoch 5/10, Train Loss: 0.158, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0775, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9409\n",
      "Epoch 8/10, Train Loss: 0.102, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9349\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9093\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        79\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.93      0.95      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9011\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.81      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 89.4813904762268 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5838, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5243, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.502, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4331, Accuracy: 0.8132, F1 Micro: 0.8937, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3909, Accuracy: 0.8787, F1 Micro: 0.927, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3288, Accuracy: 0.9189, F1 Micro: 0.95, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.25, Accuracy: 0.9345, F1 Micro: 0.959, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2002, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.9582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1701, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1306, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9697\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5563, Accuracy: 0.6857, F1 Micro: 0.6857, F1 Macro: 0.419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4274, Accuracy: 0.8776, F1 Micro: 0.8776, F1 Macro: 0.8641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1166, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9327\n",
      "Epoch 5/10, Train Loss: 0.1089, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9137\n",
      "Epoch 6/10, Train Loss: 0.0584, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.8978\n",
      "Epoch 7/10, Train Loss: 0.1148, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.8945\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.9057\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9267\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9185\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.87      0.91        78\n",
      "    positive       0.94      0.98      0.96       167\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.94      0.92      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.8837\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.90      0.93      0.92       152\n",
      "    positive       0.78      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.78      0.82       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.78      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.85      0.85      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 86.95916867256165 s\n",
      "Averaged - Iteration 274: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8892\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 343.2579659525836\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 28.824418544769287 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.582, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4868, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4299, Accuracy: 0.843, F1 Micro: 0.9094, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3514, Accuracy: 0.91, F1 Micro: 0.9445, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2845, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2258, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1716, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1429, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1161, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0964, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9736\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5565, Accuracy: 0.6598, F1 Micro: 0.6598, F1 Macro: 0.3975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3874, Accuracy: 0.8755, F1 Micro: 0.8755, F1 Macro: 0.8664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2094, Accuracy: 0.9212, F1 Micro: 0.9212, F1 Macro: 0.9152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1136, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0881, Accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.9287\n",
      "Epoch 6/10, Train Loss: 0.0903, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9071\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9212, F1 Micro: 0.9212, F1 Macro: 0.913\n",
      "Epoch 8/10, Train Loss: 0.0441, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8857\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.888, F1 Micro: 0.888, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9394\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.90      0.92        82\n",
      "    positive       0.95      0.97      0.96       159\n",
      "\n",
      "    accuracy                           0.95       241\n",
      "   macro avg       0.94      0.94      0.94       241\n",
      "weighted avg       0.95      0.95      0.95       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.903\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.87      0.91        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 97.92677783966064 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5728, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4892, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4447, Accuracy: 0.8229, F1 Micro: 0.8991, F1 Macro: 0.8977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3721, Accuracy: 0.8951, F1 Micro: 0.9371, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3045, Accuracy: 0.9397, F1 Micro: 0.9627, F1 Macro: 0.9613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2341, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1718, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1418, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1189, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "Epoch 10/10, Train Loss: 0.0966, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9717\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.95      0.98      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5644, Accuracy: 0.6653, F1 Micro: 0.6653, F1 Macro: 0.3995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4192, Accuracy: 0.8694, F1 Micro: 0.8694, F1 Macro: 0.8505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9358\n",
      "Epoch 4/10, Train Loss: 0.1232, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9118\n",
      "Epoch 5/10, Train Loss: 0.0821, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9315\n",
      "Epoch 6/10, Train Loss: 0.1259, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1088, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.946\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9333\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0337, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9287\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.94      0.96       163\n",
      "\n",
      "    accuracy                           0.95       245\n",
      "   macro avg       0.94      0.95      0.95       245\n",
      "weighted avg       0.95      0.95      0.95       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9064\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.82      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 92.47738575935364 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5775, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4983, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4704, Accuracy: 0.8125, F1 Micro: 0.8928, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4003, Accuracy: 0.875, F1 Micro: 0.926, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3238, Accuracy: 0.9308, F1 Micro: 0.9569, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2534, Accuracy: 0.9405, F1 Micro: 0.9627, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1845, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1499, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1219, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0997, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9724\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.94      0.97      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5313, Accuracy: 0.6967, F1 Micro: 0.6967, F1 Macro: 0.487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.352, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1616, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.0899, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9256\n",
      "Epoch 5/10, Train Loss: 0.1072, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9158\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0687, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.059, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        81\n",
      "    positive       0.97      0.94      0.96       163\n",
      "\n",
      "    accuracy                           0.94       244\n",
      "   macro avg       0.93      0.94      0.94       244\n",
      "weighted avg       0.94      0.94      0.94       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9009\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 98.02396535873413 s\n",
      "Averaged - Iteration 333: Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9035\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 351.7577730498899\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 26.05573081970215 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5612, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4935, Accuracy: 0.8036, F1 Micro: 0.8894, F1 Macro: 0.8881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3935, Accuracy: 0.8832, F1 Micro: 0.9294, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3281, Accuracy: 0.9278, F1 Micro: 0.9556, F1 Macro: 0.9541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2439, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1942, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1512, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1225, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0995, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0826, Accuracy: 0.9658, F1 Micro: 0.9787, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9787, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.89      0.99      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.541, Accuracy: 0.6598, F1 Micro: 0.6598, F1 Macro: 0.3975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3479, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1684, Accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1073, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9404\n",
      "Epoch 5/10, Train Loss: 0.0981, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9452\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9367\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9414\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0275, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9397\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        82\n",
      "    positive       0.97      0.95      0.96       159\n",
      "\n",
      "    accuracy                           0.95       241\n",
      "   macro avg       0.94      0.95      0.95       241\n",
      "weighted avg       0.95      0.95      0.95       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.95      0.69      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.78      0.82       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 105.49690675735474 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5603, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4934, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4167, Accuracy: 0.8646, F1 Micro: 0.9205, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3438, Accuracy: 0.9263, F1 Micro: 0.9546, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2561, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Epoch 6/10, Train Loss: 0.1949, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1542, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1222, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9734\n",
      "Epoch 9/10, Train Loss: 0.1, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0821, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.98      0.94       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.544, Accuracy: 0.6898, F1 Micro: 0.6898, F1 Macro: 0.4828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3587, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1832, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9248\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9206\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9206\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9189\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.8972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9326\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0383, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9228\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        83\n",
      "    positive       0.97      0.94      0.95       162\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.94      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9113\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.82      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.95      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 100.44148516654968 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5655, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.5151, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4418, Accuracy: 0.8318, F1 Micro: 0.9032, F1 Macro: 0.9021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3716, Accuracy: 0.9167, F1 Micro: 0.9489, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2742, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2129, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.164, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1278, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.1023, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0842, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.94      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.7437, F1 Micro: 0.7437, F1 Macro: 0.6167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3285, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1475, Accuracy: 0.9664, F1 Micro: 0.9664, F1 Macro: 0.9628\n",
      "Epoch 4/10, Train Loss: 0.126, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9523\n",
      "Epoch 5/10, Train Loss: 0.103, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9529\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9495\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9585\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.954\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.954\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9439\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9664, F1 Micro: 0.9664, F1 Macro: 0.9628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.96      0.95        81\n",
      "    positive       0.98      0.97      0.97       157\n",
      "\n",
      "    accuracy                           0.97       238\n",
      "   macro avg       0.96      0.97      0.96       238\n",
      "weighted avg       0.97      0.97      0.97       238\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.918\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.86      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 99.57689499855042 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9159\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 333.99250186893465\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 23.83315086364746 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5684, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4765, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.389, Accuracy: 0.8966, F1 Micro: 0.9371, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3015, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2244, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1769, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1409, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1124, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0869, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0772, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5376, Accuracy: 0.8161, F1 Micro: 0.8161, F1 Macro: 0.7462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.338, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.8937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2138, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1274, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1812, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9237\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1082, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0683, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        83\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9094\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.91      0.91       152\n",
      "    positive       0.76      0.79      0.77        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.81      0.82      0.81       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.48342728614807 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5623, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4836, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4107, Accuracy: 0.8787, F1 Micro: 0.9283, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3196, Accuracy: 0.9345, F1 Micro: 0.9594, F1 Macro: 0.9577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2304, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1771, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.972\n",
      "Epoch 7/10, Train Loss: 0.1442, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1169, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0905, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0775, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.534, Accuracy: 0.8069, F1 Micro: 0.8069, F1 Macro: 0.7433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3226, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.918\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1582, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Epoch 7/10, Train Loss: 0.0988, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9375\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9281\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        83\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.96      0.95       259\n",
      "weighted avg       0.96      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 102.82748103141785 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5622, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.4977, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4369, Accuracy: 0.846, F1 Micro: 0.9101, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3509, Accuracy: 0.9338, F1 Micro: 0.9591, F1 Macro: 0.9576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2432, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1801, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1406, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1136, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0886, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5618, Accuracy: 0.8525, F1 Micro: 0.8525, F1 Macro: 0.8227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.322, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.176, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9372\n",
      "Epoch 4/10, Train Loss: 0.1357, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1094, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9353\n",
      "Epoch 6/10, Train Loss: 0.0943, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.914\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9285\n",
      "Epoch 8/10, Train Loss: 0.0764, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0288, Accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9546\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9412\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        81\n",
      "    positive       0.99      0.95      0.97       163\n",
      "\n",
      "    accuracy                           0.96       244\n",
      "   macro avg       0.95      0.96      0.95       244\n",
      "weighted avg       0.96      0.96      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9193\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.93      0.98      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 103.27289700508118 s\n",
      "Averaged - Iteration 435: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9166\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 348.3414464989355\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 22.900959730148315 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5632, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4733, Accuracy: 0.8028, F1 Micro: 0.889, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3905, Accuracy: 0.9144, F1 Micro: 0.9473, F1 Macro: 0.9451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2914, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2193, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1757, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1315, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Epoch 8/10, Train Loss: 0.1049, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0912, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5465, Accuracy: 0.771, F1 Micro: 0.771, F1 Macro: 0.6593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3409, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9079\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1498, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1424, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "Epoch 10/10, Train Loss: 0.0885, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9303\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.96      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9231\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.95      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 108.56864643096924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5607, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4754, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4065, Accuracy: 0.904, F1 Micro: 0.9414, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.302, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2182, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1701, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1283, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.972\n",
      "Epoch 8/10, Train Loss: 0.0993, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.8431, F1 Micro: 0.8431, F1 Macro: 0.8111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3137, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1087, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.929\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9281\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        83\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.96      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9272\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 106.73005151748657 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5674, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4897, Accuracy: 0.7984, F1 Micro: 0.8864, F1 Macro: 0.8848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4417, Accuracy: 0.8519, F1 Micro: 0.9131, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3306, Accuracy: 0.9293, F1 Micro: 0.956, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.241, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1806, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Epoch 7/10, Train Loss: 0.1349, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1055, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0904, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4834, Accuracy: 0.8779, F1 Micro: 0.8779, F1 Macro: 0.856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2441, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1847, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1448, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9345\n",
      "Epoch 5/10, Train Loss: 0.1516, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0785, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0824, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "Epoch 10/10, Train Loss: 0.0987, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9341\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9208\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 113.89686703681946 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9586, F1 Micro: 0.9586, F1 Macro: 0.9237\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 366.8982137715445\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 19.70021343231201 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5506, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4642, Accuracy: 0.8304, F1 Micro: 0.9028, F1 Macro: 0.9019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3513, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.248, Accuracy: 0.9405, F1 Micro: 0.9629, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1856, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1394, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9757\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5308, Accuracy: 0.8814, F1 Micro: 0.8814, F1 Macro: 0.8567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1573, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9345\n",
      "Epoch 4/10, Train Loss: 0.1352, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Epoch 5/10, Train Loss: 0.1752, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9097\n",
      "Epoch 6/10, Train Loss: 0.1182, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9462\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9472\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        82\n",
      "    positive       0.99      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.96      0.95       253\n",
      "weighted avg       0.96      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9189\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.81      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.90880966186523 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.8073, F1 Micro: 0.8912, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3693, Accuracy: 0.9278, F1 Micro: 0.9553, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2585, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1829, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5968, Accuracy: 0.7395, F1 Micro: 0.7395, F1 Macro: 0.6031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3189, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1645, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1571, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9233\n",
      "Epoch 6/10, Train Loss: 0.1315, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9192\n",
      "Epoch 7/10, Train Loss: 0.1097, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Epoch 8/10, Train Loss: 0.0963, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9348\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        84\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9161\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.73266768455505 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4933, Accuracy: 0.7932, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4032, Accuracy: 0.9025, F1 Micro: 0.9404, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2802, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1942, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9747\n",
      "Epoch 7/10, Train Loss: 0.1097, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.7605, F1 Micro: 0.7605, F1 Macro: 0.6403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2823, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 3/10, Train Loss: 0.1819, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "Epoch 5/10, Train Loss: 0.1145, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1115, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "Epoch 8/10, Train Loss: 0.0991, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.962, F1 Micro: 0.962, F1 Macro: 0.9563\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9582, F1 Micro: 0.9582, F1 Macro: 0.9532\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.962, F1 Micro: 0.962, F1 Macro: 0.9563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.93      0.94        85\n",
      "    positive       0.97      0.98      0.97       178\n",
      "\n",
      "    accuracy                           0.96       263\n",
      "   macro avg       0.96      0.95      0.96       263\n",
      "weighted avg       0.96      0.96      0.96       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9299\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.93      0.90      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 111.00162124633789 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9588, F1 Micro: 0.9588, F1 Macro: 0.9217\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 335.0567875760551\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 20.167572259902954 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5585, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4687, Accuracy: 0.8274, F1 Micro: 0.9014, F1 Macro: 0.9006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3578, Accuracy: 0.9301, F1 Micro: 0.9566, F1 Macro: 0.9546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2596, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1471, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.1164, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5214, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8862\n",
      "Epoch 2/10, Train Loss: 0.2921, Accuracy: 0.884, F1 Micro: 0.884, F1 Macro: 0.8568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0972, Accuracy: 0.964, F1 Micro: 0.964, F1 Macro: 0.96\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.964, F1 Micro: 0.964, F1 Macro: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        83\n",
      "    positive       0.99      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.97      0.96       250\n",
      "weighted avg       0.97      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.931\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.80      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 117.27027177810669 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5544, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.808, F1 Micro: 0.8915, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3753, Accuracy: 0.9249, F1 Micro: 0.9534, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2634, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1876, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1404, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0755, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5466, Accuracy: 0.82, F1 Micro: 0.82, F1 Macro: 0.7778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.321, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1802, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1434, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1165, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9516\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9389\n",
      "Epoch 7/10, Train Loss: 0.1166, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9474\n",
      "Epoch 8/10, Train Loss: 0.0823, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9474\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 10/10, Train Loss: 0.0247, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9299\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       165\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.96      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      1.00      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.63619709014893 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5603, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4963, Accuracy: 0.8043, F1 Micro: 0.8893, F1 Macro: 0.8879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4122, Accuracy: 0.907, F1 Micro: 0.9439, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.289, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2038, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1571, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Epoch 7/10, Train Loss: 0.1185, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0969, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.99      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5233, Accuracy: 0.877, F1 Micro: 0.877, F1 Macro: 0.8603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2648, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9389\n",
      "Epoch 3/10, Train Loss: 0.2175, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9309\n",
      "Epoch 4/10, Train Loss: 0.1704, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9133\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1142, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9347\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.93      0.96       168\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.93      0.96      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9226\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.95      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 109.96585869789124 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9263\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 374.4780240558169\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 16.92480158805847 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5419, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4424, Accuracy: 0.8326, F1 Micro: 0.9036, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3357, Accuracy: 0.9256, F1 Micro: 0.9537, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2501, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.184, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1386, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "Epoch 9/10, Train Loss: 0.0677, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9736\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5634, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2681, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 4/10, Train Loss: 0.1451, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9167\n",
      "Epoch 5/10, Train Loss: 0.1249, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9223\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9109\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 9/10, Train Loss: 0.0937, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9126\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9324\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        86\n",
      "    positive       0.97      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.94      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9283\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 116.8416633605957 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4511, Accuracy: 0.8244, F1 Micro: 0.8997, F1 Macro: 0.899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3457, Accuracy: 0.9293, F1 Micro: 0.9563, F1 Macro: 0.9546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2558, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1802, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.135, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9768\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.068, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.557, Accuracy: 0.8502, F1 Micro: 0.8502, F1 Macro: 0.8228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2924, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1646, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9297\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9175\n",
      "Epoch 5/10, Train Loss: 0.1611, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1247, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9282\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9269\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9242\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9188\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.90        86\n",
      "    positive       0.95      0.96      0.95       181\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.93      0.93       267\n",
      "weighted avg       0.94      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9149\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.83      0.85      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.94493198394775 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.541, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4683, Accuracy: 0.8118, F1 Micro: 0.8931, F1 Macro: 0.8918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3817, Accuracy: 0.9196, F1 Micro: 0.951, F1 Macro: 0.9497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2803, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1937, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0615, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.503, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2814, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1695, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 4/10, Train Loss: 0.1346, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1125, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1356, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9561\n",
      "Epoch 7/10, Train Loss: 0.0918, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9317\n",
      "Epoch 8/10, Train Loss: 0.0686, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9073\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9354\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9522\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        85\n",
      "    positive       0.98      0.96      0.97       170\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.96       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9359\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 119.85014820098877 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9264\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 379.8333914724316\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 15.880436420440674 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4502, Accuracy: 0.8787, F1 Micro: 0.928, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3243, Accuracy: 0.9353, F1 Micro: 0.9601, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2369, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1723, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1334, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1003, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0592, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5148, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2646, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1975, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1425, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9426\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9196\n",
      "Epoch 7/10, Train Loss: 0.0677, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9233\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9352\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9212\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9356\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        85\n",
      "    positive       0.96      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.94      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9211\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.93      0.90      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.64922642707825 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5427, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4681, Accuracy: 0.8438, F1 Micro: 0.9098, F1 Macro: 0.9093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3452, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.24, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9695\n",
      "Epoch 5/10, Train Loss: 0.1736, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1318, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1034, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5114, Accuracy: 0.8808, F1 Micro: 0.8808, F1 Macro: 0.8657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1369, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1516, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9399\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9577, F1 Micro: 0.9577, F1 Macro: 0.9526\n",
      "Epoch 9/10, Train Loss: 0.0635, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9577, F1 Micro: 0.9577, F1 Macro: 0.9526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.95      0.96      0.95       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9306\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.71887707710266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5475, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.49, Accuracy: 0.8162, F1 Micro: 0.8953, F1 Macro: 0.8939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.387, Accuracy: 0.9301, F1 Micro: 0.9572, F1 Macro: 0.9558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2627, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1842, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.138, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1092, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9789\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5398, Accuracy: 0.8842, F1 Micro: 0.8842, F1 Macro: 0.8694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2815, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1849, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1708, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 5/10, Train Loss: 0.1019, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9161\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1248, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0988, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0435, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9475\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        85\n",
      "    positive       0.97      0.97      0.97       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.95      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9267\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 126.51438856124878 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9261\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 389.81584590959505\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 14.136383056640625 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5328, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4352, Accuracy: 0.8884, F1 Micro: 0.9336, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3105, Accuracy: 0.9442, F1 Micro: 0.9656, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2239, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1636, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1185, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.0941, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0753, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5235, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2887, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1648, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9476\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9148\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.909\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9107\n",
      "Epoch 10/10, Train Loss: 0.0545, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9243\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.96      0.95       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9202\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.81      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 122.36968398094177 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.453, Accuracy: 0.8512, F1 Micro: 0.9134, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3324, Accuracy: 0.9382, F1 Micro: 0.962, F1 Macro: 0.9607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1684, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1203, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0666, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5344, Accuracy: 0.8794, F1 Micro: 0.8794, F1 Macro: 0.8589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2971, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2068, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9476\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0932, Accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9601\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9023\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9459\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9572, F1 Micro: 0.9572, F1 Macro: 0.9518\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9365\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9463\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9601\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.94      0.95        84\n",
      "    positive       0.97      0.98      0.97       173\n",
      "\n",
      "    accuracy                           0.96       257\n",
      "   macro avg       0.96      0.96      0.96       257\n",
      "weighted avg       0.96      0.96      0.96       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.928\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.88      0.88      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.92      0.80      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      1.00      0.98        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.85089087486267 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5354, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4677, Accuracy: 0.814, F1 Micro: 0.8927, F1 Macro: 0.8908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3602, Accuracy: 0.9308, F1 Micro: 0.9569, F1 Macro: 0.9544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2479, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.177, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0626, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4857, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1697, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1414, Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9565\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9439\n",
      "Epoch 7/10, Train Loss: 0.092, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.943\n",
      "Epoch 8/10, Train Loss: 0.0448, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9395\n",
      "Epoch 10/10, Train Loss: 0.0835, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9275\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        85\n",
      "    positive       0.99      0.95      0.97       169\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.95      0.96      0.96       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9322\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 126.04928827285767 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9268\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 391.78328873847113\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 13.126485109329224 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5447, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4334, Accuracy: 0.8832, F1 Micro: 0.9309, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3053, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2159, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1645, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4956, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3065, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1753, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9375\n",
      "Epoch 4/10, Train Loss: 0.153, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 7/10, Train Loss: 0.0917, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9056\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        84\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9185\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 127.97709035873413 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4394, Accuracy: 0.8728, F1 Micro: 0.9253, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3083, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2122, Accuracy: 0.9487, F1 Micro: 0.9679, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1629, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0939, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5229, Accuracy: 0.8803, F1 Micro: 0.8803, F1 Macro: 0.863\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.8659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1941, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1688, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.952\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1558, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9565\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        85\n",
      "    positive       0.98      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.96       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9297\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.5836844444275 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5484, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.8125, F1 Micro: 0.8936, F1 Macro: 0.8923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3668, Accuracy: 0.9196, F1 Micro: 0.9497, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2391, Accuracy: 0.9405, F1 Micro: 0.9628, F1 Macro: 0.96\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1802, Accuracy: 0.9591, F1 Micro: 0.9746, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.0975, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0633, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4942, Accuracy: 0.9031, F1 Micro: 0.9031, F1 Macro: 0.8931\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9569\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9406\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9435\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.97      0.94        86\n",
      "    positive       0.98      0.96      0.97       172\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9349\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.93      0.90      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.86724305152893 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9619, F1 Micro: 0.9619, F1 Macro: 0.9277\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 405.6309066415479\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 12.354401350021362 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5274, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4291, Accuracy: 0.9137, F1 Micro: 0.9473, F1 Macro: 0.9459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.289, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1889, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1096, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.979\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9791\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5285, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2948, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1746, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.944\n",
      "Epoch 4/10, Train Loss: 0.1802, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9323\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9477\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9358\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.944\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       169\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.95      0.95      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9302\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.0689070224762 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.529, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4416, Accuracy: 0.8921, F1 Micro: 0.9354, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3016, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1132, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.086, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9802\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.8943, F1 Micro: 0.8943, F1 Macro: 0.8828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2984, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1865, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9439\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9176\n",
      "Epoch 5/10, Train Loss: 0.144, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9323\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9234\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9251\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9266\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.8277554512024 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5343, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4702, Accuracy: 0.8542, F1 Micro: 0.9135, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3334, Accuracy: 0.942, F1 Micro: 0.9638, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2029, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4938, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2251, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1849, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9417\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1085, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0736, Accuracy: 0.9646, F1 Micro: 0.9646, F1 Macro: 0.9599\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "Epoch 8/10, Train Loss: 0.0756, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9335\n",
      "Epoch 9/10, Train Loss: 0.092, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9436\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9646, F1 Micro: 0.9646, F1 Macro: 0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.93      0.95        85\n",
      "    positive       0.97      0.98      0.97       169\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.96      0.96      0.96       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9308\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.90      0.94        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 124.32577562332153 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9635, F1 Micro: 0.9635, F1 Macro: 0.9292\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 387.6833703292049\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 11.231191635131836 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4192, Accuracy: 0.8936, F1 Micro: 0.9351, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2049, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1551, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0874, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9776\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9793\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5239, Accuracy: 0.8792, F1 Micro: 0.8792, F1 Macro: 0.8687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2323, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1783, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1186, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9364\n",
      "Epoch 5/10, Train Loss: 0.1275, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1221, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9455\n",
      "Epoch 8/10, Train Loss: 0.0884, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9411\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        86\n",
      "    positive       0.99      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.96      0.95       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9281\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.06892228126526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4314, Accuracy: 0.8951, F1 Micro: 0.9367, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2995, Accuracy: 0.9442, F1 Micro: 0.9652, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2106, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1564, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0767, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4848, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.8917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2382, Accuracy: 0.9386, F1 Micro: 0.9386, F1 Macro: 0.9298\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1336, Accuracy: 0.9458, F1 Micro: 0.9458, F1 Macro: 0.9385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9458, F1 Micro: 0.9458, F1 Macro: 0.9391\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.9314, F1 Micro: 0.9314, F1 Macro: 0.9202\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9268\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.9422, F1 Micro: 0.9422, F1 Macro: 0.9353\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.9386, F1 Micro: 0.9386, F1 Macro: 0.931\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9458, F1 Micro: 0.9458, F1 Macro: 0.9391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        88\n",
      "    positive       0.98      0.94      0.96       189\n",
      "\n",
      "    accuracy                           0.95       277\n",
      "   macro avg       0.93      0.95      0.94       277\n",
      "weighted avg       0.95      0.95      0.95       277\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9292\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.90      0.93       152\n",
      "    positive       0.74      0.81      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.88      0.84       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.57166147232056 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5452, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4619, Accuracy: 0.8356, F1 Micro: 0.9054, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3392, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2345, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1693, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0784, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5243, Accuracy: 0.8885, F1 Micro: 0.8885, F1 Macro: 0.8777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2457, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.129, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1372, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9198\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9207\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 10/10, Train Loss: 0.066, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9127\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        86\n",
      "    positive       0.97      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.94      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9235\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.21364378929138 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9269\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 377.19367477219646\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 9.96744179725647 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5326, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4216, Accuracy: 0.904, F1 Micro: 0.9401, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2772, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.141, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 10/10, Train Loss: 0.0538, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5028, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2189, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1422, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1441, Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1203, Accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9609\n",
      "Epoch 6/10, Train Loss: 0.1019, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9483\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9478\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.927\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9356\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        85\n",
      "    positive       0.99      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.96      0.97      0.96       256\n",
      "weighted avg       0.97      0.96      0.97       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9367\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.1515347957611 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5317, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4388, Accuracy: 0.9219, F1 Micro: 0.9519, F1 Macro: 0.95\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2883, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1954, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 6/10, Train Loss: 0.1077, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.971, F1 Micro: 0.9816, F1 Macro: 0.9804\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.95      0.94      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4716, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1948, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 6/10, Train Loss: 0.0679, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9335\n",
      "Epoch 9/10, Train Loss: 0.0357, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9236\n",
      "Epoch 10/10, Train Loss: 0.0266, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9217\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        87\n",
      "    positive       0.98      0.95      0.96       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.95      0.95       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9297\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.34920358657837 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.541, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4678, Accuracy: 0.8534, F1 Micro: 0.9141, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3181, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2173, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1147, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4984, Accuracy: 0.8962, F1 Micro: 0.8962, F1 Macro: 0.8785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2258, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1904, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 5/10, Train Loss: 0.1333, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        85\n",
      "    positive       0.98      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.94      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.72243809700012 s\n",
      "Averaged - Iteration 723: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9282\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 372.19582818760074\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 8.687522172927856 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.532, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4075, Accuracy: 0.9085, F1 Micro: 0.9433, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2651, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4855, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2743, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1833, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.139, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9268\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9297\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.83      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.909907579422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5347, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4227, Accuracy: 0.9182, F1 Micro: 0.9495, F1 Macro: 0.9475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2761, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.9625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1992, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "Epoch 6/10, Train Loss: 0.104, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9747, F1 Micro: 0.9841, F1 Macro: 0.9832\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9747, F1 Micro: 0.9841, F1 Macro: 0.9832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.94      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4811, Accuracy: 0.8821, F1 Micro: 0.8821, F1 Macro: 0.8757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2683, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9372\n",
      "Epoch 4/10, Train Loss: 0.1446, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9094\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Epoch 9/10, Train Loss: 0.0789, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9248\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9323\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.5874834060669 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5383, Accuracy: 0.7917, F1 Micro: 0.8832, F1 Macro: 0.8817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4468, Accuracy: 0.8795, F1 Micro: 0.9281, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3018, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5095, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.197, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9364\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 7/10, Train Loss: 0.0986, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9299\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Epoch 9/10, Train Loss: 0.0407, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "Epoch 10/10, Train Loss: 0.0396, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9234\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.82      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.22668194770813 s\n",
      "Averaged - Iteration 748: Accuracy: 0.964, F1 Micro: 0.964, F1 Macro: 0.9285\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 382.4258108886488\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 7.545981407165527 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5257, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3908, Accuracy: 0.9241, F1 Micro: 0.9528, F1 Macro: 0.9505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2641, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5101, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2436, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9017\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1581, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Epoch 6/10, Train Loss: 0.1077, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9273\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.048, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0566, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9451\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        87\n",
      "    positive       0.98      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.95      0.95       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9272\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.67440533638 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5266, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4117, Accuracy: 0.9249, F1 Micro: 0.9538, F1 Macro: 0.9518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.271, Accuracy: 0.9427, F1 Micro: 0.9641, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1929, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0603, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5024, Accuracy: 0.903, F1 Micro: 0.903, F1 Macro: 0.8924\n",
      "Epoch 2/10, Train Loss: 0.2472, Accuracy: 0.8918, F1 Micro: 0.8918, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1548, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1155, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9408\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9056\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9364\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       182\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.94      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9256\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.24032878875732 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7954, F1 Micro: 0.8851, F1 Macro: 0.8835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4451, Accuracy: 0.8705, F1 Micro: 0.9227, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2987, Accuracy: 0.942, F1 Micro: 0.9636, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2031, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9731\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1043, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "Epoch 7/10, Train Loss: 0.0825, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0708, Accuracy: 0.9717, F1 Micro: 0.9821, F1 Macro: 0.9808\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9821, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4535, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Epoch 2/10, Train Loss: 0.2612, Accuracy: 0.9033, F1 Micro: 0.9033, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1939, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.124, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Epoch 6/10, Train Loss: 0.1077, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 7/10, Train Loss: 0.1096, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0635, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9453\n",
      "Epoch 9/10, Train Loss: 0.1, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9416\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9376\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.96       182\n",
      "\n",
      "    accuracy                           0.95       269\n",
      "   macro avg       0.94      0.95      0.95       269\n",
      "weighted avg       0.95      0.95      0.95       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9332\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.88      0.88      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.46413612365723 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9619, F1 Micro: 0.9619, F1 Macro: 0.9286\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 396.7320972445561\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 7.217921733856201 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5387, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4021, Accuracy: 0.9204, F1 Micro: 0.9504, F1 Macro: 0.9486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2621, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.9606, F1 Micro: 0.9755, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.133, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9749\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9775\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4595, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9224\n",
      "Epoch 2/10, Train Loss: 0.2276, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1841, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9302\n",
      "Epoch 4/10, Train Loss: 0.1357, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1087, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9155\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.0841, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.95      0.95      0.95       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9281\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.50981879234314 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.418, Accuracy: 0.9278, F1 Micro: 0.9552, F1 Macro: 0.9534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2641, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 6/10, Train Loss: 0.1017, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9761\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4941, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1817, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1422, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.942\n",
      "Epoch 5/10, Train Loss: 0.1277, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1041, Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.946\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9184\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9184\n",
      "Epoch 10/10, Train Loss: 0.0697, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9354\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        87\n",
      "    positive       0.98      0.95      0.96       183\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.96      0.95       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9316\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.75      0.77      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.624573469162 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5459, Accuracy: 0.7954, F1 Micro: 0.8849, F1 Macro: 0.8832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4572, Accuracy: 0.878, F1 Micro: 0.9274, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.294, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1323, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9789\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4851, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2388, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.188, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1479, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 8/10, Train Loss: 0.0811, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9241\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        86\n",
      "    positive       0.96      0.96      0.96       173\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.94      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9218\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.93676137924194 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9271\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 357.63872101670734\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.722740888595581 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5253, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3798, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2522, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1748, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4854, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2527, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1674, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Epoch 4/10, Train Loss: 0.1484, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9327\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.9009\n",
      "Epoch 6/10, Train Loss: 0.1183, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9327\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9258\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.32806134223938 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5258, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4029, Accuracy: 0.9271, F1 Micro: 0.9547, F1 Macro: 0.9522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2574, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9823\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4688, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2515, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 3/10, Train Loss: 0.1603, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1782, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9359\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.86       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.91      0.81      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.95      0.87      0.91       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.22593092918396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5365, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4315, Accuracy: 0.907, F1 Micro: 0.9431, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2804, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.965, F1 Micro: 0.9782, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9827\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4701, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2496, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1641, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1278, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9446\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.8885, F1 Micro: 0.8885, F1 Macro: 0.882\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        87\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9352\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.11040830612183 s\n",
      "Averaged - Iteration 806: Accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9323\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 228.75344262866778\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.305154085159302 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5214, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3845, Accuracy: 0.9271, F1 Micro: 0.9546, F1 Macro: 0.9524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2497, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1736, Accuracy: 0.965, F1 Micro: 0.9782, F1 Macro: 0.9773\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.90      0.98      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4515, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1994, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1344, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "Epoch 4/10, Train Loss: 0.1321, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9344\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9105\n",
      "Epoch 6/10, Train Loss: 0.0774, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9238\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9123\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        85\n",
      "    positive       0.98      0.94      0.96       170\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9276\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.95      0.71      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.74899101257324 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5256, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4095, Accuracy: 0.9204, F1 Micro: 0.9503, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2606, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1743, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 5/10, Train Loss: 0.1249, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4928, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 3/10, Train Loss: 0.1967, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9095\n",
      "Epoch 4/10, Train Loss: 0.1423, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9108\n",
      "Epoch 5/10, Train Loss: 0.1631, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9299\n",
      "Epoch 7/10, Train Loss: 0.1051, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0676, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9335\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        88\n",
      "    positive       0.96      0.95      0.95       178\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.93      0.93       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9244\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.97213554382324 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5319, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4459, Accuracy: 0.907, F1 Micro: 0.9432, F1 Macro: 0.9415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2862, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1874, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9754\n",
      "Epoch 5/10, Train Loss: 0.1278, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1021, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9725, F1 Micro: 0.9826, F1 Macro: 0.9814\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9801\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9826, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.95      0.94      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4929, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.9487, F1 Micro: 0.9487, F1 Macro: 0.9417\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9373\n",
      "Epoch 4/10, Train Loss: 0.1345, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9387\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1318, Accuracy: 0.9487, F1 Micro: 0.9487, F1 Macro: 0.9423\n",
      "Epoch 7/10, Train Loss: 0.0931, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9154\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9387\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9045\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9487, F1 Micro: 0.9487, F1 Macro: 0.9423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        88\n",
      "    positive       0.98      0.95      0.96       185\n",
      "\n",
      "    accuracy                           0.95       273\n",
      "   macro avg       0.94      0.95      0.94       273\n",
      "weighted avg       0.95      0.95      0.95       273\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9361\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.96      0.94      0.95       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.5708556175232 s\n",
      "Averaged - Iteration 831: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9294\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 164.26221357984642\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.3475799560546875 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5212, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3745, Accuracy: 0.9323, F1 Micro: 0.9579, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2425, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1599, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9815\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9809\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.426, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2083, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1574, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1173, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1381, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9258\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9088\n",
      "Epoch 8/10, Train Loss: 0.055, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9327\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.6494152545929 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5212, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3989, Accuracy: 0.9278, F1 Micro: 0.9553, F1 Macro: 0.9534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2469, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1593, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4611, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2132, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1564, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1268, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.0875, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.8821, F1 Micro: 0.8821, F1 Macro: 0.8613\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9331\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9244\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9269\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.95       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.0638666152954 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5237, Accuracy: 0.7961, F1 Micro: 0.8855, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4304, Accuracy: 0.9115, F1 Micro: 0.9459, F1 Macro: 0.9444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.266, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4346, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "Epoch 2/10, Train Loss: 0.2617, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1499, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Epoch 4/10, Train Loss: 0.1231, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1011, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1186, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9093\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9064\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.92      0.90        86\n",
      "    positive       0.96      0.94      0.95       179\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.92      0.93      0.93       265\n",
      "weighted avg       0.94      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9169\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.95       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.9955234527588 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9619, F1 Micro: 0.9619, F1 Macro: 0.9255\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 40.337334096099404\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.4191131591796875 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5167, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3672, Accuracy: 0.9301, F1 Micro: 0.9563, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2362, Accuracy: 0.9539, F1 Micro: 0.9715, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4522, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1883, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1233, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9369\n",
      "Epoch 5/10, Train Loss: 0.1103, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9189\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9361\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9262\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9341\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8961\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.90      0.91        87\n",
      "    positive       0.95      0.97      0.96       186\n",
      "\n",
      "    accuracy                           0.95       273\n",
      "   macro avg       0.94      0.93      0.94       273\n",
      "weighted avg       0.94      0.95      0.94       273\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9183\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.75      0.87      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.6343195438385 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5204, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3907, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2409, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1664, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0635, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4977, Accuracy: 0.8843, F1 Micro: 0.8843, F1 Macro: 0.8761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2249, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1358, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9075\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0988, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9203\n",
      "Epoch 10/10, Train Loss: 0.0675, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9138\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.97      0.90        86\n",
      "    positive       0.98      0.92      0.95       182\n",
      "\n",
      "    accuracy                           0.93       268\n",
      "   macro avg       0.91      0.94      0.93       268\n",
      "weighted avg       0.94      0.93      0.93       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9205\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.82      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.95565962791443 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5233, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4253, Accuracy: 0.8951, F1 Micro: 0.9351, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2678, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1305, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9782\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4418, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9514\n",
      "Epoch 3/10, Train Loss: 0.1549, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.0967, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9514\n",
      "Epoch 5/10, Train Loss: 0.1047, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9511\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "Epoch 8/10, Train Loss: 0.0484, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9278\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9477\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.94      0.93        84\n",
      "    positive       0.97      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.95      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9264\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.76745772361755 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9236\n",
      "Total runtime: 9977.750111818314 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9814e",
   "metadata": {
    "papermill": {
     "duration": 0.012027,
     "end_time": "2025-06-08T19:09:42.460772",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.448745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de448d649c2d43cbb70872a4900b987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6796, Accuracy: 0.776, F1 Micro: 0.8715, F1 Macro: 0.8667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5844, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5397, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4843, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4798, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4852, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4178, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Epoch 10/10, Train Loss: 0.4018, Accuracy: 0.7932, F1 Micro: 0.8837, F1 Macro: 0.8819\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6006, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.431, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3108, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2303, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1708, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.38      0.50      0.43         4\n",
      "weighted avg       0.56      0.75      0.64         4\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3012\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.67      0.05      0.09        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.46      0.35      0.31       216\n",
      "weighted avg       0.63      0.71      0.60       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 66.59182786941528 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7078, Accuracy: 0.7024, F1 Micro: 0.8155, F1 Macro: 0.7878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6021, Accuracy: 0.7872, F1 Micro: 0.8807, F1 Macro: 0.8791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5268, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4897, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Epoch 6/10, Train Loss: 0.4716, Accuracy: 0.7879, F1 Micro: 0.8805, F1 Macro: 0.8784\n",
      "Epoch 7/10, Train Loss: 0.4659, Accuracy: 0.7827, F1 Micro: 0.8767, F1 Macro: 0.8734\n",
      "Epoch 8/10, Train Loss: 0.4462, Accuracy: 0.7827, F1 Micro: 0.8759, F1 Macro: 0.872\n",
      "Epoch 9/10, Train Loss: 0.4178, Accuracy: 0.7812, F1 Micro: 0.8752, F1 Macro: 0.8715\n",
      "Epoch 10/10, Train Loss: 0.3894, Accuracy: 0.7812, F1 Micro: 0.8751, F1 Macro: 0.8716\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.72      0.99      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.814, Accuracy: 0.0, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6518, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5041, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3836, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2981, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2348, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1847, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1552, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1217, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1058, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         6\n",
      "   macro avg       0.50      0.50      0.50         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7924, F1 Micro: 0.7924, F1 Macro: 0.3036\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.83      0.10      0.17        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.52      0.36      0.34       216\n",
      "weighted avg       0.71      0.72      0.63       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 58.21936559677124 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6904, Accuracy: 0.7835, F1 Micro: 0.8752, F1 Macro: 0.8707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5708, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5489, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5207, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4748, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4754, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.48, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4383, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4139, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3882, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8826\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.98      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4833, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3188, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1606, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1135, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2591, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2411, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1873, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0508, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         5\n",
      "    positive       0.62      1.00      0.76         8\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.31      0.50      0.38        13\n",
      "weighted avg       0.38      0.62      0.47        13\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.3068\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.29      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.36      0.35      0.33       216\n",
      "weighted avg       0.65      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.40      0.05      0.09        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.37      0.35      0.31       216\n",
      "weighted avg       0.58      0.71      0.60       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 71.28321242332458 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3039\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 583.5546446409595\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 24.032009601593018 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6214, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5218, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4895, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4693, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4336, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4069, Accuracy: 0.8065, F1 Micro: 0.8904, F1 Macro: 0.8888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.39, Accuracy: 0.843, F1 Micro: 0.9085, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.32, Accuracy: 0.8661, F1 Micro: 0.9204, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2841, Accuracy: 0.875, F1 Micro: 0.9243, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2296, Accuracy: 0.8862, F1 Micro: 0.9304, F1 Macro: 0.9277\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8862, F1 Micro: 0.9304, F1 Macro: 0.9277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.95      0.99      0.97       187\n",
      "     machine       0.91      0.95      0.93       175\n",
      "      others       0.84      0.85      0.84       158\n",
      "        part       0.88      0.97      0.92       158\n",
      "       price       0.87      1.00      0.93       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.90      0.96      0.93      1061\n",
      "   macro avg       0.90      0.96      0.93      1061\n",
      "weighted avg       0.90      0.96      0.93      1061\n",
      " samples avg       0.90      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5611, Accuracy: 0.7121, F1 Micro: 0.7121, F1 Macro: 0.4159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5025, Accuracy: 0.798, F1 Micro: 0.798, F1 Macro: 0.725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3358, Accuracy: 0.8283, F1 Micro: 0.8283, F1 Macro: 0.786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2228, Accuracy: 0.8384, F1 Micro: 0.8384, F1 Macro: 0.8168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2024, Accuracy: 0.8485, F1 Micro: 0.8485, F1 Macro: 0.8239\n",
      "Epoch 6/10, Train Loss: 0.2157, Accuracy: 0.8232, F1 Micro: 0.8232, F1 Macro: 0.7759\n",
      "Epoch 7/10, Train Loss: 0.1617, Accuracy: 0.8283, F1 Micro: 0.8283, F1 Macro: 0.8145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1697, Accuracy: 0.8485, F1 Micro: 0.8485, F1 Macro: 0.8308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.8636, F1 Micro: 0.8636, F1 Macro: 0.8422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0959, Accuracy: 0.8788, F1 Micro: 0.8788, F1 Macro: 0.8615\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.8788, F1 Micro: 0.8788, F1 Macro: 0.8615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.91      0.81        57\n",
      "    positive       0.96      0.87      0.91       141\n",
      "\n",
      "    accuracy                           0.88       198\n",
      "   macro avg       0.85      0.89      0.86       198\n",
      "weighted avg       0.89      0.88      0.88       198\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8789, F1 Micro: 0.8789, F1 Macro: 0.714\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.95      0.99      0.97       181\n",
      "    positive       0.89      0.71      0.79        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.69      0.69        16\n",
      "     neutral       0.90      0.96      0.93       167\n",
      "    positive       0.78      0.55      0.64        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.73      0.75       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.58      0.45        12\n",
      "     neutral       0.84      0.85      0.84       152\n",
      "    positive       0.67      0.56      0.61        52\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.63      0.66      0.64       216\n",
      "weighted avg       0.77      0.76      0.77       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.65      0.75        23\n",
      "     neutral       0.88      0.97      0.92       152\n",
      "    positive       0.83      0.61      0.70        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.86      0.75      0.79       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.87      1.00      0.93       186\n",
      "    positive       1.00      0.18      0.30        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.62      0.39      0.41       216\n",
      "weighted avg       0.83      0.88      0.83       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.89      0.47      0.62        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.73      0.81       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Total train time: 82.81615281105042 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6316, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5294, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4938, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4696, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4299, Accuracy: 0.7984, F1 Micro: 0.8863, F1 Macro: 0.8846\n",
      "Epoch 6/10, Train Loss: 0.4047, Accuracy: 0.7999, F1 Micro: 0.8854, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3742, Accuracy: 0.8281, F1 Micro: 0.9011, F1 Macro: 0.8994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3262, Accuracy: 0.8579, F1 Micro: 0.9148, F1 Macro: 0.9122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.289, Accuracy: 0.875, F1 Micro: 0.9237, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2297, Accuracy: 0.8899, F1 Micro: 0.932, F1 Macro: 0.9283\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8899, F1 Micro: 0.932, F1 Macro: 0.9283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.90      0.94      0.92       175\n",
      "      others       0.87      0.82      0.84       158\n",
      "        part       0.85      0.97      0.91       158\n",
      "       price       0.90      0.99      0.95       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.91      0.96      0.93      1061\n",
      "   macro avg       0.91      0.95      0.93      1061\n",
      "weighted avg       0.91      0.96      0.93      1061\n",
      " samples avg       0.91      0.96      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6527, Accuracy: 0.7281, F1 Micro: 0.7281, F1 Macro: 0.4213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6044, Accuracy: 0.7281, F1 Micro: 0.7281, F1 Macro: 0.4213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5028, Accuracy: 0.8249, F1 Micro: 0.8249, F1 Macro: 0.7686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3224, Accuracy: 0.8479, F1 Micro: 0.8479, F1 Macro: 0.8026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2339, Accuracy: 0.8756, F1 Micro: 0.8756, F1 Macro: 0.8483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1824, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1886, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1205, Accuracy: 0.8986, F1 Micro: 0.8986, F1 Macro: 0.877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1453, Accuracy: 0.9078, F1 Micro: 0.9078, F1 Macro: 0.8882\n",
      "Epoch 10/10, Train Loss: 0.1437, Accuracy: 0.894, F1 Micro: 0.894, F1 Macro: 0.8682\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9078, F1 Micro: 0.9078, F1 Macro: 0.8882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.90      0.84        59\n",
      "    positive       0.96      0.91      0.94       158\n",
      "\n",
      "    accuracy                           0.91       217\n",
      "   macro avg       0.88      0.90      0.89       217\n",
      "weighted avg       0.91      0.91      0.91       217\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.7487\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.89      0.94      0.92       167\n",
      "    positive       0.64      0.48      0.55        33\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.76      0.70      0.73       216\n",
      "weighted avg       0.84      0.85      0.84       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.67      0.57        12\n",
      "     neutral       0.87      0.82      0.84       152\n",
      "    positive       0.62      0.67      0.65        52\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.66      0.72      0.69       216\n",
      "weighted avg       0.79      0.78      0.78       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.57      0.68        23\n",
      "     neutral       0.85      0.98      0.91       152\n",
      "    positive       0.84      0.51      0.64        41\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.85      0.69      0.74       216\n",
      "weighted avg       0.85      0.85      0.83       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.08      0.14        13\n",
      "     neutral       0.90      0.99      0.94       186\n",
      "    positive       0.73      0.47      0.57        17\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.51      0.55       216\n",
      "weighted avg       0.89      0.89      0.87       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.75      0.82       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Total train time: 79.76303720474243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6074, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.52, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.486, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4684, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4236, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3958, Accuracy: 0.8088, F1 Micro: 0.8918, F1 Macro: 0.8903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3703, Accuracy: 0.8423, F1 Micro: 0.9089, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3195, Accuracy: 0.8787, F1 Micro: 0.9272, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.283, Accuracy: 0.8943, F1 Micro: 0.9359, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2294, Accuracy: 0.904, F1 Micro: 0.9408, F1 Macro: 0.9376\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.904, F1 Micro: 0.9408, F1 Macro: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      1.00      0.98       187\n",
      "     machine       0.92      0.97      0.94       175\n",
      "      others       0.89      0.84      0.86       158\n",
      "        part       0.87      0.97      0.92       158\n",
      "       price       0.90      1.00      0.95       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.94      1061\n",
      "   macro avg       0.92      0.96      0.94      1061\n",
      "weighted avg       0.92      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6225, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4693, Accuracy: 0.8056, F1 Micro: 0.8056, F1 Macro: 0.7305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3781, Accuracy: 0.8287, F1 Micro: 0.8287, F1 Macro: 0.7876\n",
      "Epoch 4/10, Train Loss: 0.2942, Accuracy: 0.8194, F1 Micro: 0.8194, F1 Macro: 0.7947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.175, Accuracy: 0.8287, F1 Micro: 0.8287, F1 Macro: 0.7973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1508, Accuracy: 0.8333, F1 Micro: 0.8333, F1 Macro: 0.8036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1583, Accuracy: 0.838, F1 Micro: 0.838, F1 Macro: 0.8183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.121, Accuracy: 0.8519, F1 Micro: 0.8519, F1 Macro: 0.8172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1515, Accuracy: 0.8611, F1 Micro: 0.8611, F1 Macro: 0.8377\n",
      "Epoch 10/10, Train Loss: 0.0833, Accuracy: 0.8472, F1 Micro: 0.8472, F1 Macro: 0.825\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8611, F1 Micro: 0.8611, F1 Macro: 0.8377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.83      0.78        63\n",
      "    positive       0.92      0.88      0.90       153\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.87      0.86      0.86       216\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.892, F1 Micro: 0.892, F1 Macro: 0.7518\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.96      1.00      0.98       181\n",
      "    positive       0.95      0.75      0.84        24\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.97      0.86      0.91       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.92      0.97      0.94       167\n",
      "    positive       0.80      0.61      0.69        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.75      0.79       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.67      0.48        12\n",
      "     neutral       0.89      0.85      0.87       152\n",
      "    positive       0.70      0.67      0.69        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.66      0.73      0.68       216\n",
      "weighted avg       0.82      0.80      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.52      0.63        23\n",
      "     neutral       0.87      0.97      0.92       152\n",
      "    positive       0.75      0.59      0.66        41\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.81      0.69      0.74       216\n",
      "weighted avg       0.84      0.85      0.84       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.08      0.14        13\n",
      "     neutral       0.89      1.00      0.94       186\n",
      "    positive       0.86      0.35      0.50        17\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.92      0.48      0.53       216\n",
      "weighted avg       0.90      0.89      0.86       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.81      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 80.03162550926208 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8845, F1 Micro: 0.8845, F1 Macro: 0.7382\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 572.4128186307058\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 30.99651861190796 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5046, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4994, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4563, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3947, Accuracy: 0.8408, F1 Micro: 0.9075, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3456, Accuracy: 0.8698, F1 Micro: 0.9221, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2721, Accuracy: 0.8876, F1 Micro: 0.9311, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2295, Accuracy: 0.9174, F1 Micro: 0.9487, F1 Macro: 0.9458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1935, Accuracy: 0.9204, F1 Micro: 0.9503, F1 Macro: 0.9471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1613, Accuracy: 0.933, F1 Micro: 0.9584, F1 Macro: 0.9564\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.933, F1 Micro: 0.9584, F1 Macro: 0.9564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.92      0.97      0.95       175\n",
      "      others       0.87      0.92      0.90       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.97      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5871, Accuracy: 0.6795, F1 Micro: 0.6795, F1 Macro: 0.4171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4644, Accuracy: 0.859, F1 Micro: 0.859, F1 Macro: 0.8375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2707, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.8889, F1 Micro: 0.8889, F1 Macro: 0.8805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8989\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9017, F1 Micro: 0.9017, F1 Macro: 0.8917\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8773\n",
      "Epoch 8/10, Train Loss: 0.0827, Accuracy: 0.8932, F1 Micro: 0.8932, F1 Macro: 0.8848\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.8932, F1 Micro: 0.8932, F1 Macro: 0.8854\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9017, F1 Micro: 0.9017, F1 Macro: 0.8935\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.99      0.87        76\n",
      "    positive       0.99      0.87      0.93       158\n",
      "\n",
      "    accuracy                           0.91       234\n",
      "   macro avg       0.89      0.93      0.90       234\n",
      "weighted avg       0.92      0.91      0.91       234\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.8523\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.79      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.90      0.93       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.92      0.97      0.94       167\n",
      "    positive       0.81      0.64      0.71        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.81      0.84       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.75      0.56        12\n",
      "     neutral       0.88      0.92      0.90       152\n",
      "    positive       0.81      0.56      0.66        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.71      0.74      0.71       216\n",
      "weighted avg       0.83      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.91      0.84        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.93      0.66      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 89.24261021614075 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5996, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5125, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5089, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4601, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3997, Accuracy: 0.8311, F1 Micro: 0.9025, F1 Macro: 0.9006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3586, Accuracy: 0.8854, F1 Micro: 0.9306, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2882, Accuracy: 0.9033, F1 Micro: 0.9404, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2464, Accuracy: 0.9107, F1 Micro: 0.9443, F1 Macro: 0.9409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2111, Accuracy: 0.9167, F1 Micro: 0.9478, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1701, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9544\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.94      0.93       175\n",
      "      others       0.88      0.94      0.91       158\n",
      "        part       0.92      0.96      0.94       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.96      1061\n",
      "   macro avg       0.94      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.96      1061\n",
      " samples avg       0.94      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6698, Accuracy: 0.682, F1 Micro: 0.682, F1 Macro: 0.4055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5407, Accuracy: 0.8117, F1 Micro: 0.8117, F1 Macro: 0.7611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.403, Accuracy: 0.8577, F1 Micro: 0.8577, F1 Macro: 0.8478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.217, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9034\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8985\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8979\n",
      "Epoch 8/10, Train Loss: 0.077, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8741\n",
      "Epoch 9/10, Train Loss: 0.0871, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8929\n",
      "Epoch 10/10, Train Loss: 0.1001, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8936\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.96      0.87        76\n",
      "    positive       0.98      0.89      0.93       163\n",
      "\n",
      "    accuracy                           0.91       239\n",
      "   macro avg       0.89      0.93      0.90       239\n",
      "weighted avg       0.92      0.91      0.91       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.8519\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.92      0.93      0.93       167\n",
      "    positive       0.71      0.67      0.69        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.78      0.79       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.88      0.95      0.91       152\n",
      "    positive       0.83      0.58      0.68        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.78      0.79      0.77       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.87      0.82        23\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.63      0.74        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.83      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 87.35475254058838 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.589, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5041, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4936, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4328, Accuracy: 0.8058, F1 Micro: 0.8903, F1 Macro: 0.8888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3764, Accuracy: 0.8549, F1 Micro: 0.9155, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3328, Accuracy: 0.9003, F1 Micro: 0.9392, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2653, Accuracy: 0.9055, F1 Micro: 0.9417, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2199, Accuracy: 0.9278, F1 Micro: 0.955, F1 Macro: 0.9526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1893, Accuracy: 0.9323, F1 Micro: 0.9577, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1587, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9592\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.95       175\n",
      "      others       0.87      0.92      0.90       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6413, Accuracy: 0.6754, F1 Micro: 0.6754, F1 Macro: 0.4031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4548, Accuracy: 0.8728, F1 Micro: 0.8728, F1 Macro: 0.8583\n",
      "Epoch 3/10, Train Loss: 0.3082, Accuracy: 0.7895, F1 Micro: 0.7895, F1 Macro: 0.7847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2151, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9123\n",
      "Epoch 5/10, Train Loss: 0.1187, Accuracy: 0.9123, F1 Micro: 0.9123, F1 Macro: 0.9037\n",
      "Epoch 6/10, Train Loss: 0.1321, Accuracy: 0.9123, F1 Micro: 0.9123, F1 Macro: 0.9043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1579, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9133\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.9008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.923\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.97      0.90        74\n",
      "    positive       0.99      0.91      0.95       154\n",
      "\n",
      "    accuracy                           0.93       228\n",
      "   macro avg       0.91      0.94      0.92       228\n",
      "weighted avg       0.94      0.93      0.93       228\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.8672\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.80      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.88      0.93      0.90       152\n",
      "    positive       0.81      0.65      0.72        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.79      0.78      0.78       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.83      0.79        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       1.00      0.63      0.78        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.82      0.84       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 89.07025623321533 s\n",
      "Averaged - Iteration 208: Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.8571\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 325.7199445070351\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 29.664657831192017 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5812, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4968, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4592, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4054, Accuracy: 0.8512, F1 Micro: 0.9134, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3392, Accuracy: 0.8876, F1 Micro: 0.9325, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.265, Accuracy: 0.9226, F1 Micro: 0.9521, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2229, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9612\n",
      "Epoch 8/10, Train Loss: 0.1751, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.96\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.142, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1192, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9654\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.91      0.97      0.94       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6048, Accuracy: 0.7983, F1 Micro: 0.7983, F1 Macro: 0.7111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3868, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9214\n",
      "Epoch 3/10, Train Loss: 0.1882, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9127\n",
      "Epoch 4/10, Train Loss: 0.206, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9229\n",
      "Epoch 6/10, Train Loss: 0.1749, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9048\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9143\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9138\n",
      "Epoch 9/10, Train Loss: 0.0878, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9184\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9048\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.93      0.90        75\n",
      "    positive       0.97      0.93      0.95       158\n",
      "\n",
      "    accuracy                           0.93       233\n",
      "   macro avg       0.92      0.93      0.92       233\n",
      "weighted avg       0.93      0.93      0.93       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.8736\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.81      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 91.20143008232117 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5924, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4996, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4734, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4285, Accuracy: 0.817, F1 Micro: 0.8952, F1 Macro: 0.8936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3615, Accuracy: 0.9003, F1 Micro: 0.94, F1 Macro: 0.9381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2881, Accuracy: 0.9263, F1 Micro: 0.9547, F1 Macro: 0.9527\n",
      "Epoch 7/10, Train Loss: 0.2408, Accuracy: 0.9249, F1 Micro: 0.9532, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1885, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1541, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1288, Accuracy: 0.9435, F1 Micro: 0.9649, F1 Macro: 0.9633\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9649, F1 Macro: 0.9633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.90      1.00      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.645, Accuracy: 0.671, F1 Micro: 0.671, F1 Macro: 0.4139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5236, Accuracy: 0.8701, F1 Micro: 0.8701, F1 Macro: 0.8509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2677, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9099\n",
      "Epoch 4/10, Train Loss: 0.1579, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1288, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9092\n",
      "Epoch 7/10, Train Loss: 0.1504, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8976\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9065\n",
      "Epoch 9/10, Train Loss: 0.1071, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9094\n",
      "Epoch 10/10, Train Loss: 0.0675, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.902\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9092\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.82      0.88        77\n",
      "    positive       0.91      0.97      0.94       154\n",
      "\n",
      "    accuracy                           0.92       231\n",
      "   macro avg       0.93      0.90      0.91       231\n",
      "weighted avg       0.92      0.92      0.92       231\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.852\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.97      0.95       167\n",
      "    positive       0.76      0.67      0.71        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.77      0.81       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.74      0.85        23\n",
      "     neutral       0.89      1.00      0.94       152\n",
      "    positive       0.93      0.66      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.94      0.80      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.81      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 97.35512232780457 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5769, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4969, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4521, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3958, Accuracy: 0.8527, F1 Micro: 0.9137, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3312, Accuracy: 0.9115, F1 Micro: 0.946, F1 Macro: 0.944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2571, Accuracy: 0.9256, F1 Micro: 0.9535, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.222, Accuracy: 0.9368, F1 Micro: 0.9608, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.175, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1427, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9651\n",
      "Epoch 10/10, Train Loss: 0.1172, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9647\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.89      0.96      0.92       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6475, Accuracy: 0.6652, F1 Micro: 0.6652, F1 Macro: 0.3995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5226, Accuracy: 0.8609, F1 Micro: 0.8609, F1 Macro: 0.837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2826, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9017\n",
      "Epoch 4/10, Train Loss: 0.1835, Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.8979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.172, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9052\n",
      "Epoch 6/10, Train Loss: 0.104, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8972\n",
      "Epoch 7/10, Train Loss: 0.0627, Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.8974\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8978\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.8969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0743, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.903\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.88      0.87        77\n",
      "    positive       0.94      0.93      0.93       153\n",
      "\n",
      "    accuracy                           0.91       230\n",
      "   macro avg       0.90      0.91      0.90       230\n",
      "weighted avg       0.91      0.91      0.91       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.8625\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71        16\n",
      "     neutral       0.92      0.99      0.95       167\n",
      "    positive       0.84      0.64      0.72        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.75      0.80       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.90      0.67      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.80      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.88      0.71      0.78        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 91.66650199890137 s\n",
      "Averaged - Iteration 274: Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.8627\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 358.6858395078404\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 26.33828830718994 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5651, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.475, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4369, Accuracy: 0.8088, F1 Micro: 0.8918, F1 Macro: 0.8903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3731, Accuracy: 0.875, F1 Micro: 0.9246, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.303, Accuracy: 0.9085, F1 Micro: 0.9441, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2381, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1753, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9694\n",
      "Epoch 8/10, Train Loss: 0.1349, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1112, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0994, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9703\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5945, Accuracy: 0.8, F1 Micro: 0.8, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3646, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9206\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.8936, F1 Micro: 0.8936, F1 Macro: 0.8862\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9131\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9142\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1206, Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9237\n",
      "Epoch 10/10, Train Loss: 0.0878, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.897\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        76\n",
      "    positive       0.97      0.93      0.95       159\n",
      "\n",
      "    accuracy                           0.93       235\n",
      "   macro avg       0.92      0.93      0.92       235\n",
      "weighted avg       0.93      0.93      0.93       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8877\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.80      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.96      0.93       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.81      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.86      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 103.29522705078125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5691, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4798, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4479, Accuracy: 0.8013, F1 Micro: 0.8877, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3907, Accuracy: 0.8847, F1 Micro: 0.9312, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3119, Accuracy: 0.9167, F1 Micro: 0.9483, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2462, Accuracy: 0.9353, F1 Micro: 0.9595, F1 Macro: 0.9571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1904, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9678\n",
      "Epoch 8/10, Train Loss: 0.1433, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.963\n",
      "Epoch 9/10, Train Loss: 0.1204, Accuracy: 0.9442, F1 Micro: 0.965, F1 Macro: 0.9627\n",
      "Epoch 10/10, Train Loss: 0.1038, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9633\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6287, Accuracy: 0.7983, F1 Micro: 0.7983, F1 Macro: 0.7404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4031, Accuracy: 0.9056, F1 Micro: 0.9056, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2244, Accuracy: 0.9399, F1 Micro: 0.9399, F1 Macro: 0.9325\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.8866\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.897, F1 Micro: 0.897, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9399, F1 Micro: 0.9399, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0952, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.86\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.927\n",
      "Epoch 9/10, Train Loss: 0.0672, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9167\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9198\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9399, F1 Micro: 0.9399, F1 Macro: 0.9317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.91        78\n",
      "    positive       0.94      0.97      0.96       155\n",
      "\n",
      "    accuracy                           0.94       233\n",
      "   macro avg       0.94      0.93      0.93       233\n",
      "weighted avg       0.94      0.94      0.94       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8807\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.82      0.70      0.75        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.81      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.94      0.82      0.87       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.69      0.75        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 97.2238256931305 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5562, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.476, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4248, Accuracy: 0.84, F1 Micro: 0.9078, F1 Macro: 0.9065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.357, Accuracy: 0.901, F1 Micro: 0.9394, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2858, Accuracy: 0.9256, F1 Micro: 0.954, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2215, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1674, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9694\n",
      "Epoch 8/10, Train Loss: 0.1351, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9617\n",
      "Epoch 9/10, Train Loss: 0.1106, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0971, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.93      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5989, Accuracy: 0.8049, F1 Micro: 0.8049, F1 Macro: 0.7916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4027, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.23, Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.9177\n",
      "Epoch 4/10, Train Loss: 0.2197, Accuracy: 0.8862, F1 Micro: 0.8862, F1 Macro: 0.8768\n",
      "Epoch 5/10, Train Loss: 0.1337, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9128\n",
      "Epoch 6/10, Train Loss: 0.073, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9006\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9024, F1 Micro: 0.9024, F1 Macro: 0.8933\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9024, F1 Micro: 0.9024, F1 Macro: 0.8915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0885, Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9259\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        77\n",
      "    positive       0.97      0.93      0.95       169\n",
      "\n",
      "    accuracy                           0.93       246\n",
      "   macro avg       0.92      0.93      0.93       246\n",
      "weighted avg       0.94      0.93      0.94       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8895\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 100.61614608764648 s\n",
      "Averaged - Iteration 333: Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.8859\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 350.935479785056\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 25.786097526550293 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5581, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4689, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4214, Accuracy: 0.8534, F1 Micro: 0.9137, F1 Macro: 0.912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3319, Accuracy: 0.9129, F1 Micro: 0.9467, F1 Macro: 0.9452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2425, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1883, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1463, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.097, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9768\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5581, Accuracy: 0.8926, F1 Micro: 0.8926, F1 Macro: 0.8694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3577, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9432\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9218\n",
      "Epoch 4/10, Train Loss: 0.1222, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9358\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9266\n",
      "Epoch 6/10, Train Loss: 0.0819, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9253\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9209\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9266\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9302\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9338\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        78\n",
      "    positive       0.96      0.96      0.96       164\n",
      "\n",
      "    accuracy                           0.95       242\n",
      "   macro avg       0.94      0.94      0.94       242\n",
      "weighted avg       0.95      0.95      0.95       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9102\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.86      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 106.3420181274414 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4758, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4365, Accuracy: 0.8393, F1 Micro: 0.9071, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3566, Accuracy: 0.9196, F1 Micro: 0.9508, F1 Macro: 0.949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2578, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1984, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1562, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.121, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1023, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0834, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6014, Accuracy: 0.8327, F1 Micro: 0.8327, F1 Macro: 0.7798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4066, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "Epoch 3/10, Train Loss: 0.2279, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.9071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9402\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9258\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9358\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9592, F1 Micro: 0.9592, F1 Macro: 0.9542\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9402\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9395\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9592, F1 Micro: 0.9592, F1 Macro: 0.9542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        80\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.96       245\n",
      "   macro avg       0.95      0.96      0.95       245\n",
      "weighted avg       0.96      0.96      0.96       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9114\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 114.30852746963501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.556, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4633, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4104, Accuracy: 0.8661, F1 Micro: 0.9207, F1 Macro: 0.9189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.323, Accuracy: 0.9234, F1 Micro: 0.9523, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.236, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1814, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1453, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1116, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0976, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 10/10, Train Loss: 0.0784, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5476, Accuracy: 0.9065, F1 Micro: 0.9065, F1 Macro: 0.8924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2966, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9187, F1 Micro: 0.9187, F1 Macro: 0.912\n",
      "Epoch 4/10, Train Loss: 0.1167, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9328\n",
      "Epoch 5/10, Train Loss: 0.1026, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9367\n",
      "Epoch 6/10, Train Loss: 0.0766, Accuracy: 0.8821, F1 Micro: 0.8821, F1 Macro: 0.8749\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9273\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9254\n",
      "Epoch 9/10, Train Loss: 0.0832, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9032\n",
      "Epoch 10/10, Train Loss: 0.0938, Accuracy: 0.9187, F1 Micro: 0.9187, F1 Macro: 0.9115\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        80\n",
      "    positive       0.99      0.94      0.96       166\n",
      "\n",
      "    accuracy                           0.95       246\n",
      "   macro avg       0.94      0.96      0.95       246\n",
      "weighted avg       0.95      0.95      0.95       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9108\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 108.69923639297485 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9108\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 379.42869808064233\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 23.85597848892212 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5461, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4849, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4109, Accuracy: 0.8601, F1 Micro: 0.9167, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.312, Accuracy: 0.9301, F1 Micro: 0.9568, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2424, Accuracy: 0.9338, F1 Micro: 0.9584, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1854, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1416, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1119, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0939, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3187, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.229, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1065, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9374\n",
      "Epoch 7/10, Train Loss: 0.0597, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9288\n",
      "Epoch 8/10, Train Loss: 0.1227, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9457\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9113, F1 Micro: 0.9113, F1 Macro: 0.9041\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        82\n",
      "    positive       0.97      0.96      0.96       166\n",
      "\n",
      "    accuracy                           0.95       248\n",
      "   macro avg       0.94      0.95      0.95       248\n",
      "weighted avg       0.95      0.95      0.95       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9117\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 118.09336400032043 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5607, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4991, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4276, Accuracy: 0.8571, F1 Micro: 0.9162, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3307, Accuracy: 0.9271, F1 Micro: 0.9547, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2569, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1951, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1498, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1169, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9736\n",
      "Epoch 9/10, Train Loss: 0.0984, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0804, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.617, Accuracy: 0.6706, F1 Micro: 0.6706, F1 Macro: 0.4127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4092, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2621, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9464\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9222\n",
      "Epoch 10/10, Train Loss: 0.0749, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9305\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        84\n",
      "    positive       0.96      0.96      0.96       168\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.95      0.95      0.95       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9129\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.90      0.90       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 117.18924617767334 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4852, Accuracy: 0.8006, F1 Micro: 0.8876, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4078, Accuracy: 0.8847, F1 Micro: 0.9295, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3067, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2374, Accuracy: 0.9442, F1 Micro: 0.965, F1 Macro: 0.9633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1835, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1403, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Epoch 8/10, Train Loss: 0.1127, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0806, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5585, Accuracy: 0.8112, F1 Micro: 0.8112, F1 Macro: 0.7463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3393, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9324\n",
      "Epoch 3/10, Train Loss: 0.2342, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9215\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9076, F1 Micro: 0.9076, F1 Macro: 0.9007\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9268\n",
      "Epoch 6/10, Train Loss: 0.1315, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9277\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1015, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9458\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        82\n",
      "    positive       0.97      0.96      0.96       167\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.95      0.95       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9171\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 119.27306842803955 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9139\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 369.2517066892086\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 21.562068223953247 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5526, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4724, Accuracy: 0.8065, F1 Micro: 0.8908, F1 Macro: 0.8894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3946, Accuracy: 0.8876, F1 Micro: 0.932, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2966, Accuracy: 0.9323, F1 Micro: 0.9584, F1 Macro: 0.9569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2168, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1638, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1271, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.1072, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.542, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3086, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9411\n",
      "Epoch 3/10, Train Loss: 0.2093, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.916\n",
      "Epoch 4/10, Train Loss: 0.1551, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9347\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9243\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.932\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9273\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9411\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        81\n",
      "    positive       0.97      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.95      0.94       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9056\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.79      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 117.1065788269043 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5557, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4783, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4059, Accuracy: 0.8951, F1 Micro: 0.9369, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3002, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2234, Accuracy: 0.942, F1 Micro: 0.9634, F1 Macro: 0.9609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1703, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9741\n",
      "Epoch 7/10, Train Loss: 0.1347, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.1117, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0898, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6295, Accuracy: 0.8721, F1 Micro: 0.8721, F1 Macro: 0.85\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3452, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2846, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9257\n",
      "Epoch 4/10, Train Loss: 0.1694, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1774, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9524\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       173\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.95       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.926\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 125.33994030952454 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5414, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4661, Accuracy: 0.8058, F1 Micro: 0.8903, F1 Macro: 0.8888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3866, Accuracy: 0.9085, F1 Micro: 0.9434, F1 Macro: 0.9409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2774, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2057, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1604, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1295, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 8/10, Train Loss: 0.1074, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9713\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5522, Accuracy: 0.8413, F1 Micro: 0.8413, F1 Macro: 0.7986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3155, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.241, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9277\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9309\n",
      "Epoch 7/10, Train Loss: 0.1302, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9454\n",
      "Epoch 10/10, Train Loss: 0.0866, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.89      0.93        84\n",
      "    positive       0.95      0.98      0.96       168\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.95      0.94      0.95       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9166\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.81        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 119.27640986442566 s\n",
      "Averaged - Iteration 478: Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9161\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 376.8599965853275\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 19.780832290649414 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5417, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4695, Accuracy: 0.8095, F1 Micro: 0.8922, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3681, Accuracy: 0.901, F1 Micro: 0.9389, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2692, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1886, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1452, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "Epoch 8/10, Train Loss: 0.1016, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.972\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9708\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.92      0.92       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5185, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2806, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9221\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0655, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9345\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0349, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9387\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.93      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9085\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.79      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.3513400554657 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5492, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4794, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.378, Accuracy: 0.9055, F1 Micro: 0.942, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2813, Accuracy: 0.9412, F1 Micro: 0.9631, F1 Macro: 0.9609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1946, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9554, F1 Micro: 0.9718, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1033, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0782, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5954, Accuracy: 0.861, F1 Micro: 0.861, F1 Macro: 0.8324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3308, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1946, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1646, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1248, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Epoch 8/10, Train Loss: 0.0623, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0498, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        85\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.92      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9049\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.80      0.73      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.4644536972046 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5366, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4684, Accuracy: 0.817, F1 Micro: 0.8956, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3588, Accuracy: 0.9129, F1 Micro: 0.9456, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2661, Accuracy: 0.9449, F1 Micro: 0.9654, F1 Macro: 0.9631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1847, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1461, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1088, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9728\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9718\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5484, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2835, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1679, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9397\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9071\n",
      "Epoch 5/10, Train Loss: 0.0985, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0874, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9266\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9188\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9144\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.80      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.67021536827087 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9093\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 355.3322228326493\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 19.032470703125 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5428, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4697, Accuracy: 0.8103, F1 Micro: 0.8926, F1 Macro: 0.8912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3729, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2556, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1865, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1492, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Epoch 7/10, Train Loss: 0.1202, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9725\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5319, Accuracy: 0.8755, F1 Micro: 0.8755, F1 Macro: 0.8516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9107\n",
      "Epoch 3/10, Train Loss: 0.2, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9036\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1303, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0781, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9159\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9234\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9117\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.90        82\n",
      "    positive       0.96      0.94      0.95       167\n",
      "\n",
      "    accuracy                           0.93       249\n",
      "   macro avg       0.92      0.93      0.92       249\n",
      "weighted avg       0.93      0.93      0.93       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9024\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.81      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.8777687549591 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4809, Accuracy: 0.7976, F1 Micro: 0.8863, F1 Macro: 0.8847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3835, Accuracy: 0.9085, F1 Micro: 0.9443, F1 Macro: 0.9424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2674, Accuracy: 0.9375, F1 Micro: 0.9609, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.9721\n",
      "Epoch 6/10, Train Loss: 0.1557, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1245, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0969, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5891, Accuracy: 0.8661, F1 Micro: 0.8661, F1 Macro: 0.8324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3734, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.927\n",
      "Epoch 3/10, Train Loss: 0.218, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9361\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9259\n",
      "Epoch 7/10, Train Loss: 0.0919, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9417\n",
      "Epoch 10/10, Train Loss: 0.058, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.928\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        82\n",
      "    positive       0.96      0.96      0.96       172\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.94      0.94      0.94       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.81        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.81      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.95      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 130.0640847682953 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4644, Accuracy: 0.8274, F1 Micro: 0.9011, F1 Macro: 0.8997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.357, Accuracy: 0.9301, F1 Micro: 0.9569, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2415, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1771, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1477, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5554, Accuracy: 0.8664, F1 Micro: 0.8664, F1 Macro: 0.8414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3225, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1902, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9188\n",
      "Epoch 4/10, Train Loss: 0.1131, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9419\n",
      "Epoch 6/10, Train Loss: 0.1071, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9325\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9183\n",
      "Epoch 8/10, Train Loss: 0.0817, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.921\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9206\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9197\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       165\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.93      0.95      0.94       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9148\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.81      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.35635805130005 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9094\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 370.756439795538\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 17.235383987426758 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5399, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4672, Accuracy: 0.8244, F1 Micro: 0.8992, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3526, Accuracy: 0.9263, F1 Micro: 0.9546, F1 Macro: 0.9531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2538, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1824, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 6/10, Train Loss: 0.1396, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5837, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.292, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2214, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1694, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.941\n",
      "Epoch 5/10, Train Loss: 0.1786, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9277\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9373\n",
      "Epoch 7/10, Train Loss: 0.1033, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9362\n",
      "Epoch 8/10, Train Loss: 0.1071, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.937\n",
      "Epoch 9/10, Train Loss: 0.08, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9294\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9211\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        87\n",
      "    positive       0.97      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.94      0.94       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9242\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.76      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.76369452476501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5483, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4725, Accuracy: 0.814, F1 Micro: 0.8939, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3634, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2554, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1864, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1442, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1118, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5745, Accuracy: 0.8808, F1 Micro: 0.8808, F1 Macro: 0.8615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3155, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1726, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1435, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9191\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9191\n",
      "Epoch 7/10, Train Loss: 0.0862, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1146, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9182\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.906\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.94      0.89        83\n",
      "    positive       0.97      0.92      0.94       177\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.91      0.93      0.92       260\n",
      "weighted avg       0.93      0.93      0.93       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9042\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 138.09898734092712 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5336, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4548, Accuracy: 0.8408, F1 Micro: 0.9073, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3373, Accuracy: 0.9293, F1 Micro: 0.956, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2445, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1824, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1413, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.976\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5343, Accuracy: 0.8973, F1 Micro: 0.8973, F1 Macro: 0.8792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2824, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9342\n",
      "Epoch 3/10, Train Loss: 0.1719, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Epoch 4/10, Train Loss: 0.1313, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9268\n",
      "Epoch 5/10, Train Loss: 0.1163, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9191\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9346\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        84\n",
      "    positive       0.96      0.96      0.96       179\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9155\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.86      0.85       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 131.0794689655304 s\n",
      "Averaged - Iteration 573: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9146\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 378.10100692801586\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 15.739907264709473 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5328, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4537, Accuracy: 0.8378, F1 Micro: 0.906, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3424, Accuracy: 0.9315, F1 Micro: 0.9574, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2305, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Epoch 7/10, Train Loss: 0.1038, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0848, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 9/10, Train Loss: 0.0698, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.501, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9131\n",
      "Epoch 2/10, Train Loss: 0.3016, Accuracy: 0.8833, F1 Micro: 0.8833, F1 Macro: 0.8763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1081, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1006, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9347\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9255\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.96      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9149\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.63866114616394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5398, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4568, Accuracy: 0.8311, F1 Micro: 0.9031, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3432, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2406, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.138, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9749\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5887, Accuracy: 0.803, F1 Micro: 0.803, F1 Macro: 0.7196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3407, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1399, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9168\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9132\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8959\n",
      "Epoch 7/10, Train Loss: 0.117, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.908\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 10/10, Train Loss: 0.0774, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9132\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        85\n",
      "    positive       0.95      0.94      0.95       179\n",
      "\n",
      "    accuracy                           0.93       264\n",
      "   macro avg       0.91      0.92      0.92       264\n",
      "weighted avg       0.93      0.93      0.93       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9065\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 141.06765747070312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.528, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4479, Accuracy: 0.8601, F1 Micro: 0.918, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3294, Accuracy: 0.9412, F1 Micro: 0.9632, F1 Macro: 0.9609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2268, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.129, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Epoch 10/10, Train Loss: 0.0592, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5539, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9048\n",
      "Epoch 2/10, Train Loss: 0.2773, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1933, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1062, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.901\n",
      "Epoch 8/10, Train Loss: 0.1194, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9337\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9271\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.89      0.91        84\n",
      "    positive       0.95      0.97      0.96       178\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.94      0.93      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9096\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.76      0.79      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.80      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 143.43993306159973 s\n",
      "Averaged - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9104\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 394.3309346757086\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 13.777406454086304 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5368, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4504, Accuracy: 0.8519, F1 Micro: 0.9127, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3289, Accuracy: 0.9375, F1 Micro: 0.9614, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.234, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.973\n",
      "Epoch 5/10, Train Loss: 0.1673, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9708\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4706, Accuracy: 0.8945, F1 Micro: 0.8945, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.268, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.9017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1776, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 4/10, Train Loss: 0.1273, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9291\n",
      "Epoch 6/10, Train Loss: 0.111, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8902\n",
      "Epoch 7/10, Train Loss: 0.0928, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9053\n",
      "Epoch 8/10, Train Loss: 0.1113, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9088\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Epoch 10/10, Train Loss: 0.082, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9093\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.92      0.90        83\n",
      "    positive       0.96      0.95      0.95       173\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.93      0.93      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9078\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.99867749214172 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5424, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4654, Accuracy: 0.8348, F1 Micro: 0.9048, F1 Macro: 0.9036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3432, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2466, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1768, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1379, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0824, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "Epoch 9/10, Train Loss: 0.069, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.99      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.557, Accuracy: 0.8406, F1 Micro: 0.8406, F1 Macro: 0.7903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2974, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2421, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9237\n",
      "Epoch 4/10, Train Loss: 0.1782, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9213\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9022\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9167\n",
      "Epoch 7/10, Train Loss: 0.1033, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.912\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.912\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9362\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.89      0.91        83\n",
      "    positive       0.95      0.97      0.96       168\n",
      "\n",
      "    accuracy                           0.94       251\n",
      "   macro avg       0.94      0.93      0.94       251\n",
      "weighted avg       0.94      0.94      0.94       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9048\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80        12\n",
      "     neutral       0.92      0.99      0.95       152\n",
      "    positive       0.93      0.81      0.87        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.95      0.82      0.87       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 138.6988880634308 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5349, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.447, Accuracy: 0.875, F1 Micro: 0.9255, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3139, Accuracy: 0.9442, F1 Micro: 0.9652, F1 Macro: 0.9633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2294, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Epoch 5/10, Train Loss: 0.1638, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1325, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0975, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 9/10, Train Loss: 0.0665, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4854, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9042\n",
      "Epoch 2/10, Train Loss: 0.2488, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.9031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 4/10, Train Loss: 0.1273, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9143\n",
      "Epoch 5/10, Train Loss: 0.1347, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9249\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9148\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9221\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9058\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        84\n",
      "    positive       0.96      0.94      0.95       172\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.93      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9117\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.80      0.83       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.77852034568787 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9081\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 382.4009780166216\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 12.637481689453125 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5362, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4473, Accuracy: 0.8564, F1 Micro: 0.9147, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3215, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2215, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1588, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1202, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9752\n",
      "Epoch 8/10, Train Loss: 0.0823, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9728\n",
      "Epoch 9/10, Train Loss: 0.0635, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5334, Accuracy: 0.8884, F1 Micro: 0.8884, F1 Macro: 0.8787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2421, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9296\n",
      "Epoch 3/10, Train Loss: 0.1595, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9157\n",
      "Epoch 4/10, Train Loss: 0.1548, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9167\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9105\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8996\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9162\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8955\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.912\n",
      "Epoch 10/10, Train Loss: 0.0743, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9047\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        82\n",
      "    positive       0.98      0.92      0.95       169\n",
      "\n",
      "    accuracy                           0.94       251\n",
      "   macro avg       0.92      0.94      0.93       251\n",
      "weighted avg       0.94      0.94      0.94       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9091\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.28551316261292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5394, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4517, Accuracy: 0.8787, F1 Micro: 0.928, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3196, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2189, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1614, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.9554, F1 Micro: 0.9717, F1 Macro: 0.9696\n",
      "Epoch 8/10, Train Loss: 0.0796, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5872, Accuracy: 0.8386, F1 Micro: 0.8386, F1 Macro: 0.8021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2904, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2153, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.91\n",
      "Epoch 4/10, Train Loss: 0.2067, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1201, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9131\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.95      0.89        83\n",
      "    positive       0.97      0.91      0.94       171\n",
      "\n",
      "    accuracy                           0.93       254\n",
      "   macro avg       0.91      0.93      0.92       254\n",
      "weighted avg       0.93      0.93      0.93       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9039\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 147.19889092445374 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5333, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4373, Accuracy: 0.9018, F1 Micro: 0.94, F1 Macro: 0.9379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3048, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2131, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1551, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5177, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.256, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1242, Accuracy: 0.9624, F1 Micro: 0.9624, F1 Macro: 0.957\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9488\n",
      "Epoch 6/10, Train Loss: 0.0839, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.91\n",
      "Epoch 7/10, Train Loss: 0.1129, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "Epoch 8/10, Train Loss: 0.1026, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9201\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9484\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9624, F1 Micro: 0.9624, F1 Macro: 0.957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        85\n",
      "    positive       0.98      0.97      0.97       181\n",
      "\n",
      "    accuracy                           0.96       266\n",
      "   macro avg       0.95      0.96      0.96       266\n",
      "weighted avg       0.96      0.96      0.96       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9293\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.56727576255798 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9573, F1 Micro: 0.9573, F1 Macro: 0.9141\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 368.3477093737461\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 12.039717435836792 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5442, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4407, Accuracy: 0.8638, F1 Micro: 0.9193, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3033, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2016, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4876, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2332, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 3/10, Train Loss: 0.1654, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1295, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9267\n",
      "Epoch 5/10, Train Loss: 0.1119, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9186\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0817, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 9/10, Train Loss: 0.0666, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9213\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9134\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.91        86\n",
      "    positive       0.98      0.93      0.95       180\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.92      0.94      0.93       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9194\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.67390847206116 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5456, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4506, Accuracy: 0.8765, F1 Micro: 0.9263, F1 Macro: 0.9244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.312, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2103, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.99      0.95       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5733, Accuracy: 0.8863, F1 Micro: 0.8863, F1 Macro: 0.8739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2821, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1784, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1256, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9298\n",
      "Epoch 6/10, Train Loss: 0.1119, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 7/10, Train Loss: 0.0675, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9182\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9154\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        83\n",
      "    positive       0.96      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.94       255\n",
      "   macro avg       0.93      0.94      0.93       255\n",
      "weighted avg       0.94      0.94      0.94       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9122\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.99      0.95       152\n",
      "    positive       0.95      0.77      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.88      0.85      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 148.05363392829895 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5396, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4363, Accuracy: 0.8966, F1 Micro: 0.9378, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2941, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1984, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0753, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Epoch 10/10, Train Loss: 0.0534, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4865, Accuracy: 0.8774, F1 Micro: 0.8774, F1 Macro: 0.8687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2777, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1827, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1472, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9205\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9161\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9306\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9215\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.53990006446838 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9586, F1 Micro: 0.9586, F1 Macro: 0.9177\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 364.0375340160719\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 10.989689111709595 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5438, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4363, Accuracy: 0.875, F1 Micro: 0.9248, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3042, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2067, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9741\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9755\n",
      "Epoch 10/10, Train Loss: 0.0538, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9769\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1632, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "Epoch 4/10, Train Loss: 0.1114, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9249\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9144\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9186\n",
      "Epoch 9/10, Train Loss: 0.0655, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9063\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9221\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.98      0.92        85\n",
      "    positive       0.99      0.93      0.96       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.44456124305725 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4485, Accuracy: 0.8676, F1 Micro: 0.9218, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3155, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2116, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2693, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9135\n",
      "Epoch 6/10, Train Loss: 0.0824, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9135\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9085\n",
      "Epoch 10/10, Train Loss: 0.0935, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9259\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        84\n",
      "    positive       0.98      0.94      0.95       170\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.94      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9192\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 146.89566349983215 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4333, Accuracy: 0.8921, F1 Micro: 0.9345, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3046, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2026, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1087, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0796, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4661, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 2/10, Train Loss: 0.2302, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9259\n",
      "Epoch 3/10, Train Loss: 0.175, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9244\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 7/10, Train Loss: 0.0894, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        87\n",
      "    positive       0.96      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.94       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9233\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.76      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.57651329040527 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9588, F1 Micro: 0.9588, F1 Macro: 0.921\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 377.9274229949336\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 9.428237676620483 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5299, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4333, Accuracy: 0.8943, F1 Micro: 0.9363, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.286, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1953, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9765\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4831, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2394, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1939, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9188\n",
      "Epoch 4/10, Train Loss: 0.1328, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9145\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9112\n",
      "Epoch 6/10, Train Loss: 0.1179, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0695, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9141\n",
      "Epoch 8/10, Train Loss: 0.0523, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.09, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        87\n",
      "    positive       0.98      0.93      0.96       183\n",
      "\n",
      "    accuracy                           0.94       270\n",
      "   macro avg       0.93      0.94      0.93       270\n",
      "weighted avg       0.94      0.94      0.94       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9245\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.10419273376465 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4371, Accuracy: 0.9025, F1 Micro: 0.9411, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2929, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2019, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1425, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1116, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 9/10, Train Loss: 0.0603, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5677, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2936, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9247\n",
      "Epoch 3/10, Train Loss: 0.1909, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1183, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1166, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9177\n",
      "Epoch 8/10, Train Loss: 0.1031, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9051\n",
      "Epoch 9/10, Train Loss: 0.1023, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.91        86\n",
      "    positive       0.98      0.93      0.95       182\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.92      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9181\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      1.00      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 157.3927083015442 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5242, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4166, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2709, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1893, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1072, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9783\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0605, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4978, Accuracy: 0.9111, F1 Micro: 0.9111, F1 Macro: 0.9011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9254\n",
      "Epoch 3/10, Train Loss: 0.1893, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1689, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1043, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Epoch 7/10, Train Loss: 0.1037, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9294\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9366\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        87\n",
      "    positive       0.96      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.94       270\n",
      "   macro avg       0.94      0.94      0.94       270\n",
      "weighted avg       0.94      0.94      0.94       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9234\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.92      0.94       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.7527425289154 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9594, F1 Micro: 0.9594, F1 Macro: 0.922\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 363.6565069837473\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 9.176557779312134 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5333, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4237, Accuracy: 0.8966, F1 Micro: 0.9375, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2762, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1942, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1028, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0878, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4583, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2306, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.944\n",
      "Epoch 3/10, Train Loss: 0.1663, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1643, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9436\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9395\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9409\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.91      0.92        86\n",
      "    positive       0.96      0.97      0.96       180\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.95      0.94      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9195\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.69976019859314 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5359, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4337, Accuracy: 0.8988, F1 Micro: 0.9391, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2921, Accuracy: 0.9516, F1 Micro: 0.9701, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1984, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1447, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 6/10, Train Loss: 0.105, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0687, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5184, Accuracy: 0.9116, F1 Micro: 0.9116, F1 Macro: 0.9028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2569, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9209\n",
      "Epoch 3/10, Train Loss: 0.1918, Accuracy: 0.9076, F1 Micro: 0.9076, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9286\n",
      "Epoch 5/10, Train Loss: 0.1336, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1162, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9301\n",
      "Epoch 7/10, Train Loss: 0.1021, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9424\n",
      "Epoch 9/10, Train Loss: 0.0896, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9301\n",
      "Epoch 10/10, Train Loss: 0.069, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9144\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.97      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.95      0.94       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.52963423728943 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5281, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4092, Accuracy: 0.9241, F1 Micro: 0.9534, F1 Macro: 0.9513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2694, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1869, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9716\n",
      "Epoch 8/10, Train Loss: 0.0687, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9728\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5081, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.88\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2531, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1901, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9341\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1527, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9394\n",
      "Epoch 7/10, Train Loss: 0.1009, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9162\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        85\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9204\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.87      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 160.7042920589447 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9586, F1 Micro: 0.9586, F1 Macro: 0.9189\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 376.6345819434133\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 7.634409666061401 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5357, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4361, Accuracy: 0.8914, F1 Micro: 0.9342, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.293, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2006, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1369, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4848, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9579, F1 Micro: 0.9579, F1 Macro: 0.9524\n",
      "Epoch 3/10, Train Loss: 0.1688, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9489\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1141, Accuracy: 0.9617, F1 Micro: 0.9617, F1 Macro: 0.9564\n",
      "Epoch 6/10, Train Loss: 0.1203, Accuracy: 0.9579, F1 Micro: 0.9579, F1 Macro: 0.9527\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 8/10, Train Loss: 0.1061, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Epoch 9/10, Train Loss: 0.0808, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9444\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9617, F1 Micro: 0.9617, F1 Macro: 0.9564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.94      0.94        85\n",
      "    positive       0.97      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.96       261\n",
      "   macro avg       0.96      0.96      0.96       261\n",
      "weighted avg       0.96      0.96      0.96       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9323\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.05570220947266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4389, Accuracy: 0.8951, F1 Micro: 0.937, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3015, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2078, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1418, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.0694, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Epoch 9/10, Train Loss: 0.0583, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2524, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1439, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1367, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Epoch 10/10, Train Loss: 0.0735, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        84\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9271\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.70632886886597 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5264, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4172, Accuracy: 0.9196, F1 Micro: 0.9505, F1 Macro: 0.9479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1955, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1369, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1108, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4659, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1838, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9525\n",
      "Epoch 4/10, Train Loss: 0.1552, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9484\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9448\n",
      "Epoch 7/10, Train Loss: 0.0932, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9481\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.94      0.94        86\n",
      "    positive       0.97      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.95      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9279\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.37518072128296 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9291\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 387.0611276657497\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 7.104519844055176 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5401, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4157, Accuracy: 0.9122, F1 Micro: 0.9464, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.268, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.974\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0832, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9805\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.99      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4468, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2761, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1638, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9348\n",
      "Epoch 4/10, Train Loss: 0.1299, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1118, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "Epoch 6/10, Train Loss: 0.0839, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9345\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9207\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9086\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9249\n",
      "Epoch 10/10, Train Loss: 0.0846, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9341\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        84\n",
      "    positive       0.98      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.94       253\n",
      "   macro avg       0.93      0.95      0.94       253\n",
      "weighted avg       0.95      0.94      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9185\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.47858834266663 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5485, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4226, Accuracy: 0.9107, F1 Micro: 0.9454, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2787, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1877, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1403, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9791\n",
      "Epoch 8/10, Train Loss: 0.0665, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4938, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.227, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Epoch 3/10, Train Loss: 0.193, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9058\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9173\n",
      "Epoch 5/10, Train Loss: 0.1082, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.921\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0865, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9222\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Epoch 9/10, Train Loss: 0.067, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9214\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.94506096839905 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4156, Accuracy: 0.9241, F1 Micro: 0.9531, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2664, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1825, Accuracy: 0.965, F1 Micro: 0.9782, F1 Macro: 0.977\n",
      "Epoch 5/10, Train Loss: 0.1403, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0851, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.96      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4717, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2569, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 3/10, Train Loss: 0.1871, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9198\n",
      "Epoch 4/10, Train Loss: 0.1381, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9347\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.0997, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9423\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9434\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.919\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.96      0.97       152\n",
      "    positive       0.87      0.83      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.93      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.37048029899597 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9596, F1 Micro: 0.9596, F1 Macro: 0.9196\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 340.88863845575236\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.898282289505005 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.529, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.408, Accuracy: 0.9107, F1 Micro: 0.9459, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2625, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1718, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9728\n",
      "Epoch 6/10, Train Loss: 0.0947, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9725\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.9606, F1 Micro: 0.9749, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.92      0.92       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4347, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9177\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1526, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1358, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.936\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.906\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9068\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9138\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8983\n",
      "Epoch 9/10, Train Loss: 0.0855, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9199\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9168\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.97      0.95      0.96       183\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.94      0.94       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9152\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.82      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.7492413520813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5359, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4151, Accuracy: 0.9152, F1 Micro: 0.9483, F1 Macro: 0.9464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2695, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1279, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9732\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5017, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2533, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1886, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9171\n",
      "Epoch 4/10, Train Loss: 0.1314, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.945\n",
      "Epoch 6/10, Train Loss: 0.0824, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9624, F1 Micro: 0.9624, F1 Macro: 0.957\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8984\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9358\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9624, F1 Micro: 0.9624, F1 Macro: 0.957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.93      0.94        87\n",
      "    positive       0.97      0.98      0.97       179\n",
      "\n",
      "    accuracy                           0.96       266\n",
      "   macro avg       0.96      0.95      0.96       266\n",
      "weighted avg       0.96      0.96      0.96       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9321\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.76      0.87      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.94      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.44816827774048 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5228, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3898, Accuracy: 0.9308, F1 Micro: 0.9574, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.255, Accuracy: 0.9576, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9722\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.93      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.44, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2185, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1826, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1192, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9449\n",
      "Epoch 5/10, Train Loss: 0.1115, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9315\n",
      "Epoch 6/10, Train Loss: 0.1207, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.922\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9411\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       182\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.94      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9244\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 169.77733206748962 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9586, F1 Micro: 0.9586, F1 Macro: 0.9239\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 240.96009616604468\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.334700584411621 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5287, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3964, Accuracy: 0.9249, F1 Micro: 0.9539, F1 Macro: 0.9524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2658, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1749, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0686, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4268, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 2/10, Train Loss: 0.1903, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9269\n",
      "Epoch 3/10, Train Loss: 0.1698, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Epoch 4/10, Train Loss: 0.1607, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Epoch 5/10, Train Loss: 0.108, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9042\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.934\n",
      "Epoch 8/10, Train Loss: 0.0994, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Epoch 9/10, Train Loss: 0.0785, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9087\n",
      "Epoch 10/10, Train Loss: 0.0714, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9269\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.88      0.91        86\n",
      "    positive       0.94      0.97      0.96       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.94      0.93      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.90      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 169.0566051006317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5287, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4005, Accuracy: 0.9182, F1 Micro: 0.9491, F1 Macro: 0.9463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.266, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1752, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4937, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2321, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Epoch 3/10, Train Loss: 0.1665, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0899, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9369\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9265\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9256\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        86\n",
      "    positive       0.99      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9237\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 176.17081093788147 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5227, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3885, Accuracy: 0.9256, F1 Micro: 0.9537, F1 Macro: 0.9513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2529, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1671, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.126, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4737, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2496, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1481, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9321\n",
      "Epoch 4/10, Train Loss: 0.1166, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0886, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9354\n",
      "Epoch 7/10, Train Loss: 0.061, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "Epoch 9/10, Train Loss: 0.0661, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9328\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.935\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9237\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.86      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 174.026789188385 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9217\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 165.49506112568014\n",
      "Nearest checkpoint: \n",
      "864Acquired samples: 25\n",
      "Sampling duration: 3.3880436420440674 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.531, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3995, Accuracy: 0.9234, F1 Micro: 0.953, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2547, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1234, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4325, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2454, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Epoch 3/10, Train Loss: 0.1517, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9175\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1122, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9442\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9132\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.905\n",
      "Epoch 8/10, Train Loss: 0.0852, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.0597, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9316\n",
      "Epoch 10/10, Train Loss: 0.0745, Accuracy: 0.8981, F1 Micro: 0.8981, F1 Macro: 0.8787\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        87\n",
      "    positive       0.96      0.97      0.96       178\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.95      0.94      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.928\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.67285799980164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4052, Accuracy: 0.9234, F1 Micro: 0.9532, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2554, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1621, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.979\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4796, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9221\n",
      "Epoch 2/10, Train Loss: 0.2381, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Epoch 3/10, Train Loss: 0.1802, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9077\n",
      "Epoch 4/10, Train Loss: 0.1324, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.918\n",
      "Epoch 5/10, Train Loss: 0.1117, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "Epoch 7/10, Train Loss: 0.0931, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8993\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8993\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.889\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        84\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 169.95822286605835 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.525, Accuracy: 0.7984, F1 Micro: 0.8866, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3811, Accuracy: 0.9315, F1 Micro: 0.9578, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2429, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1565, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4392, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2377, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "Epoch 3/10, Train Loss: 0.1551, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.905\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Epoch 5/10, Train Loss: 0.0994, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9205\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8998\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "Epoch 9/10, Train Loss: 0.0671, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9348\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        85\n",
      "    positive       0.96      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9162\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.08094453811646 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9218\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 41.25782539905463\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.3677330017089844 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5216, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3994, Accuracy: 0.9278, F1 Micro: 0.956, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2466, Accuracy: 0.9576, F1 Micro: 0.9737, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1722, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      1.00      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4069, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2211, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9344\n",
      "Epoch 3/10, Train Loss: 0.1536, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9252\n",
      "Epoch 4/10, Train Loss: 0.1583, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1043, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9471\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.951\n",
      "Epoch 10/10, Train Loss: 0.0373, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9281\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.92      0.93        85\n",
      "    positive       0.96      0.98      0.97       170\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.96      0.95      0.95       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9273\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.95      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      1.00      0.98        23\n",
      "     neutral       0.97      1.00      0.98       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.82        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 173.5594367980957 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4107, Accuracy: 0.9182, F1 Micro: 0.9501, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2629, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4629, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2314, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1778, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9298\n",
      "Epoch 4/10, Train Loss: 0.1099, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9224\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9224\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9327\n",
      "Epoch 9/10, Train Loss: 0.0893, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9146\n",
      "Epoch 10/10, Train Loss: 0.0607, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9138\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        88\n",
      "    positive       0.96      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9198\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 174.93267846107483 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.516, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3886, Accuracy: 0.9226, F1 Micro: 0.9523, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2381, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1248, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.462, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1872, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9491\n",
      "Epoch 3/10, Train Loss: 0.1714, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9212\n",
      "Epoch 4/10, Train Loss: 0.1218, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9406\n",
      "Epoch 5/10, Train Loss: 0.1109, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Epoch 6/10, Train Loss: 0.0588, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.077, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9479\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9111\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.92      0.93        86\n",
      "    positive       0.96      0.97      0.97       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.95      0.95      0.95       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.77      0.82      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.04596376419067 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.923\n",
      "Total runtime: 11587.529206752777 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5658e",
   "metadata": {
    "papermill": {
     "duration": 0.011913,
     "end_time": "2025-06-08T19:09:42.514381",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.502468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbe52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f12dfa6a11345ea833de7fd52c63cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6645, Accuracy: 0.7865, F1 Micro: 0.8804, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5755, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5509, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5294, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4827, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4817, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 7/10, Train Loss: 0.4898, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4555, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4326, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8825\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.73      0.96      0.83       158\n",
      "        part       0.72      1.00      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.88      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7166, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5281, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4526, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3997, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3523, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3078, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2888, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2379, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1964, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1981, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.95      1.00      0.97        18\n",
      "\n",
      "    accuracy                           0.95        19\n",
      "   macro avg       0.47      0.50      0.49        19\n",
      "weighted avg       0.90      0.95      0.92        19\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.73      0.96      0.83       152\n",
      "    positive       0.60      0.17      0.27        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.44      0.38      0.37       216\n",
      "weighted avg       0.66      0.72      0.65       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.75      0.07      0.13        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.49      0.36      0.32       216\n",
      "weighted avg       0.65      0.72      0.61       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 63.85185241699219 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6903, Accuracy: 0.7671, F1 Micro: 0.8669, F1 Macro: 0.8648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5862, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 3/10, Train Loss: 0.5474, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5251, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.4831, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4764, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4815, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8803\n",
      "Epoch 8/10, Train Loss: 0.4411, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4087, Accuracy: 0.7969, F1 Micro: 0.8856, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3783, Accuracy: 0.8043, F1 Micro: 0.889, F1 Macro: 0.8873\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8043, F1 Micro: 0.889, F1 Macro: 0.8873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.81      0.97      0.88       175\n",
      "      others       0.73      0.99      0.84       158\n",
      "        part       0.74      0.99      0.85       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.81      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.89      1061\n",
      "weighted avg       0.81      0.99      0.89      1061\n",
      " samples avg       0.81      0.99      0.89      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6552, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5933, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5783, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5062, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4427, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3945, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4686, Accuracy: 0.6857, F1 Micro: 0.6857, F1 Macro: 0.4804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4872, Accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.6504\n",
      "Epoch 9/10, Train Loss: 0.3907, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.6196\n",
      "Epoch 10/10, Train Loss: 0.4138, Accuracy: 0.6286, F1 Micro: 0.6286, F1 Macro: 0.5956\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.6504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.33      0.47        12\n",
      "    positive       0.73      0.96      0.83        23\n",
      "\n",
      "    accuracy                           0.74        35\n",
      "   macro avg       0.77      0.64      0.65        35\n",
      "weighted avg       0.76      0.74      0.71        35\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7994, F1 Micro: 0.7994, F1 Macro: 0.3498\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.25      0.38        16\n",
      "     neutral       0.80      0.98      0.88       167\n",
      "    positive       0.25      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.62      0.43      0.45       216\n",
      "weighted avg       0.72      0.78      0.72       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.73      0.99      0.84       152\n",
      "    positive       0.70      0.13      0.23        52\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.48      0.37      0.35       216\n",
      "weighted avg       0.68      0.73      0.64       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.74      0.99      0.85       152\n",
      "    positive       0.58      0.17      0.26        41\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.44      0.39      0.37       216\n",
      "weighted avg       0.63      0.73      0.65       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 53.117534160614014 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6548, Accuracy: 0.7827, F1 Micro: 0.878, F1 Macro: 0.8767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5764, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5377, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5305, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4931, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4768, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4845, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8799\n",
      "Epoch 8/10, Train Loss: 0.4606, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4249, Accuracy: 0.7924, F1 Micro: 0.8834, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4039, Accuracy: 0.7969, F1 Micro: 0.8852, F1 Macro: 0.8834\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7969, F1 Micro: 0.8852, F1 Macro: 0.8834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.99      0.88       175\n",
      "      others       0.72      0.97      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6588, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5725, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4665, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5629, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4718, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3767, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3798, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3906, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Epoch 9/10, Train Loss: 0.4707, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.326, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.5214\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.5214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.14      0.22         7\n",
      "    positive       0.73      0.94      0.82        17\n",
      "\n",
      "    accuracy                           0.71        24\n",
      "   macro avg       0.61      0.54      0.52        24\n",
      "weighted avg       0.66      0.71      0.65        24\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3226\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.43      0.35      0.33       216\n",
      "weighted avg       0.65      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.98      0.83       152\n",
      "    positive       0.70      0.13      0.23        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.47      0.37      0.35       216\n",
      "weighted avg       0.68      0.72      0.64       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.50      0.10      0.16        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.41      0.36      0.33       216\n",
      "weighted avg       0.60      0.71      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 62.17392587661743 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7958, F1 Micro: 0.7958, F1 Macro: 0.3296\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 950.516608463651\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 29.932151317596436 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6148, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5374, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.5204, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4885, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4584, Accuracy: 0.8006, F1 Micro: 0.8871, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4293, Accuracy: 0.8132, F1 Micro: 0.8924, F1 Macro: 0.8906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3816, Accuracy: 0.8251, F1 Micro: 0.8988, F1 Macro: 0.8971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3382, Accuracy: 0.8482, F1 Micro: 0.9102, F1 Macro: 0.9083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2849, Accuracy: 0.872, F1 Micro: 0.9233, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.253, Accuracy: 0.8914, F1 Micro: 0.9339, F1 Macro: 0.9317\n",
      "\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8914, F1 Micro: 0.9339, F1 Macro: 0.9317              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.88      0.99      0.93       187\n",
      "     machine       0.91      0.97      0.94       175\n",
      "      others       0.86      0.87      0.87       158\n",
      "        part       0.88      0.98      0.93       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.89      1.00      0.94       191\n",
      "\n",
      "   micro avg       0.90      0.97      0.93      1061\n",
      "   macro avg       0.90      0.97      0.93      1061\n",
      "weighted avg       0.90      0.97      0.93      1061\n",
      " samples avg       0.90      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6337, Accuracy: 0.7619, F1 Micro: 0.7619, F1 Macro: 0.4324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4827, Accuracy: 0.7619, F1 Micro: 0.7619, F1 Macro: 0.4324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4825, Accuracy: 0.7884, F1 Micro: 0.7884, F1 Macro: 0.554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3191, Accuracy: 0.873, F1 Micro: 0.873, F1 Macro: 0.8097\n",
      "Epoch 5/10, Train Loss: 0.2072, Accuracy: 0.8519, F1 Micro: 0.8519, F1 Macro: 0.8096\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8102\n",
      "Epoch 7/10, Train Loss: 0.1258, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1331, Accuracy: 0.873, F1 Micro: 0.873, F1 Macro: 0.8324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.8783, F1 Micro: 0.8783, F1 Macro: 0.8383\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.8677, F1 Micro: 0.8677, F1 Macro: 0.8217\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.8783, F1 Micro: 0.8783, F1 Macro: 0.8383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.80      0.76        45\n",
      "    positive       0.94      0.90      0.92       144\n",
      "\n",
      "    accuracy                           0.88       189\n",
      "   macro avg       0.83      0.85      0.84       189\n",
      "weighted avg       0.88      0.88      0.88       189\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8773, F1 Micro: 0.8773, F1 Macro: 0.67\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.89      0.99      0.94       181\n",
      "    positive       0.85      0.46      0.59        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.58      0.48      0.51       216\n",
      "weighted avg       0.84      0.88      0.85       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.62      0.69        16\n",
      "     neutral       0.91      0.98      0.94       167\n",
      "    positive       0.79      0.58      0.67        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.73      0.77       216\n",
      "weighted avg       0.88      0.89      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.42      0.42        12\n",
      "     neutral       0.86      0.88      0.87       152\n",
      "    positive       0.66      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.65      0.64      0.64       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.39      0.51        23\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.82      0.68      0.75        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.82      0.68      0.73       216\n",
      "weighted avg       0.85      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.88      0.77      0.81       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        14\n",
      "     neutral       0.89      1.00      0.94       185\n",
      "    positive       1.00      0.12      0.21        17\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.96      0.49      0.56       216\n",
      "weighted avg       0.90      0.89      0.85       216\n",
      "\n",
      "Total train time: 74.01990270614624 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6296, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5318, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5152, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4825, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4531, Accuracy: 0.8065, F1 Micro: 0.8907, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.412, Accuracy: 0.8274, F1 Micro: 0.9008, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.364, Accuracy: 0.8579, F1 Micro: 0.9166, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3238, Accuracy: 0.8884, F1 Micro: 0.9329, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2717, Accuracy: 0.9048, F1 Micro: 0.9421, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2413, Accuracy: 0.9122, F1 Micro: 0.9464, F1 Macro: 0.9446\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9122, F1 Micro: 0.9464, F1 Macro: 0.9446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.90      1.00      0.95       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.87      0.93      0.90       158\n",
      "        part       0.86      0.99      0.92       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.91      0.98      0.95      1061\n",
      "   macro avg       0.91      0.98      0.94      1061\n",
      "weighted avg       0.91      0.98      0.95      1061\n",
      " samples avg       0.92      0.98      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6247, Accuracy: 0.7077, F1 Micro: 0.7077, F1 Macro: 0.4144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5271, Accuracy: 0.7077, F1 Micro: 0.7077, F1 Macro: 0.4144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5058, Accuracy: 0.7538, F1 Micro: 0.7538, F1 Macro: 0.5623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3957, Accuracy: 0.8923, F1 Micro: 0.8923, F1 Macro: 0.8648\n",
      "Epoch 5/10, Train Loss: 0.2227, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.8796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1478, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8872\n",
      "Epoch 8/10, Train Loss: 0.1122, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8884\n",
      "Epoch 10/10, Train Loss: 0.0775, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8862\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.84      0.84        57\n",
      "    positive       0.93      0.93      0.93       138\n",
      "\n",
      "    accuracy                           0.91       195\n",
      "   macro avg       0.89      0.89      0.89       195\n",
      "weighted avg       0.91      0.91      0.91       195\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.7576\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.91      1.00      0.95       181\n",
      "    positive       1.00      0.71      0.83        24\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.64      0.57      0.59       216\n",
      "weighted avg       0.87      0.92      0.89       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.76      0.58      0.66        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.75      0.77       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.87      0.93      0.90       152\n",
      "    positive       0.76      0.62      0.68        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.79      0.74      0.76       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.48      0.63        23\n",
      "     neutral       0.86      0.99      0.92       152\n",
      "    positive       0.83      0.61      0.70        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.87      0.69      0.75       216\n",
      "weighted avg       0.86      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.75      0.80       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.95      1.00      0.97       185\n",
      "    positive       1.00      0.59      0.74        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.98      0.79      0.86       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Total train time: 80.81842565536499 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6239, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5418, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5148, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4933, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4669, Accuracy: 0.7909, F1 Micro: 0.8829, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4372, Accuracy: 0.8065, F1 Micro: 0.8903, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4003, Accuracy: 0.8199, F1 Micro: 0.8961, F1 Macro: 0.8942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3644, Accuracy: 0.8482, F1 Micro: 0.9111, F1 Macro: 0.9094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3115, Accuracy: 0.8832, F1 Micro: 0.93, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2729, Accuracy: 0.8981, F1 Micro: 0.9385, F1 Macro: 0.9369\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8981, F1 Micro: 0.9385, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.91      1.00      0.95       187\n",
      "     machine       0.91      0.96      0.94       175\n",
      "      others       0.84      0.97      0.90       158\n",
      "        part       0.84      0.99      0.91       158\n",
      "       price       0.94      0.99      0.97       192\n",
      "     service       0.93      0.99      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.98      0.94      1061\n",
      "   macro avg       0.89      0.98      0.94      1061\n",
      "weighted avg       0.90      0.98      0.94      1061\n",
      " samples avg       0.90      0.99      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5937, Accuracy: 0.7176, F1 Micro: 0.7176, F1 Macro: 0.4178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5408, Accuracy: 0.7294, F1 Micro: 0.7294, F1 Macro: 0.4607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4553, Accuracy: 0.8471, F1 Micro: 0.8471, F1 Macro: 0.8199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.305, Accuracy: 0.8824, F1 Micro: 0.8824, F1 Macro: 0.853\n",
      "Epoch 5/10, Train Loss: 0.1944, Accuracy: 0.8765, F1 Micro: 0.8765, F1 Macro: 0.8521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8694\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.8882, F1 Micro: 0.8882, F1 Macro: 0.8594\n",
      "Epoch 9/10, Train Loss: 0.088, Accuracy: 0.8824, F1 Micro: 0.8824, F1 Macro: 0.8599\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.8765, F1 Micro: 0.8765, F1 Macro: 0.8538\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        48\n",
      "    positive       0.93      0.93      0.93       122\n",
      "\n",
      "    accuracy                           0.89       170\n",
      "   macro avg       0.87      0.87      0.87       170\n",
      "weighted avg       0.89      0.89      0.89       170\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8873, F1 Micro: 0.8873, F1 Macro: 0.7082\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       0.94      0.71      0.81        24\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.95      0.60      0.64       216\n",
      "weighted avg       0.93      0.92      0.90       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.72      0.55      0.62        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.73      0.76       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.25      0.33        12\n",
      "     neutral       0.84      0.97      0.90       152\n",
      "    positive       0.85      0.54      0.66        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.73      0.59      0.63       216\n",
      "weighted avg       0.82      0.83      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.39      0.55        23\n",
      "     neutral       0.83      0.99      0.91       152\n",
      "    positive       0.84      0.51      0.64        41\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.86      0.63      0.70       216\n",
      "weighted avg       0.84      0.84      0.82       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.38      0.53        13\n",
      "     neutral       0.94      0.99      0.97       186\n",
      "    positive       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.68      0.74       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.93      0.99      0.96       185\n",
      "    positive       0.88      0.41      0.56        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.71      0.77       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 78.15199398994446 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8889, F1 Micro: 0.8889, F1 Macro: 0.7119\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 948.4091497115924\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: \n",
      "73Sampling duration: 38.95800542831421 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5735, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5032, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.503, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4478, Accuracy: 0.8028, F1 Micro: 0.8886, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3953, Accuracy: 0.8371, F1 Micro: 0.9043, F1 Macro: 0.9021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3406, Accuracy: 0.8646, F1 Micro: 0.9193, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2916, Accuracy: 0.9055, F1 Micro: 0.9416, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.243, Accuracy: 0.9241, F1 Micro: 0.9533, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.194, Accuracy: 0.9301, F1 Micro: 0.9562, F1 Macro: 0.9531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1632, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.9597\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.9597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      1.00      0.98       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.89      0.91      0.90       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5782, Accuracy: 0.6864, F1 Micro: 0.6864, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4271, Accuracy: 0.7034, F1 Micro: 0.7034, F1 Macro: 0.4624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3579, Accuracy: 0.8856, F1 Micro: 0.8856, F1 Macro: 0.8666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2309, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.118, Accuracy: 0.9322, F1 Micro: 0.9322, F1 Macro: 0.9248\n",
      "Epoch 7/10, Train Loss: 0.1252, Accuracy: 0.9025, F1 Micro: 0.9025, F1 Macro: 0.8837\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8892\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.8947\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.8998\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9322, F1 Micro: 0.9322, F1 Macro: 0.9248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.99      0.90        74\n",
      "    positive       0.99      0.91      0.95       162\n",
      "\n",
      "    accuracy                           0.93       236\n",
      "   macro avg       0.91      0.95      0.92       236\n",
      "weighted avg       0.94      0.93      0.93       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.8619\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.97      1.00      0.98       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       1.00      0.61      0.75        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.83      0.85       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.90      0.91      0.91       152\n",
      "    positive       0.75      0.63      0.69        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.75      0.79      0.76       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      0.99      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.85      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 89.10804867744446 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5943, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5057, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4962, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4359, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3782, Accuracy: 0.872, F1 Micro: 0.924, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3251, Accuracy: 0.8929, F1 Micro: 0.9351, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2692, Accuracy: 0.9144, F1 Micro: 0.9473, F1 Macro: 0.9457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2282, Accuracy: 0.9368, F1 Micro: 0.9605, F1 Macro: 0.9588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1837, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1587, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9653\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.91      0.91       158\n",
      "        part       0.94      0.99      0.97       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5795, Accuracy: 0.6855, F1 Micro: 0.6855, F1 Macro: 0.4067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5603, Accuracy: 0.7258, F1 Micro: 0.7258, F1 Macro: 0.5385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3588, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2304, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.179, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0815, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8949\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.8835\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.8858\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8862\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8766\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.90      0.86        78\n",
      "    positive       0.95      0.91      0.93       170\n",
      "\n",
      "    accuracy                           0.91       248\n",
      "   macro avg       0.89      0.90      0.89       248\n",
      "weighted avg       0.91      0.91      0.91       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.8623\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.58      0.56        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.73      0.71      0.72        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.73      0.74      0.73       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.87      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 90.5122926235199 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5781, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5059, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5065, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.455, Accuracy: 0.7969, F1 Micro: 0.8858, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4109, Accuracy: 0.8259, F1 Micro: 0.8995, F1 Macro: 0.8976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.367, Accuracy: 0.8713, F1 Micro: 0.9238, F1 Macro: 0.9225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3068, Accuracy: 0.9182, F1 Micro: 0.9498, F1 Macro: 0.9481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2564, Accuracy: 0.9308, F1 Micro: 0.9572, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2093, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9643\n",
      "Epoch 10/10, Train Loss: 0.1741, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9633\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.93      1.00      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5893, Accuracy: 0.6809, F1 Micro: 0.6809, F1 Macro: 0.4051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5338, Accuracy: 0.7702, F1 Micro: 0.7702, F1 Macro: 0.657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3426, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2159, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.8894, F1 Micro: 0.8894, F1 Macro: 0.8666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1525, Accuracy: 0.8894, F1 Micro: 0.8894, F1 Macro: 0.8808\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.817, F1 Micro: 0.817, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1443, Accuracy: 0.8979, F1 Micro: 0.8979, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8923\n",
      "Epoch 10/10, Train Loss: 0.1317, Accuracy: 0.8979, F1 Micro: 0.8979, F1 Macro: 0.8789\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        75\n",
      "    positive       0.93      0.93      0.93       160\n",
      "\n",
      "    accuracy                           0.91       235\n",
      "   macro avg       0.89      0.89      0.89       235\n",
      "weighted avg       0.91      0.91      0.91       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8486\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.98      0.85      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.79      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.93      1.00      0.97       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.69      0.75        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.96      0.99      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.80      0.86       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 91.62948822975159 s\n",
      "Averaged - Iteration 208: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8576\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 526.5985582189278\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 39.49849724769592 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5701, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5152, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4793, Accuracy: 0.7939, F1 Micro: 0.8842, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.427, Accuracy: 0.8229, F1 Micro: 0.8979, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3679, Accuracy: 0.8661, F1 Micro: 0.9199, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2922, Accuracy: 0.9174, F1 Micro: 0.9492, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2296, Accuracy: 0.936, F1 Micro: 0.9605, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1966, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1574, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9683\n",
      "Epoch 10/10, Train Loss: 0.124, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9658\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      1.00      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.96      0.99      0.97       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5818, Accuracy: 0.6681, F1 Micro: 0.6681, F1 Macro: 0.4005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4827, Accuracy: 0.834, F1 Micro: 0.834, F1 Macro: 0.7955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2797, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1412, Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9255\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9362, F1 Micro: 0.9362, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9227\n",
      "Epoch 8/10, Train Loss: 0.0499, Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9404, F1 Micro: 0.9404, F1 Macro: 0.9328\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.9362, F1 Micro: 0.9362, F1 Macro: 0.9283\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9404, F1 Micro: 0.9404, F1 Macro: 0.9328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        78\n",
      "    positive       0.96      0.96      0.96       157\n",
      "\n",
      "    accuracy                           0.94       235\n",
      "   macro avg       0.93      0.93      0.93       235\n",
      "weighted avg       0.94      0.94      0.94       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8776\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      1.00      0.97       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.81      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.94      0.93       152\n",
      "    positive       0.81      0.73      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.81      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.96      0.99      0.97       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.77      0.82       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 98.96864676475525 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5771, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5079, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4625, Accuracy: 0.8103, F1 Micro: 0.8923, F1 Macro: 0.8908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3975, Accuracy: 0.8698, F1 Micro: 0.9232, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3286, Accuracy: 0.91, F1 Micro: 0.9449, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2546, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2068, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1768, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1443, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9695\n",
      "Epoch 10/10, Train Loss: 0.1181, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9685\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      1.00      0.97       175\n",
      "      others       0.91      0.93      0.92       158\n",
      "        part       0.95      1.00      0.97       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5985, Accuracy: 0.6624, F1 Micro: 0.6624, F1 Macro: 0.3985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.447, Accuracy: 0.8481, F1 Micro: 0.8481, F1 Macro: 0.8127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2394, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.9044\n",
      "Epoch 4/10, Train Loss: 0.0844, Accuracy: 0.9072, F1 Micro: 0.9072, F1 Macro: 0.9006\n",
      "Epoch 5/10, Train Loss: 0.0919, Accuracy: 0.8819, F1 Micro: 0.8819, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9441\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9392\n",
      "Epoch 8/10, Train Loss: 0.0555, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9245\n",
      "Epoch 9/10, Train Loss: 0.0339, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9266\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9344\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        80\n",
      "    positive       0.97      0.95      0.96       157\n",
      "\n",
      "    accuracy                           0.95       237\n",
      "   macro avg       0.94      0.95      0.94       237\n",
      "weighted avg       0.95      0.95      0.95       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.896\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.94      1.00      0.97       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.86      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.94      1.00      0.97       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.87      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 97.52587604522705 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5749, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5146, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4817, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4441, Accuracy: 0.8073, F1 Micro: 0.8909, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3894, Accuracy: 0.8891, F1 Micro: 0.9335, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3069, Accuracy: 0.9301, F1 Micro: 0.9569, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2392, Accuracy: 0.9427, F1 Micro: 0.9645, F1 Macro: 0.9633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2079, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1646, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.968\n",
      "Epoch 10/10, Train Loss: 0.1278, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9666\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.94      1.00      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6205, Accuracy: 0.6638, F1 Micro: 0.6638, F1 Macro: 0.399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4497, Accuracy: 0.8511, F1 Micro: 0.8511, F1 Macro: 0.8375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3418, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9187\n",
      "Epoch 5/10, Train Loss: 0.088, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9122\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.8936, F1 Micro: 0.8936, F1 Macro: 0.8763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0663, Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9251\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.893\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9202\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9035\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.9251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        79\n",
      "    positive       0.97      0.93      0.95       156\n",
      "\n",
      "    accuracy                           0.93       235\n",
      "   macro avg       0.92      0.93      0.93       235\n",
      "weighted avg       0.93      0.93      0.93       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8822\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.90      0.95      0.92       152\n",
      "    positive       0.84      0.69      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.94      1.00      0.97       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 97.08960342407227 s\n",
      "Averaged - Iteration 274: Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.8853\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 573.8661227958931\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 35.01092290878296 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5621, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.497, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4497, Accuracy: 0.8266, F1 Micro: 0.8994, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3986, Accuracy: 0.8705, F1 Micro: 0.9225, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2956, Accuracy: 0.9249, F1 Micro: 0.9535, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2353, Accuracy: 0.9405, F1 Micro: 0.9626, F1 Macro: 0.961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1659, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9705\n",
      "Epoch 8/10, Train Loss: 0.1432, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.9676\n",
      "Epoch 9/10, Train Loss: 0.115, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9669\n",
      "Epoch 10/10, Train Loss: 0.0952, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9688\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5577, Accuracy: 0.6811, F1 Micro: 0.6811, F1 Macro: 0.4052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4012, Accuracy: 0.8976, F1 Micro: 0.8976, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2477, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1881, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9503\n",
      "Epoch 7/10, Train Loss: 0.1004, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9366\n",
      "Epoch 8/10, Train Loss: 0.0555, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9251\n",
      "Epoch 9/10, Train Loss: 0.0975, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9366\n",
      "Epoch 10/10, Train Loss: 0.0798, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9293\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.94      0.93        81\n",
      "    positive       0.97      0.97      0.97       173\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.95      0.95      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8991\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.78      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.96      0.96        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.88      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 102.73628568649292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5694, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4912, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4323, Accuracy: 0.8452, F1 Micro: 0.9103, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.359, Accuracy: 0.9018, F1 Micro: 0.9402, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.275, Accuracy: 0.9382, F1 Micro: 0.9612, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.221, Accuracy: 0.9405, F1 Micro: 0.9626, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1593, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9709\n",
      "Epoch 8/10, Train Loss: 0.1338, Accuracy: 0.9531, F1 Micro: 0.9705, F1 Macro: 0.969\n",
      "Epoch 9/10, Train Loss: 0.1109, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9704\n",
      "Epoch 10/10, Train Loss: 0.0944, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9697\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6235, Accuracy: 0.668, F1 Micro: 0.668, F1 Macro: 0.4005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4337, Accuracy: 0.9016, F1 Micro: 0.9016, F1 Macro: 0.8898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2775, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1495, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9116\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9345\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9412\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9212\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.97      0.96      0.97       163\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.95      0.95      0.95       244\n",
      "weighted avg       0.96      0.95      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9027\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.94      0.93       152\n",
      "    positive       0.81      0.75      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.81      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 99.71839714050293 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5694, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5027, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4642, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4185, Accuracy: 0.8631, F1 Micro: 0.9194, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.322, Accuracy: 0.933, F1 Micro: 0.9588, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2463, Accuracy: 0.9449, F1 Micro: 0.9656, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1796, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Epoch 8/10, Train Loss: 0.1473, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "Epoch 9/10, Train Loss: 0.1158, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9682\n",
      "Epoch 10/10, Train Loss: 0.0965, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5995, Accuracy: 0.6624, F1 Micro: 0.6624, F1 Macro: 0.3985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4256, Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2532, Accuracy: 0.9487, F1 Micro: 0.9487, F1 Macro: 0.9419\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1477, Accuracy: 0.9487, F1 Micro: 0.9487, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.953, F1 Micro: 0.953, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9569\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9359, F1 Micro: 0.9359, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0881, Accuracy: 0.953, F1 Micro: 0.953, F1 Macro: 0.9485\n",
      "Epoch 10/10, Train Loss: 0.0819, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9335\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.94      0.94        79\n",
      "    positive       0.97      0.97      0.97       155\n",
      "\n",
      "    accuracy                           0.96       234\n",
      "   macro avg       0.96      0.96      0.96       234\n",
      "weighted avg       0.96      0.96      0.96       234\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.906\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.96      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 102.35845470428467 s\n",
      "Averaged - Iteration 333: Accuracy: 0.9501, F1 Micro: 0.9501, F1 Macro: 0.9026\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 543.6367915646756\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 33.57900953292847 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5636, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4882, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4348, Accuracy: 0.8318, F1 Micro: 0.9027, F1 Macro: 0.9011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3526, Accuracy: 0.9025, F1 Micro: 0.9406, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2516, Accuracy: 0.9375, F1 Micro: 0.961, F1 Macro: 0.9589\n",
      "Epoch 6/10, Train Loss: 0.1968, Accuracy: 0.9353, F1 Micro: 0.9594, F1 Macro: 0.9556\n",
      "Epoch 7/10, Train Loss: 0.1516, Accuracy: 0.936, F1 Micro: 0.9597, F1 Macro: 0.9553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1199, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.099, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9686\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      1.00      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5629, Accuracy: 0.6875, F1 Micro: 0.6875, F1 Macro: 0.4304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.399, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Epoch 3/10, Train Loss: 0.2262, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 5/10, Train Loss: 0.107, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1231, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9472\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9329\n",
      "Epoch 9/10, Train Loss: 0.095, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9143\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9345\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.96      0.95       256\n",
      "weighted avg       0.96      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9027\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      1.00      0.97       167\n",
      "    positive       0.96      0.76      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.75      0.83      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 110.66271376609802 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5705, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4842, Accuracy: 0.8021, F1 Micro: 0.8883, F1 Macro: 0.8868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4108, Accuracy: 0.8869, F1 Micro: 0.9327, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3177, Accuracy: 0.9129, F1 Micro: 0.9466, F1 Macro: 0.9454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2286, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1817, Accuracy: 0.9479, F1 Micro: 0.9673, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1459, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1127, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0955, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0804, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6034, Accuracy: 0.6957, F1 Micro: 0.6957, F1 Macro: 0.4556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3641, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.917\n",
      "Epoch 3/10, Train Loss: 0.1926, Accuracy: 0.9012, F1 Micro: 0.9012, F1 Macro: 0.893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1575, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1128, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Epoch 8/10, Train Loss: 0.1017, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0628, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        81\n",
      "    positive       0.99      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.96      0.95       253\n",
      "weighted avg       0.96      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.908\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 116.40661668777466 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5623, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4899, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4457, Accuracy: 0.8147, F1 Micro: 0.8945, F1 Macro: 0.893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3676, Accuracy: 0.8973, F1 Micro: 0.9383, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2624, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2005, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9643\n",
      "Epoch 7/10, Train Loss: 0.1564, Accuracy: 0.9457, F1 Micro: 0.9658, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1246, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1004, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "Epoch 10/10, Train Loss: 0.0815, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5813, Accuracy: 0.8254, F1 Micro: 0.8254, F1 Macro: 0.7805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3796, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2371, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 4/10, Train Loss: 0.1846, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9177\n",
      "Epoch 5/10, Train Loss: 0.1946, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9177\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.101, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9375\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.087, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9234\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       170\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.95      0.94       252\n",
      "weighted avg       0.95      0.94      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9062\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 110.97281575202942 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9527, F1 Micro: 0.9527, F1 Macro: 0.9056\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 588.0324315682021\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 31.708819150924683 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.484, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.429, Accuracy: 0.8467, F1 Micro: 0.9096, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3288, Accuracy: 0.9189, F1 Micro: 0.9498, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2421, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1886, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9704\n",
      "Epoch 7/10, Train Loss: 0.142, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9697\n",
      "Epoch 8/10, Train Loss: 0.1111, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9667\n",
      "Epoch 9/10, Train Loss: 0.0944, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0766, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6034, Accuracy: 0.6759, F1 Micro: 0.6759, F1 Macro: 0.4033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3895, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2275, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1958, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.091, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9465\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.936\n",
      "Epoch 10/10, Train Loss: 0.0365, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9291\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        82\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9097\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 116.78007555007935 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5555, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4745, Accuracy: 0.8028, F1 Micro: 0.8888, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4, Accuracy: 0.8914, F1 Micro: 0.9346, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2959, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2159, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1735, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1405, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9746\n",
      "Epoch 8/10, Train Loss: 0.1057, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.1004, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Epoch 10/10, Train Loss: 0.078, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9717\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.57, Accuracy: 0.6761, F1 Micro: 0.6761, F1 Macro: 0.4034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3649, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.191, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1229, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9496\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9496\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9365\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9493\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.94      0.93        80\n",
      "    positive       0.97      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.95      0.95      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9051\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.94      0.83      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.84      0.86       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 118.45199632644653 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5493, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4906, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4442, Accuracy: 0.8318, F1 Micro: 0.9032, F1 Macro: 0.9017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3477, Accuracy: 0.9211, F1 Micro: 0.9516, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2555, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1942, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9735\n",
      "Epoch 7/10, Train Loss: 0.1489, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.97\n",
      "Epoch 8/10, Train Loss: 0.114, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1018, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9743\n",
      "Epoch 10/10, Train Loss: 0.0783, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9703\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5986, Accuracy: 0.8859, F1 Micro: 0.8859, F1 Macro: 0.8612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3592, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.944\n",
      "Epoch 4/10, Train Loss: 0.2039, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Epoch 7/10, Train Loss: 0.1109, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9399\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9338\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.92        82\n",
      "    positive       0.99      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.96      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9127\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.89      0.88       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 113.40933394432068 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9092\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 574.8689221861491\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 29.984111547470093 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4819, Accuracy: 0.8036, F1 Micro: 0.8887, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4185, Accuracy: 0.8549, F1 Micro: 0.9148, F1 Macro: 0.9135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3187, Accuracy: 0.9204, F1 Micro: 0.9503, F1 Macro: 0.9471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.241, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9681\n",
      "Epoch 6/10, Train Loss: 0.177, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1284, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1029, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0849, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5558, Accuracy: 0.7701, F1 Micro: 0.7701, F1 Macro: 0.659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3703, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1943, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Epoch 5/10, Train Loss: 0.1621, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "Epoch 7/10, Train Loss: 0.0715, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9274\n",
      "Epoch 8/10, Train Loss: 0.1028, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0899, Accuracy: 0.9579, F1 Micro: 0.9579, F1 Macro: 0.9519\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9424\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9579, F1 Micro: 0.9579, F1 Macro: 0.9519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        83\n",
      "    positive       0.98      0.96      0.97       178\n",
      "\n",
      "    accuracy                           0.96       261\n",
      "   macro avg       0.95      0.96      0.95       261\n",
      "weighted avg       0.96      0.96      0.96       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.67871236801147 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5528, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4707, Accuracy: 0.8289, F1 Micro: 0.902, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3758, Accuracy: 0.9062, F1 Micro: 0.943, F1 Macro: 0.9423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2785, Accuracy: 0.9308, F1 Micro: 0.9568, F1 Macro: 0.9553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2144, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1607, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0974, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.0854, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5868, Accuracy: 0.6772, F1 Micro: 0.6772, F1 Macro: 0.4038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3559, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "Epoch 4/10, Train Loss: 0.1844, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "Epoch 7/10, Train Loss: 0.0622, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9388\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        82\n",
      "    positive       0.99      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.94      0.96      0.95       254\n",
      "weighted avg       0.96      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9205\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 122.91140913963318 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5486, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4839, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4263, Accuracy: 0.8579, F1 Micro: 0.9169, F1 Macro: 0.9159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3326, Accuracy: 0.9368, F1 Micro: 0.9611, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2487, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1746, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1339, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Epoch 8/10, Train Loss: 0.1061, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5553, Accuracy: 0.8706, F1 Micro: 0.8706, F1 Macro: 0.8531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3432, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.252, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9507\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0855, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9599\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9362\n",
      "Epoch 9/10, Train Loss: 0.0927, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9418\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9418\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.96      0.95        82\n",
      "    positive       0.98      0.97      0.97       173\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.96      0.96      0.96       255\n",
      "weighted avg       0.97      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9225\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.38170123100281 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9212\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 604.5587807191453\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 25.62284803390503 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4829, Accuracy: 0.8043, F1 Micro: 0.8889, F1 Macro: 0.8871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3888, Accuracy: 0.8847, F1 Micro: 0.9301, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2641, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1852, Accuracy: 0.9479, F1 Micro: 0.9673, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1376, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5646, Accuracy: 0.7326, F1 Micro: 0.7326, F1 Macro: 0.5688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3012, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9302\n",
      "Epoch 3/10, Train Loss: 0.238, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.922\n",
      "Epoch 4/10, Train Loss: 0.1399, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9215\n",
      "Epoch 5/10, Train Loss: 0.1057, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9106\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9183\n",
      "Epoch 8/10, Train Loss: 0.0782, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9224\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9247\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        82\n",
      "    positive       0.98      0.93      0.95       176\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.92      0.94      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9024\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.81      0.85      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 127.12264347076416 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5428, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4602, Accuracy: 0.8445, F1 Micro: 0.9101, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3425, Accuracy: 0.9174, F1 Micro: 0.9494, F1 Macro: 0.9486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2353, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1704, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1292, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9792\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.97      1.00      0.98       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5905, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3604, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2601, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1546, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9336\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8942\n",
      "Epoch 7/10, Train Loss: 0.0839, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0382, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        84\n",
      "    positive       0.97      0.94      0.95       168\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.93       252\n",
      "weighted avg       0.94      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9141\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      1.00      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.75      0.88      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 138.50308322906494 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5441, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4914, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4021, Accuracy: 0.9018, F1 Micro: 0.9408, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2709, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1905, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1449, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.579, Accuracy: 0.86, F1 Micro: 0.86, F1 Macro: 0.8224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3303, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9412\n",
      "Epoch 4/10, Train Loss: 0.1589, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9291\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9269\n",
      "Epoch 6/10, Train Loss: 0.0881, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0775, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9459\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9365\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        82\n",
      "    positive       0.97      0.96      0.96       168\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.94      0.95      0.95       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9124\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.47943234443665 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9096\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 607.3364050054687\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 22.728342294692993 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5397, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4686, Accuracy: 0.8095, F1 Micro: 0.892, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3859, Accuracy: 0.8951, F1 Micro: 0.9366, F1 Macro: 0.9348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2724, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1954, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1079, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5566, Accuracy: 0.8702, F1 Micro: 0.8702, F1 Macro: 0.846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3247, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9039\n",
      "Epoch 9/10, Train Loss: 0.0825, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        84\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.92      0.94      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9161\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.44021248817444 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4483, Accuracy: 0.8557, F1 Micro: 0.9157, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.35, Accuracy: 0.9323, F1 Micro: 0.9582, F1 Macro: 0.9569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2464, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1834, Accuracy: 0.9576, F1 Micro: 0.9737, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1431, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0895, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.677, F1 Micro: 0.677, F1 Macro: 0.4037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3828, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9297\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.9027, F1 Micro: 0.9027, F1 Macro: 0.8949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.943\n",
      "Epoch 5/10, Train Loss: 0.1481, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "Epoch 6/10, Train Loss: 0.1179, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9377\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9347\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        83\n",
      "    positive       0.98      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9172\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.62798857688904 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5467, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4741, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.395, Accuracy: 0.9122, F1 Micro: 0.947, F1 Macro: 0.9457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.278, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1995, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.9722\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1109, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9716\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5582, Accuracy: 0.8784, F1 Micro: 0.8784, F1 Macro: 0.8519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3181, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1768, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1352, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9174\n",
      "Epoch 7/10, Train Loss: 0.0993, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9164\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.9005\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9248\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9243\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        81\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9098\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.9722876548767 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9143\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 607.1275235681286\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 22.093350887298584 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4797, Accuracy: 0.8095, F1 Micro: 0.8907, F1 Macro: 0.8888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3785, Accuracy: 0.8996, F1 Micro: 0.9394, F1 Macro: 0.938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.274, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1976, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0826, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.975\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5519, Accuracy: 0.7977, F1 Micro: 0.7977, F1 Macro: 0.7105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3121, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1842, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9093\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9144, F1 Micro: 0.9144, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1486, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9222\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9255\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        82\n",
      "    positive       0.97      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.93       257\n",
      "   macro avg       0.92      0.94      0.93       257\n",
      "weighted avg       0.94      0.93      0.93       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9066\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.92758345603943 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5394, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4475, Accuracy: 0.8542, F1 Micro: 0.9152, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3333, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2408, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1804, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0607, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.976\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.99      1.00      0.99       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5741, Accuracy: 0.8794, F1 Micro: 0.8794, F1 Macro: 0.8558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2945, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2481, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9476\n",
      "Epoch 5/10, Train Loss: 0.1115, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9423\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "Epoch 8/10, Train Loss: 0.0544, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9434\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        83\n",
      "    positive       0.98      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.96      0.95       257\n",
      "weighted avg       0.96      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9237\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.99      1.00      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.94      0.93       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.28161811828613 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4838, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3929, Accuracy: 0.91, F1 Micro: 0.9456, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.274, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1969, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1155, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.93      0.92       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5835, Accuracy: 0.8923, F1 Micro: 0.8923, F1 Macro: 0.8761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.275, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1743, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9155\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0576, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9437\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.916\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.023508310318 s\n",
      "Averaged - Iteration 573: Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9154\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 588.8982562845845\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 20.435794591903687 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5342, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4626, Accuracy: 0.817, F1 Micro: 0.8948, F1 Macro: 0.893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3551, Accuracy: 0.9256, F1 Micro: 0.9543, F1 Macro: 0.9524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2392, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.168, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1354, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0607, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5129, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3037, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2271, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9143\n",
      "Epoch 4/10, Train Loss: 0.1782, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Epoch 5/10, Train Loss: 0.1712, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9256\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9178\n",
      "Epoch 10/10, Train Loss: 0.0826, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        83\n",
      "    positive       0.96      0.94      0.95       172\n",
      "\n",
      "    accuracy                           0.94       255\n",
      "   macro avg       0.92      0.93      0.93       255\n",
      "weighted avg       0.94      0.94      0.94       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9079\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.8572371006012 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4434, Accuracy: 0.8616, F1 Micro: 0.9191, F1 Macro: 0.9187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3193, Accuracy: 0.9308, F1 Micro: 0.9568, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2183, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1564, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.974\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5476, Accuracy: 0.6757, F1 Micro: 0.6757, F1 Macro: 0.4251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3168, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9166\n",
      "Epoch 3/10, Train Loss: 0.1921, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1668, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Epoch 5/10, Train Loss: 0.1809, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9042\n",
      "Epoch 6/10, Train Loss: 0.1077, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1003, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        85\n",
      "    positive       0.97      0.94      0.95       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.92      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9187\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.99      0.99      0.99       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.16436505317688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5353, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4721, Accuracy: 0.8058, F1 Micro: 0.8904, F1 Macro: 0.889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3665, Accuracy: 0.936, F1 Micro: 0.9605, F1 Macro: 0.9593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.243, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1713, Accuracy: 0.9606, F1 Micro: 0.9755, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1367, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0688, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5509, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3273, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2102, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1674, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9351\n",
      "Epoch 5/10, Train Loss: 0.1616, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9157\n",
      "Epoch 6/10, Train Loss: 0.1132, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9113\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9073\n",
      "Epoch 8/10, Train Loss: 0.0758, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.926\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9314\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        84\n",
      "    positive       0.97      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.94      0.94       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9157\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 138.8937451839447 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9141\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 601.6374916431563\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 18.47448968887329 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4611, Accuracy: 0.8311, F1 Micro: 0.9023, F1 Macro: 0.9006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3527, Accuracy: 0.9241, F1 Micro: 0.9527, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.238, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.165, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0793, Accuracy: 0.965, F1 Micro: 0.9782, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0701, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5284, Accuracy: 0.8687, F1 Micro: 0.8687, F1 Macro: 0.8365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2707, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Epoch 3/10, Train Loss: 0.2331, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8998\n",
      "Epoch 9/10, Train Loss: 0.0986, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9149\n",
      "Epoch 10/10, Train Loss: 0.0592, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9099\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        84\n",
      "    positive       0.99      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9214\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.84      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.67410612106323 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5393, Accuracy: 0.7932, F1 Micro: 0.8839, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4306, Accuracy: 0.8683, F1 Micro: 0.922, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3151, Accuracy: 0.9368, F1 Micro: 0.9603, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2161, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.8471, F1 Micro: 0.8471, F1 Macro: 0.8054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2883, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.8991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1956, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1736, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 5/10, Train Loss: 0.1449, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1384, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "Epoch 7/10, Train Loss: 0.1213, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9261\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        83\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9204\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.1700038909912 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5419, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4703, Accuracy: 0.8147, F1 Micro: 0.8948, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3595, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2379, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1606, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0993, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5529, Accuracy: 0.8798, F1 Micro: 0.8798, F1 Macro: 0.8619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3291, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2242, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.921\n",
      "Epoch 4/10, Train Loss: 0.1685, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9035\n",
      "Epoch 5/10, Train Loss: 0.1618, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Epoch 6/10, Train Loss: 0.1055, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1315, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9196\n",
      "Epoch 10/10, Train Loss: 0.0698, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9057\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9196\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        85\n",
      "    positive       0.93      0.97      0.95       173\n",
      "\n",
      "    accuracy                           0.93       258\n",
      "   macro avg       0.93      0.91      0.92       258\n",
      "weighted avg       0.93      0.93      0.93       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9035\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.26277780532837 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9573, F1 Micro: 0.9573, F1 Macro: 0.9151\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 595.7518139216006\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 16.989439964294434 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5344, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4527, Accuracy: 0.8311, F1 Micro: 0.9016, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3409, Accuracy: 0.9196, F1 Micro: 0.9499, F1 Macro: 0.9457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2271, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1267, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1011, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0792, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5192, Accuracy: 0.8972, F1 Micro: 0.8972, F1 Macro: 0.8879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3039, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1933, Accuracy: 0.9605, F1 Micro: 0.9605, F1 Macro: 0.9554\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9177\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9299\n",
      "Epoch 7/10, Train Loss: 0.1198, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9338\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9299\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9341\n",
      "Epoch 10/10, Train Loss: 0.1034, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9345\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9605, F1 Micro: 0.9605, F1 Macro: 0.9554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        83\n",
      "    positive       0.98      0.96      0.97       170\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.96       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9229\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.90      0.94        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 148.11943435668945 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4322, Accuracy: 0.8735, F1 Micro: 0.925, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3124, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2092, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 7/10, Train Loss: 0.1028, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5741, Accuracy: 0.8263, F1 Micro: 0.8263, F1 Macro: 0.7767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3494, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2361, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1248, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "Epoch 5/10, Train Loss: 0.1644, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9082\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8998\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 10/10, Train Loss: 0.086, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9121\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9266\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 148.82996559143066 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5348, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4676, Accuracy: 0.8214, F1 Micro: 0.8983, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3535, Accuracy: 0.939, F1 Micro: 0.9619, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2292, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1624, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1264, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.068, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5621, Accuracy: 0.878, F1 Micro: 0.878, F1 Macro: 0.8649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3388, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2234, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "Epoch 4/10, Train Loss: 0.148, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9267\n",
      "Epoch 5/10, Train Loss: 0.1825, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9304\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9067\n",
      "Epoch 8/10, Train Loss: 0.1039, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9107\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 10/10, Train Loss: 0.0851, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8982\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.96      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.94      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9197\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.84      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.88      0.85      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.39774799346924 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9231\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 576.234575883472\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 16.224570751190186 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5358, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4568, Accuracy: 0.843, F1 Micro: 0.9057, F1 Macro: 0.9022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3183, Accuracy: 0.9435, F1 Micro: 0.9649, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2106, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9729\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5163, Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8921\n",
      "Epoch 2/10, Train Loss: 0.2854, Accuracy: 0.8864, F1 Micro: 0.8864, F1 Macro: 0.8783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 4/10, Train Loss: 0.1404, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "Epoch 7/10, Train Loss: 0.1118, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "Epoch 9/10, Train Loss: 0.0818, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.91        85\n",
      "    positive       0.98      0.93      0.95       179\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9112\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.87      0.91      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.05292177200317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5397, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4314, Accuracy: 0.8914, F1 Micro: 0.9338, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2868, Accuracy: 0.9487, F1 Micro: 0.9679, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1975, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.087, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0635, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5358, Accuracy: 0.8949, F1 Micro: 0.8949, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.269, Accuracy: 0.8988, F1 Micro: 0.8988, F1 Macro: 0.8924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1887, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1974, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1572, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9343\n",
      "Epoch 6/10, Train Loss: 0.1116, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Epoch 7/10, Train Loss: 0.1161, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8989\n",
      "Epoch 10/10, Train Loss: 0.0744, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9264\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        85\n",
      "    positive       0.96      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9199\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 156.19432425498962 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4642, Accuracy: 0.8452, F1 Micro: 0.9091, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3227, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2162, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1573, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Epoch 8/10, Train Loss: 0.0753, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0633, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5679, Accuracy: 0.8863, F1 Micro: 0.8863, F1 Macro: 0.8758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2669, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2076, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9141\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1555, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "Epoch 6/10, Train Loss: 0.1514, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.915\n",
      "Epoch 8/10, Train Loss: 0.1101, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9065\n",
      "Epoch 9/10, Train Loss: 0.0931, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.915\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        85\n",
      "    positive       0.95      0.95      0.95       170\n",
      "\n",
      "    accuracy                           0.94       255\n",
      "   macro avg       0.93      0.93      0.93       255\n",
      "weighted avg       0.94      0.94      0.94       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.80      0.73      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.99      0.99      0.99       152\n",
      "    positive       0.93      0.90      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 149.61300015449524 s\n",
      "Averaged - Iteration 673: Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.914\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 631.569714239088\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 13.674556970596313 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5355, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4657, Accuracy: 0.8534, F1 Micro: 0.9125, F1 Macro: 0.9104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3277, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1577, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1158, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5866, Accuracy: 0.8808, F1 Micro: 0.8808, F1 Macro: 0.8665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.9\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2064, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1668, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9434\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9159\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.907\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9226\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.90      0.90      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.81214332580566 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5378, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4339, Accuracy: 0.8966, F1 Micro: 0.9375, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2929, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1972, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "Epoch 8/10, Train Loss: 0.0754, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5052, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2449, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9125\n",
      "Epoch 3/10, Train Loss: 0.1878, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1273, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1109, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9121\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9429\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9082\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        86\n",
      "    positive       0.97      0.97      0.97       173\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.95      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9301\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.90      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.00569081306458 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5391, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4775, Accuracy: 0.8296, F1 Micro: 0.9016, F1 Macro: 0.9\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.34, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2184, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9749\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0766, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.99      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5549, Accuracy: 0.8864, F1 Micro: 0.8864, F1 Macro: 0.8755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2823, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 3/10, Train Loss: 0.2014, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1541, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9197\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9074\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9013\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9018\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9087\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0734, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9237\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        86\n",
      "    positive       0.97      0.93      0.95       178\n",
      "\n",
      "    accuracy                           0.93       264\n",
      "   macro avg       0.92      0.93      0.92       264\n",
      "weighted avg       0.93      0.93      0.93       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9151\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.92      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 151.62478852272034 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9226\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 613.0610211339073\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 12.514368057250977 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4383, Accuracy: 0.8795, F1 Micro: 0.927, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2976, Accuracy: 0.9442, F1 Micro: 0.9655, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9771\n",
      "Epoch 7/10, Train Loss: 0.0857, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9739\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5343, Accuracy: 0.888, F1 Micro: 0.888, F1 Macro: 0.8781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1804, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1283, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1048, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9217\n",
      "Epoch 6/10, Train Loss: 0.0729, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8993\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9225\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        85\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.92      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9158\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.21325039863586 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5345, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4108, Accuracy: 0.9152, F1 Micro: 0.948, F1 Macro: 0.9458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2721, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0851, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5075, Accuracy: 0.8851, F1 Micro: 0.8851, F1 Macro: 0.8741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2464, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1884, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.148, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9579, F1 Micro: 0.9579, F1 Macro: 0.9519\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9273\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9579, F1 Micro: 0.9579, F1 Macro: 0.9519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.93      0.93        85\n",
      "    positive       0.97      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.96       261\n",
      "   macro avg       0.95      0.95      0.95       261\n",
      "weighted avg       0.96      0.96      0.96       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9241\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.99      0.99      0.99       152\n",
      "    positive       0.97      0.93      0.95        41\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.97      0.97       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.27455306053162 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4468, Accuracy: 0.872, F1 Micro: 0.924, F1 Macro: 0.9228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3051, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5123, Accuracy: 0.8843, F1 Micro: 0.8843, F1 Macro: 0.8739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2525, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9298\n",
      "Epoch 3/10, Train Loss: 0.2105, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9065\n",
      "Epoch 4/10, Train Loss: 0.1437, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9452\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Epoch 8/10, Train Loss: 0.0857, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9138\n",
      "Epoch 9/10, Train Loss: 0.0768, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       182\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.94      0.95      0.95       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9261\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 151.9339303970337 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9604, F1 Micro: 0.9604, F1 Macro: 0.922\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 600.3788893394872\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 10.524255990982056 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5277, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4446, Accuracy: 0.8698, F1 Micro: 0.9213, F1 Macro: 0.9187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2908, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1089, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5537, Accuracy: 0.8885, F1 Micro: 0.8885, F1 Macro: 0.8777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1684, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 4/10, Train Loss: 0.1573, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Epoch 5/10, Train Loss: 0.1633, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Epoch 6/10, Train Loss: 0.1108, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Epoch 7/10, Train Loss: 0.092, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9115\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.908\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        85\n",
      "    positive       0.99      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.97962594032288 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.528, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4124, Accuracy: 0.9077, F1 Micro: 0.9432, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2577, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1831, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1089, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.8092, F1 Micro: 0.8092, F1 Macro: 0.7416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2767, Accuracy: 0.8702, F1 Micro: 0.8702, F1 Macro: 0.8635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.238, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1705, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9123\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9079\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9429\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "Epoch 10/10, Train Loss: 0.0937, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.90      0.92        86\n",
      "    positive       0.95      0.98      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.95      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9211\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.99      0.99      0.99       152\n",
      "    positive       0.93      0.93      0.93        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.46748208999634 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4519, Accuracy: 0.8713, F1 Micro: 0.923, F1 Macro: 0.9214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2937, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2006, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1395, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5073, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2413, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.8947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.158, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "Epoch 4/10, Train Loss: 0.1463, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1057, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9566\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        86\n",
      "    positive       0.98      0.97      0.97       172\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9274\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.66563296318054 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9604, F1 Micro: 0.9604, F1 Macro: 0.924\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 486.7773311335749\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 8.070528507232666 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5236, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4349, Accuracy: 0.8884, F1 Micro: 0.9322, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2906, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1909, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4951, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2661, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "Epoch 3/10, Train Loss: 0.154, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9317\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9471\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9399\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9399\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9443\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.92      0.93        86\n",
      "    positive       0.96      0.97      0.96       169\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.95      0.94      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9249\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 162.81253671646118 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5271, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4011, Accuracy: 0.9263, F1 Micro: 0.9546, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.266, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1782, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9769\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0848, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9781\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.46, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2273, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.18, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9418\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9176\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9169\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "Epoch 9/10, Train Loss: 0.0884, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        87\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9312\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.88      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.55444025993347 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5289, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.443, Accuracy: 0.8958, F1 Micro: 0.9375, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2943, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1942, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9729\n",
      "Epoch 5/10, Train Loss: 0.1328, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1065, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4921, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Epoch 2/10, Train Loss: 0.2078, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9196\n",
      "Epoch 3/10, Train Loss: 0.1712, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9229\n",
      "Epoch 4/10, Train Loss: 0.109, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.927\n",
      "Epoch 5/10, Train Loss: 0.1478, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9085\n",
      "Epoch 6/10, Train Loss: 0.1348, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9107\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9263\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9144\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9226\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        86\n",
      "    positive       0.95      0.95      0.95       168\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.93      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9154\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.83      0.88        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.83067178726196 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9238\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 434.62976804679374\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 7.356229066848755 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5253, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.43, Accuracy: 0.9018, F1 Micro: 0.9405, F1 Macro: 0.9383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2841, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.975\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4584, Accuracy: 0.8918, F1 Micro: 0.8918, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1931, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9381\n",
      "Epoch 4/10, Train Loss: 0.1409, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9338\n",
      "Epoch 5/10, Train Loss: 0.1469, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9341\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9263\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "Epoch 9/10, Train Loss: 0.0685, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9338\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.3543975353241 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.7976, F1 Micro: 0.8863, F1 Macro: 0.8848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4069, Accuracy: 0.9174, F1 Micro: 0.9495, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2581, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0548, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4825, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9039\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.8969, F1 Micro: 0.8969, F1 Macro: 0.8893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1862, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9439\n",
      "Epoch 4/10, Train Loss: 0.1329, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Epoch 5/10, Train Loss: 0.0892, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9253\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9387\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9525\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.96      0.97       178\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.96      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9298\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.61714220046997 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5281, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4478, Accuracy: 0.8891, F1 Micro: 0.9337, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2875, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1924, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5199, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9105\n",
      "Epoch 3/10, Train Loss: 0.1746, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1431, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9272\n",
      "Epoch 5/10, Train Loss: 0.1381, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9033\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9109\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9224\n",
      "Epoch 8/10, Train Loss: 0.1143, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9179\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.94        85\n",
      "    positive       0.98      0.96      0.97       168\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9235\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 164.86532044410706 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9263\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 353.08585002729114\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.697381258010864 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.526, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4192, Accuracy: 0.9092, F1 Micro: 0.9437, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2657, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0623, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4692, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2245, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9447\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Epoch 4/10, Train Loss: 0.1436, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 5/10, Train Loss: 0.1093, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9435\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9121\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        85\n",
      "    positive       0.97      0.96      0.96       176\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.94      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9208\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.15240669250488 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5293, Accuracy: 0.7954, F1 Micro: 0.8851, F1 Macro: 0.8835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3827, Accuracy: 0.9338, F1 Micro: 0.9585, F1 Macro: 0.9571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0924, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0635, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9725, F1 Micro: 0.9826, F1 Macro: 0.9816\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9826, F1 Macro: 0.9816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5446, Accuracy: 0.8383, F1 Micro: 0.8383, F1 Macro: 0.7886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2745, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1882, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1596, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9313\n",
      "Epoch 6/10, Train Loss: 0.1231, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9244\n",
      "Epoch 7/10, Train Loss: 0.1086, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9209\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.92        87\n",
      "    positive       0.97      0.95      0.96       179\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.94      0.94       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.8255331516266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5317, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4344, Accuracy: 0.9226, F1 Micro: 0.9524, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2708, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1788, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9776\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.483, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2396, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Epoch 3/10, Train Loss: 0.1746, Accuracy: 0.9071, F1 Micro: 0.9071, F1 Macro: 0.8928\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9152\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9339\n",
      "Epoch 6/10, Train Loss: 0.1302, Accuracy: 0.9033, F1 Micro: 0.9033, F1 Macro: 0.8965\n",
      "Epoch 7/10, Train Loss: 0.0855, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9257\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9071\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9244\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        88\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9233\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      1.00      0.83        12\n",
      "     neutral       0.95      0.94      0.95       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.91      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.37767696380615 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9604, F1 Micro: 0.9604, F1 Macro: 0.9231\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 243.61962631382667\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.214489698410034 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4208, Accuracy: 0.8958, F1 Micro: 0.936, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2806, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0495, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9786\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4929, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.155, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1342, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9259\n",
      "Epoch 5/10, Train Loss: 0.0951, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9186\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9186\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9299, F1 Micro: 0.9299, F1 Macro: 0.9224\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9299, F1 Micro: 0.9299, F1 Macro: 0.9212\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9299, F1 Micro: 0.9299, F1 Macro: 0.9224\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.9299, F1 Micro: 0.9299, F1 Macro: 0.9224\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        88\n",
      "    positive       0.97      0.93      0.95       183\n",
      "\n",
      "    accuracy                           0.93       271\n",
      "   macro avg       0.92      0.94      0.93       271\n",
      "weighted avg       0.94      0.93      0.93       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9202\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.88      0.85       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 180.19174885749817 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5296, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.383, Accuracy: 0.9271, F1 Micro: 0.9547, F1 Macro: 0.9534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2585, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.129, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9753\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.493, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1942, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1569, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.146, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9157\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Epoch 7/10, Train Loss: 0.0943, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9338\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9049\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9044\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.89      0.91        85\n",
      "    positive       0.95      0.97      0.96       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.94      0.93      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9209\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.88      0.90      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 178.5526614189148 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5365, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4276, Accuracy: 0.9115, F1 Micro: 0.9456, F1 Macro: 0.9432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2782, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1755, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1053, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0801, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9757\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4706, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2197, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1451, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1119, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.921\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        86\n",
      "    positive       0.97      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.93       261\n",
      "   macro avg       0.92      0.94      0.93       261\n",
      "weighted avg       0.94      0.93      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9107\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 183.26778888702393 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9581, F1 Micro: 0.9581, F1 Macro: 0.9173\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 156.3880834328198\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.22676682472229 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5193, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4128, Accuracy: 0.907, F1 Micro: 0.9436, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2694, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1216, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9767\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4451, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.215, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9185\n",
      "Epoch 3/10, Train Loss: 0.1983, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1435, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9378\n",
      "Epoch 6/10, Train Loss: 0.1153, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9142\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0665, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9259\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9276\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 177.62071061134338 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5159, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3785, Accuracy: 0.9375, F1 Micro: 0.9615, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2483, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5093, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2413, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "Epoch 3/10, Train Loss: 0.1812, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 6/10, Train Loss: 0.1225, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Epoch 8/10, Train Loss: 0.0685, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9138\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 180.93153882026672 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4202, Accuracy: 0.9092, F1 Micro: 0.9448, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2756, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4978, Accuracy: 0.8876, F1 Micro: 0.8876, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.21, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1684, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1234, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0961, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.947\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9411\n",
      "Epoch 10/10, Train Loss: 0.0743, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9473\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        83\n",
      "    positive       0.98      0.96      0.97       166\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.95      0.95      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9215\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 173.65270614624023 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9248\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 37.267055331121256\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.2984402179718018 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5172, Accuracy: 0.7976, F1 Micro: 0.8853, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4022, Accuracy: 0.9174, F1 Micro: 0.9494, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2451, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 6/10, Train Loss: 0.0912, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4849, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2229, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1766, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9454\n",
      "Epoch 4/10, Train Loss: 0.1548, Accuracy: 0.8989, F1 Micro: 0.8989, F1 Macro: 0.8909\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9403\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Epoch 9/10, Train Loss: 0.0652, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9223\n",
      "Epoch 10/10, Train Loss: 0.0571, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9202\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.95       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9336\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.82      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.6520926952362 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5217, Accuracy: 0.8065, F1 Micro: 0.8902, F1 Macro: 0.8888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3677, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0943, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5255, Accuracy: 0.8405, F1 Micro: 0.8405, F1 Macro: 0.8014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2842, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9231\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1333, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1137, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9343\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "Epoch 8/10, Train Loss: 0.0487, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9186\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9038\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.95       170\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.94      0.94       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 179.54118251800537 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5229, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4116, Accuracy: 0.9278, F1 Micro: 0.9557, F1 Macro: 0.9542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2477, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1203, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9712\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4727, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1635, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1559, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0756, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 7/10, Train Loss: 0.0952, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9213\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9237\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 181.57375621795654 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9262\n",
      "Total runtime: 10836.733971595764 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd261a",
   "metadata": {
    "papermill": {
     "duration": 0.012285,
     "end_time": "2025-06-08T19:09:42.567885",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.555600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2c7be50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T19:09:42.592952Z",
     "iopub.status.busy": "2025-06-08T19:09:42.592708Z",
     "iopub.status.idle": "2025-06-08T22:10:47.296997Z",
     "shell.execute_reply": "2025-06-08T22:10:47.296210Z"
    },
    "papermill": {
     "duration": 10864.718382,
     "end_time": "2025-06-08T22:10:47.298419",
     "exception": false,
     "start_time": "2025-06-08T19:09:42.580037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6559, Accuracy: 0.7463, F1 Micro: 0.8496, F1 Macro: 0.8267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5772, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5239, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.476, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4654, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4704, Accuracy: 0.7879, F1 Micro: 0.8808, F1 Macro: 0.8789\n",
      "Epoch 8/10, Train Loss: 0.4263, Accuracy: 0.7827, F1 Micro: 0.8771, F1 Macro: 0.8742\n",
      "Epoch 9/10, Train Loss: 0.4009, Accuracy: 0.7812, F1 Micro: 0.8761, F1 Macro: 0.8731\n",
      "Epoch 10/10, Train Loss: 0.3867, Accuracy: 0.7798, F1 Micro: 0.8749, F1 Macro: 0.8721\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      0.99      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.8458, Accuracy: 0.4, F1 Micro: 0.4, F1 Macro: 0.2857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6455, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4744, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3486, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2564, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2002, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1521, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       0.50      0.50      0.50         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7901, F1 Micro: 0.7901, F1 Macro: 0.2994\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.60      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.44      0.35      0.31       216\n",
      "weighted avg       0.64      0.71      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 59.0232195854187 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6744, Accuracy: 0.7552, F1 Micro: 0.8555, F1 Macro: 0.8479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5918, Accuracy: 0.785, F1 Micro: 0.8794, F1 Macro: 0.8779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5453, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5309, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4879, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4745, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4859, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4599, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4311, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 10/10, Train Loss: 0.3935, Accuracy: 0.7909, F1 Micro: 0.8827, F1 Macro: 0.8811\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6444, Accuracy: 0.8, F1 Micro: 0.8, F1 Macro: 0.4444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5577, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.531, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4857, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4167, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3914, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3635, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2985, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3418, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2611, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       0.50      0.50      0.50         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3025\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.57      0.35      0.31       216\n",
      "weighted avg       0.74      0.72      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.50      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.40      0.34      0.29       216\n",
      "weighted avg       0.59      0.70      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 63.59353280067444 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7408, Accuracy: 0.5982, F1 Micro: 0.7065, F1 Macro: 0.6096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6394, Accuracy: 0.7723, F1 Micro: 0.8672, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.6003, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 4/10, Train Loss: 0.5587, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.5047, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.5075, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.5115, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4764, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 9/10, Train Loss: 0.4545, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4405, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7828, Accuracy: 0.0, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6312, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4728, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3577, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3441, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.307, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2576, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1603, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2043, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1563, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       0.50      0.50      0.50         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7901, F1 Micro: 0.7901, F1 Macro: 0.2958\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 57.30021333694458 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7906, F1 Micro: 0.7906, F1 Macro: 0.2992\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 567.4611538787026\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 22.54823327064514 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5997, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4738, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8826\n",
      "Epoch 4/10, Train Loss: 0.4732, Accuracy: 0.7924, F1 Micro: 0.8806, F1 Macro: 0.8768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4404, Accuracy: 0.7976, F1 Micro: 0.8846, F1 Macro: 0.8819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3982, Accuracy: 0.8043, F1 Micro: 0.8875, F1 Macro: 0.8847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3773, Accuracy: 0.8222, F1 Micro: 0.8963, F1 Macro: 0.8939\n",
      "Epoch 8/10, Train Loss: 0.3295, Accuracy: 0.8237, F1 Micro: 0.895, F1 Macro: 0.8901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2934, Accuracy: 0.8475, F1 Micro: 0.9078, F1 Macro: 0.9027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.243, Accuracy: 0.8728, F1 Micro: 0.922, F1 Macro: 0.9171\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8728, F1 Micro: 0.922, F1 Macro: 0.9171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.88      1.00      0.94       187\n",
      "     machine       0.89      0.97      0.93       175\n",
      "      others       0.91      0.74      0.82       158\n",
      "        part       0.88      0.97      0.92       158\n",
      "       price       0.89      1.00      0.94       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.89      0.95      0.92      1061\n",
      "   macro avg       0.89      0.95      0.92      1061\n",
      "weighted avg       0.89      0.95      0.92      1061\n",
      " samples avg       0.90      0.95      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6048, Accuracy: 0.7685, F1 Micro: 0.7685, F1 Macro: 0.4345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4758, Accuracy: 0.7685, F1 Micro: 0.7685, F1 Macro: 0.4345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3924, Accuracy: 0.8522, F1 Micro: 0.8522, F1 Macro: 0.7748\n",
      "Epoch 4/10, Train Loss: 0.3307, Accuracy: 0.8424, F1 Micro: 0.8424, F1 Macro: 0.7905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1892, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.8818, F1 Micro: 0.8818, F1 Macro: 0.8486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1294, Accuracy: 0.8867, F1 Micro: 0.8867, F1 Macro: 0.8541\n",
      "Epoch 8/10, Train Loss: 0.1277, Accuracy: 0.8522, F1 Micro: 0.8522, F1 Macro: 0.8258\n",
      "Epoch 9/10, Train Loss: 0.0976, Accuracy: 0.8818, F1 Micro: 0.8818, F1 Macro: 0.8566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8739\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.94      0.81        47\n",
      "    positive       0.98      0.89      0.93       156\n",
      "\n",
      "    accuracy                           0.90       203\n",
      "   macro avg       0.85      0.91      0.87       203\n",
      "weighted avg       0.92      0.90      0.91       203\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8657, F1 Micro: 0.8657, F1 Macro: 0.6711\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.89      1.00      0.94       181\n",
      "    positive       1.00      0.46      0.63        24\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.96      0.52      0.58       216\n",
      "weighted avg       0.91      0.89      0.87       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.89      0.98      0.93       167\n",
      "    positive       0.93      0.42      0.58        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.74      0.77       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.91      0.74      0.81       152\n",
      "    positive       0.56      0.77      0.65        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.65      0.78      0.69       216\n",
      "weighted avg       0.80      0.75      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.52      0.65        23\n",
      "     neutral       0.88      0.97      0.92       152\n",
      "    positive       0.82      0.66      0.73        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.85      0.72      0.77       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.08      0.14        13\n",
      "     neutral       0.89      1.00      0.94       186\n",
      "    positive       1.00      0.35      0.52        17\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.96      0.48      0.54       216\n",
      "weighted avg       0.91      0.89      0.86       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       1.00      0.29      0.45        17\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.97      0.60      0.69       216\n",
      "weighted avg       0.92      0.91      0.89       216\n",
      "\n",
      "Total train time: 79.3147521018982 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6344, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5061, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4759, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4823, Accuracy: 0.8021, F1 Micro: 0.888, F1 Macro: 0.8865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4453, Accuracy: 0.8073, F1 Micro: 0.8907, F1 Macro: 0.8893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3941, Accuracy: 0.8214, F1 Micro: 0.8975, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3759, Accuracy: 0.84, F1 Micro: 0.9044, F1 Macro: 0.9002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3209, Accuracy: 0.8564, F1 Micro: 0.914, F1 Macro: 0.9116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2904, Accuracy: 0.8616, F1 Micro: 0.9164, F1 Macro: 0.9135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2449, Accuracy: 0.869, F1 Micro: 0.9204, F1 Macro: 0.9178\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.869, F1 Micro: 0.9204, F1 Macro: 0.9178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.85      1.00      0.92       187\n",
      "     machine       0.91      0.94      0.92       175\n",
      "      others       0.90      0.82      0.86       158\n",
      "        part       0.86      0.97      0.91       158\n",
      "       price       0.87      1.00      0.93       192\n",
      "     service       0.92      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.88      0.96      0.92      1061\n",
      "   macro avg       0.89      0.96      0.92      1061\n",
      "weighted avg       0.89      0.96      0.92      1061\n",
      " samples avg       0.89      0.96      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5639, Accuracy: 0.7581, F1 Micro: 0.7581, F1 Macro: 0.4312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5352, Accuracy: 0.7581, F1 Micro: 0.7581, F1 Macro: 0.4312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4964, Accuracy: 0.7581, F1 Micro: 0.7581, F1 Macro: 0.4312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4454, Accuracy: 0.7957, F1 Micro: 0.7957, F1 Macro: 0.5884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2933, Accuracy: 0.8441, F1 Micro: 0.8441, F1 Macro: 0.8076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1435, Accuracy: 0.8548, F1 Micro: 0.8548, F1 Macro: 0.8165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1221, Accuracy: 0.8548, F1 Micro: 0.8548, F1 Macro: 0.8229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.8602, F1 Micro: 0.8602, F1 Macro: 0.8304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1026, Accuracy: 0.8763, F1 Micro: 0.8763, F1 Macro: 0.8417\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.871, F1 Micro: 0.871, F1 Macro: 0.8417\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8763, F1 Micro: 0.8763, F1 Macro: 0.8417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.84      0.77        45\n",
      "    positive       0.95      0.89      0.92       141\n",
      "\n",
      "    accuracy                           0.88       186\n",
      "   macro avg       0.83      0.87      0.84       186\n",
      "weighted avg       0.89      0.88      0.88       186\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8596, F1 Micro: 0.8596, F1 Macro: 0.6232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.86      1.00      0.92       181\n",
      "    positive       1.00      0.21      0.34        24\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.62      0.40      0.42       216\n",
      "weighted avg       0.83      0.86      0.81       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.69      0.69        16\n",
      "     neutral       0.91      0.94      0.92       167\n",
      "    positive       0.74      0.61      0.67        33\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.78      0.74      0.76       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.89      0.82      0.86       152\n",
      "    positive       0.62      0.69      0.65        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.67      0.75      0.70       216\n",
      "weighted avg       0.81      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.43      0.57        23\n",
      "     neutral       0.86      0.97      0.91       152\n",
      "    positive       0.75      0.59      0.66        41\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.81      0.66      0.71       216\n",
      "weighted avg       0.84      0.84      0.83       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.87      1.00      0.93       186\n",
      "    positive       1.00      0.18      0.30        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.62      0.39      0.41       216\n",
      "weighted avg       0.83      0.88      0.83       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.57      0.73        14\n",
      "     neutral       0.92      1.00      0.96       185\n",
      "    positive       0.86      0.35      0.50        17\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.93      0.64      0.73       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Total train time: 83.27658009529114 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6796, Accuracy: 0.7344, F1 Micro: 0.8384, F1 Macro: 0.7845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5332, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4931, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5018, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4691, Accuracy: 0.7999, F1 Micro: 0.8871, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4226, Accuracy: 0.808, F1 Micro: 0.8911, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4084, Accuracy: 0.8185, F1 Micro: 0.8943, F1 Macro: 0.8915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.352, Accuracy: 0.8445, F1 Micro: 0.9074, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3169, Accuracy: 0.8586, F1 Micro: 0.9135, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2606, Accuracy: 0.8802, F1 Micro: 0.9261, F1 Macro: 0.9218\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8802, F1 Micro: 0.9261, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.90      1.00      0.95       187\n",
      "     machine       0.91      0.94      0.92       175\n",
      "      others       0.92      0.75      0.83       158\n",
      "        part       0.90      0.98      0.94       158\n",
      "       price       0.87      1.00      0.93       192\n",
      "     service       0.93      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.95      0.93      1061\n",
      "   macro avg       0.90      0.95      0.92      1061\n",
      "weighted avg       0.90      0.95      0.92      1061\n",
      " samples avg       0.91      0.95      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7253, Accuracy: 0.7465, F1 Micro: 0.7465, F1 Macro: 0.4274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5463, Accuracy: 0.7465, F1 Micro: 0.7465, F1 Macro: 0.4274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4677, Accuracy: 0.7742, F1 Micro: 0.7742, F1 Macro: 0.5874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3249, Accuracy: 0.8387, F1 Micro: 0.8387, F1 Macro: 0.8015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2385, Accuracy: 0.8525, F1 Micro: 0.8525, F1 Macro: 0.8227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.8618, F1 Micro: 0.8618, F1 Macro: 0.8338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1134, Accuracy: 0.8664, F1 Micro: 0.8664, F1 Macro: 0.8338\n",
      "Epoch 8/10, Train Loss: 0.1809, Accuracy: 0.8341, F1 Micro: 0.8341, F1 Macro: 0.8142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1853, Accuracy: 0.894, F1 Micro: 0.894, F1 Macro: 0.8743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1266, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.8943\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.8943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.98      0.85        55\n",
      "    positive       0.99      0.89      0.94       162\n",
      "\n",
      "    accuracy                           0.91       217\n",
      "   macro avg       0.87      0.94      0.89       217\n",
      "weighted avg       0.93      0.91      0.92       217\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8758, F1 Micro: 0.8758, F1 Macro: 0.7107\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.45      0.62        11\n",
      "     neutral       0.91      1.00      0.95       181\n",
      "    positive       1.00      0.46      0.63        24\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.97      0.64      0.73       216\n",
      "weighted avg       0.92      0.91      0.90       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.88      0.78        16\n",
      "     neutral       0.90      0.95      0.92       167\n",
      "    positive       0.81      0.52      0.63        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.78      0.78       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.92      0.67        12\n",
      "     neutral       0.92      0.75      0.83       152\n",
      "    positive       0.58      0.79      0.67        52\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.67      0.82      0.72       216\n",
      "weighted avg       0.82      0.77      0.78       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.70      0.78        23\n",
      "     neutral       0.89      0.98      0.93       152\n",
      "    positive       0.90      0.68      0.78        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.08      0.14        13\n",
      "     neutral       0.87      1.00      0.93       186\n",
      "    positive       1.00      0.12      0.21        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.96      0.40      0.43       216\n",
      "weighted avg       0.89      0.88      0.83       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.93      1.00      0.96       185\n",
      "    positive       1.00      0.53      0.69        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.97      0.68      0.77       216\n",
      "weighted avg       0.94      0.93      0.92       216\n",
      "\n",
      "Total train time: 90.02972078323364 s\n",
      "Averaged - Iteration 135: Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.6683\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 618.3471310194395\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 31.62076449394226 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5805, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.491, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.488, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4424, Accuracy: 0.8132, F1 Micro: 0.894, F1 Macro: 0.8926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.396, Accuracy: 0.8237, F1 Micro: 0.8988, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3505, Accuracy: 0.8668, F1 Micro: 0.921, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.278, Accuracy: 0.8847, F1 Micro: 0.9306, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2328, Accuracy: 0.9115, F1 Micro: 0.9458, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1966, Accuracy: 0.9226, F1 Micro: 0.9521, F1 Macro: 0.9493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1622, Accuracy: 0.9278, F1 Micro: 0.9552, F1 Macro: 0.9525\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9278, F1 Micro: 0.9552, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      1.00      0.98       187\n",
      "     machine       0.91      0.99      0.95       175\n",
      "      others       0.91      0.86      0.89       158\n",
      "        part       0.92      0.98      0.95       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.96      1061\n",
      "   macro avg       0.93      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.95      1061\n",
      " samples avg       0.94      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.623, Accuracy: 0.6983, F1 Micro: 0.6983, F1 Macro: 0.4112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4667, Accuracy: 0.8448, F1 Micro: 0.8448, F1 Macro: 0.8188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3341, Accuracy: 0.8836, F1 Micro: 0.8836, F1 Macro: 0.8684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1918, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1831, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9171\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.9138, F1 Micro: 0.9138, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9166\n",
      "Epoch 8/10, Train Loss: 0.1412, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.94\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9177\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        70\n",
      "    positive       0.98      0.94      0.96       162\n",
      "\n",
      "    accuracy                           0.95       232\n",
      "   macro avg       0.93      0.95      0.94       232\n",
      "weighted avg       0.95      0.95      0.95       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.8483\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.97      1.00      0.98       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.87      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.90      0.99      0.94       167\n",
      "    positive       0.94      0.52      0.67        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.77      0.82       216\n",
      "weighted avg       0.91      0.90      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.86      0.88       152\n",
      "    positive       0.67      0.79      0.73        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.79      0.83      0.80       216\n",
      "weighted avg       0.85      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.91      0.73      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.95      1.00      0.98       186\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.75      0.82       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.98      0.81      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 94.39145612716675 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5986, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4933, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4896, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.442, Accuracy: 0.8073, F1 Micro: 0.891, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3988, Accuracy: 0.84, F1 Micro: 0.9074, F1 Macro: 0.9064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.354, Accuracy: 0.8624, F1 Micro: 0.9176, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2857, Accuracy: 0.8795, F1 Micro: 0.9272, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2548, Accuracy: 0.9048, F1 Micro: 0.9417, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2109, Accuracy: 0.91, F1 Micro: 0.9444, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1739, Accuracy: 0.9315, F1 Micro: 0.9575, F1 Macro: 0.9556\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9315, F1 Micro: 0.9575, F1 Macro: 0.9556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.94      0.94      0.94       175\n",
      "      others       0.90      0.92      0.91       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.97      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5962, Accuracy: 0.694, F1 Micro: 0.694, F1 Macro: 0.4097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5333, Accuracy: 0.8017, F1 Micro: 0.8017, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3351, Accuracy: 0.8836, F1 Micro: 0.8836, F1 Macro: 0.87\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1883, Accuracy: 0.8966, F1 Micro: 0.8966, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.145, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0591, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9154\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.908\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.88        71\n",
      "    positive       0.96      0.93      0.95       161\n",
      "\n",
      "    accuracy                           0.93       232\n",
      "   macro avg       0.91      0.92      0.92       232\n",
      "weighted avg       0.93      0.93      0.93       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.843\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.90      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.93      0.94      0.94       167\n",
      "    positive       0.77      0.73      0.75        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.81      0.80       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.58      0.70        12\n",
      "     neutral       0.90      0.92      0.91       152\n",
      "    positive       0.72      0.73      0.72        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.83      0.75      0.78       216\n",
      "weighted avg       0.86      0.86      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.74      0.77        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.97      0.71      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.95      1.00      0.98       186\n",
      "    positive       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.76      0.82       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.95      1.00      0.97       185\n",
      "    positive       1.00      0.59      0.74        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.98      0.79      0.86       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Total train time: 100.9301393032074 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6474, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5077, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.505, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4676, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4339, Accuracy: 0.8088, F1 Micro: 0.8918, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3916, Accuracy: 0.8467, F1 Micro: 0.91, F1 Macro: 0.9083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3178, Accuracy: 0.8847, F1 Micro: 0.9297, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2792, Accuracy: 0.9077, F1 Micro: 0.943, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.232, Accuracy: 0.9129, F1 Micro: 0.9455, F1 Macro: 0.9423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.196, Accuracy: 0.9256, F1 Micro: 0.9536, F1 Macro: 0.9514\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9256, F1 Micro: 0.9536, F1 Macro: 0.9514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.91      0.93      0.92       175\n",
      "      others       0.89      0.89      0.89       158\n",
      "        part       0.92      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.95      1061\n",
      "   macro avg       0.94      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.95      1061\n",
      " samples avg       0.94      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6314, Accuracy: 0.6904, F1 Micro: 0.6904, F1 Macro: 0.4084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5381, Accuracy: 0.795, F1 Micro: 0.795, F1 Macro: 0.7448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.343, Accuracy: 0.8577, F1 Micro: 0.8577, F1 Macro: 0.8456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1937, Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1365, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9247, F1 Micro: 0.9247, F1 Macro: 0.9138\n",
      "Epoch 7/10, Train Loss: 0.036, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.8651\n",
      "Epoch 8/10, Train Loss: 0.1207, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9022\n",
      "Epoch 9/10, Train Loss: 0.1319, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9105\n",
      "Epoch 10/10, Train Loss: 0.0783, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8973\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9247, F1 Micro: 0.9247, F1 Macro: 0.9138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        74\n",
      "    positive       0.96      0.93      0.94       165\n",
      "\n",
      "    accuracy                           0.92       239\n",
      "   macro avg       0.91      0.92      0.91       239\n",
      "weighted avg       0.93      0.92      0.93       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.8468\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.81      0.72        16\n",
      "     neutral       0.90      0.93      0.91       167\n",
      "    positive       0.75      0.55      0.63        33\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.77      0.76      0.76       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.58      0.67        12\n",
      "     neutral       0.89      0.89      0.89       152\n",
      "    positive       0.69      0.73      0.71        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.79      0.74      0.76       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.92      0.99      0.96       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.98      0.81      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 88.90123224258423 s\n",
      "Averaged - Iteration 208: Accuracy: 0.921, F1 Micro: 0.921, F1 Macro: 0.846\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 330.7603945327504\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 30.11376714706421 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.575, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4997, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4492, Accuracy: 0.8051, F1 Micro: 0.8896, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4081, Accuracy: 0.8199, F1 Micro: 0.8951, F1 Macro: 0.8926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3552, Accuracy: 0.8683, F1 Micro: 0.9214, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2847, Accuracy: 0.9033, F1 Micro: 0.9411, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2438, Accuracy: 0.9167, F1 Micro: 0.9487, F1 Macro: 0.9466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1873, Accuracy: 0.9293, F1 Micro: 0.9556, F1 Macro: 0.9521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.151, Accuracy: 0.9345, F1 Micro: 0.959, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1242, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.9619\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.9619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.99      0.95       175\n",
      "      others       0.89      0.91      0.90       158\n",
      "        part       0.96      0.94      0.95       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6286, Accuracy: 0.7004, F1 Micro: 0.7004, F1 Macro: 0.459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4537, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2372, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9322\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0784, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0543, Accuracy: 0.9636, F1 Micro: 0.9636, F1 Macro: 0.958\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9282\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9278\n",
      "Epoch 10/10, Train Loss: 0.0551, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9274\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9636, F1 Micro: 0.9636, F1 Macro: 0.958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.95      0.94        78\n",
      "    positive       0.98      0.97      0.97       169\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.96      0.96      0.96       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.8923\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.92      0.99      0.95       167\n",
      "    positive       0.91      0.64      0.75        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.79      0.84       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.90      0.91      0.91       152\n",
      "    positive       0.73      0.69      0.71        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.82      0.81      0.82       216\n",
      "weighted avg       0.85      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        23\n",
      "     neutral       0.95      0.94      0.95       152\n",
      "    positive       0.80      0.85      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 105.33915686607361 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5818, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4529, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3998, Accuracy: 0.8385, F1 Micro: 0.9056, F1 Macro: 0.9038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3506, Accuracy: 0.8802, F1 Micro: 0.9275, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2929, Accuracy: 0.8996, F1 Micro: 0.9384, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2364, Accuracy: 0.9189, F1 Micro: 0.95, F1 Macro: 0.9481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1934, Accuracy: 0.9308, F1 Micro: 0.9566, F1 Macro: 0.9541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1584, Accuracy: 0.9405, F1 Micro: 0.9629, F1 Macro: 0.961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1284, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9627\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.90      0.93      0.91       158\n",
      "        part       0.94      0.96      0.95       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6214, Accuracy: 0.6844, F1 Micro: 0.6844, F1 Macro: 0.4063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.575, Accuracy: 0.7049, F1 Micro: 0.7049, F1 Macro: 0.4723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3281, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1988, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9222\n",
      "Epoch 5/10, Train Loss: 0.1643, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.914\n",
      "Epoch 6/10, Train Loss: 0.1321, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9183\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9261\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.8852, F1 Micro: 0.8852, F1 Macro: 0.8768\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        77\n",
      "    positive       0.98      0.93      0.95       167\n",
      "\n",
      "    accuracy                           0.94       244\n",
      "   macro avg       0.92      0.94      0.93       244\n",
      "weighted avg       0.94      0.94      0.94       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.8826\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.90      0.93      0.91       152\n",
      "    positive       0.82      0.69      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.79      0.82      0.80       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.84      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 101.73589396476746 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6255, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5128, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4674, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4311, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3813, Accuracy: 0.8735, F1 Micro: 0.9239, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3127, Accuracy: 0.9003, F1 Micro: 0.9383, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2571, Accuracy: 0.9204, F1 Micro: 0.9503, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2054, Accuracy: 0.9315, F1 Micro: 0.957, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.167, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1338, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9664\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5756, Accuracy: 0.8016, F1 Micro: 0.8016, F1 Macro: 0.7549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4137, Accuracy: 0.873, F1 Micro: 0.873, F1 Macro: 0.858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2098, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9367\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9045\n",
      "Epoch 5/10, Train Loss: 0.1145, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9367\n",
      "Epoch 7/10, Train Loss: 0.0474, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8973\n",
      "Epoch 8/10, Train Loss: 0.0296, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9234\n",
      "Epoch 9/10, Train Loss: 0.0439, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.909\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9214\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        81\n",
      "    positive       0.96      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.94       252\n",
      "weighted avg       0.94      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.893\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.92      0.91      0.91       152\n",
      "    positive       0.76      0.79      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.88      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 103.04962182044983 s\n",
      "Averaged - Iteration 274: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8893\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 341.03887131541677\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 29.53044867515564 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5603, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5009, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4371, Accuracy: 0.8229, F1 Micro: 0.8986, F1 Macro: 0.8974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3747, Accuracy: 0.8728, F1 Micro: 0.9245, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2938, Accuracy: 0.9085, F1 Micro: 0.9441, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2295, Accuracy: 0.9271, F1 Micro: 0.9545, F1 Macro: 0.9524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1743, Accuracy: 0.9382, F1 Micro: 0.9613, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1376, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9673\n",
      "Epoch 9/10, Train Loss: 0.112, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9649\n",
      "Epoch 10/10, Train Loss: 0.0886, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.965\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.89      0.94      0.92       158\n",
      "        part       0.95      0.97      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6011, Accuracy: 0.6736, F1 Micro: 0.6736, F1 Macro: 0.4025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4355, Accuracy: 0.9247, F1 Micro: 0.9247, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1396, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9485\n",
      "Epoch 5/10, Train Loss: 0.1152, Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9582, F1 Micro: 0.9582, F1 Macro: 0.9527\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9398\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9623, F1 Micro: 0.9623, F1 Macro: 0.957\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9446\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9623, F1 Micro: 0.9623, F1 Macro: 0.957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.94      0.94        78\n",
      "    positive       0.97      0.98      0.97       161\n",
      "\n",
      "    accuracy                           0.96       239\n",
      "   macro avg       0.96      0.96      0.96       239\n",
      "weighted avg       0.96      0.96      0.96       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.9026\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.82      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.89      0.95      0.92       152\n",
      "    positive       0.82      0.71      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.91      0.83      0.86       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.87      0.83      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.92      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 105.9782133102417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5756, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5001, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4393, Accuracy: 0.8251, F1 Micro: 0.9001, F1 Macro: 0.899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3676, Accuracy: 0.8743, F1 Micro: 0.924, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3032, Accuracy: 0.8996, F1 Micro: 0.9389, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2336, Accuracy: 0.9241, F1 Micro: 0.953, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1857, Accuracy: 0.933, F1 Micro: 0.9579, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1468, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.969\n",
      "Epoch 9/10, Train Loss: 0.1206, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9655\n",
      "Epoch 10/10, Train Loss: 0.0939, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9684\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.95      0.97      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.605, Accuracy: 0.6695, F1 Micro: 0.6695, F1 Macro: 0.401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5116, Accuracy: 0.887, F1 Micro: 0.887, F1 Macro: 0.8728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2836, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9139\n",
      "Epoch 4/10, Train Loss: 0.3312, Accuracy: 0.8787, F1 Micro: 0.8787, F1 Macro: 0.8716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1765, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.171, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1098, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9443\n",
      "Epoch 9/10, Train Loss: 0.1129, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9395\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9405\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        79\n",
      "    positive       0.98      0.94      0.96       160\n",
      "\n",
      "    accuracy                           0.95       239\n",
      "   macro avg       0.94      0.95      0.94       239\n",
      "weighted avg       0.95      0.95      0.95       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8912\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.96      0.76      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.71      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.89      0.78      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.83      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 106.36647152900696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.605, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5087, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4589, Accuracy: 0.808, F1 Micro: 0.8913, F1 Macro: 0.8898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.39, Accuracy: 0.8713, F1 Micro: 0.9234, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3166, Accuracy: 0.8973, F1 Micro: 0.9368, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2491, Accuracy: 0.9219, F1 Micro: 0.9507, F1 Macro: 0.9475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1952, Accuracy: 0.9368, F1 Micro: 0.9603, F1 Macro: 0.9572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1542, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1244, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0991, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9704\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.95      0.89      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5907, Accuracy: 0.7034, F1 Micro: 0.7034, F1 Macro: 0.4678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4826, Accuracy: 0.8859, F1 Micro: 0.8859, F1 Macro: 0.8746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2599, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9191\n",
      "Epoch 4/10, Train Loss: 0.2172, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1465, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9376\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9041\n",
      "Epoch 7/10, Train Loss: 0.1309, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9319\n",
      "Epoch 8/10, Train Loss: 0.1469, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "Epoch 9/10, Train Loss: 0.0958, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.90      0.91        82\n",
      "    positive       0.96      0.97      0.96       181\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.93      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.9023\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.75      0.88      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.90      0.88       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.83      0.83      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 102.72987127304077 s\n",
      "Averaged - Iteration 333: Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.8987\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 347.9409281187821\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 26.057722568511963 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4647, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4033, Accuracy: 0.846, F1 Micro: 0.9104, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3359, Accuracy: 0.8973, F1 Micro: 0.9384, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2527, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9601\n",
      "Epoch 6/10, Train Loss: 0.1853, Accuracy: 0.9338, F1 Micro: 0.9584, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1423, Accuracy: 0.9435, F1 Micro: 0.9646, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1123, Accuracy: 0.9472, F1 Micro: 0.9667, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0925, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0798, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9674\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.89      0.91       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.97      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6098, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3851, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2451, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2077, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9475\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9383\n",
      "Epoch 6/10, Train Loss: 0.1085, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9383\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 8/10, Train Loss: 0.1031, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.918\n",
      "Epoch 9/10, Train Loss: 0.0957, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9406\n",
      "Epoch 10/10, Train Loss: 0.0875, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.918\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        81\n",
      "    positive       0.99      0.94      0.97       178\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.96      0.95       259\n",
      "weighted avg       0.96      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.9064\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.94      0.89      0.92       152\n",
      "    positive       0.73      0.79      0.76        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.80      0.87      0.83       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.57880735397339 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5595, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4645, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3954, Accuracy: 0.8408, F1 Micro: 0.9064, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3276, Accuracy: 0.8876, F1 Micro: 0.9322, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2605, Accuracy: 0.9368, F1 Micro: 0.9608, F1 Macro: 0.9592\n",
      "Epoch 6/10, Train Loss: 0.1936, Accuracy: 0.936, F1 Micro: 0.9597, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1502, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.116, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.099, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0845, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.94      0.99      0.96       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6039, Accuracy: 0.7951, F1 Micro: 0.7951, F1 Macro: 0.7139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3906, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2082, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9446\n",
      "Epoch 4/10, Train Loss: 0.1769, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1304, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9116\n",
      "Epoch 9/10, Train Loss: 0.0933, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9243\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9116\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.97      0.96      0.97       163\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.95      0.95      0.95       244\n",
      "weighted avg       0.96      0.95      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.907\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 115.65564846992493 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5902, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4827, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4393, Accuracy: 0.814, F1 Micro: 0.8943, F1 Macro: 0.8929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3743, Accuracy: 0.8958, F1 Micro: 0.9368, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.288, Accuracy: 0.9308, F1 Micro: 0.9568, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2162, Accuracy: 0.9412, F1 Micro: 0.9631, F1 Macro: 0.9607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.163, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1243, Accuracy: 0.9531, F1 Micro: 0.9705, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1077, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "Epoch 10/10, Train Loss: 0.0911, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6064, Accuracy: 0.7349, F1 Micro: 0.7349, F1 Macro: 0.5871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4134, Accuracy: 0.9036, F1 Micro: 0.9036, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2254, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.129, Accuracy: 0.9598, F1 Micro: 0.9598, F1 Macro: 0.954\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9385\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9367\n",
      "Epoch 9/10, Train Loss: 0.0429, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9297\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9375\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9598, F1 Micro: 0.9598, F1 Macro: 0.954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.91      0.94        82\n",
      "    positive       0.96      0.98      0.97       167\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.96      0.95      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9116\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.84      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.83      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.87      0.83      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 112.75622367858887 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9084\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 356.40044259119196\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 23.718722581863403 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5462, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4786, Accuracy: 0.8065, F1 Micro: 0.8907, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4014, Accuracy: 0.8445, F1 Micro: 0.9094, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3283, Accuracy: 0.9055, F1 Micro: 0.9425, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2318, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1664, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.13, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1054, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9689\n",
      "Epoch 9/10, Train Loss: 0.0896, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0766, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.607, Accuracy: 0.8532, F1 Micro: 0.8532, F1 Macro: 0.8073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.339, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9248\n",
      "Epoch 3/10, Train Loss: 0.2131, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9181\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0985, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9375\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9218\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9218\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.929\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9336\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        82\n",
      "    positive       0.97      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.94       252\n",
      "weighted avg       0.95      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.898\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.91      0.73      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.75      0.88      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 114.7390353679657 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5564, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4786, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3961, Accuracy: 0.8646, F1 Micro: 0.9197, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3241, Accuracy: 0.8981, F1 Micro: 0.938, F1 Macro: 0.9368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2396, Accuracy: 0.9382, F1 Micro: 0.9621, F1 Macro: 0.9612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1775, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.138, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1108, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Epoch 9/10, Train Loss: 0.0959, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9689\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9524, F1 Micro: 0.9699, F1 Macro: 0.9679\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6443, Accuracy: 0.6777, F1 Micro: 0.6777, F1 Macro: 0.4039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.436, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.247, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.166, Accuracy: 0.9587, F1 Micro: 0.9587, F1 Macro: 0.9536\n",
      "Epoch 6/10, Train Loss: 0.1616, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9432\n",
      "Epoch 7/10, Train Loss: 0.0972, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9432\n",
      "Epoch 8/10, Train Loss: 0.0966, Accuracy: 0.9463, F1 Micro: 0.9463, F1 Macro: 0.9391\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9306\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9314\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9587, F1 Micro: 0.9587, F1 Macro: 0.9536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.94        78\n",
      "    positive       0.99      0.95      0.97       164\n",
      "\n",
      "    accuracy                           0.96       242\n",
      "   macro avg       0.95      0.96      0.95       242\n",
      "weighted avg       0.96      0.96      0.96       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 113.93314599990845 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5889, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.49, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4276, Accuracy: 0.8281, F1 Micro: 0.9013, F1 Macro: 0.9002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3556, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2585, Accuracy: 0.9338, F1 Micro: 0.9587, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1902, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1509, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1211, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1007, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0864, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5771, Accuracy: 0.7826, F1 Micro: 0.7826, F1 Macro: 0.6826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3579, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2242, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1503, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9511\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9307\n",
      "Epoch 7/10, Train Loss: 0.1208, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Epoch 8/10, Train Loss: 0.0744, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.084, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9514\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9345\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        83\n",
      "    positive       0.98      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9052\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 120.67618989944458 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9011\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 347.9158532176955\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 23.16019892692566 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5406, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4822, Accuracy: 0.8095, F1 Micro: 0.8923, F1 Macro: 0.8908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3944, Accuracy: 0.8653, F1 Micro: 0.9209, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3086, Accuracy: 0.9338, F1 Micro: 0.9592, F1 Macro: 0.9578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2188, Accuracy: 0.9405, F1 Micro: 0.9629, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1604, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.123, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9718\n",
      "Epoch 8/10, Train Loss: 0.107, Accuracy: 0.9494, F1 Micro: 0.9681, F1 Macro: 0.9659\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9697\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9689\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.93      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6091, Accuracy: 0.7882, F1 Micro: 0.7882, F1 Macro: 0.6987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3412, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "Epoch 3/10, Train Loss: 0.2318, Accuracy: 0.9059, F1 Micro: 0.9059, F1 Macro: 0.8993\n",
      "Epoch 4/10, Train Loss: 0.1703, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9425\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9146\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.9025\n",
      "Epoch 10/10, Train Loss: 0.1082, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.915\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        84\n",
      "    positive       0.96      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9067\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.80      0.75      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.84      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.87      0.91        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.87      0.83      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 116.05058860778809 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5501, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4758, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3913, Accuracy: 0.8765, F1 Micro: 0.9258, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3077, Accuracy: 0.9249, F1 Micro: 0.9535, F1 Macro: 0.9519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2204, Accuracy: 0.9382, F1 Micro: 0.9614, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.174, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.132, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1128, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0898, Accuracy: 0.9524, F1 Micro: 0.9699, F1 Macro: 0.9678\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9692\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5916, Accuracy: 0.8156, F1 Micro: 0.8156, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3479, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2182, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.206, Accuracy: 0.9631, F1 Micro: 0.9631, F1 Macro: 0.9593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.9672, F1 Micro: 0.9672, F1 Macro: 0.9639\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9551\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9464\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9507\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9464\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9672, F1 Micro: 0.9672, F1 Macro: 0.9639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.98      0.95        83\n",
      "    positive       0.99      0.96      0.97       161\n",
      "\n",
      "    accuracy                           0.97       244\n",
      "   macro avg       0.96      0.97      0.96       244\n",
      "weighted avg       0.97      0.97      0.97       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.90      0.69      0.78        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 122.53935980796814 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5814, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4933, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4282, Accuracy: 0.8475, F1 Micro: 0.9109, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3406, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2402, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.9578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1865, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1455, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1222, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0951, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9722\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9716\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.97      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5926, Accuracy: 0.8246, F1 Micro: 0.8246, F1 Macro: 0.7738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3614, Accuracy: 0.8918, F1 Micro: 0.8918, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2479, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1764, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1542, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.076, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9452\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0911, Accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.9567\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 10/10, Train Loss: 0.0823, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.9567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.92      0.94        86\n",
      "    positive       0.96      0.98      0.97       182\n",
      "\n",
      "    accuracy                           0.96       268\n",
      "   macro avg       0.96      0.95      0.96       268\n",
      "weighted avg       0.96      0.96      0.96       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9192\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.79      0.87      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.90      0.91       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.87      0.91        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.83      0.85      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 127.31292510032654 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.916\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 352.0236715535982\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 20.222673416137695 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5438, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4698, Accuracy: 0.814, F1 Micro: 0.8945, F1 Macro: 0.8932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.383, Accuracy: 0.8787, F1 Micro: 0.9276, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.267, Accuracy: 0.9405, F1 Micro: 0.9627, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1921, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.137, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0875, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9731\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9723\n",
      "Epoch 10/10, Train Loss: 0.0598, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9711\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.96      0.97       158\n",
      "       price       0.99      0.97      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5866, Accuracy: 0.8855, F1 Micro: 0.8855, F1 Macro: 0.8641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3175, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2269, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9439\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 6/10, Train Loss: 0.1013, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9299\n",
      "Epoch 7/10, Train Loss: 0.1345, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9258\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9316\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9345\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9079\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9105\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.95      0.97       152\n",
      "    positive       0.88      0.90      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.95      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.97      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.93      0.90       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.63618159294128 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5537, Accuracy: 0.7909, F1 Micro: 0.8813, F1 Macro: 0.8787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4698, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3773, Accuracy: 0.8802, F1 Micro: 0.9271, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2814, Accuracy: 0.933, F1 Micro: 0.958, F1 Macro: 0.9553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2055, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Epoch 7/10, Train Loss: 0.1164, Accuracy: 0.9501, F1 Micro: 0.9686, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6191, Accuracy: 0.845, F1 Micro: 0.845, F1 Macro: 0.8149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3733, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2532, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.939\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1604, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9522\n",
      "Epoch 7/10, Train Loss: 0.1207, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0797, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9522\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 10/10, Train Loss: 0.0663, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.94        85\n",
      "    positive       0.98      0.96      0.97       173\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.95       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9197\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.65973043441772 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5765, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4807, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3971, Accuracy: 0.8966, F1 Micro: 0.9375, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2879, Accuracy: 0.939, F1 Micro: 0.9618, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2119, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1554, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9741\n",
      "Epoch 7/10, Train Loss: 0.1274, Accuracy: 0.9531, F1 Micro: 0.9705, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9714\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5757, Accuracy: 0.8263, F1 Micro: 0.8263, F1 Macro: 0.8134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3263, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2475, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9201\n",
      "Epoch 6/10, Train Loss: 0.1395, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0943, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1144, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9486\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9241\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.96      0.95       259\n",
      "weighted avg       0.96      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9145\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 128.58431100845337 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9149\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 367.3502501706856\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 18.49841547012329 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5308, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4735, Accuracy: 0.8088, F1 Micro: 0.8917, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3878, Accuracy: 0.8854, F1 Micro: 0.9316, F1 Macro: 0.9312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2805, Accuracy: 0.939, F1 Micro: 0.9625, F1 Macro: 0.9614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1984, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1166, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9501, F1 Micro: 0.9683, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.0599, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9733\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.93      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5697, Accuracy: 0.8918, F1 Micro: 0.8918, F1 Macro: 0.8694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2864, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1878, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9455\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9319\n",
      "Epoch 6/10, Train Loss: 0.1431, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9323\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 8/10, Train Loss: 0.1014, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9411\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9104, F1 Micro: 0.9104, F1 Macro: 0.9022\n",
      "Epoch 10/10, Train Loss: 0.0987, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9256\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.96       182\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.94      0.96      0.95       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9169\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.96      0.93      0.94       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.89      0.86       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.21302580833435 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5429, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4702, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3753, Accuracy: 0.8936, F1 Micro: 0.9353, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2788, Accuracy: 0.9315, F1 Micro: 0.9578, F1 Macro: 0.956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2018, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1474, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1208, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.9702\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.594, Accuracy: 0.8659, F1 Micro: 0.8659, F1 Macro: 0.8467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3386, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2257, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1567, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9464\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9285\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9429\n",
      "Epoch 7/10, Train Loss: 0.057, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9387\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9299\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9414\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9381\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        84\n",
      "    positive       0.97      0.95      0.96       162\n",
      "\n",
      "    accuracy                           0.95       246\n",
      "   macro avg       0.94      0.95      0.95       246\n",
      "weighted avg       0.95      0.95      0.95       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9119\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.90      0.97      0.94       152\n",
      "    positive       0.90      0.69      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.81      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 129.3955864906311 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5666, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4745, Accuracy: 0.7999, F1 Micro: 0.8873, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3881, Accuracy: 0.9033, F1 Micro: 0.9407, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2844, Accuracy: 0.9256, F1 Micro: 0.9534, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2111, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1593, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1217, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1003, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9728\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.97      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5824, Accuracy: 0.8556, F1 Micro: 0.8556, F1 Macro: 0.8428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2999, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.22, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2075, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.942\n",
      "Epoch 5/10, Train Loss: 0.1841, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9417\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9227\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9184\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9262\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9111, F1 Micro: 0.9111, F1 Macro: 0.9039\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        88\n",
      "    positive       0.97      0.95      0.96       182\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.95      0.94       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9158\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.96      0.91      0.93       152\n",
      "    positive       0.76      0.85      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.89      0.86       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.97      0.98       186\n",
      "    positive       0.71      0.88      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.90      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.71503067016602 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9149\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 366.10173643619885\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 16.86532425880432 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5364, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4663, Accuracy: 0.8304, F1 Micro: 0.9026, F1 Macro: 0.9016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3785, Accuracy: 0.9107, F1 Micro: 0.9459, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2476, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1881, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9701\n",
      "Epoch 6/10, Train Loss: 0.1426, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1034, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9732\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9501, F1 Micro: 0.9684, F1 Macro: 0.9659\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.8859, F1 Micro: 0.8859, F1 Macro: 0.8746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2634, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9218\n",
      "Epoch 4/10, Train Loss: 0.235, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9095\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1571, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9406\n",
      "Epoch 8/10, Train Loss: 0.1042, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9328\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9164\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 136.02791261672974 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5453, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4681, Accuracy: 0.8155, F1 Micro: 0.895, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3821, Accuracy: 0.8914, F1 Micro: 0.9341, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2582, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1987, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.147, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9705\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5852, Accuracy: 0.8372, F1 Micro: 0.8372, F1 Macro: 0.7883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3219, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1937, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1365, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1413, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9323\n",
      "Epoch 7/10, Train Loss: 0.1389, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9317\n",
      "Epoch 9/10, Train Loss: 0.0976, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9564\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        84\n",
      "    positive       0.98      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.919\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.81      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 136.1123161315918 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.8073, F1 Micro: 0.8909, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3926, Accuracy: 0.9159, F1 Micro: 0.9484, F1 Macro: 0.9471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2645, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2018, Accuracy: 0.9472, F1 Micro: 0.9668, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.151, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9711\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.9708\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9539, F1 Micro: 0.9708, F1 Macro: 0.9687\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9546, F1 Micro: 0.9712, F1 Macro: 0.969\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5748, Accuracy: 0.8425, F1 Micro: 0.8425, F1 Macro: 0.7973\n",
      "Epoch 2/10, Train Loss: 0.3548, Accuracy: 0.8268, F1 Micro: 0.8268, F1 Macro: 0.8209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2373, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1275, Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.955\n",
      "Epoch 6/10, Train Loss: 0.1381, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "Epoch 7/10, Train Loss: 0.0911, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9424\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9304\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9353\n",
      "Epoch 10/10, Train Loss: 0.0845, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.93      0.94        83\n",
      "    positive       0.97      0.98      0.97       171\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.96      0.95      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9171\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.84      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.86      0.88      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 127.87196731567383 s\n",
      "Averaged - Iteration 573: Accuracy: 0.955, F1 Micro: 0.955, F1 Macro: 0.9175\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 366.8545772277198\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 15.75601315498352 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4634, Accuracy: 0.8304, F1 Micro: 0.9024, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3617, Accuracy: 0.9167, F1 Micro: 0.949, F1 Macro: 0.9473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2432, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1703, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9516, F1 Micro: 0.9694, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1033, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0652, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0566, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9761\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.93      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5531, Accuracy: 0.9071, F1 Micro: 0.9071, F1 Macro: 0.8922\n",
      "Epoch 2/10, Train Loss: 0.2779, Accuracy: 0.8885, F1 Micro: 0.8885, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1407, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9376\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9182\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.926\n",
      "Epoch 8/10, Train Loss: 0.0686, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9105\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.6844072341919 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.543, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.461, Accuracy: 0.8311, F1 Micro: 0.9028, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3601, Accuracy: 0.9144, F1 Micro: 0.9472, F1 Macro: 0.9448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2474, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1762, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Epoch 6/10, Train Loss: 0.1383, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5585, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.8513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.205, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9379\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.115, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9523\n",
      "Epoch 7/10, Train Loss: 0.1158, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1008, Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9612\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        85\n",
      "    positive       0.99      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.97       259\n",
      "   macro avg       0.96      0.97      0.96       259\n",
      "weighted avg       0.97      0.97      0.97       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9271\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.9850401878357 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5656, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4799, Accuracy: 0.8021, F1 Micro: 0.8883, F1 Macro: 0.8867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3884, Accuracy: 0.9085, F1 Micro: 0.9434, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2661, Accuracy: 0.9308, F1 Micro: 0.9563, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1906, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Epoch 6/10, Train Loss: 0.1485, Accuracy: 0.9554, F1 Micro: 0.9718, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.112, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5647, Accuracy: 0.8858, F1 Micro: 0.8858, F1 Macro: 0.8644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1942, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1671, Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.956\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9476\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9685, F1 Micro: 0.9685, F1 Macro: 0.9642\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.9646, F1 Micro: 0.9646, F1 Macro: 0.9596\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9685, F1 Micro: 0.9685, F1 Macro: 0.9646\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9685, F1 Micro: 0.9685, F1 Macro: 0.9646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.98      0.95        83\n",
      "    positive       0.99      0.96      0.98       171\n",
      "\n",
      "    accuracy                           0.97       254\n",
      "   macro avg       0.96      0.97      0.96       254\n",
      "weighted avg       0.97      0.97      0.97       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.86      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.93      0.85      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.14338898658752 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9578, F1 Micro: 0.9578, F1 Macro: 0.9244\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 365.2519638679115\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 14.254901885986328 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5275, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4532, Accuracy: 0.8274, F1 Micro: 0.9012, F1 Macro: 0.9001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3526, Accuracy: 0.9293, F1 Micro: 0.9564, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2256, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.158, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9576, F1 Micro: 0.9731, F1 Macro: 0.9709\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5093, Accuracy: 0.8817, F1 Micro: 0.8817, F1 Macro: 0.8534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2994, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1817, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1393, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9481\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "Epoch 9/10, Train Loss: 0.0908, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 10/10, Train Loss: 0.0873, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9364\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.97       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.95       262\n",
      "weighted avg       0.96      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9185\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.1390504837036 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4515, Accuracy: 0.8497, F1 Micro: 0.9126, F1 Macro: 0.9112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3443, Accuracy: 0.9345, F1 Micro: 0.9592, F1 Macro: 0.9576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2283, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1623, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9561, F1 Micro: 0.9721, F1 Macro: 0.9695\n",
      "Epoch 8/10, Train Loss: 0.0827, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0662, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9744\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5687, Accuracy: 0.8458, F1 Micro: 0.8458, F1 Macro: 0.8115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.27, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9426\n",
      "Epoch 3/10, Train Loss: 0.2202, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Epoch 5/10, Train Loss: 0.1699, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9187\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9426\n",
      "Epoch 7/10, Train Loss: 0.125, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9272\n",
      "Epoch 8/10, Train Loss: 0.0974, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9314\n",
      "Epoch 9/10, Train Loss: 0.0871, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9432\n",
      "Epoch 10/10, Train Loss: 0.0902, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9376\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9216\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 141.0196509361267 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5583, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4755, Accuracy: 0.7991, F1 Micro: 0.8869, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.377, Accuracy: 0.9219, F1 Micro: 0.9515, F1 Macro: 0.95\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2461, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1788, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1315, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9729\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9738\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.587, Accuracy: 0.8922, F1 Micro: 0.8922, F1 Macro: 0.8741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3256, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1551, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1436, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9453\n",
      "Epoch 5/10, Train Loss: 0.1463, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9199\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9534\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9446\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.9453\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.96      0.97       184\n",
      "\n",
      "    accuracy                           0.96       269\n",
      "   macro avg       0.95      0.96      0.95       269\n",
      "weighted avg       0.96      0.96      0.96       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9244\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.75      0.83      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.86      0.83       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.2619001865387 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9573, F1 Micro: 0.9573, F1 Macro: 0.9215\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 392.11818420304553\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 12.612253189086914 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5366, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4559, Accuracy: 0.846, F1 Micro: 0.9104, F1 Macro: 0.9096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3275, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.9599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2148, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Epoch 6/10, Train Loss: 0.112, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9727\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9719\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5494, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9406\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9329\n",
      "Epoch 5/10, Train Loss: 0.1031, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Epoch 6/10, Train Loss: 0.063, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Epoch 8/10, Train Loss: 0.0878, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9444\n",
      "Epoch 10/10, Train Loss: 0.056, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9125\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9201\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.05589747428894 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5427, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4576, Accuracy: 0.8475, F1 Micro: 0.9116, F1 Macro: 0.9104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3322, Accuracy: 0.9375, F1 Micro: 0.9612, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2202, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1566, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.9554, F1 Micro: 0.9717, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5136, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9339\n",
      "Epoch 3/10, Train Loss: 0.1746, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1343, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1237, Accuracy: 0.9681, F1 Micro: 0.9681, F1 Macro: 0.9638\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9503\n",
      "Epoch 7/10, Train Loss: 0.114, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9245\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9258\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9424\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9602, F1 Micro: 0.9602, F1 Macro: 0.9555\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9681, F1 Micro: 0.9681, F1 Macro: 0.9638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.95      0.95        82\n",
      "    positive       0.98      0.98      0.98       169\n",
      "\n",
      "    accuracy                           0.97       251\n",
      "   macro avg       0.96      0.96      0.96       251\n",
      "weighted avg       0.97      0.97      0.97       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9219\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.92      0.81      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 144.16092228889465 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5651, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4696, Accuracy: 0.8274, F1 Micro: 0.9011, F1 Macro: 0.9003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3434, Accuracy: 0.9308, F1 Micro: 0.9567, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2347, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1638, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0932, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.98      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5734, Accuracy: 0.8852, F1 Micro: 0.8852, F1 Macro: 0.8705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3103, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.929\n",
      "Epoch 3/10, Train Loss: 0.1834, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1704, Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.946\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9294\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9254\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.938\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9377\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9254\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9219\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        86\n",
      "    positive       0.99      0.94      0.96       184\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.96      0.95       270\n",
      "weighted avg       0.96      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9275\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.81      0.85      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.90      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.90      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.00738382339478 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9232\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 390.47408535129443\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 11.606046676635742 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.8028, F1 Micro: 0.8888, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4464, Accuracy: 0.8579, F1 Micro: 0.9166, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3105, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2036, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.148, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5445, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.91\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2732, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1816, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9339\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9296\n",
      "Epoch 5/10, Train Loss: 0.1028, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9307\n",
      "Epoch 6/10, Train Loss: 0.0842, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9258\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9258\n",
      "Epoch 8/10, Train Loss: 0.0841, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9262\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9179\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9262\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.96      0.95      0.95       166\n",
      "\n",
      "    accuracy                           0.94       251\n",
      "   macro avg       0.93      0.94      0.93       251\n",
      "weighted avg       0.94      0.94      0.94       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9099\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.86      0.69      0.77        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.38067960739136 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5374, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4405, Accuracy: 0.8713, F1 Micro: 0.9233, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3119, Accuracy: 0.942, F1 Micro: 0.9641, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2091, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5471, Accuracy: 0.7313, F1 Micro: 0.7313, F1 Macro: 0.7279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3073, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1855, Accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9531\n",
      "Epoch 4/10, Train Loss: 0.137, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9445\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9367\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9367\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9486\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9367\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9367\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9327\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.94        85\n",
      "    positive       0.98      0.96      0.97       183\n",
      "\n",
      "    accuracy                           0.96       268\n",
      "   macro avg       0.95      0.96      0.95       268\n",
      "weighted avg       0.96      0.96      0.96       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9254\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.74      0.83      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 149.39524936676025 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5552, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4711, Accuracy: 0.8237, F1 Micro: 0.8992, F1 Macro: 0.898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3416, Accuracy: 0.9368, F1 Micro: 0.9609, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2271, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0784, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9737\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.529, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2278, Accuracy: 0.9621, F1 Micro: 0.9621, F1 Macro: 0.9574\n",
      "Epoch 3/10, Train Loss: 0.1804, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9621, F1 Micro: 0.9621, F1 Macro: 0.9574\n",
      "Epoch 6/10, Train Loss: 0.0915, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9413\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9413\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9535\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9524\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9413\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9621, F1 Micro: 0.9621, F1 Macro: 0.9574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        87\n",
      "    positive       0.98      0.97      0.97       177\n",
      "\n",
      "    accuracy                           0.96       264\n",
      "   macro avg       0.95      0.96      0.96       264\n",
      "weighted avg       0.96      0.96      0.96       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9314\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.88      0.88       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.96      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.38029551506042 s\n",
      "Averaged - Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9222\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 365.92626672737606\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 10.727838039398193 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.528, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4398, Accuracy: 0.8676, F1 Micro: 0.9214, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3047, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.205, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9735\n",
      "Epoch 6/10, Train Loss: 0.116, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4962, Accuracy: 0.9104, F1 Micro: 0.9104, F1 Macro: 0.9022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.225, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "Epoch 3/10, Train Loss: 0.1687, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "Epoch 4/10, Train Loss: 0.1309, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0912, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 6/10, Train Loss: 0.1231, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "Epoch 9/10, Train Loss: 0.0385, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9323\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9225\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 154.34989285469055 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.536, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4334, Accuracy: 0.8832, F1 Micro: 0.9292, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3051, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2109, Accuracy: 0.9449, F1 Micro: 0.9654, F1 Macro: 0.9632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1475, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1175, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0845, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5284, Accuracy: 0.8851, F1 Micro: 0.8851, F1 Macro: 0.8707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.273, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9393\n",
      "Epoch 3/10, Train Loss: 0.1808, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Epoch 4/10, Train Loss: 0.1499, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9655, F1 Micro: 0.9655, F1 Macro: 0.9609\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9617, F1 Micro: 0.9617, F1 Macro: 0.9571\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9617, F1 Micro: 0.9617, F1 Macro: 0.9571\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9483\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9617, F1 Micro: 0.9617, F1 Macro: 0.9571\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9655, F1 Micro: 0.9655, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.95      0.95        85\n",
      "    positive       0.98      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.97       261\n",
      "   macro avg       0.96      0.96      0.96       261\n",
      "weighted avg       0.97      0.97      0.97       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9346\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.95803451538086 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5576, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4538, Accuracy: 0.8571, F1 Micro: 0.916, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3273, Accuracy: 0.9382, F1 Micro: 0.9615, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2308, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.158, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0933, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 8/10, Train Loss: 0.0817, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9733\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0566, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5035, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2366, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1588, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1506, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9522\n",
      "Epoch 5/10, Train Loss: 0.1129, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9358\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.944\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9276\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9235\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.95       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.928\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 154.9147663116455 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9596, F1 Micro: 0.9596, F1 Macro: 0.9284\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 366.88014913643633\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 9.66479754447937 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.521, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4396, Accuracy: 0.8668, F1 Micro: 0.9217, F1 Macro: 0.9214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3106, Accuracy: 0.939, F1 Micro: 0.9622, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.201, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1499, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.107, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5162, Accuracy: 0.8973, F1 Micro: 0.8973, F1 Macro: 0.8885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2596, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1808, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9443\n",
      "Epoch 4/10, Train Loss: 0.1625, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9114\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.0665, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9244\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        86\n",
      "    positive       0.97      0.97      0.97       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.95      0.95      0.95       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9259\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.87      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.35170245170593 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5347, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4441, Accuracy: 0.8757, F1 Micro: 0.9257, F1 Macro: 0.9244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3184, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2118, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1128, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0621, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.492, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2493, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2056, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9433\n",
      "Epoch 5/10, Train Loss: 0.1177, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9605\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9426\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 10/10, Train Loss: 0.0354, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.96      0.95        84\n",
      "    positive       0.98      0.97      0.97       172\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.96      0.96      0.96       256\n",
      "weighted avg       0.97      0.96      0.96       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9293\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 169.8617901802063 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5526, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4662, Accuracy: 0.8385, F1 Micro: 0.907, F1 Macro: 0.9057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.335, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2234, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9767\n",
      "Epoch 7/10, Train Loss: 0.0941, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9729\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0534, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5152, Accuracy: 0.9019, F1 Micro: 0.9019, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2606, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9327\n",
      "Epoch 3/10, Train Loss: 0.1757, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9088\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1294, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9401\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9255\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9327\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9367\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9331\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        87\n",
      "    positive       0.96      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.94      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9191\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.83      0.85      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      1.00      0.98        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.94      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 164.47738432884216 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9604, F1 Micro: 0.9604, F1 Macro: 0.9248\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 388.8291579927215\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 8.753942251205444 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5398, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4404, Accuracy: 0.8772, F1 Micro: 0.9275, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2816, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1278, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9729\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5041, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.245, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1639, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9248\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1094, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "Epoch 8/10, Train Loss: 0.0823, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9175\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 164.3631763458252 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5487, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4332, Accuracy: 0.8847, F1 Micro: 0.9307, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2867, Accuracy: 0.9323, F1 Micro: 0.9573, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1985, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.444, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.8473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.186, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9568\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 6/10, Train Loss: 0.1413, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9446\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9577, F1 Micro: 0.9577, F1 Macro: 0.9521\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9301\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        85\n",
      "    positive       0.98      0.96      0.97       175\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.95      0.96      0.96       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9333\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.45841717720032 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5676, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4668, Accuracy: 0.8504, F1 Micro: 0.9129, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3184, Accuracy: 0.9368, F1 Micro: 0.9602, F1 Macro: 0.958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2183, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1451, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.117, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0911, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5116, Accuracy: 0.9042, F1 Micro: 0.9042, F1 Macro: 0.8972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2066, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9489\n",
      "Epoch 5/10, Train Loss: 0.1073, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 6/10, Train Loss: 0.0885, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Epoch 8/10, Train Loss: 0.1015, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9125\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.96      0.95       261\n",
      "weighted avg       0.96      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9315\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 174.24139952659607 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9274\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 382.78653538452215\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 7.330883741378784 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5366, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4287, Accuracy: 0.8683, F1 Micro: 0.9216, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2881, Accuracy: 0.942, F1 Micro: 0.9638, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1949, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1364, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0995, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.97      0.90      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4724, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 2/10, Train Loss: 0.2521, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9101\n",
      "Epoch 3/10, Train Loss: 0.174, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 4/10, Train Loss: 0.1401, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9186\n",
      "Epoch 5/10, Train Loss: 0.1475, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1303, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9349\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 8/10, Train Loss: 0.052, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9174\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9365\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        85\n",
      "    positive       0.97      0.95      0.96       184\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.94      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9152\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.97      0.90      0.94       152\n",
      "    positive       0.76      0.90      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.91      0.88       216\n",
      "weighted avg       0.91      0.90      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.72061014175415 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5406, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4187, Accuracy: 0.8802, F1 Micro: 0.9271, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2953, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2021, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1431, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9643, F1 Micro: 0.9773, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9761\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.8551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2713, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9486\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9486\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9525\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        85\n",
      "    positive       0.99      0.95      0.97       174\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.95       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9292\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 166.59678483009338 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5586, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4427, Accuracy: 0.8839, F1 Micro: 0.93, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3123, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1119, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4845, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2329, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.151, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9528\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Epoch 6/10, Train Loss: 0.1267, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "Epoch 7/10, Train Loss: 0.0674, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9048\n",
      "Epoch 9/10, Train Loss: 0.0872, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Epoch 10/10, Train Loss: 0.0663, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.96      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9318\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.60080075263977 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9601, F1 Micro: 0.9601, F1 Macro: 0.9254\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 378.4891931300732\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.869110107421875 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.8095, F1 Micro: 0.8923, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.429, Accuracy: 0.8876, F1 Micro: 0.9322, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2732, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1791, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9758\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.473, Accuracy: 0.8764, F1 Micro: 0.8764, F1 Macro: 0.8687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2601, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.9003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1967, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1302, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.936\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1166, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.936\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0759, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9481\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0367, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        85\n",
      "    positive       0.98      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.96       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9284\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.5162172317505 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5379, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.422, Accuracy: 0.9003, F1 Micro: 0.9388, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2784, Accuracy: 0.9464, F1 Micro: 0.9669, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1365, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9798\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5665, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.18, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Epoch 4/10, Train Loss: 0.1369, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0952, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9308\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.926\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9554, F1 Micro: 0.9554, F1 Macro: 0.9493\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9554, F1 Micro: 0.9554, F1 Macro: 0.9493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.97       182\n",
      "\n",
      "    accuracy                           0.96       269\n",
      "   macro avg       0.95      0.95      0.95       269\n",
      "weighted avg       0.96      0.96      0.96       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9329\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.36355924606323 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.555, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4469, Accuracy: 0.8876, F1 Micro: 0.9318, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2973, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1994, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1432, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.8842, F1 Micro: 0.8842, F1 Macro: 0.867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1819, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Epoch 4/10, Train Loss: 0.1585, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9202\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8972\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9445\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9233\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.2203552722931 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9282\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 365.2041363009914\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.615616083145142 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.416, Accuracy: 0.9226, F1 Micro: 0.9526, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2589, Accuracy: 0.9539, F1 Micro: 0.9715, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0721, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5145, Accuracy: 0.8985, F1 Micro: 0.8985, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.229, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1593, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.134, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9321\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9061\n",
      "Epoch 6/10, Train Loss: 0.133, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9191\n",
      "Epoch 7/10, Train Loss: 0.0693, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9056\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        87\n",
      "    positive       0.96      0.95      0.96       179\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.93      0.93       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9238\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.89      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.62043952941895 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5306, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4233, Accuracy: 0.9025, F1 Micro: 0.941, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2694, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Epoch 5/10, Train Loss: 0.1314, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9717, F1 Micro: 0.9823, F1 Macro: 0.9812\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9823, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4664, Accuracy: 0.9059, F1 Micro: 0.9059, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2237, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.176, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "Epoch 4/10, Train Loss: 0.1299, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 5/10, Train Loss: 0.1031, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9396\n",
      "Epoch 6/10, Train Loss: 0.0696, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9354\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.96      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.93\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 169.39561986923218 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.552, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4425, Accuracy: 0.9055, F1 Micro: 0.9428, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2892, Accuracy: 0.9494, F1 Micro: 0.9688, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.974, F1 Micro: 0.9837, F1 Macro: 0.9828\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.974, F1 Micro: 0.9837, F1 Macro: 0.9828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.457, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 2/10, Train Loss: 0.2309, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9203\n",
      "Epoch 3/10, Train Loss: 0.1512, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.8965\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1298, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.1245, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9483\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9523\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.94      0.94        87\n",
      "    positive       0.97      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.95      0.95      0.95       256\n",
      "weighted avg       0.96      0.96      0.96       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9668, F1 Micro: 0.9668, F1 Macro: 0.9349\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 180.8970446586609 s\n",
      "Averaged - Iteration 806: Accuracy: 0.964, F1 Micro: 0.964, F1 Macro: 0.9296\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 247.00378938965835\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.226810455322266 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5353, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.418, Accuracy: 0.9092, F1 Micro: 0.9447, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2635, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4563, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Epoch 2/10, Train Loss: 0.2157, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "Epoch 3/10, Train Loss: 0.1664, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Epoch 5/10, Train Loss: 0.099, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.943\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9241\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9223\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 184.24588537216187 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4142, Accuracy: 0.9062, F1 Micro: 0.942, F1 Macro: 0.9388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2753, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9782\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4917, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9155\n",
      "Epoch 2/10, Train Loss: 0.2336, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9442\n",
      "Epoch 4/10, Train Loss: 0.1231, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9523\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9153\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        85\n",
      "    positive       0.99      0.95      0.97       174\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.95       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9325\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.7189610004425 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5617, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4436, Accuracy: 0.9077, F1 Micro: 0.9434, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2943, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 7/10, Train Loss: 0.0855, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5078, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8828\n",
      "Epoch 2/10, Train Loss: 0.2387, Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.8856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1716, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.9011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.083, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9444\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9291\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.35152339935303 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.928\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 164.91823287206861\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.2959816455841064 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5196, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4003, Accuracy: 0.9375, F1 Micro: 0.9615, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.25, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1584, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4878, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2158, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 3/10, Train Loss: 0.1213, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.125, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 5/10, Train Loss: 0.0997, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9244\n",
      "Epoch 7/10, Train Loss: 0.092, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 9/10, Train Loss: 0.0574, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9156\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9096\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       183\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9234\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.5494589805603 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5288, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3968, Accuracy: 0.9249, F1 Micro: 0.9535, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2569, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0623, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4557, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2056, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Epoch 3/10, Train Loss: 0.1312, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Epoch 4/10, Train Loss: 0.1268, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9224\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1087, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9355\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9314\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        86\n",
      "    positive       0.96      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.94      0.94       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 174.0315637588501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5425, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4171, Accuracy: 0.9204, F1 Micro: 0.9509, F1 Macro: 0.9491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.268, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9788\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.95      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4526, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2201, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 3/10, Train Loss: 0.1627, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9098\n",
      "Epoch 4/10, Train Loss: 0.1303, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9182\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9116\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1044, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.071, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9186\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        86\n",
      "    positive       0.97      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.94       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.41692066192627 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9604, F1 Micro: 0.9604, F1 Macro: 0.9227\n",
      "Launching training on 2 GPUs.\n",
      "BESRA Uncertainty Score Threshold 42.22116630817719\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.350233554840088 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.513, Accuracy: 0.7991, F1 Micro: 0.8871, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4053, Accuracy: 0.9308, F1 Micro: 0.9577, F1 Macro: 0.9565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2495, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1569, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4511, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2342, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1517, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1351, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Epoch 5/10, Train Loss: 0.1068, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9242\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 184.5093104839325 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5206, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4065, Accuracy: 0.91, F1 Micro: 0.9445, F1 Macro: 0.9426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2573, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1643, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9765\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9805\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.91      0.98      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.943\n",
      "Epoch 4/10, Train Loss: 0.1246, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.141, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9602, F1 Micro: 0.9602, F1 Macro: 0.9558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9602, F1 Micro: 0.9602, F1 Macro: 0.956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9602, F1 Micro: 0.9602, F1 Macro: 0.956\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9413\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9602, F1 Micro: 0.9602, F1 Macro: 0.956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.99      0.94        83\n",
      "    positive       0.99      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       251\n",
      "   macro avg       0.95      0.97      0.96       251\n",
      "weighted avg       0.96      0.96      0.96       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9318\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 179.971444606781 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5399, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.434, Accuracy: 0.91, F1 Micro: 0.9447, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2723, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1731, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.101, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9806\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9811\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.519, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2261, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 3/10, Train Loss: 0.1793, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1262, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1075, Accuracy: 0.9643, F1 Micro: 0.9643, F1 Macro: 0.9606\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9467\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9519\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9643, F1 Micro: 0.9643, F1 Macro: 0.9606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        85\n",
      "    positive       0.99      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.97      0.96       252\n",
      "weighted avg       0.97      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9355\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.99      0.95       152\n",
      "    positive       0.97      0.75      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 176.59075498580933 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9266\n",
      "Total runtime: 10863.809392929077 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6175447,
     "sourceId": 10843157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10894.839497,
   "end_time": "2025-06-08T22:10:51.053204",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-08T19:09:16.213707",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01c31fe3a5a74d44affec207b3ac8e64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d7f2c73c6e224bed82d43c0740732220",
       "placeholder": "",
       "style": "IPY_MODEL_abee1ddf99f7446097dcac1e0ac84175",
       "tabbable": null,
       "tooltip": null,
       "value": "1.53k/1.53k[00:00&lt;00:00,158kB/s]"
      }
     },
     "052d4d1427c84e6788546fa94aa48724": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "176d83eff0c14a248f2a8ec85fc66491": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2cdaf0611b9e4f38bca1e7768d793abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "313bc7deba424e368ee488a5afd2f15d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3839c9740a1345389bfbbeecea20c12b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e68c15a05b6f475b90050ba4e1ed8eb2",
        "IPY_MODEL_d0b1f178b4c8441cba85f708030467ec",
        "IPY_MODEL_ab1960932ddd42adb3f06879e3b7bd07"
       ],
       "layout": "IPY_MODEL_4390765173c74f1cbde9fc222c135953",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3aadddd578174dac926ce015e4e92e7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f0fbd969454a42eab186d1ebfe164999",
       "placeholder": "",
       "style": "IPY_MODEL_c0667b2d17d6444b902c4760fbc28e83",
       "tabbable": null,
       "tooltip": null,
       "value": "2.00/2.00[00:00&lt;00:00,160B/s]"
      }
     },
     "3cc11914054749eb8f32d0e54d6d7472": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d7bf86dcd0c4f3ea6ac8d412ca8ec3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dd942ab234524617bd27210e92efbb9d",
       "placeholder": "",
       "style": "IPY_MODEL_7193508bd432469fbf9702cdc06ae90f",
       "tabbable": null,
       "tooltip": null,
       "value": "112/112[00:00&lt;00:00,12.8kB/s]"
      }
     },
     "40eb2559291a4cee876d9d0af8646855": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4390765173c74f1cbde9fc222c135953": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5287a55a8d5242a7a1980d366574a44c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d12a5e333bf644a7a94797a6c2dee154",
        "IPY_MODEL_b9f203d8a0c54c75aabfaaf8fc8a5ad0",
        "IPY_MODEL_01c31fe3a5a74d44affec207b3ac8e64"
       ],
       "layout": "IPY_MODEL_c75f842a584049a390ee2f0c83add176",
       "tabbable": null,
       "tooltip": null
      }
     },
     "57408c741e0e46a79617122ce57eec65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "608aa8eb0e92477f9e33169e030f5e08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b5fd02283279445ab04dbd2ce7500140",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6ddf2e45684f47b3828ccc937d2a44ef",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "6ddf2e45684f47b3828ccc937d2a44ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7158b64de98c4bd4bbe3af65d86d2db6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8d2fd29e0da4941a512df70c2602575",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_052d4d1427c84e6788546fa94aa48724",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "7193508bd432469fbf9702cdc06ae90f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "72bc89d562674894883d34e956e00c50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75a9e0f0378a4d24afbe96320bb3beb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "80a22c00e2f54121bda844d0031728f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "875ff10da14848ed9f2b4e401f67817f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8e3ea796961c45ffae89d64cc6d68629": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a9aa56f237244afa9635e986ecb7fc26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa50720e6fd84463aa5d203dec0f0058": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab1960932ddd42adb3f06879e3b7bd07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd12de78e4864af6abf2df03c66388a2",
       "placeholder": "",
       "style": "IPY_MODEL_75a9e0f0378a4d24afbe96320bb3beb1",
       "tabbable": null,
       "tooltip": null,
       "value": "229k/229k[00:00&lt;00:00,5.68MB/s]"
      }
     },
     "abee1ddf99f7446097dcac1e0ac84175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b5fd02283279445ab04dbd2ce7500140": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b607acabff834f6391dca2329e820523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_313bc7deba424e368ee488a5afd2f15d",
       "placeholder": "",
       "style": "IPY_MODEL_875ff10da14848ed9f2b4e401f67817f",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:100%"
      }
     },
     "b8d2fd29e0da4941a512df70c2602575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9f203d8a0c54c75aabfaaf8fc8a5ad0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3cc11914054749eb8f32d0e54d6d7472",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8e3ea796961c45ffae89d64cc6d68629",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "c0667b2d17d6444b902c4760fbc28e83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c4f4b1a77005431d90f296e45b907836": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b607acabff834f6391dca2329e820523",
        "IPY_MODEL_7158b64de98c4bd4bbe3af65d86d2db6",
        "IPY_MODEL_3aadddd578174dac926ce015e4e92e7e"
       ],
       "layout": "IPY_MODEL_176d83eff0c14a248f2a8ec85fc66491",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c75f842a584049a390ee2f0c83add176": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbfc5cc631e040be820d4d48335b637b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e671580b75494cda8dd3f989540dc40f",
       "placeholder": "",
       "style": "IPY_MODEL_2cdaf0611b9e4f38bca1e7768d793abc",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:100%"
      }
     },
     "cd12de78e4864af6abf2df03c66388a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0b1f178b4c8441cba85f708030467ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_80a22c00e2f54121bda844d0031728f3",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_40eb2559291a4cee876d9d0af8646855",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "d12a5e333bf644a7a94797a6c2dee154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a9aa56f237244afa9635e986ecb7fc26",
       "placeholder": "",
       "style": "IPY_MODEL_d25707554b2944b180bcabdb642cf954",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "d25707554b2944b180bcabdb642cf954": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d47db88806b1404e9aaaa97ed0188005": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cbfc5cc631e040be820d4d48335b637b",
        "IPY_MODEL_608aa8eb0e92477f9e33169e030f5e08",
        "IPY_MODEL_3d7bf86dcd0c4f3ea6ac8d412ca8ec3b"
       ],
       "layout": "IPY_MODEL_72bc89d562674894883d34e956e00c50",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d7f2c73c6e224bed82d43c0740732220": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd942ab234524617bd27210e92efbb9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e671580b75494cda8dd3f989540dc40f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e68c15a05b6f475b90050ba4e1ed8eb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aa50720e6fd84463aa5d203dec0f0058",
       "placeholder": "",
       "style": "IPY_MODEL_57408c741e0e46a79617122ce57eec65",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:100%"
      }
     },
     "f0fbd969454a42eab186d1ebfe164999": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
