{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2110bf",
   "metadata": {
    "papermill": {
     "duration": 0.012612,
     "end_time": "2025-04-01T05:00:06.243302",
     "exception": false,
     "start_time": "2025-04-01T05:00:06.230690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66ab67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:06.266992Z",
     "iopub.status.busy": "2025-04-01T05:00:06.266682Z",
     "iopub.status.idle": "2025-04-01T05:00:29.291136Z",
     "shell.execute_reply": "2025-04-01T05:00:29.290379Z"
    },
    "papermill": {
     "duration": 23.03804,
     "end_time": "2025-04-01T05:00:29.292805",
     "exception": false,
     "start_time": "2025-04-01T05:00:06.254765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba36703",
   "metadata": {
    "papermill": {
     "duration": 0.011122,
     "end_time": "2025-04-01T05:00:29.315740",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.304618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a7ee0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.344047Z",
     "iopub.status.busy": "2025-04-01T05:00:29.343511Z",
     "iopub.status.idle": "2025-04-01T05:00:29.347006Z",
     "shell.execute_reply": "2025-04-01T05:00:29.346383Z"
    },
    "papermill": {
     "duration": 0.021303,
     "end_time": "2025-04-01T05:00:29.348209",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.326906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c55dff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.371635Z",
     "iopub.status.busy": "2025-04-01T05:00:29.371400Z",
     "iopub.status.idle": "2025-04-01T05:00:29.374999Z",
     "shell.execute_reply": "2025-04-01T05:00:29.374369Z"
    },
    "papermill": {
     "duration": 0.016569,
     "end_time": "2025-04-01T05:00:29.376227",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.359658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b5d9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.400061Z",
     "iopub.status.busy": "2025-04-01T05:00:29.399823Z",
     "iopub.status.idle": "2025-04-01T05:00:29.408568Z",
     "shell.execute_reply": "2025-04-01T05:00:29.407773Z"
    },
    "papermill": {
     "duration": 0.02222,
     "end_time": "2025-04-01T05:00:29.409904",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.387684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4155fc",
   "metadata": {
    "papermill": {
     "duration": 0.010779,
     "end_time": "2025-04-01T05:00:29.433212",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.422433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da01387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.456057Z",
     "iopub.status.busy": "2025-04-01T05:00:29.455808Z",
     "iopub.status.idle": "2025-04-01T05:00:29.528602Z",
     "shell.execute_reply": "2025-04-01T05:00:29.526747Z"
    },
    "papermill": {
     "duration": 0.086875,
     "end_time": "2025-04-01T05:00:29.531077",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.444202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "aspect_accuracies = manager.list()\n",
    "aspect_f1_micros = manager.list()\n",
    "aspect_f1_macros = manager.list()\n",
    "sentiment_accuracies = manager.list()\n",
    "sentiment_f1_micros = manager.list()\n",
    "sentiment_f1_macros = manager.list()\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'casa-kmeans'\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "sequence_length = 48\n",
    "\n",
    "aspect_list = ['fuel', 'machine', 'others', 'part', 'price', 'service']\n",
    "aspect_mapping = {'fuel': 0, 'machine': 1, 'others': 2, 'part': 3, 'price': 4, 'service': 5 }\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, 'positive': 2}\n",
    "ignored_keys = ['labels', 'ori_text', 'ori_label', 'ori_indices', 'aspect']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3515158",
   "metadata": {
    "papermill": {
     "duration": 0.011464,
     "end_time": "2025-04-01T05:00:29.558691",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.547227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5b05ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.582432Z",
     "iopub.status.busy": "2025-04-01T05:00:29.582085Z",
     "iopub.status.idle": "2025-04-01T05:00:29.652964Z",
     "shell.execute_reply": "2025-04-01T05:00:29.652144Z"
    },
    "papermill": {
     "duration": 0.084327,
     "end_time": "2025-04-01T05:00:29.654222",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.569895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/casa-dataset/train_preprocess.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/casa-dataset/valid_preprocess.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('/kaggle/input/casa-dataset/test_preprocess.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652b81f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.678047Z",
     "iopub.status.busy": "2025-04-01T05:00:29.677818Z",
     "iopub.status.idle": "2025-04-01T05:00:29.686849Z",
     "shell.execute_reply": "2025-04-01T05:00:29.686061Z"
    },
    "papermill": {
     "duration": 0.022367,
     "end_time": "2025-04-01T05:00:29.688273",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.665906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cad4273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.712063Z",
     "iopub.status.busy": "2025-04-01T05:00:29.711849Z",
     "iopub.status.idle": "2025-04-01T05:00:29.719084Z",
     "shell.execute_reply": "2025-04-01T05:00:29.718516Z"
    },
    "papermill": {
     "duration": 0.020409,
     "end_time": "2025-04-01T05:00:29.720289",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.699880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c880f14d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.744049Z",
     "iopub.status.busy": "2025-04-01T05:00:29.743832Z",
     "iopub.status.idle": "2025-04-01T05:00:29.754408Z",
     "shell.execute_reply": "2025-04-01T05:00:29.753704Z"
    },
    "papermill": {
     "duration": 0.023811,
     "end_time": "2025-04-01T05:00:29.755739",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.731928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864,) (864, 6)\n",
      "(216,) (216, 6)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data.columns[1:]\n",
    "val_labels = val_data.columns[1:]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['sentence'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['sentence'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc732b",
   "metadata": {
    "papermill": {
     "duration": 0.0111,
     "end_time": "2025-04-01T05:00:29.778246",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.767146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec7ae5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.801894Z",
     "iopub.status.busy": "2025-04-01T05:00:29.801644Z",
     "iopub.status.idle": "2025-04-01T05:00:29.807640Z",
     "shell.execute_reply": "2025-04-01T05:00:29.807028Z"
    },
    "papermill": {
     "duration": 0.019212,
     "end_time": "2025-04-01T05:00:29.808802",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.789590",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AspectDetectionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, label_mapping, tokenizer, max_length=sequence_length, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        original_labels = [self.label_mapping[label] for label in self.labels[idx]]\n",
    "        encoded_labels = [1 if label == 1 else 0 for label in original_labels]\n",
    "        \n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['ori_indices'] = idx\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(original_labels, dtype=torch.float)\n",
    "        item['labels'] = torch.tensor(encoded_labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acaeef30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.832559Z",
     "iopub.status.busy": "2025-04-01T05:00:29.832271Z",
     "iopub.status.idle": "2025-04-01T05:00:29.839004Z",
     "shell.execute_reply": "2025-04-01T05:00:29.838404Z"
    },
    "papermill": {
     "duration": 0.019951,
     "end_time": "2025-04-01T05:00:29.840198",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.820247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, texts, labels, aspects, indices, label_mapping, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.aspects = aspects\n",
    "        self.indices = indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = aspect_token + ' ' + self.aspects[idx] + ' ' + review_token + ' ' + self.texts[idx] \n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        if isinstance(self.labels[idx], str):\n",
    "            self.labels[idx] = self.label_mapping[self.labels[idx]]\n",
    "        elif torch.is_tensor(self.labels[idx]):\n",
    "            self.labels[idx] = int(self.labels[idx].item())\n",
    "\n",
    "        encoded_label = 1 if self.labels[idx] == 2 else self.labels[idx]\n",
    "        one_hot_label = F.one_hot(torch.tensor(encoded_label, dtype=torch.long), num_classes=2).float()\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['aspect'] = self.aspects[idx]\n",
    "        item['labels'] = one_hot_label\n",
    "        item['ori_indices'] = self.indices[idx]\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963cf303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:29.863949Z",
     "iopub.status.busy": "2025-04-01T05:00:29.863729Z",
     "iopub.status.idle": "2025-04-01T05:00:30.510770Z",
     "shell.execute_reply": "2025-04-01T05:00:30.510068Z"
    },
    "papermill": {
     "duration": 0.660637,
     "end_time": "2025-04-01T05:00:30.512282",
     "exception": false,
     "start_time": "2025-04-01T05:00:29.851645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6e8d858c364d5f846accf2120ca2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a4d90356a64ca79556e7390f49e421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce40ae5e9c245e3864942a0774a2b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaef4551401a454fa836cbe819c24711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "review_token = '[REVIEW]'\n",
    "aspect_token = '[ASPECT]'\n",
    "special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168cd806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.538604Z",
     "iopub.status.busy": "2025-04-01T05:00:30.538341Z",
     "iopub.status.idle": "2025-04-01T05:00:30.542846Z",
     "shell.execute_reply": "2025-04-01T05:00:30.542172Z"
    },
    "papermill": {
     "duration": 0.019024,
     "end_time": "2025-04-01T05:00:30.544053",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.525029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_aspect_dataset(X_train, y_train, X_val, y_val, sequence_length, num_workers=4):\n",
    "    train_dataset = AspectDetectionDataset(X_train, y_train, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = AspectDetectionDataset(X_val, y_val, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "505d8992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.569234Z",
     "iopub.status.busy": "2025-04-01T05:00:30.569017Z",
     "iopub.status.idle": "2025-04-01T05:00:30.578556Z",
     "shell.execute_reply": "2025-04-01T05:00:30.577869Z"
    },
    "papermill": {
     "duration": 0.023602,
     "end_time": "2025-04-01T05:00:30.579779",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.556177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_sentiment_dataset(device, train_dataset, val_dataset, aspect_detection_model, tokenizer, max_length=sequence_length):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    aspect_detection_model.to(device)\n",
    "    aspect_detection_model.eval()\n",
    "\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_aspects = []\n",
    "    train_indices = []\n",
    "\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    val_aspects = []\n",
    "    val_indices = []\n",
    "\n",
    "    # Transform train set\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        train_aspects.append(aspect_list[j])\n",
    "                        train_data.append(batch['ori_text'][i])\n",
    "                        train_labels.append(batch['ori_label'][i][j])\n",
    "                        train_indices.append(batch['ori_indices'][i])\n",
    "            \n",
    "        # Transform validation set\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        val_aspects.append(aspect_list[j])\n",
    "                        val_data.append(batch['ori_text'][i])\n",
    "                        val_labels.append(batch['ori_label'][i][j])\n",
    "                        val_indices.append(batch['ori_indices'][i])\n",
    "\n",
    "    train_dataset = SentimentAnalysisDataset(train_data, train_labels, train_aspects, train_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "    val_dataset = SentimentAnalysisDataset(val_data, val_labels, val_aspects, val_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da23c42",
   "metadata": {
    "papermill": {
     "duration": 0.011696,
     "end_time": "2025-04-01T05:00:30.603389",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.591693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5e1c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.628171Z",
     "iopub.status.busy": "2025-04-01T05:00:30.627926Z",
     "iopub.status.idle": "2025-04-01T05:00:30.631435Z",
     "shell.execute_reply": "2025-04-01T05:00:30.630805Z"
    },
    "papermill": {
     "duration": 0.017352,
     "end_time": "2025-04-01T05:00:30.632598",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.615246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "917309f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.656912Z",
     "iopub.status.busy": "2025-04-01T05:00:30.656687Z",
     "iopub.status.idle": "2025-04-01T05:00:30.661130Z",
     "shell.execute_reply": "2025-04-01T05:00:30.660523Z"
    },
    "papermill": {
     "duration": 0.017966,
     "end_time": "2025-04-01T05:00:30.662359",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.644393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p, label, classes):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=label,\n",
    "        target_names=classes,\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b9545e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.686569Z",
     "iopub.status.busy": "2025-04-01T05:00:30.686342Z",
     "iopub.status.idle": "2025-04-01T05:00:30.692220Z",
     "shell.execute_reply": "2025-04-01T05:00:30.691649Z"
    },
    "papermill": {
     "duration": 0.019518,
     "end_time": "2025-04-01T05:00:30.693529",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.674011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_overall(p, classes):\n",
    "    preds = torch.tensor(p.predictions)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Ensure it's in the correct shape\n",
    "    if preds.shape != labels.shape:\n",
    "        raise ValueError(\"Shape mismatch: predictions and labels must have the same shape.\")\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Compute per-label (column-wise) precision, recall, F1\n",
    "    precision_list, recall_list, f1_micro_list, f1_macro_list = [], [], [], []\n",
    "    \n",
    "    for i in range(labels.shape[1]):  # Loop through each column (multi-output)\n",
    "        prec, rec, f1_micro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='micro', zero_division=0\n",
    "        )\n",
    "        _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        precision_list.append(prec)\n",
    "        recall_list.append(rec)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        f1_macro_list.append(f1_macro)\n",
    "\n",
    "    # Compute average metrics across all outputs\n",
    "    precision = sum(precision_list) / len(precision_list)\n",
    "    recall = sum(recall_list) / len(recall_list)\n",
    "    f1_micro = sum(f1_micro_list) / len(f1_micro_list)\n",
    "    f1_macro = sum(f1_macro_list) / len(f1_macro_list)\n",
    "\n",
    "    # Generate classification report per output\n",
    "    reports = [classification_report(labels[:, i], preds[:, i], target_names=classes, zero_division=0) for i in range(labels.shape[1])]\n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'reports': reports  # Returns list of reports, one for each output label\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada7b11d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.725390Z",
     "iopub.status.busy": "2025-04-01T05:00:30.725071Z",
     "iopub.status.idle": "2025-04-01T05:00:30.753940Z",
     "shell.execute_reply": "2025-04-01T05:00:30.753046Z"
    },
    "papermill": {
     "duration": 0.044539,
     "end_time": "2025-04-01T05:00:30.755536",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.710997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, aspect_metrics, sentiment_metrics, metrics, trials, seed):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    aspect_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=len(train_labels),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in aspect_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    aspect_optimizer = torch.optim.AdamW(aspect_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    aspect_train_loader, aspect_val_loader, aspect_train_dataset, aspect_val_dataset = build_aspect_dataset(current_X_train, current_y_train, X_val, y_val, sequence_length)\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    aspect_model, aspect_optimizer, aspect_train_loader, aspect_val_loader = accelerator.prepare(\n",
    "        aspect_model, aspect_optimizer, aspect_train_loader, aspect_val_loader\n",
    "    )\n",
    "\n",
    "    aspect_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ASPECT DETECTION\n",
    "    accelerator.print(\"ASPECT DETECTION\")\n",
    "    for epoch in range(epochs):\n",
    "        aspect_model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in aspect_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            aspect_optimizer.zero_grad()\n",
    "            outputs = aspect_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            aspect_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        aspect_model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in aspect_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = aspect_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}),\n",
    "            None,\n",
    "            ['fuel', 'machine', 'others', 'part', 'price', 'service']\n",
    "        )\n",
    "\n",
    "        if aspect_result is None or result['f1_micro'] >= aspect_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(aspect_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-aspect-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            aspect_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(aspect_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"\\nAspect detection accuracy: {round(aspect_result['accuracy'], 4)}, F1 Micro: {round(aspect_result['f1_micro'], 4)}, F1 Macro: {round(aspect_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(aspect_result['report'])\n",
    "\n",
    "    best_aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{trials + 1}-model')\n",
    "    best_aspect_model = accelerator.prepare(best_aspect_model)\n",
    "\n",
    "    # SENTIMENT ANALYSIS ON NON NEUTRAL ASPECTS\n",
    "    accelerator.print(\"--------------------------------------------------\")\n",
    "    accelerator.print(\"SENTIMENT ANALYSIS\")\n",
    "\n",
    "    sentiment_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=2,\n",
    "    )\n",
    "    sentiment_optimizer = torch.optim.AdamW(sentiment_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    for name, param in sentiment_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    sentiment_train_loader, sentiment_val_loader, sentiment_train_dataset, sentiment_val_dataset = build_sentiment_dataset(\n",
    "        device, aspect_train_dataset, aspect_val_dataset, best_aspect_model, tokenizer, max_length=sequence_length\n",
    "    )\n",
    "    sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader = accelerator.prepare(\n",
    "        sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader\n",
    "    )\n",
    "    sentiment_result = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sentiment_model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in sentiment_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            sentiment_optimizer.zero_grad()\n",
    "            outputs = sentiment_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            sentiment_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        sentiment_model.eval()\n",
    "        sentiment_val_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in sentiment_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                \n",
    "                outputs = sentiment_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                for i in range(len(preds)):\n",
    "                    val_output = {\n",
    "                        'label': batch['labels'][i],\n",
    "                        'aspect': batch['aspect'][i],\n",
    "                        'ori_indices': batch['ori_indices'][i],\n",
    "                        'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                    }\n",
    "                    sentiment_val_outputs.append(val_output)\n",
    "\n",
    "        sentiment_val_outputs = accelerator.gather_for_metrics(sentiment_val_outputs)\n",
    "        unique_val_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_val_outputs}\n",
    "        sentiment_val_outputs = list(unique_val_outputs.values())\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': [item['pred'] for item in sentiment_val_outputs], 'label_ids': [np.argmax(item['label'].cpu().numpy()) for item in sentiment_val_outputs]}),\n",
    "            [0, 1],\n",
    "            ['negative', 'positive']\n",
    "        )\n",
    "\n",
    "        if sentiment_result is None or result['f1_micro'] >= sentiment_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            sentiment_result = result\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(sentiment_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                 f'{filename}-sentiment-{trials + 1}-model',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(sentiment_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    accelerator.print(f\"\\nSentiment analysis accuracy: {round(sentiment_result['accuracy'], 4)}, F1 Micro: {round(sentiment_result['f1_micro'], 4)}, F1 Macro: {round(sentiment_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(sentiment_result['report'])\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    best_sentiment_model = BertForSequenceClassification.from_pretrained( f'{filename}-sentiment-{trials + 1}-model')\n",
    "    best_sentiment_model = accelerator.prepare(best_sentiment_model)\n",
    "\n",
    "    # Compute overall metrics\n",
    "    aspect_labels = []\n",
    "    aspect_indices = []\n",
    "    aspect_preds = []\n",
    "\n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = []\n",
    "    \n",
    "    best_aspect_model.eval()\n",
    "    best_sentiment_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in aspect_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = best_aspect_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            aspect_indices.append(accelerator.gather(batch['ori_indices']))\n",
    "            aspect_labels.append(accelerator.gather(batch['ori_label']))\n",
    "            aspect_preds.append(accelerator.gather(preds))\n",
    "\n",
    "        aspect_indices = torch.cat(aspect_indices).cpu().numpy()\n",
    "        aspect_labels = torch.cat(aspect_labels).cpu().numpy()\n",
    "        aspect_preds = torch.cat(aspect_preds).cpu().numpy()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        aspect_outputs = [\n",
    "            {'ori_indices': aspect_indices[i], \n",
    "             'ori_labels': aspect_labels[i], \n",
    "             'pred': aspect_preds[i]}\n",
    "            for i in range(len(aspect_preds))\n",
    "        ]\n",
    "        aspect_outputs = {x['ori_indices'].item(): x for x in aspect_outputs}\n",
    "    \n",
    "        for batch in sentiment_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = best_sentiment_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "    \n",
    "            for i in range(len(preds)):\n",
    "                output = {\n",
    "                    'aspect': batch['aspect'][i],\n",
    "                    'ori_indices': batch['ori_indices'][i],\n",
    "                    'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                }\n",
    "                sentiment_outputs.append(output)\n",
    "\n",
    "        sentiment_outputs = accelerator.gather_for_metrics(sentiment_outputs)\n",
    "        sentiment_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_outputs}\n",
    "\n",
    "    # Replcae non neutral aspect to its predicted sentiment\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        i = -1\n",
    "        for (ori_index, aspect), value in sentiment_outputs.items():\n",
    "            aspect = aspect_mapping[aspect]\n",
    "            aspect_outputs[ori_index]['pred'][aspect] = 2 if value['pred'] == 1.0 else value['pred']\n",
    "\n",
    "        result = compute_metrics_overall(\n",
    "            type('EvalOutput', (object,), {'predictions': [output['pred'] for output in aspect_outputs.values()], 'label_ids': [output['ori_labels'] for output in aspect_outputs.values()]}),\n",
    "            ['negative', 'neutral', 'positive'],\n",
    "        )\n",
    "\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        accelerator.print(f\"Iteration {current_train_size}: Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        for i in range(len(train_labels)):\n",
    "            accelerator.print(f\"Aspect {aspect_list[i]} report:\")\n",
    "            accelerator.print(result['reports'][i])\n",
    "       \n",
    "        \n",
    "        aspect_metrics[0].append(aspect_result['accuracy'])\n",
    "        aspect_metrics[1].append(aspect_result['f1_micro'])\n",
    "        aspect_metrics[2].append(aspect_result['f1_macro'])\n",
    "        sentiment_metrics[0].append(sentiment_result['accuracy'])\n",
    "        sentiment_metrics[1].append(sentiment_result['f1_micro'])\n",
    "        sentiment_metrics[2].append(sentiment_result['f1_macro'])\n",
    "        metrics[0].append(current_train_size)\n",
    "        metrics[1].append(result['accuracy'])\n",
    "        metrics[2].append(result['f1_micro'])\n",
    "        metrics[3].append(result['f1_macro'])\n",
    "        \n",
    "    accelerator.print(f\"Total train time: {duration} s\")\n",
    "    accelerator.end_training()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb4f3c",
   "metadata": {
    "papermill": {
     "duration": 0.01235,
     "end_time": "2025-04-01T05:00:30.780104",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.767754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec21baa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.808828Z",
     "iopub.status.busy": "2025-04-01T05:00:30.808317Z",
     "iopub.status.idle": "2025-04-01T05:00:30.814564Z",
     "shell.execute_reply": "2025-04-01T05:00:30.813707Z"
    },
    "papermill": {
     "duration": 0.020995,
     "end_time": "2025-04-01T05:00:30.815973",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.794978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee7028",
   "metadata": {
    "papermill": {
     "duration": 0.012667,
     "end_time": "2025-04-01T05:00:30.841105",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.828438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344cff37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.866891Z",
     "iopub.status.busy": "2025-04-01T05:00:30.866608Z",
     "iopub.status.idle": "2025-04-01T05:00:30.887210Z",
     "shell.execute_reply": "2025-04-01T05:00:30.886393Z"
    },
    "papermill": {
     "duration": 0.034798,
     "end_time": "2025-04-01T05:00:30.888515",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.853717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kmeans_clustering_sampling(aspect_model, sentiment_model, X_pool, train_indices, remaining_indices, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    aspect_model.to(device)\n",
    "    aspect_model.eval()\n",
    "    sentiment_model.to(device)\n",
    "    sentiment_model.eval()\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    aspect_dataset = AspectDetectionDataset(\n",
    "        X_pool, \n",
    "        [['neutral' for i in range(len(train_labels))] for x in range(len(X_pool))], \n",
    "        label_mapping, \n",
    "        tokenizer, \n",
    "        max_length=sequence_length\n",
    "    )\n",
    "    aspect_loader = DataLoader(\n",
    "        aspect_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = {}\n",
    "\n",
    "    aspects = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    indices = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Pass through aspect detction model\n",
    "    for batch in aspect_loader:\n",
    "        token_type_ids = batch['token_type_ids'].to(device, non_blocking=True)\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = aspect_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).logits\n",
    "            embeddings = aspect_model.base_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            aspect_outputs[batch['ori_indices'][i].item()] = embeddings.last_hidden_state[i].mean(dim=1).cpu().numpy()\n",
    "            \n",
    "            for j in range(len(outputs[i])):\n",
    "                if int(outputs[i][j].round()) != 1:\n",
    "                    aspects.append(aspect_list[j])\n",
    "                    data.append(batch['ori_text'][i])\n",
    "                    labels.append(batch['ori_label'][i][j])\n",
    "                    indices.append(batch['ori_indices'][i])\n",
    "\n",
    "    if len(data) > 0:\n",
    "        sentiment_dataset = SentimentAnalysisDataset(data, labels, aspects, indices, label_mapping, tokenizer, max_length=sequence_length)\n",
    "        sentiment_loader = torch.utils.data.DataLoader(\n",
    "            sentiment_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4,\n",
    "        )\n",
    "    \n",
    "        # Pass through sentiment analysis model\n",
    "        for batch in sentiment_loader:\n",
    "            token_type_ids = batch['token_type_ids'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                outputs = sentiment_model.base_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    \n",
    "            for i in range(len(outputs.last_hidden_state)):\n",
    "                ori_index = batch['ori_indices'][i].item()\n",
    "                if ori_index in sentiment_outputs.keys():\n",
    "                    sentiment_outputs[ori_index].append(outputs.last_hidden_state[i].mean(dim=1).cpu().numpy())\n",
    "                else:\n",
    "                    sentiment_outputs[ori_index] = [outputs.last_hidden_state[i].mean(dim=1).cpu().numpy()]\n",
    "\n",
    "    for key, val in sentiment_outputs.items():\n",
    "        sentiment_outputs[key] = np.mean(val, axis=0)\n",
    "\n",
    "    collected_indices = set()  # Initialize set to store selected indices\n",
    "    thresholds = []\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        aspect_outputs = dict(sorted(aspect_outputs.items()))\n",
    "\n",
    "        if len(data) > 0:\n",
    "            for key, val in sentiment_outputs.items():\n",
    "                aspect_outputs[key] = np.mean([val, aspect_outputs[key]], axis=0)\n",
    "\n",
    "        embeddings = np.array(list(aspect_outputs.values()))\n",
    "        target_samples = len(embeddings[:math.ceil(0.1 * len(embeddings))])\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "                \n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'fuel': [y_train[i][0] for i in temp],\n",
    "                'machine': [y_train[i][1] for i in temp],\n",
    "                'others': [y_train[i][2] for i in temp],\n",
    "                'part': [y_train[i][3] for i in temp],\n",
    "                'price': [y_train[i][4] for i in temp],\n",
    "                'service': [y_train[i][5] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(embeddings)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(embeddings[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'fuel': [y_train[i][0] for i in temp],\n",
    "                    'machine': [y_train[i][1] for i in temp],\n",
    "                    'others': [y_train[i][2] for i in temp],\n",
    "                    'part': [y_train[i][3] for i in temp],\n",
    "                    'price': [y_train[i][4] for i in temp],\n",
    "                    'service': [y_train[i][5] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc873e77",
   "metadata": {
    "papermill": {
     "duration": 0.011809,
     "end_time": "2025-04-01T05:00:30.912398",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.900589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2166010b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.937288Z",
     "iopub.status.busy": "2025-04-01T05:00:30.936996Z",
     "iopub.status.idle": "2025-04-01T05:00:30.946882Z",
     "shell.execute_reply": "2025-04-01T05:00:30.946028Z"
    },
    "papermill": {
     "duration": 0.023707,
     "end_time": "2025-04-01T05:00:30.948059",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.924352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    aspect_accuracies = manager.list()\n",
    "    aspect_f1_micros = manager.list()\n",
    "    aspect_f1_macros = manager.list()\n",
    "    sentiment_accuracies = manager.list()\n",
    "    sentiment_f1_micros = manager.list()\n",
    "    sentiment_f1_macros = manager.list()\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "\n",
    "    set_seed(seed)\n",
    "    \n",
    "    print(\"===============================================\")\n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        # Train the model\n",
    "        args = (\n",
    "            current_train_size, \n",
    "            train_indices, \n",
    "            (aspect_accuracies, aspect_f1_micros, aspect_f1_macros), \n",
    "            (sentiment_accuracies, sentiment_f1_micros, sentiment_f1_macros),\n",
    "            (data_used, accuracies, f1_micros, f1_macros), \n",
    "            i,\n",
    "            seed\n",
    "        )\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{i+1}-model')\n",
    "        sentiment_model = BertForSequenceClassification.from_pretrained(f'{filename}-sentiment-{i+1}-model')\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (\n",
    "            aspect_model, \n",
    "            sentiment_model, \n",
    "            [X_train[i] for i in remaining_indices], \n",
    "            train_indices, \n",
    "            remaining_indices, \n",
    "            sampling_dur, \n",
    "            new_samples, \n",
    "            i\n",
    "        )\n",
    "        notebook_launcher(kmeans_clustering_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    args = (\n",
    "        current_train_size, \n",
    "        train_indices, \n",
    "        (aspect_accuracies, aspect_f1_micros, aspect_f1_macros), \n",
    "        (sentiment_accuracies, sentiment_f1_micros, sentiment_f1_macros),\n",
    "        (data_used, accuracies, f1_micros, f1_macros), \n",
    "        i,\n",
    "        seed\n",
    "    )\n",
    "    notebook_launcher(train_model, args, num_processes=2)\n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Aspect Accuracy': aspect_accuracies,\n",
    "        'Aspect F1 Micro': aspect_f1_micros,\n",
    "        'Aspect F1 Macro': aspect_f1_macros,\n",
    "        'Sentiment Accuracy': sentiment_accuracies,\n",
    "        'Sentiment F1 Micro': sentiment_f1_micros,\n",
    "        'Sentiment F1 Macro': sentiment_f1_macros,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14454fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:30.972865Z",
     "iopub.status.busy": "2025-04-01T05:00:30.972621Z",
     "iopub.status.idle": "2025-04-01T05:00:30.975924Z",
     "shell.execute_reply": "2025-04-01T05:00:30.975120Z"
    },
    "papermill": {
     "duration": 0.016741,
     "end_time": "2025-04-01T05:00:30.977169",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.960428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [50, 81, 14, 3, 94]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c38d9",
   "metadata": {
    "papermill": {
     "duration": 0.011592,
     "end_time": "2025-04-01T05:00:31.000537",
     "exception": false,
     "start_time": "2025-04-01T05:00:30.988945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "909a7fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:00:31.025571Z",
     "iopub.status.busy": "2025-04-01T05:00:31.025286Z",
     "iopub.status.idle": "2025-04-01T05:52:17.802845Z",
     "shell.execute_reply": "2025-04-01T05:52:17.802078Z"
    },
    "papermill": {
     "duration": 3106.791567,
     "end_time": "2025-04-01T05:52:17.804219",
     "exception": false,
     "start_time": "2025-04-01T05:00:31.012652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 1\n",
      "Random seed: 50\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6806e75d34e2f9c0d4b6c0aa90ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6756, Accuracy: 0.7731, F1 Micro: 0.8711, F1 Macro: 0.8698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5958, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5785, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.513, Accuracy: 0.7939, F1 Micro: 0.8843, F1 Macro: 0.8828\n",
      "Epoch 5/10, Train Loss: 0.5006, Accuracy: 0.7924, F1 Micro: 0.8829, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4695, Accuracy: 0.7954, F1 Micro: 0.8846, F1 Macro: 0.8828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4328, Accuracy: 0.7969, F1 Micro: 0.8857, F1 Macro: 0.8842\n",
      "Epoch 8/10, Train Loss: 0.4369, Accuracy: 0.7932, F1 Micro: 0.8832, F1 Macro: 0.8812\n",
      "Epoch 9/10, Train Loss: 0.415, Accuracy: 0.7917, F1 Micro: 0.8816, F1 Macro: 0.8789\n",
      "Epoch 10/10, Train Loss: 0.3884, Accuracy: 0.7894, F1 Micro: 0.88, F1 Macro: 0.8769\n",
      "\n",
      "Aspect detection accuracy: 0.7969, F1 Micro: 0.8857, F1 Macro: 0.8842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.75      0.98      0.85       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      1.00      0.89      1061\n",
      "   macro avg       0.80      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.80      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7162, Accuracy: 0.2857, F1 Micro: 0.2857, F1 Macro: 0.2222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6764, Accuracy: 0.5, F1 Micro: 0.5, F1 Macro: 0.4974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.6621, Accuracy: 0.7143, F1 Micro: 0.7143, F1 Macro: 0.65\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.6053, Accuracy: 0.7857, F1 Micro: 0.7857, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.5803, Accuracy: 0.7857, F1 Micro: 0.7857, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5674, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4789, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4895, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.825\n",
      "Epoch 9/10, Train Loss: 0.4054, Accuracy: 0.7143, F1 Micro: 0.7143, F1 Macro: 0.6889\n",
      "Epoch 10/10, Train Loss: 0.3482, Accuracy: 0.5, F1 Micro: 0.5, F1 Macro: 0.4974\n",
      "\n",
      "Sentiment analysis accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75         4\n",
      "    positive       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.86        14\n",
      "   macro avg       0.82      0.82      0.82        14\n",
      "weighted avg       0.86      0.86      0.86        14\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3199\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.13      0.22        23\n",
      "     neutral       0.74      0.98      0.84       152\n",
      "    positive       0.60      0.15      0.24        41\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.70      0.42      0.43       216\n",
      "weighted avg       0.71      0.73      0.66       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 53.38333511352539 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 9.23673415184021 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6289, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5062, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4717, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4605, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4017, Accuracy: 0.8155, F1 Micro: 0.8954, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3899, Accuracy: 0.8408, F1 Micro: 0.9081, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3503, Accuracy: 0.8631, F1 Micro: 0.9192, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2863, Accuracy: 0.8772, F1 Micro: 0.9269, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2636, Accuracy: 0.8988, F1 Micro: 0.9385, F1 Macro: 0.9363\n",
      "\n",
      "Aspect detection accuracy: 0.8988, F1 Micro: 0.9385, F1 Macro: 0.9363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.87      0.98      0.92       175\n",
      "      others       0.88      0.89      0.88       158\n",
      "        part       0.87      1.00      0.93       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.93      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.98      0.94      1061\n",
      "   macro avg       0.90      0.98      0.94      1061\n",
      "weighted avg       0.90      0.98      0.94      1061\n",
      " samples avg       0.91      0.98      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6232, Accuracy: 0.7027, F1 Micro: 0.7027, F1 Macro: 0.4127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5092, Accuracy: 0.7027, F1 Micro: 0.7027, F1 Macro: 0.4127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4147, Accuracy: 0.8054, F1 Micro: 0.8054, F1 Macro: 0.7316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3192, Accuracy: 0.8432, F1 Micro: 0.8432, F1 Macro: 0.7887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1935, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.8976\n",
      "Epoch 6/10, Train Loss: 0.1722, Accuracy: 0.8865, F1 Micro: 0.8865, F1 Macro: 0.8587\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.8757, F1 Micro: 0.8757, F1 Macro: 0.8414\n",
      "Epoch 8/10, Train Loss: 0.1024, Accuracy: 0.8649, F1 Micro: 0.8649, F1 Macro: 0.823\n",
      "Epoch 9/10, Train Loss: 0.1013, Accuracy: 0.8541, F1 Micro: 0.8541, F1 Macro: 0.7971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9044\n",
      "\n",
      "Sentiment analysis accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.89      0.87        55\n",
      "    positive       0.95      0.93      0.94       130\n",
      "\n",
      "    accuracy                           0.92       185\n",
      "   macro avg       0.90      0.91      0.90       185\n",
      "weighted avg       0.92      0.92      0.92       185\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 135: Accuracy: 0.8873, F1 Micro: 0.8873, F1 Macro: 0.7377\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.27      0.43        11\n",
      "     neutral       0.90      1.00      0.95       181\n",
      "    positive       1.00      0.50      0.67        24\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.97      0.59      0.68       216\n",
      "weighted avg       0.92      0.91      0.89       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.38      0.55        16\n",
      "     neutral       0.87      0.98      0.92       167\n",
      "    positive       0.77      0.52      0.62        33\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.88      0.62      0.69       216\n",
      "weighted avg       0.86      0.86      0.84       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.88      0.89      0.88       152\n",
      "    positive       0.67      0.62      0.64        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.73      0.75      0.74       216\n",
      "weighted avg       0.81      0.81      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.87      1.00      0.93       152\n",
      "    positive       0.95      0.49      0.65        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.89      0.74      0.79       216\n",
      "weighted avg       0.88      0.88      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.73      0.78       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.93      1.00      0.96       185\n",
      "    positive       0.80      0.47      0.59        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.66      0.74       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 72.48848748207092 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 13.271495342254639 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5963, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5211, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4964, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4456, Accuracy: 0.8006, F1 Micro: 0.8877, F1 Macro: 0.8862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4271, Accuracy: 0.8341, F1 Micro: 0.9047, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.383, Accuracy: 0.8564, F1 Micro: 0.9157, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.327, Accuracy: 0.8966, F1 Micro: 0.9373, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2632, Accuracy: 0.9152, F1 Micro: 0.9478, F1 Macro: 0.9456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2381, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1889, Accuracy: 0.9323, F1 Micro: 0.9577, F1 Macro: 0.9547\n",
      "\n",
      "Aspect detection accuracy: 0.9323, F1 Micro: 0.9577, F1 Macro: 0.9547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      1.00      0.98       187\n",
      "     machine       0.91      0.98      0.95       175\n",
      "      others       0.91      0.84      0.88       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.96      1061\n",
      "   macro avg       0.94      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.603, Accuracy: 0.698, F1 Micro: 0.698, F1 Macro: 0.4111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5329, Accuracy: 0.7102, F1 Micro: 0.7102, F1 Macro: 0.4851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3852, Accuracy: 0.8449, F1 Micro: 0.8449, F1 Macro: 0.8048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2065, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1806, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0847, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9428\n",
      "Epoch 7/10, Train Loss: 0.0956, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9108\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9\n",
      "Epoch 9/10, Train Loss: 0.0901, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9271\n",
      "\n",
      "Sentiment analysis accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        74\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       245\n",
      "   macro avg       0.94      0.95      0.94       245\n",
      "weighted avg       0.95      0.95      0.95       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 208: Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.8603\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.96      1.00      0.98       181\n",
      "    positive       1.00      0.79      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.87      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.91      0.98      0.95       167\n",
      "    positive       0.83      0.61      0.70        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.76      0.81       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.84      0.88       152\n",
      "    positive       0.65      0.79      0.71        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.75      0.79      0.77       216\n",
      "weighted avg       0.84      0.82      0.83       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 80.76309037208557 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 13.246726751327515 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5788, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 2/10, Train Loss: 0.5075, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4627, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4154, Accuracy: 0.8348, F1 Micro: 0.905, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3744, Accuracy: 0.8586, F1 Micro: 0.9162, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3049, Accuracy: 0.8973, F1 Micro: 0.9378, F1 Macro: 0.936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2469, Accuracy: 0.9241, F1 Micro: 0.9526, F1 Macro: 0.9493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2096, Accuracy: 0.9338, F1 Micro: 0.9588, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1656, Accuracy: 0.9375, F1 Micro: 0.9607, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1323, Accuracy: 0.9449, F1 Micro: 0.9656, F1 Macro: 0.9634\n",
      "\n",
      "Aspect detection accuracy: 0.9449, F1 Micro: 0.9656, F1 Macro: 0.9634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.93      0.90      0.92       158\n",
      "        part       0.91      0.98      0.95       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6145, Accuracy: 0.6885, F1 Micro: 0.6885, F1 Macro: 0.4078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5025, Accuracy: 0.8156, F1 Micro: 0.8156, F1 Macro: 0.7696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3226, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9212\n",
      "Epoch 4/10, Train Loss: 0.1623, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.899\n",
      "Epoch 5/10, Train Loss: 0.1175, Accuracy: 0.9016, F1 Micro: 0.9016, F1 Macro: 0.8765\n",
      "Epoch 6/10, Train Loss: 0.1682, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8953\n",
      "Epoch 7/10, Train Loss: 0.0941, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9331\n",
      "\n",
      "Sentiment analysis accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9331\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        76\n",
      "    positive       0.96      0.96      0.96       168\n",
      "\n",
      "    accuracy                           0.94       244\n",
      "   macro avg       0.93      0.93      0.93       244\n",
      "weighted avg       0.94      0.94      0.94       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 274: Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.8739\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.79      0.84       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.76      0.87      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.81      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.84      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 92.9099006652832 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 12.27755880355835 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5797, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Epoch 2/10, Train Loss: 0.5011, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4534, Accuracy: 0.8229, F1 Micro: 0.899, F1 Macro: 0.8979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4065, Accuracy: 0.8564, F1 Micro: 0.9151, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3077, Accuracy: 0.9107, F1 Micro: 0.9452, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2522, Accuracy: 0.9345, F1 Micro: 0.9591, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1961, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1603, Accuracy: 0.9501, F1 Micro: 0.9686, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1292, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "Epoch 10/10, Train Loss: 0.1042, Accuracy: 0.9472, F1 Micro: 0.9668, F1 Macro: 0.9642\n",
      "\n",
      "Aspect detection accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.94      0.99      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6306, Accuracy: 0.6842, F1 Micro: 0.6842, F1 Macro: 0.4701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4878, Accuracy: 0.8907, F1 Micro: 0.8907, F1 Macro: 0.8748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2932, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.8962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2458, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.8939\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.8785, F1 Micro: 0.8785, F1 Macro: 0.8689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1986, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1458, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9309\n",
      "Epoch 8/10, Train Loss: 0.0988, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.926\n",
      "Epoch 9/10, Train Loss: 0.0642, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9195\n",
      "Epoch 10/10, Train Loss: 0.0915, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9056\n",
      "\n",
      "Sentiment analysis accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        77\n",
      "    positive       0.98      0.94      0.95       170\n",
      "\n",
      "    accuracy                           0.94       247\n",
      "   macro avg       0.92      0.94      0.93       247\n",
      "weighted avg       0.94      0.94      0.94       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8903\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.82      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.83      0.81       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.85      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 94.40810680389404 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 11.167758226394653 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4703, Accuracy: 0.8006, F1 Micro: 0.8878, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4236, Accuracy: 0.8504, F1 Micro: 0.9128, F1 Macro: 0.913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3582, Accuracy: 0.9018, F1 Micro: 0.9404, F1 Macro: 0.9393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2822, Accuracy: 0.9397, F1 Micro: 0.9629, F1 Macro: 0.9617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2036, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1551, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9704\n",
      "Epoch 8/10, Train Loss: 0.1258, Accuracy: 0.9531, F1 Micro: 0.9705, F1 Macro: 0.9685\n",
      "Epoch 9/10, Train Loss: 0.1011, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0907, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "\n",
      "Aspect detection accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5493, Accuracy: 0.6895, F1 Micro: 0.6895, F1 Macro: 0.4905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3651, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2015, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1101, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9247\n",
      "Epoch 6/10, Train Loss: 0.1589, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1528, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9315\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9115\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0814, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "\n",
      "Sentiment analysis accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        80\n",
      "    positive       0.98      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       248\n",
      "   macro avg       0.93      0.95      0.94       248\n",
      "weighted avg       0.95      0.95      0.95       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 387: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9049\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.83      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 100.07888174057007 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 10.416502952575684 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5519, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4903, Accuracy: 0.8028, F1 Micro: 0.8889, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4247, Accuracy: 0.8549, F1 Micro: 0.9149, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3446, Accuracy: 0.9271, F1 Micro: 0.9553, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2629, Accuracy: 0.939, F1 Micro: 0.9623, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.197, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9705\n",
      "Epoch 7/10, Train Loss: 0.1486, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9642\n",
      "Epoch 8/10, Train Loss: 0.119, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1035, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0869, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "\n",
      "Aspect detection accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5572, Accuracy: 0.8353, F1 Micro: 0.8353, F1 Macro: 0.8066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3437, Accuracy: 0.9036, F1 Micro: 0.9036, F1 Macro: 0.8956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.131, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.11, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9464\n",
      "Epoch 6/10, Train Loss: 0.132, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1374, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0971, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "\n",
      "Sentiment analysis accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.93        81\n",
      "    positive       0.99      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.94      0.96      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 435: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9204\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       1.00      0.71      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.88      0.88       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 104.99367308616638 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 9.540494441986084 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5532, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4787, Accuracy: 0.8051, F1 Micro: 0.8899, F1 Macro: 0.8887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4184, Accuracy: 0.8787, F1 Micro: 0.9284, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3185, Accuracy: 0.936, F1 Micro: 0.9607, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.234, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.18, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.1384, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1171, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0936, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9732\n",
      "Epoch 10/10, Train Loss: 0.077, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9723\n",
      "\n",
      "Aspect detection accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.94      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.7925, F1 Micro: 0.7925, F1 Macro: 0.7483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3845, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.936\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9273\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.932\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0855, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1197, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9541\n",
      "Epoch 9/10, Train Loss: 0.0966, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9452\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9367\n",
      "\n",
      "Sentiment analysis accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        81\n",
      "    positive       0.98      0.96      0.97       160\n",
      "\n",
      "    accuracy                           0.96       241\n",
      "   macro avg       0.95      0.96      0.95       241\n",
      "weighted avg       0.96      0.96      0.96       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 478: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9151\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       1.00      0.71      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 101.8316478729248 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 8.585940837860107 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5544, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4748, Accuracy: 0.8266, F1 Micro: 0.901, F1 Macro: 0.9001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3883, Accuracy: 0.9092, F1 Micro: 0.9449, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.275, Accuracy: 0.9427, F1 Micro: 0.9645, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1938, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.1014, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9776\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.98      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6197, Accuracy: 0.7388, F1 Micro: 0.7388, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3867, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2557, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9416\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9503\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9376\n",
      "Epoch 7/10, Train Loss: 0.1446, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9462\n",
      "Epoch 8/10, Train Loss: 0.1038, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9453\n",
      "Epoch 9/10, Train Loss: 0.1073, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0863, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9506\n",
      "\n",
      "Sentiment analysis accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        82\n",
      "    positive       0.99      0.94      0.97       163\n",
      "\n",
      "    accuracy                           0.96       245\n",
      "   macro avg       0.94      0.96      0.95       245\n",
      "weighted avg       0.96      0.96      0.96       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 517: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9207\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 107.59231090545654 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 7.917744398117065 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.553, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4665, Accuracy: 0.817, F1 Micro: 0.896, F1 Macro: 0.8948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3663, Accuracy: 0.9077, F1 Micro: 0.9442, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2785, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2036, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1509, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9722\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9718\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "\n",
      "Aspect detection accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.92      0.92       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5582, Accuracy: 0.8769, F1 Micro: 0.8769, F1 Macro: 0.8545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3341, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9343\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9155\n",
      "Epoch 5/10, Train Loss: 0.1787, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9186\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9427\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.935\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8951\n",
      "\n",
      "Sentiment analysis accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        83\n",
      "    positive       0.97      0.96      0.96       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.94      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 540: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9138\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.76      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.83      0.83       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 110.38497805595398 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 7.431884288787842 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5445, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4644, Accuracy: 0.8251, F1 Micro: 0.8999, F1 Macro: 0.8986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3609, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2609, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1822, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1482, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0782, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9721\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9704\n",
      "\n",
      "Aspect detection accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5571, Accuracy: 0.8484, F1 Micro: 0.8484, F1 Macro: 0.817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3137, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1569, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9455\n",
      "Epoch 4/10, Train Loss: 0.1599, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9328\n",
      "Epoch 5/10, Train Loss: 0.1053, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9375\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9415\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9418\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.058, Accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9546\n",
      "\n",
      "Sentiment analysis accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        82\n",
      "    positive       0.98      0.96      0.97       162\n",
      "\n",
      "    accuracy                           0.96       244\n",
      "   macro avg       0.95      0.96      0.95       244\n",
      "weighted avg       0.96      0.96      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 573: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.914\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 109.75931692123413 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 6.746323823928833 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5395, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4564, Accuracy: 0.8393, F1 Micro: 0.9075, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3503, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2543, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1799, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9706\n",
      "Epoch 6/10, Train Loss: 0.1416, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1129, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "\n",
      "Aspect detection accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.98      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5396, Accuracy: 0.8589, F1 Micro: 0.8589, F1 Macro: 0.8266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2767, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1946, Accuracy: 0.9597, F1 Micro: 0.9597, F1 Macro: 0.9545\n",
      "Epoch 4/10, Train Loss: 0.1603, Accuracy: 0.9556, F1 Micro: 0.9556, F1 Macro: 0.9497\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0832, Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9677, F1 Micro: 0.9677, F1 Macro: 0.964\n",
      "Epoch 9/10, Train Loss: 0.0376, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9597, F1 Micro: 0.9597, F1 Macro: 0.9545\n",
      "\n",
      "Sentiment analysis accuracy: 0.9677, F1 Micro: 0.9677, F1 Macro: 0.964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.98      0.95        82\n",
      "    positive       0.99      0.96      0.98       166\n",
      "\n",
      "    accuracy                           0.97       248\n",
      "   macro avg       0.96      0.97      0.96       248\n",
      "weighted avg       0.97      0.97      0.97       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 603: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9216\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.79      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 118.22323489189148 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.2547101974487305 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5429, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4569, Accuracy: 0.8534, F1 Micro: 0.9142, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3407, Accuracy: 0.9167, F1 Micro: 0.9485, F1 Macro: 0.9459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.231, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1729, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 10/10, Train Loss: 0.0586, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "\n",
      "Aspect detection accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5692, Accuracy: 0.8539, F1 Micro: 0.8539, F1 Macro: 0.8214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2839, Accuracy: 0.8914, F1 Micro: 0.8914, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1767, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 5/10, Train Loss: 0.1387, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.9024\n",
      "Epoch 6/10, Train Loss: 0.1572, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8948\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9157\n",
      "Epoch 8/10, Train Loss: 0.0737, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9264\n",
      "Epoch 9/10, Train Loss: 0.0688, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9322\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.921\n",
      "\n",
      "Sentiment analysis accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.95      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 630: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.86      0.84       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 122.01930141448975 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 5.69404935836792 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5437, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4626, Accuracy: 0.8631, F1 Micro: 0.9193, F1 Macro: 0.9189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3292, Accuracy: 0.9338, F1 Micro: 0.9586, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.223, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1326, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9715\n",
      "Epoch 8/10, Train Loss: 0.0785, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 9/10, Train Loss: 0.0672, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5373, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2676, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1971, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1448, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9432\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9103\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9198\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9295\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9333\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9386\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9236\n",
      "\n",
      "Sentiment analysis accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.93      0.96       166\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.93      0.95      0.94       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 648: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9234\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.12502694129944 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.31294584274292 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5363, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4416, Accuracy: 0.8772, F1 Micro: 0.9272, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3128, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2035, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1527, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5467, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2813, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1836, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9355\n",
      "Epoch 4/10, Train Loss: 0.1704, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Epoch 5/10, Train Loss: 0.1418, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9196\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9155\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9136\n",
      "Epoch 8/10, Train Loss: 0.1035, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 10/10, Train Loss: 0.0389, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.929\n",
      "\n",
      "Sentiment analysis accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.96      0.95       258\n",
      "weighted avg       0.96      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 673: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9304\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.30922722816467 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.990342855453491 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.532, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4367, Accuracy: 0.8787, F1 Micro: 0.9283, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3105, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2081, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1566, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1163, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5292, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.9076, F1 Micro: 0.9076, F1 Macro: 0.893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.176, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1488, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9301\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1137, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0923, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9385\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9385\n",
      "\n",
      "Sentiment analysis accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        84\n",
      "    positive       0.98      0.93      0.96       165\n",
      "\n",
      "    accuracy                           0.94       249\n",
      "   macro avg       0.93      0.95      0.94       249\n",
      "weighted avg       0.95      0.94      0.94       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 698: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9152\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.3634753227234 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.3438379764556885 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5396, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4231, Accuracy: 0.8996, F1 Micro: 0.9391, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2878, Accuracy: 0.9457, F1 Micro: 0.966, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1425, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1087, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9764\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.512, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2559, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.185, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.138, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9479\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0882, Accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9473\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "\n",
      "Sentiment analysis accuracy: 0.9533, F1 Micro: 0.9533, F1 Macro: 0.9473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        84\n",
      "    positive       0.97      0.96      0.97       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.95       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 723: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.24463820457458 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.8620378971099854 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5479, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4451, Accuracy: 0.9085, F1 Micro: 0.9449, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3001, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1949, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9744\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0597, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "\n",
      "Aspect detection accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4808, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.9033, F1 Micro: 0.9033, F1 Macro: 0.8952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1746, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.932\n",
      "Epoch 4/10, Train Loss: 0.1379, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9174\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9253\n",
      "Epoch 8/10, Train Loss: 0.0493, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "\n",
      "Sentiment analysis accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        86\n",
      "    positive       0.96      0.95      0.96       183\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.93      0.93       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 748: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9168\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.74      0.81      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.85      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.26044011116028 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.3615317344665527 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5293, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4174, Accuracy: 0.9003, F1 Micro: 0.9402, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2761, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5133, Accuracy: 0.8855, F1 Micro: 0.8855, F1 Macro: 0.8761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2475, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1689, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1594, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1086, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "\n",
      "Sentiment analysis accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 756: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9203\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 142.11685585975647 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.1655588150024414 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5398, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4167, Accuracy: 0.9129, F1 Micro: 0.9474, F1 Macro: 0.9465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2789, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5336, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2449, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.193, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1434, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9375\n",
      "Epoch 5/10, Train Loss: 0.11, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0866, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9328\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9213\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9209\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9204\n",
      "\n",
      "Sentiment analysis accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9274\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.04310059547424 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.8009676933288574 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5181, Accuracy: 0.7999, F1 Micro: 0.8874, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4001, Accuracy: 0.9308, F1 Micro: 0.9574, F1 Macro: 0.9559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2623, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1712, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0958, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4615, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2098, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.178, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1334, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.939\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9075\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9187\n",
      "\n",
      "Sentiment analysis accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.93      0.96      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.34780621528625 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.117936372756958 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4088, Accuracy: 0.9241, F1 Micro: 0.9537, F1 Macro: 0.9526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2589, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1646, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9769\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4655, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9071\n",
      "Epoch 2/10, Train Loss: 0.2229, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1755, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1493, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "Epoch 6/10, Train Loss: 0.0821, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9257\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.08, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Epoch 9/10, Train Loss: 0.0445, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9102\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9393\n",
      "\n",
      "Sentiment analysis accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        85\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 831: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9239\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.01809811592102 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.7254812717437744 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5215, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3865, Accuracy: 0.9323, F1 Micro: 0.9586, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2487, Accuracy: 0.9524, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4881, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2263, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9369\n",
      "Epoch 3/10, Train Loss: 0.1544, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1246, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9413\n",
      "Epoch 5/10, Train Loss: 0.0985, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Epoch 6/10, Train Loss: 0.0745, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0597, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.042, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 10/10, Train Loss: 0.0588, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "\n",
      "Sentiment analysis accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9282\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.4358696937561 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.0268604755401611 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5275, Accuracy: 0.8095, F1 Micro: 0.8922, F1 Macro: 0.8911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3798, Accuracy: 0.9345, F1 Micro: 0.9594, F1 Macro: 0.9576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2459, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1647, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1278, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 8/10, Train Loss: 0.0602, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Aspect detection accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4851, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Epoch 2/10, Train Loss: 0.2378, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1629, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1345, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1713, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "\n",
      "Sentiment analysis accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        87\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 864: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9277\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 144.24017548561096 s\n",
      "Total runtime: 3105.911245584488 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15a8d2",
   "metadata": {
    "papermill": {
     "duration": 0.195932,
     "end_time": "2025-04-01T05:52:18.150416",
     "exception": false,
     "start_time": "2025-04-01T05:52:17.954484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90382948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T05:52:18.445059Z",
     "iopub.status.busy": "2025-04-01T05:52:18.444706Z",
     "iopub.status.idle": "2025-04-01T06:44:30.031263Z",
     "shell.execute_reply": "2025-04-01T06:44:30.030150Z"
    },
    "papermill": {
     "duration": 3131.734557,
     "end_time": "2025-04-01T06:44:30.032859",
     "exception": false,
     "start_time": "2025-04-01T05:52:18.298302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 2\n",
      "Random seed: 81\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.647, Accuracy: 0.7054, F1 Micro: 0.8127, F1 Macro: 0.7262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5731, Accuracy: 0.7374, F1 Micro: 0.8408, F1 Macro: 0.7949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5493, Accuracy: 0.7917, F1 Micro: 0.8824, F1 Macro: 0.8802\n",
      "Epoch 4/10, Train Loss: 0.5049, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Epoch 5/10, Train Loss: 0.4781, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4364, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4061, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 8/10, Train Loss: 0.4421, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4208, Accuracy: 0.7939, F1 Micro: 0.8842, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3864, Accuracy: 0.8036, F1 Micro: 0.8886, F1 Macro: 0.8869\n",
      "\n",
      "Aspect detection accuracy: 0.8036, F1 Micro: 0.8886, F1 Macro: 0.8869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.79      1.00      0.88       175\n",
      "      others       0.78      0.95      0.86       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.89      1061\n",
      "weighted avg       0.81      0.99      0.89      1061\n",
      " samples avg       0.81      0.99      0.89      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6351, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5577, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5004, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4905, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4593, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4341, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4188, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2323, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2613, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2353, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "\n",
      "Sentiment analysis accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.4769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         3\n",
      "    positive       0.91      1.00      0.95        31\n",
      "\n",
      "    accuracy                           0.91        34\n",
      "   macro avg       0.46      0.50      0.48        34\n",
      "weighted avg       0.83      0.91      0.87        34\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 54: Accuracy: 0.8009, F1 Micro: 0.8009, F1 Macro: 0.3313\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.78      1.00      0.88       167\n",
      "    positive       1.00      0.06      0.11        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.59      0.35      0.33       216\n",
      "weighted avg       0.76      0.78      0.70       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.78      0.95      0.85       152\n",
      "    positive       0.65      0.38      0.48        52\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.47      0.44      0.45       216\n",
      "weighted avg       0.70      0.76      0.72       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.69      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 60.170979022979736 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 13.147248029708862 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.614, Accuracy: 0.7932, F1 Micro: 0.8838, F1 Macro: 0.8821\n",
      "Epoch 2/10, Train Loss: 0.5052, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.4787, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.4596, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3939, Accuracy: 0.8006, F1 Micro: 0.8877, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3859, Accuracy: 0.8445, F1 Micro: 0.9099, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3431, Accuracy: 0.8757, F1 Micro: 0.9262, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.308, Accuracy: 0.8996, F1 Micro: 0.9392, F1 Macro: 0.9381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2651, Accuracy: 0.9152, F1 Micro: 0.948, F1 Macro: 0.9463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2254, Accuracy: 0.9219, F1 Micro: 0.9521, F1 Macro: 0.9507\n",
      "\n",
      "Aspect detection accuracy: 0.9219, F1 Micro: 0.9521, F1 Macro: 0.9507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.93      0.98      0.95       175\n",
      "      others       0.85      0.94      0.89       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.94      1.00      0.97       192\n",
      "     service       0.94      0.99      0.97       191\n",
      "\n",
      "   micro avg       0.92      0.98      0.95      1061\n",
      "   macro avg       0.92      0.98      0.95      1061\n",
      "weighted avg       0.92      0.98      0.95      1061\n",
      " samples avg       0.93      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5811, Accuracy: 0.6683, F1 Micro: 0.6683, F1 Macro: 0.4006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6044, Accuracy: 0.6683, F1 Micro: 0.6683, F1 Macro: 0.4006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5635, Accuracy: 0.6683, F1 Micro: 0.6683, F1 Macro: 0.4006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.499, Accuracy: 0.678, F1 Micro: 0.678, F1 Macro: 0.444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4325, Accuracy: 0.7317, F1 Micro: 0.7317, F1 Macro: 0.6176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2584, Accuracy: 0.8049, F1 Micro: 0.8049, F1 Macro: 0.7596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.177, Accuracy: 0.8439, F1 Micro: 0.8439, F1 Macro: 0.8212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1569, Accuracy: 0.8537, F1 Micro: 0.8537, F1 Macro: 0.8281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0914, Accuracy: 0.8585, F1 Micro: 0.8585, F1 Macro: 0.8373\n",
      "Epoch 10/10, Train Loss: 0.1255, Accuracy: 0.839, F1 Micro: 0.839, F1 Macro: 0.8046\n",
      "\n",
      "Sentiment analysis accuracy: 0.8585, F1 Micro: 0.8585, F1 Macro: 0.8373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.75      0.78        68\n",
      "    positive       0.88      0.91      0.90       137\n",
      "\n",
      "    accuracy                           0.86       205\n",
      "   macro avg       0.84      0.83      0.84       205\n",
      "weighted avg       0.86      0.86      0.86       205\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 135: Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.7635\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.94      1.00      0.97       181\n",
      "    positive       0.88      0.62      0.73        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.75      0.83       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.56      0.69        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.77      0.70      0.73        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.75      0.79       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.42      0.45        12\n",
      "     neutral       0.84      0.95      0.89       152\n",
      "    positive       0.74      0.50      0.60        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.69      0.62      0.65       216\n",
      "weighted avg       0.80      0.81      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.74      0.76        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.83      0.71      0.76        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.89      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.38      0.56        13\n",
      "     neutral       0.94      1.00      0.97       186\n",
      "    positive       0.71      0.59      0.65        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.66      0.72       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.57      0.70        14\n",
      "     neutral       0.94      0.99      0.97       185\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.70      0.77       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Total train time: 74.86194562911987 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 14.030493974685669 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5866, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 2/10, Train Loss: 0.5081, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.4712, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4262, Accuracy: 0.811, F1 Micro: 0.8923, F1 Macro: 0.8908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.388, Accuracy: 0.8549, F1 Micro: 0.9153, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3152, Accuracy: 0.9025, F1 Micro: 0.9404, F1 Macro: 0.9379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2636, Accuracy: 0.9048, F1 Micro: 0.9409, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2274, Accuracy: 0.9167, F1 Micro: 0.9478, F1 Macro: 0.9434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1876, Accuracy: 0.9234, F1 Micro: 0.952, F1 Macro: 0.9476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1623, Accuracy: 0.9323, F1 Micro: 0.9575, F1 Macro: 0.954\n",
      "\n",
      "Aspect detection accuracy: 0.9323, F1 Micro: 0.9575, F1 Macro: 0.954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.90      0.83      0.86       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.95      0.96      0.95      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6282, Accuracy: 0.698, F1 Micro: 0.698, F1 Macro: 0.4111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5633, Accuracy: 0.702, F1 Micro: 0.702, F1 Macro: 0.4366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4541, Accuracy: 0.8235, F1 Micro: 0.8235, F1 Macro: 0.7794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3109, Accuracy: 0.8824, F1 Micro: 0.8824, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2065, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8832\n",
      "Epoch 6/10, Train Loss: 0.121, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8873\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8799\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.8824, F1 Micro: 0.8824, F1 Macro: 0.8684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8841\n",
      "\n",
      "Sentiment analysis accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.84      0.84        77\n",
      "    positive       0.93      0.93      0.93       178\n",
      "\n",
      "    accuracy                           0.90       255\n",
      "   macro avg       0.88      0.89      0.88       255\n",
      "weighted avg       0.90      0.90      0.90       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 208: Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.8439\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.77        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.80      0.84       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.89      0.83      0.86       152\n",
      "    positive       0.62      0.69      0.65        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.68      0.76      0.71       216\n",
      "weighted avg       0.81      0.79      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.74      0.77        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.96      1.00      0.98       186\n",
      "    positive       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.75      0.82       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 80.19862627983093 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 13.584611654281616 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5599, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4781, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4291, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3877, Accuracy: 0.8527, F1 Micro: 0.9142, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3182, Accuracy: 0.9055, F1 Micro: 0.9421, F1 Macro: 0.9399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2626, Accuracy: 0.9308, F1 Micro: 0.9571, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2106, Accuracy: 0.933, F1 Micro: 0.958, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1607, Accuracy: 0.9412, F1 Micro: 0.9632, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1455, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9627\n",
      "Epoch 10/10, Train Loss: 0.1202, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9597\n",
      "\n",
      "Aspect detection accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.86      0.89       158\n",
      "        part       0.92      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.97      0.97      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.97      0.97      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6322, Accuracy: 0.6969, F1 Micro: 0.6969, F1 Macro: 0.4107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5754, Accuracy: 0.7165, F1 Micro: 0.7165, F1 Macro: 0.5222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3915, Accuracy: 0.8504, F1 Micro: 0.8504, F1 Macro: 0.8373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2213, Accuracy: 0.8976, F1 Micro: 0.8976, F1 Macro: 0.8789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1766, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8961\n",
      "Epoch 6/10, Train Loss: 0.1432, Accuracy: 0.9016, F1 Micro: 0.9016, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9022\n",
      "Epoch 8/10, Train Loss: 0.0961, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9051\n",
      "Epoch 10/10, Train Loss: 0.0915, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8961\n",
      "\n",
      "Sentiment analysis accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9051\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.92      0.87        77\n",
      "    positive       0.96      0.92      0.94       177\n",
      "\n",
      "    accuracy                           0.92       254\n",
      "   macro avg       0.89      0.92      0.91       254\n",
      "weighted avg       0.92      0.92      0.92       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 274: Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.8747\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.92      0.86      0.89       152\n",
      "    positive       0.71      0.71      0.71        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.70      0.80      0.74       216\n",
      "weighted avg       0.84      0.82      0.83       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.74      0.83        23\n",
      "     neutral       0.92      0.99      0.96       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 89.65950512886047 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 12.406517028808594 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5596, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4763, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.441, Accuracy: 0.8251, F1 Micro: 0.9001, F1 Macro: 0.8994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3545, Accuracy: 0.8996, F1 Micro: 0.9392, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2813, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2263, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1734, Accuracy: 0.942, F1 Micro: 0.9636, F1 Macro: 0.9612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1394, Accuracy: 0.9457, F1 Micro: 0.9657, F1 Macro: 0.963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1134, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0964, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "\n",
      "Aspect detection accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.95      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6142, Accuracy: 0.6935, F1 Micro: 0.6935, F1 Macro: 0.4095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4812, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.286, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2011, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9378\n",
      "Epoch 6/10, Train Loss: 0.1425, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9128\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9042, F1 Micro: 0.9042, F1 Macro: 0.8785\n",
      "Epoch 8/10, Train Loss: 0.1653, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9269\n",
      "Epoch 10/10, Train Loss: 0.0768, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9231\n",
      "\n",
      "Sentiment analysis accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        80\n",
      "    positive       0.97      0.95      0.96       181\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.93      0.94      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 333: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9024\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.86      0.85       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.95      0.98      0.96       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.57      0.73        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 94.038254737854 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 11.29407787322998 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5505, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4688, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4267, Accuracy: 0.8519, F1 Micro: 0.9138, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3192, Accuracy: 0.9249, F1 Micro: 0.9533, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2497, Accuracy: 0.9375, F1 Micro: 0.961, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.189, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.148, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.969\n",
      "Epoch 8/10, Train Loss: 0.1156, Accuracy: 0.9531, F1 Micro: 0.9704, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1001, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9717\n",
      "Epoch 10/10, Train Loss: 0.0828, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.971\n",
      "\n",
      "Aspect detection accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.95      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6092, Accuracy: 0.6902, F1 Micro: 0.6902, F1 Macro: 0.4084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4373, Accuracy: 0.8549, F1 Micro: 0.8549, F1 Macro: 0.826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3076, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1897, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9238\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9281\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9248\n",
      "Epoch 9/10, Train Loss: 0.0929, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.907\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8763\n",
      "\n",
      "Sentiment analysis accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        79\n",
      "    positive       0.97      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.94       255\n",
      "   macro avg       0.92      0.94      0.93       255\n",
      "weighted avg       0.94      0.94      0.94       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 387: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8966\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.80      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.95      0.98      0.96       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.97       216\n",
      "\n",
      "Total train time: 97.33852291107178 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 10.405622482299805 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5525, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4803, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3949, Accuracy: 0.8728, F1 Micro: 0.9248, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3134, Accuracy: 0.936, F1 Micro: 0.9603, F1 Macro: 0.9588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2333, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1961, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1384, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Epoch 8/10, Train Loss: 0.1112, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0974, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0808, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9764\n",
      "\n",
      "Aspect detection accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      1.00      0.98       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6031, Accuracy: 0.6762, F1 Micro: 0.6762, F1 Macro: 0.4034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4581, Accuracy: 0.8648, F1 Micro: 0.8648, F1 Macro: 0.8406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2348, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 4/10, Train Loss: 0.1629, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8872\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1085, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0321, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "\n",
      "Sentiment analysis accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.92      0.91        79\n",
      "    positive       0.96      0.95      0.95       165\n",
      "\n",
      "    accuracy                           0.94       244\n",
      "   macro avg       0.93      0.93      0.93       244\n",
      "weighted avg       0.94      0.94      0.94       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 435: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9051\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      1.00      0.98       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.81      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.97       216\n",
      "\n",
      "Total train time: 103.5699098110199 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 9.717143774032593 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.464, Accuracy: 0.811, F1 Micro: 0.8931, F1 Macro: 0.8918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3874, Accuracy: 0.907, F1 Micro: 0.9429, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2738, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2185, Accuracy: 0.9494, F1 Micro: 0.9681, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1745, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1302, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1006, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0905, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0766, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "\n",
      "Aspect detection accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.6746, F1 Micro: 0.6746, F1 Macro: 0.4355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3684, Accuracy: 0.869, F1 Micro: 0.869, F1 Macro: 0.8609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2311, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1506, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.131, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9196\n",
      "Epoch 8/10, Train Loss: 0.1049, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9252\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9112\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9164\n",
      "\n",
      "Sentiment analysis accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        82\n",
      "    positive       0.99      0.92      0.95       170\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.92      0.95      0.93       252\n",
      "weighted avg       0.94      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 478: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9082\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.81      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.73      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 108.40385031700134 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 8.919224977493286 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5499, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4584, Accuracy: 0.8289, F1 Micro: 0.9018, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3526, Accuracy: 0.9085, F1 Micro: 0.9424, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2709, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9755\n",
      "\n",
      "Aspect detection accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5696, Accuracy: 0.8015, F1 Micro: 0.8015, F1 Macro: 0.7423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3242, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2326, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9246\n",
      "Epoch 4/10, Train Loss: 0.1444, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9161\n",
      "Epoch 5/10, Train Loss: 0.1132, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9157\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9152\n",
      "Epoch 7/10, Train Loss: 0.0966, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "Epoch 8/10, Train Loss: 0.0576, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9197\n",
      "Epoch 9/10, Train Loss: 0.071, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9097\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9118\n",
      "\n",
      "Sentiment analysis accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.96      0.90        84\n",
      "    positive       0.98      0.92      0.95       183\n",
      "\n",
      "    accuracy                           0.93       267\n",
      "   macro avg       0.91      0.94      0.92       267\n",
      "weighted avg       0.94      0.93      0.93       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 517: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9115\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 109.28285312652588 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.108110189437866 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5281, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4556, Accuracy: 0.8371, F1 Micro: 0.9057, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3499, Accuracy: 0.9189, F1 Micro: 0.9499, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.263, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2002, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1525, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Epoch 7/10, Train Loss: 0.1187, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0626, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5592, Accuracy: 0.6825, F1 Micro: 0.6825, F1 Macro: 0.4493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.34, Accuracy: 0.8968, F1 Micro: 0.8968, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2257, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1805, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9112\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.909\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9042\n",
      "Epoch 9/10, Train Loss: 0.0661, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9389\n",
      "\n",
      "Sentiment analysis accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        83\n",
      "    positive       0.99      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.95      0.94       252\n",
      "weighted avg       0.95      0.94      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 540: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9205\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 112.05121111869812 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 7.749281644821167 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5411, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4553, Accuracy: 0.8445, F1 Micro: 0.91, F1 Macro: 0.9094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3327, Accuracy: 0.9293, F1 Micro: 0.9559, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2391, Accuracy: 0.9449, F1 Micro: 0.9656, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1754, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1436, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1087, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9718\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "\n",
      "Aspect detection accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5792, Accuracy: 0.7027, F1 Micro: 0.7027, F1 Macro: 0.4868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3232, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9212\n",
      "Epoch 3/10, Train Loss: 0.1942, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.916\n",
      "Epoch 4/10, Train Loss: 0.1197, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.8949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9212\n",
      "Epoch 6/10, Train Loss: 0.0838, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9353\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0394, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "\n",
      "Sentiment analysis accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        83\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 573: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 115.08268451690674 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 6.855530261993408 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5385, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.447, Accuracy: 0.8638, F1 Micro: 0.919, F1 Macro: 0.918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3322, Accuracy: 0.9263, F1 Micro: 0.9537, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2328, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.172, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.129, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0838, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "Epoch 9/10, Train Loss: 0.0691, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5492, Accuracy: 0.7669, F1 Micro: 0.7669, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2661, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1908, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1412, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9456\n",
      "Epoch 5/10, Train Loss: 0.1052, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9296\n",
      "Epoch 6/10, Train Loss: 0.0876, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Epoch 7/10, Train Loss: 0.0581, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0724, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0433, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9212\n",
      "\n",
      "Sentiment analysis accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.95      0.95       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 603: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.929\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.83      0.88      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.83      0.85      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 123.43022036552429 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.3355302810668945 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5315, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4428, Accuracy: 0.8817, F1 Micro: 0.9292, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3004, Accuracy: 0.9382, F1 Micro: 0.9614, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2197, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5438, Accuracy: 0.8736, F1 Micro: 0.8736, F1 Macro: 0.8597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2435, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1734, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1577, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 5/10, Train Loss: 0.111, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.934\n",
      "Epoch 6/10, Train Loss: 0.0717, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0664, Accuracy: 0.9655, F1 Micro: 0.9655, F1 Macro: 0.9609\n",
      "Epoch 8/10, Train Loss: 0.0671, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9473\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.94\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9306\n",
      "\n",
      "Sentiment analysis accuracy: 0.9655, F1 Micro: 0.9655, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.95      0.95        85\n",
      "    positive       0.98      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.97       261\n",
      "   macro avg       0.96      0.96      0.96       261\n",
      "weighted avg       0.97      0.97      0.97       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 630: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9348\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.90      0.91       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.7855441570282 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 5.698177337646484 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4258, Accuracy: 0.9077, F1 Micro: 0.9433, F1 Macro: 0.9408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3047, Accuracy: 0.9375, F1 Micro: 0.9609, F1 Macro: 0.9582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1992, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1556, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.12, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0912, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0754, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      1.00      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5116, Accuracy: 0.856, F1 Micro: 0.856, F1 Macro: 0.8315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2737, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1928, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1131, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Epoch 7/10, Train Loss: 0.1164, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0801, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9194\n",
      "\n",
      "Sentiment analysis accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 648: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      1.00      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.80047631263733 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.48669958114624 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4231, Accuracy: 0.9048, F1 Micro: 0.9418, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2948, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1214, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0737, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.97      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5204, Accuracy: 0.8977, F1 Micro: 0.8977, F1 Macro: 0.8832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.266, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9131\n",
      "Epoch 4/10, Train Loss: 0.1897, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9479\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 7/10, Train Loss: 0.1144, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9393\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "\n",
      "Sentiment analysis accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        85\n",
      "    positive       0.97      0.97      0.97       179\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.95      0.95      0.95       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 673: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.98      0.97      0.97       167\n",
      "    positive       0.83      0.88      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.14016938209534 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.9057886600494385 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5305, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4141, Accuracy: 0.9048, F1 Micro: 0.9407, F1 Macro: 0.9381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2875, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9746\n",
      "Epoch 5/10, Train Loss: 0.159, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "Epoch 7/10, Train Loss: 0.0954, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9747\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.506, Accuracy: 0.8858, F1 Micro: 0.8858, F1 Macro: 0.8707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Epoch 3/10, Train Loss: 0.2001, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1488, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1105, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9308\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9226\n",
      "\n",
      "Sentiment analysis accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        83\n",
      "    positive       0.98      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.95      0.94       254\n",
      "weighted avg       0.95      0.94      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 698: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9223\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 124.74261999130249 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.2937541007995605 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3995, Accuracy: 0.9182, F1 Micro: 0.9494, F1 Macro: 0.9471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2732, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1928, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1485, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.1096, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "\n",
      "Aspect detection accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5366, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.893\n",
      "Epoch 2/10, Train Loss: 0.2961, Accuracy: 0.8669, F1 Micro: 0.8669, F1 Macro: 0.8596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1794, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1329, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 6/10, Train Loss: 0.1361, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1103, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0757, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "\n",
      "Sentiment analysis accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        86\n",
      "    positive       0.96      0.95      0.95       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.93      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 723: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.83      0.87      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.63234758377075 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.9462201595306396 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.525, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3996, Accuracy: 0.91, F1 Micro: 0.9434, F1 Macro: 0.9403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2669, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1837, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.567, Accuracy: 0.8615, F1 Micro: 0.8615, F1 Macro: 0.8407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3139, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1922, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 4/10, Train Loss: 0.1513, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9268\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Epoch 10/10, Train Loss: 0.0806, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9115\n",
      "\n",
      "Sentiment analysis accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 748: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9193\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.65947103500366 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.408379077911377 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5184, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3937, Accuracy: 0.9189, F1 Micro: 0.9496, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2619, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5084, Accuracy: 0.8889, F1 Micro: 0.8889, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2581, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 3/10, Train Loss: 0.18, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.905\n",
      "Epoch 4/10, Train Loss: 0.1337, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9122\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9289\n",
      "Epoch 9/10, Train Loss: 0.0652, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "\n",
      "Sentiment analysis accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        86\n",
      "    positive       0.98      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.92      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 756: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9229\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.680992603302 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.1960225105285645 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5196, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3867, Accuracy: 0.9189, F1 Micro: 0.9495, F1 Macro: 0.9468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2492, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9776\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9761\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5003, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2353, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1953, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1227, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1334, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9411\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9531\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9203\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9234\n",
      "\n",
      "Sentiment analysis accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.94        86\n",
      "    positive       0.98      0.96      0.97       179\n",
      "\n",
      "    accuracy                           0.96       265\n",
      "   macro avg       0.95      0.96      0.95       265\n",
      "weighted avg       0.96      0.96      0.96       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9257\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.22637486457825 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.6865437030792236 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5159, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3784, Accuracy: 0.9122, F1 Micro: 0.9445, F1 Macro: 0.9409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1761, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9736\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5762, Accuracy: 0.834, F1 Micro: 0.834, F1 Macro: 0.799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1677, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.139, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "Epoch 9/10, Train Loss: 0.0875, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9528\n",
      "Epoch 10/10, Train Loss: 0.0586, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "\n",
      "Sentiment analysis accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        85\n",
      "    positive       0.98      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.96       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 806: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9305\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.46670532226562 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.2963831424713135 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5221, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3733, Accuracy: 0.9249, F1 Micro: 0.9536, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2377, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1635, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5308, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2607, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "Epoch 3/10, Train Loss: 0.1442, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.922\n",
      "Epoch 4/10, Train Loss: 0.1655, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1042, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9332\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9455\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9339\n",
      "Epoch 9/10, Train Loss: 0.0939, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9598, F1 Micro: 0.9598, F1 Macro: 0.9551\n",
      "\n",
      "Sentiment analysis accuracy: 0.9598, F1 Micro: 0.9598, F1 Macro: 0.9551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        82\n",
      "    positive       0.98      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.95      0.96      0.96       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 831: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9217\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.24729871749878 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.6751554012298584 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5154, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3704, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2397, Accuracy: 0.9568, F1 Micro: 0.9732, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1683, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9753\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9782\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4855, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2117, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9231\n",
      "Epoch 3/10, Train Loss: 0.1827, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.91\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9289\n",
      "Epoch 6/10, Train Loss: 0.104, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.9162\n",
      "Epoch 7/10, Train Loss: 0.1028, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9294\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9112\n",
      "\n",
      "Sentiment analysis accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       186\n",
      "\n",
      "    accuracy                           0.94       273\n",
      "   macro avg       0.92      0.94      0.93       273\n",
      "weighted avg       0.94      0.94      0.94       273\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 856: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9197\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.78794646263123 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.0566556453704834 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.517, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3655, Accuracy: 0.939, F1 Micro: 0.9623, F1 Macro: 0.9607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2276, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0943, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4839, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2282, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1731, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1372, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0922, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9283\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9203\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9168\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "\n",
      "Sentiment analysis accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.93        87\n",
      "    positive       0.97      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 864: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9284\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.3767433166504 s\n",
      "Total runtime: 3130.764529466629 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c8c8c",
   "metadata": {
    "papermill": {
     "duration": 0.282524,
     "end_time": "2025-04-01T06:44:30.605995",
     "exception": false,
     "start_time": "2025-04-01T06:44:30.323471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 3\n",
      "Random seed: 14\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def97fed636f4ca9a8488d44036aa7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6791, Accuracy: 0.7746, F1 Micro: 0.8722, F1 Macro: 0.8703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5752, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.512, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4609, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4635, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4127, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3898, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4151, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.396, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3746, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "\n",
      "Aspect detection accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.8159, Accuracy: 0.3333, F1 Micro: 0.3333, F1 Macro: 0.25\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6264, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5442, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5154, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4038, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3617, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3192, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2942, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3391, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2952, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "\n",
      "Sentiment analysis accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.298\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.67      0.04      0.07        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.46      0.35      0.30       216\n",
      "weighted avg       0.66      0.71      0.60       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 64.13745999336243 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 12.346516132354736 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6273, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5032, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4712, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4598, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4209, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3777, Accuracy: 0.8125, F1 Micro: 0.8935, F1 Macro: 0.892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.341, Accuracy: 0.8378, F1 Micro: 0.9059, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3158, Accuracy: 0.8594, F1 Micro: 0.9166, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2773, Accuracy: 0.8795, F1 Micro: 0.9279, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2263, Accuracy: 0.8943, F1 Micro: 0.9362, F1 Macro: 0.9341\n",
      "\n",
      "Aspect detection accuracy: 0.8943, F1 Micro: 0.9362, F1 Macro: 0.9341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.91      1.00      0.95       187\n",
      "     machine       0.89      0.98      0.93       175\n",
      "      others       0.83      0.92      0.88       158\n",
      "        part       0.86      0.98      0.92       158\n",
      "       price       0.94      0.99      0.96       192\n",
      "     service       0.92      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.98      0.94      1061\n",
      "   macro avg       0.89      0.98      0.93      1061\n",
      "weighted avg       0.90      0.98      0.94      1061\n",
      " samples avg       0.90      0.98      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6609, Accuracy: 0.711, F1 Micro: 0.711, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5792, Accuracy: 0.711, F1 Micro: 0.711, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.525, Accuracy: 0.7225, F1 Micro: 0.7225, F1 Macro: 0.4887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4369, Accuracy: 0.763, F1 Micro: 0.763, F1 Macro: 0.6441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3334, Accuracy: 0.7977, F1 Micro: 0.7977, F1 Macro: 0.7226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.246, Accuracy: 0.8497, F1 Micro: 0.8497, F1 Macro: 0.8193\n",
      "Epoch 7/10, Train Loss: 0.196, Accuracy: 0.8382, F1 Micro: 0.8382, F1 Macro: 0.7898\n",
      "Epoch 8/10, Train Loss: 0.1586, Accuracy: 0.8439, F1 Micro: 0.8439, F1 Macro: 0.7958\n",
      "Epoch 9/10, Train Loss: 0.1135, Accuracy: 0.8439, F1 Micro: 0.8439, F1 Macro: 0.8015\n",
      "Epoch 10/10, Train Loss: 0.1033, Accuracy: 0.8382, F1 Micro: 0.8382, F1 Macro: 0.7927\n",
      "\n",
      "Sentiment analysis accuracy: 0.8497, F1 Micro: 0.8497, F1 Macro: 0.8193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.76      0.75        50\n",
      "    positive       0.90      0.89      0.89       123\n",
      "\n",
      "    accuracy                           0.85       173\n",
      "   macro avg       0.82      0.82      0.82       173\n",
      "weighted avg       0.85      0.85      0.85       173\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 135: Accuracy: 0.8812, F1 Micro: 0.8812, F1 Macro: 0.6862\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       1.00      0.75      0.86        24\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.97      0.61      0.66       216\n",
      "weighted avg       0.93      0.93      0.91       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.31      0.48        16\n",
      "     neutral       0.89      0.98      0.93       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.91      0.65      0.72       216\n",
      "weighted avg       0.89      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.58      0.50        12\n",
      "     neutral       0.83      0.93      0.88       152\n",
      "    positive       0.74      0.44      0.55        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.67      0.65      0.64       216\n",
      "weighted avg       0.79      0.79      0.78       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.74      0.76        23\n",
      "     neutral       0.86      0.98      0.91       152\n",
      "    positive       0.90      0.44      0.59        41\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.84      0.72      0.75       216\n",
      "weighted avg       0.86      0.85      0.84       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.23      0.38        13\n",
      "     neutral       0.94      0.99      0.97       186\n",
      "    positive       0.62      0.59      0.61        17\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.60      0.65       216\n",
      "weighted avg       0.92      0.92      0.90       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        14\n",
      "     neutral       0.92      1.00      0.96       185\n",
      "    positive       0.80      0.47      0.59        17\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.61      0.69       216\n",
      "weighted avg       0.92      0.92      0.90       216\n",
      "\n",
      "Total train time: 70.90604877471924 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 14.294761657714844 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5926, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.493, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4791, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4371, Accuracy: 0.8036, F1 Micro: 0.8894, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3998, Accuracy: 0.8333, F1 Micro: 0.9037, F1 Macro: 0.9023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3568, Accuracy: 0.846, F1 Micro: 0.9085, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3104, Accuracy: 0.8899, F1 Micro: 0.9333, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2573, Accuracy: 0.9025, F1 Micro: 0.9401, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2121, Accuracy: 0.9144, F1 Micro: 0.9471, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1779, Accuracy: 0.9144, F1 Micro: 0.9471, F1 Macro: 0.9438\n",
      "\n",
      "Aspect detection accuracy: 0.9144, F1 Micro: 0.9471, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.93      1.00      0.96       187\n",
      "     machine       0.90      0.99      0.95       175\n",
      "      others       0.87      0.84      0.85       158\n",
      "        part       0.91      0.98      0.95       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.95      1061\n",
      "   macro avg       0.92      0.97      0.94      1061\n",
      "weighted avg       0.92      0.97      0.95      1061\n",
      " samples avg       0.93      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6352, Accuracy: 0.7117, F1 Micro: 0.7117, F1 Macro: 0.4158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5161, Accuracy: 0.7387, F1 Micro: 0.7387, F1 Macro: 0.5662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4532, Accuracy: 0.8468, F1 Micro: 0.8468, F1 Macro: 0.8227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3046, Accuracy: 0.8604, F1 Micro: 0.8604, F1 Macro: 0.8257\n",
      "Epoch 5/10, Train Loss: 0.1928, Accuracy: 0.8559, F1 Micro: 0.8559, F1 Macro: 0.8367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0758, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8732\n",
      "Epoch 7/10, Train Loss: 0.1008, Accuracy: 0.8514, F1 Micro: 0.8514, F1 Macro: 0.8298\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.8559, F1 Micro: 0.8559, F1 Macro: 0.8227\n",
      "Epoch 9/10, Train Loss: 0.1171, Accuracy: 0.8559, F1 Micro: 0.8559, F1 Macro: 0.8356\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.8649, F1 Micro: 0.8649, F1 Macro: 0.8338\n",
      "\n",
      "Sentiment analysis accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.81      0.82        64\n",
      "    positive       0.92      0.93      0.93       158\n",
      "\n",
      "    accuracy                           0.90       222\n",
      "   macro avg       0.87      0.87      0.87       222\n",
      "weighted avg       0.90      0.90      0.90       222\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 208: Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.7847\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        11\n",
      "     neutral       0.94      1.00      0.97       181\n",
      "    positive       1.00      0.79      0.88        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.98      0.72      0.79       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.38      0.55        16\n",
      "     neutral       0.90      0.99      0.94       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.93      0.68      0.75       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.86      0.84      0.85       152\n",
      "    positive       0.63      0.63      0.63        52\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.68      0.74      0.70       216\n",
      "weighted avg       0.79      0.78      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.81      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.54      0.70        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.67      0.71      0.69        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.75      0.79       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.76      0.83       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Total train time: 79.35776233673096 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 15.737051725387573 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4869, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4429, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4297, Accuracy: 0.8326, F1 Micro: 0.9035, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3565, Accuracy: 0.8824, F1 Micro: 0.9296, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2783, Accuracy: 0.9174, F1 Micro: 0.9492, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2304, Accuracy: 0.9323, F1 Micro: 0.958, F1 Macro: 0.9557\n",
      "Epoch 8/10, Train Loss: 0.181, Accuracy: 0.9308, F1 Micro: 0.957, F1 Macro: 0.9546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1524, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1302, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9643\n",
      "\n",
      "Aspect detection accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6489, Accuracy: 0.6835, F1 Micro: 0.6835, F1 Macro: 0.406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5084, Accuracy: 0.8692, F1 Micro: 0.8692, F1 Macro: 0.8435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3556, Accuracy: 0.8903, F1 Micro: 0.8903, F1 Macro: 0.8801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2465, Accuracy: 0.903, F1 Micro: 0.903, F1 Macro: 0.8937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9095\n",
      "Epoch 6/10, Train Loss: 0.1514, Accuracy: 0.8776, F1 Micro: 0.8776, F1 Macro: 0.8686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1522, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9214\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.8819, F1 Micro: 0.8819, F1 Macro: 0.8729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0935, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9419\n",
      "Epoch 10/10, Train Loss: 0.0698, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.919\n",
      "\n",
      "Sentiment analysis accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        75\n",
      "    positive       0.97      0.96      0.96       162\n",
      "\n",
      "    accuracy                           0.95       237\n",
      "   macro avg       0.94      0.95      0.94       237\n",
      "weighted avg       0.95      0.95      0.95       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 274: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8804\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.80      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.77      0.77      0.77        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 87.73169136047363 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 14.512004613876343 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5564, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4955, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4348, Accuracy: 0.8147, F1 Micro: 0.8941, F1 Macro: 0.8923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3848, Accuracy: 0.8542, F1 Micro: 0.9127, F1 Macro: 0.9093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3046, Accuracy: 0.9085, F1 Micro: 0.9436, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2522, Accuracy: 0.9323, F1 Micro: 0.9577, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1945, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1508, Accuracy: 0.9487, F1 Micro: 0.9679, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1169, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1058, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "\n",
      "Aspect detection accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.89      0.94      0.91       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6017, Accuracy: 0.6708, F1 Micro: 0.6708, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4638, Accuracy: 0.8833, F1 Micro: 0.8833, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2917, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.206, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9145\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.9008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1247, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9295\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9089\n",
      "Epoch 9/10, Train Loss: 0.1122, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.923\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "\n",
      "Sentiment analysis accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.92      0.91        78\n",
      "    positive       0.96      0.94      0.95       162\n",
      "\n",
      "    accuracy                           0.94       240\n",
      "   macro avg       0.93      0.93      0.93       240\n",
      "weighted avg       0.94      0.94      0.94       240\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 333: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.8828\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.91      0.88      0.89        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.88      0.94      0.91       152\n",
      "    positive       0.82      0.63      0.72        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 94.58886003494263 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 13.22720193862915 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5489, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4816, Accuracy: 0.7932, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4136, Accuracy: 0.8311, F1 Micro: 0.9023, F1 Macro: 0.9006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3382, Accuracy: 0.9018, F1 Micro: 0.9394, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2655, Accuracy: 0.9315, F1 Micro: 0.9574, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2112, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1526, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1288, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Epoch 9/10, Train Loss: 0.1076, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9673\n",
      "Epoch 10/10, Train Loss: 0.0839, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "\n",
      "Aspect detection accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5998, Accuracy: 0.679, F1 Micro: 0.679, F1 Macro: 0.4044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4395, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2427, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9189\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9285\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9356\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9312\n",
      "Epoch 9/10, Train Loss: 0.0769, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9483\n",
      "\n",
      "Sentiment analysis accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        78\n",
      "    positive       0.97      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.95       243\n",
      "   macro avg       0.95      0.95      0.95       243\n",
      "weighted avg       0.95      0.95      0.95       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 387: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8993\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.94      0.92       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.81      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 97.06533980369568 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.84993314743042 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5619, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.481, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4279, Accuracy: 0.8482, F1 Micro: 0.9107, F1 Macro: 0.9088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3539, Accuracy: 0.91, F1 Micro: 0.9447, F1 Macro: 0.9423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2603, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1902, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.966\n",
      "Epoch 7/10, Train Loss: 0.1526, Accuracy: 0.942, F1 Micro: 0.9633, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1232, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.101, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9704\n",
      "\n",
      "Aspect detection accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.92      0.91       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5911, Accuracy: 0.6865, F1 Micro: 0.6865, F1 Macro: 0.441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4003, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2653, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9336\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9336\n",
      "Epoch 6/10, Train Loss: 0.1183, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.08, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9498\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9399\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.929\n",
      "\n",
      "Sentiment analysis accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.93      0.93        81\n",
      "    positive       0.97      0.97      0.97       171\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.95      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 435: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9055\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.92      0.92       152\n",
      "    positive       0.78      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.83      0.84       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 101.3166127204895 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 10.993866920471191 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5507, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4813, Accuracy: 0.7976, F1 Micro: 0.8863, F1 Macro: 0.8848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4164, Accuracy: 0.8631, F1 Micro: 0.918, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3088, Accuracy: 0.9315, F1 Micro: 0.9575, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2411, Accuracy: 0.9338, F1 Micro: 0.9584, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1749, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1425, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Epoch 8/10, Train Loss: 0.107, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9693\n",
      "Epoch 9/10, Train Loss: 0.0942, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.971\n",
      "Epoch 10/10, Train Loss: 0.0789, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9689\n",
      "\n",
      "Aspect detection accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5741, Accuracy: 0.7184, F1 Micro: 0.7184, F1 Macro: 0.5633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3583, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1526, Accuracy: 0.9592, F1 Micro: 0.9592, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1248, Accuracy: 0.9633, F1 Micro: 0.9633, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.9633, F1 Micro: 0.9633, F1 Macro: 0.9584\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9406\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9406\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9592, F1 Micro: 0.9592, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.9633, F1 Micro: 0.9633, F1 Macro: 0.9584\n",
      "\n",
      "Sentiment analysis accuracy: 0.9633, F1 Micro: 0.9633, F1 Macro: 0.9584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.97      0.94        78\n",
      "    positive       0.99      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       245\n",
      "   macro avg       0.95      0.97      0.96       245\n",
      "weighted avg       0.96      0.96      0.96       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 478: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9124\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 104.79503798484802 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.867141485214233 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5435, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4804, Accuracy: 0.8088, F1 Micro: 0.8919, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3914, Accuracy: 0.8906, F1 Micro: 0.9325, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2843, Accuracy: 0.9338, F1 Micro: 0.9586, F1 Macro: 0.956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2075, Accuracy: 0.9494, F1 Micro: 0.9682, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1532, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.114, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0927, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "\n",
      "Aspect detection accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.93      0.92      0.92       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5498, Accuracy: 0.834, F1 Micro: 0.834, F1 Macro: 0.8035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3223, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1552, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9153\n",
      "\n",
      "Sentiment analysis accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        84\n",
      "    positive       0.98      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.92      0.95      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 517: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.85      0.82       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 109.96013307571411 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 9.175907135009766 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5484, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4803, Accuracy: 0.8147, F1 Micro: 0.8947, F1 Macro: 0.8935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3779, Accuracy: 0.9055, F1 Micro: 0.9421, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2788, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2076, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1615, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1224, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0813, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5536, Accuracy: 0.8476, F1 Micro: 0.8476, F1 Macro: 0.8183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.32, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.152, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.133, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9253\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9135\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.9019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.92\n",
      "\n",
      "Sentiment analysis accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        85\n",
      "    positive       0.98      0.93      0.96       184\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.92      0.94      0.93       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 540: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9219\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.95      0.92      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.86      0.82       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 114.99294066429138 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.68211054801941 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.543, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.473, Accuracy: 0.8192, F1 Micro: 0.896, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3822, Accuracy: 0.9115, F1 Micro: 0.9453, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2648, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1959, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9732\n",
      "Epoch 6/10, Train Loss: 0.1501, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1152, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5539, Accuracy: 0.7885, F1 Micro: 0.7885, F1 Macro: 0.7222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1822, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1443, Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9573\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.106, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9446\n",
      "Epoch 8/10, Train Loss: 0.1011, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9327\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9127\n",
      "\n",
      "Sentiment analysis accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.97      0.94        87\n",
      "    positive       0.98      0.96      0.97       173\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.95      0.96      0.96       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 573: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9412\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       1.00      0.88      0.94        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.49720430374146 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.881256103515625 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4476, Accuracy: 0.8356, F1 Micro: 0.9036, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3516, Accuracy: 0.9241, F1 Micro: 0.953, F1 Macro: 0.9503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2466, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1791, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 6/10, Train Loss: 0.129, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "\n",
      "Aspect detection accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5638, Accuracy: 0.8199, F1 Micro: 0.8199, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2832, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1778, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9489\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 10/10, Train Loss: 0.0835, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "\n",
      "Sentiment analysis accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.96      0.95       261\n",
      "weighted avg       0.96      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 603: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.928\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 119.91198015213013 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 7.103718519210815 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5467, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4579, Accuracy: 0.8304, F1 Micro: 0.9006, F1 Macro: 0.8978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3344, Accuracy: 0.9315, F1 Micro: 0.9572, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2373, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.17, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0661, Accuracy: 0.9725, F1 Micro: 0.9828, F1 Macro: 0.9818\n",
      "Epoch 10/10, Train Loss: 0.0583, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "\n",
      "Aspect detection accuracy: 0.9725, F1 Micro: 0.9828, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5159, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3015, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "Epoch 3/10, Train Loss: 0.2037, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1917, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Epoch 6/10, Train Loss: 0.1479, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9309\n",
      "Epoch 8/10, Train Loss: 0.0958, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0635, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9112\n",
      "Epoch 10/10, Train Loss: 0.0545, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9267\n",
      "\n",
      "Sentiment analysis accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.93      0.96       168\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.93      0.96      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 630: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9303\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 119.7658109664917 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.546548128128052 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5334, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4458, Accuracy: 0.8341, F1 Micro: 0.9031, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3168, Accuracy: 0.9353, F1 Micro: 0.9597, F1 Macro: 0.9577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2222, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.165, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0979, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 10/10, Train Loss: 0.0505, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9798\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5752, Accuracy: 0.8649, F1 Micro: 0.8649, F1 Macro: 0.8389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3067, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9363\n",
      "Epoch 3/10, Train Loss: 0.1912, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9096\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9245\n",
      "Epoch 6/10, Train Loss: 0.0855, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9442\n",
      "Epoch 8/10, Train Loss: 0.0551, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Epoch 10/10, Train Loss: 0.0626, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "\n",
      "Sentiment analysis accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 648: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9315\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.99      0.99      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.97236156463623 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.035609483718872 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5324, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4385, Accuracy: 0.8757, F1 Micro: 0.9247, F1 Macro: 0.9214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2998, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2133, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0752, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9717, F1 Micro: 0.9823, F1 Macro: 0.9814\n",
      "\n",
      "Aspect detection accuracy: 0.9717, F1 Micro: 0.9823, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      1.00      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5309, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.8483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.241, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1877, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9522\n",
      "Epoch 4/10, Train Loss: 0.1653, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.938\n",
      "Epoch 5/10, Train Loss: 0.1117, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0814, Accuracy: 0.9644, F1 Micro: 0.9644, F1 Macro: 0.9607\n",
      "Epoch 7/10, Train Loss: 0.0808, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9423\n",
      "Epoch 9/10, Train Loss: 0.0696, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "\n",
      "Sentiment analysis accuracy: 0.9644, F1 Micro: 0.9644, F1 Macro: 0.9607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.97      0.95        86\n",
      "    positive       0.98      0.96      0.97       167\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.96      0.96      0.96       253\n",
      "weighted avg       0.97      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 673: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9369\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      1.00      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 125.56731390953064 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.645666122436523 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5258, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4358, Accuracy: 0.875, F1 Micro: 0.9247, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2994, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2118, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 6/10, Train Loss: 0.1091, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5282, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9406\n",
      "Epoch 3/10, Train Loss: 0.2008, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1598, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.939\n",
      "Epoch 5/10, Train Loss: 0.1187, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.9\n",
      "Epoch 9/10, Train Loss: 0.0708, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "\n",
      "Sentiment analysis accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 698: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9289\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.14244151115417 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.930163621902466 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5394, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4227, Accuracy: 0.8884, F1 Micro: 0.9321, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.295, Accuracy: 0.9405, F1 Micro: 0.9628, F1 Macro: 0.961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2031, Accuracy: 0.9606, F1 Micro: 0.9755, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.0896, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.979\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.94      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5035, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Epoch 3/10, Train Loss: 0.1724, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9692, F1 Micro: 0.9692, F1 Macro: 0.965\n",
      "Epoch 5/10, Train Loss: 0.1564, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9157\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9259\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9437\n",
      "\n",
      "Sentiment analysis accuracy: 0.9692, F1 Micro: 0.9692, F1 Macro: 0.965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.96      0.95        84\n",
      "    positive       0.98      0.97      0.98       176\n",
      "\n",
      "    accuracy                           0.97       260\n",
      "   macro avg       0.96      0.97      0.97       260\n",
      "weighted avg       0.97      0.97      0.97       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 723: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9356\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.96      0.94      0.95       152\n",
      "    positive       0.81      0.85      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.90      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.88      0.94        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 125.05147361755371 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.325094938278198 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5156, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4235, Accuracy: 0.9003, F1 Micro: 0.9393, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2919, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9756\n",
      "\n",
      "Aspect detection accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5096, Accuracy: 0.8774, F1 Micro: 0.8774, F1 Macro: 0.8676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2102, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1334, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 5/10, Train Loss: 0.1251, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.115, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0528, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.8889, F1 Micro: 0.8889, F1 Macro: 0.8813\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9128\n",
      "\n",
      "Sentiment analysis accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        87\n",
      "    positive       0.97      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.93       261\n",
      "   macro avg       0.92      0.94      0.93       261\n",
      "weighted avg       0.94      0.93      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 748: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9112\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.53887915611267 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.8078806400299072 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5231, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4246, Accuracy: 0.8929, F1 Micro: 0.9354, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2773, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1989, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0896, Accuracy: 0.9688, F1 Micro: 0.9805, F1 Macro: 0.9797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5074, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1863, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9397\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1123, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9486\n",
      "Epoch 7/10, Train Loss: 0.0837, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9431\n",
      "Epoch 10/10, Train Loss: 0.0951, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9298\n",
      "\n",
      "Sentiment analysis accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.96      0.95       261\n",
      "weighted avg       0.96      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 756: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9278\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.6886088848114 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.569209337234497 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5244, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4089, Accuracy: 0.901, F1 Micro: 0.9403, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2688, Accuracy: 0.9449, F1 Micro: 0.9656, F1 Macro: 0.9638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5218, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2728, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1694, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1295, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1062, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9476\n",
      "Epoch 7/10, Train Loss: 0.0817, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9577, F1 Micro: 0.9577, F1 Macro: 0.9526\n",
      "\n",
      "Sentiment analysis accuracy: 0.9577, F1 Micro: 0.9577, F1 Macro: 0.9526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.95      0.96      0.95       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 781: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9327\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.8561291694641 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.9479455947875977 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4209, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2624, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9745\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5153, Accuracy: 0.8712, F1 Micro: 0.8712, F1 Macro: 0.8632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9363\n",
      "Epoch 3/10, Train Loss: 0.1571, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 4/10, Train Loss: 0.1615, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9135\n",
      "Epoch 5/10, Train Loss: 0.1085, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9278\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "\n",
      "Sentiment analysis accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        86\n",
      "    positive       0.97      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.94      0.94       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 806: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9211\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.31331539154053 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.285081624984741 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5276, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.399, Accuracy: 0.9167, F1 Micro: 0.9491, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2542, Accuracy: 0.9576, F1 Micro: 0.9737, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1786, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Epoch 5/10, Train Loss: 0.1299, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "\n",
      "Aspect detection accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.88      0.99      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4847, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2167, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1539, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9455\n",
      "Epoch 4/10, Train Loss: 0.1015, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1027, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9507\n",
      "Epoch 6/10, Train Loss: 0.0804, Accuracy: 0.9262, F1 Micro: 0.9262, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0556, Accuracy: 0.9631, F1 Micro: 0.9631, F1 Macro: 0.9595\n",
      "Epoch 8/10, Train Loss: 0.0424, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9292\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9505\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.959, F1 Micro: 0.959, F1 Macro: 0.9551\n",
      "\n",
      "Sentiment analysis accuracy: 0.9631, F1 Micro: 0.9631, F1 Macro: 0.9595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        83\n",
      "    positive       0.99      0.96      0.97       161\n",
      "\n",
      "    accuracy                           0.96       244\n",
      "   macro avg       0.95      0.97      0.96       244\n",
      "weighted avg       0.96      0.96      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 831: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9285\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.97      0.67      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.91      0.78      0.83       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.64204668998718 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      " Acquired samples:25\n",
      "Sampling duration: 1.794628620147705 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.525, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.385, Accuracy: 0.9293, F1 Micro: 0.9565, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2356, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1243, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.981\n",
      "Epoch 6/10, Train Loss: 0.0924, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4804, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.233, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9442\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Epoch 4/10, Train Loss: 0.1358, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1001, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9565\n",
      "Epoch 6/10, Train Loss: 0.1406, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 8/10, Train Loss: 0.0784, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9483\n",
      "\n",
      "Sentiment analysis accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        86\n",
      "    positive       0.98      0.96      0.97       170\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.95      0.96      0.96       256\n",
      "weighted avg       0.96      0.96      0.96       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 856: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9325\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.56542658805847 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.028045892715454 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5298, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3954, Accuracy: 0.9234, F1 Micro: 0.9535, F1 Macro: 0.9523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2446, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.492, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Epoch 2/10, Train Loss: 0.2292, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9176\n",
      "Epoch 3/10, Train Loss: 0.1626, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1183, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9094\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "\n",
      "Sentiment analysis accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        88\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 864: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9329\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.88      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.1962594985962 s\n",
      "Total runtime: 3045.2245626449585 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a580015",
   "metadata": {
    "papermill": {
     "duration": 0.322584,
     "end_time": "2025-04-01T06:44:31.885851",
     "exception": false,
     "start_time": "2025-04-01T06:44:31.563267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 4\n",
      "Random seed: 3\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6456, Accuracy: 0.7865, F1 Micro: 0.8804, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5639, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5346, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5259, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4788, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 6/10, Train Loss: 0.477, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4707, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4588, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4215, Accuracy: 0.7932, F1 Micro: 0.8839, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3936, Accuracy: 0.7976, F1 Micro: 0.8857, F1 Macro: 0.884\n",
      "\n",
      "Aspect detection accuracy: 0.7976, F1 Micro: 0.8857, F1 Macro: 0.884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.76      0.96      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7056, Accuracy: 0.44, F1 Micro: 0.44, F1 Macro: 0.4318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5997, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5241, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5185, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.494, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4806, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4617, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.422, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3873, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2944, Accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "\n",
      "Sentiment analysis accuracy: 0.84, F1 Micro: 0.84, F1 Macro: 0.4565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         4\n",
      "    positive       0.84      1.00      0.91        21\n",
      "\n",
      "    accuracy                           0.84        25\n",
      "   macro avg       0.42      0.50      0.46        25\n",
      "weighted avg       0.71      0.84      0.77        25\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3169\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.78      1.00      0.87       167\n",
      "    positive       1.00      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.59      0.34      0.31       216\n",
      "weighted avg       0.75      0.78      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.76      0.95      0.84       152\n",
      "    positive       0.54      0.25      0.34        52\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.43      0.40      0.40       216\n",
      "weighted avg       0.66      0.73      0.68       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 61.41879725456238 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 13.200701475143433 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.593, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.497, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4743, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4641, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Epoch 5/10, Train Loss: 0.4417, Accuracy: 0.7946, F1 Micro: 0.8829, F1 Macro: 0.88\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4055, Accuracy: 0.8036, F1 Micro: 0.8879, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3751, Accuracy: 0.8162, F1 Micro: 0.8935, F1 Macro: 0.8906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3168, Accuracy: 0.8423, F1 Micro: 0.9072, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2831, Accuracy: 0.8534, F1 Micro: 0.9132, F1 Macro: 0.911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2431, Accuracy: 0.881, F1 Micro: 0.9286, F1 Macro: 0.9269\n",
      "\n",
      "Aspect detection accuracy: 0.881, F1 Micro: 0.9286, F1 Macro: 0.9269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.88      1.00      0.94       187\n",
      "     machine       0.87      0.99      0.93       175\n",
      "      others       0.83      0.92      0.87       158\n",
      "        part       0.87      0.97      0.92       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.88      1.00      0.93       191\n",
      "\n",
      "   micro avg       0.88      0.98      0.93      1061\n",
      "   macro avg       0.88      0.98      0.93      1061\n",
      "weighted avg       0.88      0.98      0.93      1061\n",
      " samples avg       0.89      0.98      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6075, Accuracy: 0.725, F1 Micro: 0.725, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5613, Accuracy: 0.725, F1 Micro: 0.725, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4213, Accuracy: 0.7312, F1 Micro: 0.7312, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3849, Accuracy: 0.7937, F1 Micro: 0.7938, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2997, Accuracy: 0.8625, F1 Micro: 0.8625, F1 Macro: 0.8224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2026, Accuracy: 0.8875, F1 Micro: 0.8875, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1503, Accuracy: 0.8875, F1 Micro: 0.8875, F1 Macro: 0.85\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1159, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8667\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8494\n",
      "Epoch 10/10, Train Loss: 0.0758, Accuracy: 0.8625, F1 Micro: 0.8625, F1 Macro: 0.8343\n",
      "\n",
      "Sentiment analysis accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.73      0.80        44\n",
      "    positive       0.90      0.97      0.93       116\n",
      "\n",
      "    accuracy                           0.90       160\n",
      "   macro avg       0.90      0.85      0.87       160\n",
      "weighted avg       0.90      0.90      0.90       160\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 135: Accuracy: 0.8688, F1 Micro: 0.8688, F1 Macro: 0.6303\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.89      1.00      0.94       181\n",
      "    positive       0.83      0.42      0.56        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.57      0.47      0.50       216\n",
      "weighted avg       0.84      0.88      0.85       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.86      0.99      0.92       167\n",
      "    positive       0.86      0.55      0.67        33\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.53      0.57       216\n",
      "weighted avg       0.83      0.86      0.82       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.50      0.60        12\n",
      "     neutral       0.83      0.91      0.87       152\n",
      "    positive       0.68      0.54      0.60        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.76      0.65      0.69       216\n",
      "weighted avg       0.79      0.80      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.57      0.70        23\n",
      "     neutral       0.87      0.97      0.92       152\n",
      "    positive       0.76      0.61      0.68        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.85      0.71      0.76       216\n",
      "weighted avg       0.85      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.96      0.99      0.97       186\n",
      "    positive       0.69      0.65      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.75      0.80       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.29      0.44        14\n",
      "     neutral       0.88      1.00      0.93       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.63      0.43      0.46       216\n",
      "weighted avg       0.82      0.88      0.83       216\n",
      "\n",
      "Total train time: 76.13567233085632 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 16.722077131271362 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5742, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5055, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4772, Accuracy: 0.7917, F1 Micro: 0.8832, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4337, Accuracy: 0.8065, F1 Micro: 0.8902, F1 Macro: 0.8884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.391, Accuracy: 0.8162, F1 Micro: 0.8929, F1 Macro: 0.8901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3565, Accuracy: 0.8549, F1 Micro: 0.914, F1 Macro: 0.912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2999, Accuracy: 0.8735, F1 Micro: 0.9242, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.247, Accuracy: 0.8958, F1 Micro: 0.9361, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2132, Accuracy: 0.9263, F1 Micro: 0.9543, F1 Macro: 0.9526\n",
      "Epoch 10/10, Train Loss: 0.1763, Accuracy: 0.9211, F1 Micro: 0.9515, F1 Macro: 0.9493\n",
      "\n",
      "Aspect detection accuracy: 0.9263, F1 Micro: 0.9543, F1 Macro: 0.9526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      0.98      0.97       187\n",
      "     machine       0.92      0.99      0.95       175\n",
      "      others       0.91      0.93      0.92       158\n",
      "        part       0.92      0.94      0.93       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.95      1061\n",
      "   macro avg       0.93      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.95      1061\n",
      " samples avg       0.94      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6131, Accuracy: 0.7112, F1 Micro: 0.7112, F1 Macro: 0.4156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4942, Accuracy: 0.806, F1 Micro: 0.806, F1 Macro: 0.7429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4043, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2387, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1369, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8788\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.8836, F1 Micro: 0.8836, F1 Macro: 0.8613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8893\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.8836, F1 Micro: 0.8836, F1 Macro: 0.8625\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9052, F1 Micro: 0.9052, F1 Macro: 0.8835\n",
      "\n",
      "Sentiment analysis accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.84      0.84        67\n",
      "    positive       0.93      0.94      0.94       165\n",
      "\n",
      "    accuracy                           0.91       232\n",
      "   macro avg       0.89      0.89      0.89       232\n",
      "weighted avg       0.91      0.91      0.91       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 208: Accuracy: 0.9113, F1 Micro: 0.9113, F1 Macro: 0.7875\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        11\n",
      "     neutral       0.96      0.98      0.97       181\n",
      "    positive       0.78      0.88      0.82        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.74      0.78       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.25      0.38        16\n",
      "     neutral       0.91      0.99      0.95       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.67      0.71       216\n",
      "weighted avg       0.89      0.90      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.91      0.93      0.92       152\n",
      "    positive       0.77      0.69      0.73        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.80      0.82      0.81       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.83      0.81        23\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.81      0.71      0.75        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.82        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.93      1.00      0.97       185\n",
      "    positive       0.75      0.35      0.48        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.69      0.76       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 80.46605610847473 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 15.518421411514282 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5543, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.4986, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4637, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4067, Accuracy: 0.8095, F1 Micro: 0.89, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3717, Accuracy: 0.84, F1 Micro: 0.9057, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3044, Accuracy: 0.8958, F1 Micro: 0.9372, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2514, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2057, Accuracy: 0.9338, F1 Micro: 0.9585, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1633, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9608\n",
      "Epoch 10/10, Train Loss: 0.1325, Accuracy: 0.9375, F1 Micro: 0.9614, F1 Macro: 0.9596\n",
      "\n",
      "Aspect detection accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      1.00      0.96       175\n",
      "      others       0.90      0.90      0.90       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5951, Accuracy: 0.678, F1 Micro: 0.678, F1 Macro: 0.404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.472, Accuracy: 0.839, F1 Micro: 0.839, F1 Macro: 0.8086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3166, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.8991\n",
      "Epoch 5/10, Train Loss: 0.1834, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.911\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.039, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9061\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8901\n",
      "\n",
      "Sentiment analysis accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.85      0.87        75\n",
      "    positive       0.93      0.95      0.94       161\n",
      "\n",
      "    accuracy                           0.92       236\n",
      "   macro avg       0.91      0.90      0.91       236\n",
      "weighted avg       0.92      0.92      0.92       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 274: Accuracy: 0.929, F1 Micro: 0.929, F1 Macro: 0.8434\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.44      0.58        16\n",
      "     neutral       0.91      1.00      0.95       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.71      0.78       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.91      0.90      0.90       152\n",
      "    positive       0.76      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.80      0.83      0.81       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.88      0.73      0.80        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.84      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.95      1.00      0.97       185\n",
      "    positive       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.77      0.84       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Total train time: 84.66360855102539 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 14.357300043106079 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5525, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5027, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4555, Accuracy: 0.8125, F1 Micro: 0.8928, F1 Macro: 0.8909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.389, Accuracy: 0.8586, F1 Micro: 0.9153, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3042, Accuracy: 0.9129, F1 Micro: 0.9469, F1 Macro: 0.9452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2451, Accuracy: 0.9345, F1 Micro: 0.9595, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1863, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1424, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1245, Accuracy: 0.9449, F1 Micro: 0.9653, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0985, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9698\n",
      "\n",
      "Aspect detection accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.95      0.89      0.92       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.97      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5911, Accuracy: 0.6856, F1 Micro: 0.6856, F1 Macro: 0.4067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4548, Accuracy: 0.8712, F1 Micro: 0.8712, F1 Macro: 0.8465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2804, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.8999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1827, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.8999\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1658, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0859, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9306\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.8977, F1 Micro: 0.8977, F1 Macro: 0.8738\n",
      "Epoch 10/10, Train Loss: 0.0746, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8908\n",
      "\n",
      "Sentiment analysis accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        82\n",
      "    positive       0.97      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 333: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8912\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.74      0.87      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.83      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 97.79040026664734 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 13.202327013015747 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4942, Accuracy: 0.7954, F1 Micro: 0.8848, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4245, Accuracy: 0.8192, F1 Micro: 0.8954, F1 Macro: 0.8932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3671, Accuracy: 0.8936, F1 Micro: 0.9357, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2876, Accuracy: 0.936, F1 Micro: 0.9599, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2063, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1673, Accuracy: 0.9487, F1 Micro: 0.9677, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1186, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1015, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9554, F1 Micro: 0.9718, F1 Macro: 0.9692\n",
      "\n",
      "Aspect detection accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      1.00      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.95      0.98      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5959, Accuracy: 0.6798, F1 Micro: 0.6798, F1 Macro: 0.4047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4324, Accuracy: 0.8775, F1 Micro: 0.8775, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2242, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9062\n",
      "Epoch 4/10, Train Loss: 0.1266, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0838, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1402, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9216\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9151\n",
      "Epoch 10/10, Train Loss: 0.0951, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9073\n",
      "\n",
      "Sentiment analysis accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.99      0.90        80\n",
      "    positive       0.99      0.90      0.95       173\n",
      "\n",
      "    accuracy                           0.93       253\n",
      "   macro avg       0.91      0.94      0.92       253\n",
      "weighted avg       0.94      0.93      0.93       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 387: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8958\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      1.00      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.80      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.77      0.77      0.77        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.81      0.77       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.95      0.98      0.96       152\n",
      "    positive       0.93      0.68      0.79        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.89      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 99.31762981414795 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 12.17076587677002 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5485, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4842, Accuracy: 0.7961, F1 Micro: 0.8855, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4274, Accuracy: 0.8423, F1 Micro: 0.9073, F1 Macro: 0.9056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3333, Accuracy: 0.9077, F1 Micro: 0.9431, F1 Macro: 0.9404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.246, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1869, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1488, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1134, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9721\n",
      "Epoch 9/10, Train Loss: 0.0992, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.0773, Accuracy: 0.9561, F1 Micro: 0.9721, F1 Macro: 0.9695\n",
      "\n",
      "Aspect detection accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9721\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.91      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6039, Accuracy: 0.815, F1 Micro: 0.815, F1 Macro: 0.7768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4231, Accuracy: 0.9016, F1 Micro: 0.9016, F1 Macro: 0.8885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2643, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9284\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9227\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9135\n",
      "Epoch 7/10, Train Loss: 0.1005, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9284\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0356, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9246\n",
      "\n",
      "Sentiment analysis accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.98      0.91        81\n",
      "    positive       0.99      0.92      0.96       173\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.92      0.95      0.93       254\n",
      "weighted avg       0.95      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9013\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.85      0.82       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 96.51087665557861 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 11.175308227539062 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5468, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4918, Accuracy: 0.7917, F1 Micro: 0.8831, F1 Macro: 0.8815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4382, Accuracy: 0.8363, F1 Micro: 0.9049, F1 Macro: 0.9034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3384, Accuracy: 0.9152, F1 Micro: 0.9474, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2441, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1798, Accuracy: 0.9501, F1 Micro: 0.9686, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1364, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1082, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0892, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9701\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9742\n",
      "\n",
      "Aspect detection accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5959, Accuracy: 0.6802, F1 Micro: 0.6802, F1 Macro: 0.4852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4438, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9462\n",
      "Epoch 5/10, Train Loss: 0.0961, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.8999\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9365\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9333\n",
      "Epoch 9/10, Train Loss: 0.0633, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9405\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9309\n",
      "\n",
      "Sentiment analysis accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.95      0.95       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 478: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9109\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 104.12779593467712 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 10.053311586380005 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4909, Accuracy: 0.8073, F1 Micro: 0.8905, F1 Macro: 0.8887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3977, Accuracy: 0.8854, F1 Micro: 0.931, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2922, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2123, Accuracy: 0.9539, F1 Micro: 0.9715, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.154, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 8/10, Train Loss: 0.0966, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9797\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5706, Accuracy: 0.7205, F1 Micro: 0.7205, F1 Macro: 0.5797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3602, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1721, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.935\n",
      "Epoch 7/10, Train Loss: 0.0933, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8932\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8961\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8905\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9255\n",
      "\n",
      "Sentiment analysis accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        84\n",
      "    positive       0.98      0.93      0.95       170\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.95      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 517: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9159\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 112.94006371498108 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 9.39717960357666 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4782, Accuracy: 0.8036, F1 Micro: 0.8888, F1 Macro: 0.8871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3946, Accuracy: 0.8869, F1 Micro: 0.9317, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2871, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1997, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1493, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0923, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "\n",
      "Aspect detection accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5513, Accuracy: 0.8543, F1 Micro: 0.8543, F1 Macro: 0.8329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3405, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1912, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1706, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9439\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9274\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9233\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9267\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9233\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9274\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9172\n",
      "\n",
      "Sentiment analysis accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       168\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.94      0.95      0.94       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 540: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       0.97      0.71      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 110.85154867172241 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.642822265625 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5442, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4652, Accuracy: 0.8095, F1 Micro: 0.8915, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3779, Accuracy: 0.904, F1 Micro: 0.9418, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2768, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1961, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1559, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1266, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.979\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5322, Accuracy: 0.747, F1 Micro: 0.747, F1 Macro: 0.6561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3204, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9209\n",
      "Epoch 3/10, Train Loss: 0.1541, Accuracy: 0.8956, F1 Micro: 0.8956, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1187, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9346\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9036, F1 Micro: 0.9036, F1 Macro: 0.8909\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9255\n",
      "Epoch 7/10, Train Loss: 0.0808, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9163\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9301\n",
      "Epoch 9/10, Train Loss: 0.0374, Accuracy: 0.9116, F1 Micro: 0.9116, F1 Macro: 0.9018\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9112\n",
      "\n",
      "Sentiment analysis accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.98      0.92        84\n",
      "    positive       0.99      0.92      0.95       165\n",
      "\n",
      "    accuracy                           0.94       249\n",
      "   macro avg       0.93      0.95      0.93       249\n",
      "weighted avg       0.95      0.94      0.94       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 573: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9172\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.85      0.89        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.95      0.71      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.84      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 113.03919124603271 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.926692485809326 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.537, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4633, Accuracy: 0.8073, F1 Micro: 0.8894, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.379, Accuracy: 0.9174, F1 Micro: 0.9493, F1 Macro: 0.9473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.261, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1791, Accuracy: 0.9606, F1 Micro: 0.9755, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9714\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9791\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5502, Accuracy: 0.8677, F1 Micro: 0.8677, F1 Macro: 0.8468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1861, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.119, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9255\n",
      "Epoch 8/10, Train Loss: 0.0519, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9218\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "\n",
      "Sentiment analysis accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 603: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       0.97      0.71      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 120.22573804855347 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "\n",
      "Sampling duration: 7.323482513427734 secondsNew train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5399, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4566, Accuracy: 0.8289, F1 Micro: 0.9006, F1 Macro: 0.8989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3551, Accuracy: 0.9219, F1 Micro: 0.9519, F1 Macro: 0.9497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2437, Accuracy: 0.9501, F1 Micro: 0.9686, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1718, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Aspect detection accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5406, Accuracy: 0.8431, F1 Micro: 0.8431, F1 Macro: 0.8065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1865, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "Epoch 5/10, Train Loss: 0.0915, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 6/10, Train Loss: 0.0836, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "Epoch 7/10, Train Loss: 0.0686, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9018\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.8853\n",
      "\n",
      "Sentiment analysis accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.96      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 630: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9265\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.85      0.89        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.99      0.95       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 121.31200933456421 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.540950536727905 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5183, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4576, Accuracy: 0.8237, F1 Micro: 0.8988, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3317, Accuracy: 0.9204, F1 Micro: 0.9508, F1 Macro: 0.9485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2285, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.16, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.096, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0571, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0538, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.522, Accuracy: 0.9036, F1 Micro: 0.9036, F1 Macro: 0.8956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9433\n",
      "Epoch 3/10, Train Loss: 0.1847, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9251\n",
      "Epoch 4/10, Train Loss: 0.1439, Accuracy: 0.9076, F1 Micro: 0.9076, F1 Macro: 0.9023\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9352\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9388\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9301\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9217\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.8835, F1 Micro: 0.8835, F1 Macro: 0.866\n",
      "\n",
      "Sentiment analysis accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       163\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.95      0.94       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 648: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.88      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 124.7160861492157 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.123209476470947 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5374, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4549, Accuracy: 0.84, F1 Micro: 0.9066, F1 Macro: 0.9051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3193, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2054, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0733, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5164, Accuracy: 0.8835, F1 Micro: 0.8835, F1 Macro: 0.8622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2307, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.185, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.947\n",
      "Epoch 4/10, Train Loss: 0.1331, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9301\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.0836, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9339\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9324\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9205\n",
      "Epoch 9/10, Train Loss: 0.0444, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9378\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9243\n",
      "\n",
      "Sentiment analysis accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        83\n",
      "    positive       0.99      0.94      0.96       166\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.96      0.95       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 673: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9209\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.38878154754639 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.580878019332886 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4414, Accuracy: 0.8497, F1 Micro: 0.9109, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3153, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0923, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5018, Accuracy: 0.8808, F1 Micro: 0.8808, F1 Macro: 0.8723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2504, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Epoch 3/10, Train Loss: 0.1993, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 4/10, Train Loss: 0.1302, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.901\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Epoch 8/10, Train Loss: 0.074, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9223\n",
      "Epoch 9/10, Train Loss: 0.067, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 10/10, Train Loss: 0.0615, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9199\n",
      "\n",
      "Sentiment analysis accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 698: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.95      0.75      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 125.98181748390198 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.003126621246338 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5325, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4473, Accuracy: 0.8549, F1 Micro: 0.9147, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3154, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2043, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1116, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.529, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2488, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Epoch 3/10, Train Loss: 0.1648, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1337, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Epoch 5/10, Train Loss: 0.1072, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1105, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "Epoch 8/10, Train Loss: 0.0756, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "\n",
      "Sentiment analysis accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        84\n",
      "    positive       0.98      0.93      0.96       178\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9219\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.47091245651245 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.336541414260864 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.7909, F1 Micro: 0.8819, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4326, Accuracy: 0.8899, F1 Micro: 0.9326, F1 Macro: 0.9302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.939, F1 Micro: 0.962, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1929, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9769\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.89      0.99      0.94       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5167, Accuracy: 0.8784, F1 Micro: 0.8784, F1 Macro: 0.8685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.211, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9371\n",
      "Epoch 3/10, Train Loss: 0.1628, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1461, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Epoch 5/10, Train Loss: 0.1097, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0668, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9561\n",
      "Epoch 7/10, Train Loss: 0.0671, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 9/10, Train Loss: 0.0473, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9464\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "\n",
      "Sentiment analysis accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       172\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.96       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 748: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9278\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.95      0.71      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.91      0.79      0.84       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.1713104248047 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.9540677070617676 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4318, Accuracy: 0.8713, F1 Micro: 0.923, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2872, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1925, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 8/10, Train Loss: 0.0635, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5256, Accuracy: 0.8953, F1 Micro: 0.8953, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1983, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "Epoch 4/10, Train Loss: 0.1335, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9569\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9524\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9522\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9228\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9215\n",
      "\n",
      "Sentiment analysis accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        85\n",
      "    positive       0.99      0.95      0.97       173\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.97      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 756: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9362\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.55350184440613 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.587949752807617 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5271, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4322, Accuracy: 0.904, F1 Micro: 0.9411, F1 Macro: 0.9383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2724, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9798\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9776\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4973, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9384\n",
      "Epoch 3/10, Train Loss: 0.1652, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1377, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9258\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9352\n",
      "\n",
      "Sentiment analysis accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        85\n",
      "    positive       0.98      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 781: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9243\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.47586941719055 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.9480836391448975 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5166, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4051, Accuracy: 0.9025, F1 Micro: 0.9393, F1 Macro: 0.9349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2603, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1861, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0572, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9673, F1 Micro: 0.9792, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Aspect detection accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.95      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.45, Accuracy: 0.8993, F1 Micro: 0.8993, F1 Macro: 0.8912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1636, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.11, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1167, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "Epoch 7/10, Train Loss: 0.0631, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9216\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Epoch 9/10, Train Loss: 0.0405, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9295\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.927\n",
      "\n",
      "Sentiment analysis accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        87\n",
      "    positive       0.98      0.93      0.95       181\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 806: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.1781027317047 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.3581597805023193 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5239, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4271, Accuracy: 0.8862, F1 Micro: 0.932, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2779, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.175, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0674, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5083, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1769, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9329\n",
      "Epoch 4/10, Train Loss: 0.1399, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1381, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9489\n",
      "Epoch 6/10, Train Loss: 0.1132, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9366\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9285\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "\n",
      "Sentiment analysis accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        87\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.96      0.95       259\n",
      "weighted avg       0.96      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9291\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.87      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.1133897304535 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.7710230350494385 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5299, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4102, Accuracy: 0.9137, F1 Micro: 0.9466, F1 Macro: 0.944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.26, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "Epoch 5/10, Train Loss: 0.1278, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.971\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9782\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4446, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.211, Accuracy: 0.9559, F1 Micro: 0.9559, F1 Macro: 0.9505\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9191\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9368\n",
      "Epoch 5/10, Train Loss: 0.1066, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9452\n",
      "Epoch 6/10, Train Loss: 0.1013, Accuracy: 0.9485, F1 Micro: 0.9485, F1 Macro: 0.9422\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9465\n",
      "Epoch 8/10, Train Loss: 0.0745, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9465\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9386\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9376\n",
      "\n",
      "Sentiment analysis accuracy: 0.9559, F1 Micro: 0.9559, F1 Macro: 0.9505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        87\n",
      "    positive       0.99      0.95      0.97       185\n",
      "\n",
      "    accuracy                           0.96       272\n",
      "   macro avg       0.94      0.96      0.95       272\n",
      "weighted avg       0.96      0.96      0.96       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9346\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.88      0.86       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.0435836315155 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 0.9784934520721436 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5164, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4062, Accuracy: 0.9144, F1 Micro: 0.9471, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2546, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1692, Accuracy: 0.9628, F1 Micro: 0.9769, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0721, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Aspect detection accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4454, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Epoch 2/10, Train Loss: 0.1915, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9074\n",
      "Epoch 3/10, Train Loss: 0.1508, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.147, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1116, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Epoch 7/10, Train Loss: 0.0557, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "Epoch 10/10, Train Loss: 0.059, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9103\n",
      "\n",
      "Sentiment analysis accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 864: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9247\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.82258296012878 s\n",
      "Total runtime: 3075.5991303920746 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741a57b",
   "metadata": {
    "papermill": {
     "duration": 0.293629,
     "end_time": "2025-04-01T06:44:33.098638",
     "exception": false,
     "start_time": "2025-04-01T06:44:32.805009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 5\n",
      "Random seed: 94\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6298, Accuracy: 0.7522, F1 Micro: 0.854, F1 Macro: 0.8362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5654, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5262, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4783, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4582, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4326, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4132, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4395, Accuracy: 0.7939, F1 Micro: 0.8842, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4306, Accuracy: 0.7999, F1 Micro: 0.887, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3956, Accuracy: 0.8036, F1 Micro: 0.8888, F1 Macro: 0.8871\n",
      "\n",
      "Aspect detection accuracy: 0.8036, F1 Micro: 0.8888, F1 Macro: 0.8871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.75      0.99      0.85       158\n",
      "        part       0.74      0.97      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.89      1061\n",
      "weighted avg       0.81      0.99      0.89      1061\n",
      " samples avg       0.81      0.99      0.89      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7142, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5719, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.6151, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5677, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.5123, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5138, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4932, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3579, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3098, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2431, Accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "\n",
      "Sentiment analysis accuracy: 0.8065, F1 Micro: 0.8065, F1 Macro: 0.4464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "    positive       0.81      1.00      0.89        25\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.40      0.50      0.45        31\n",
      "weighted avg       0.65      0.81      0.72        31\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 54: Accuracy: 0.7994, F1 Micro: 0.7994, F1 Macro: 0.3298\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.75      0.99      0.85       152\n",
      "    positive       0.67      0.19      0.30        52\n",
      "\n",
      "    accuracy                           0.74       216\n",
      "   macro avg       0.47      0.39      0.38       216\n",
      "weighted avg       0.69      0.74      0.67       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.74      0.97      0.84       152\n",
      "    positive       0.56      0.22      0.32        41\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.43      0.40      0.39       216\n",
      "weighted avg       0.63      0.73      0.65       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 66.53084969520569 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 13.325407028198242 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6089, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5125, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4697, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4728, Accuracy: 0.7946, F1 Micro: 0.8848, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4376, Accuracy: 0.808, F1 Micro: 0.8914, F1 Macro: 0.8899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4025, Accuracy: 0.8199, F1 Micro: 0.896, F1 Macro: 0.894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3712, Accuracy: 0.8371, F1 Micro: 0.9054, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3197, Accuracy: 0.8452, F1 Micro: 0.9088, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2662, Accuracy: 0.8601, F1 Micro: 0.917, F1 Macro: 0.9159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2484, Accuracy: 0.8743, F1 Micro: 0.9247, F1 Macro: 0.9232\n",
      "\n",
      "Aspect detection accuracy: 0.8743, F1 Micro: 0.9247, F1 Macro: 0.9232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.92      1.00      0.96       187\n",
      "     machine       0.80      0.99      0.89       175\n",
      "      others       0.85      0.89      0.87       158\n",
      "        part       0.89      0.97      0.93       158\n",
      "       price       0.88      1.00      0.93       192\n",
      "     service       0.92      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.88      0.98      0.92      1061\n",
      "   macro avg       0.88      0.98      0.92      1061\n",
      "weighted avg       0.88      0.98      0.93      1061\n",
      " samples avg       0.88      0.98      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6708, Accuracy: 0.7368, F1 Micro: 0.7368, F1 Macro: 0.4242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5905, Accuracy: 0.7368, F1 Micro: 0.7368, F1 Macro: 0.4242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5498, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4716, Accuracy: 0.7697, F1 Micro: 0.7697, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3896, Accuracy: 0.8224, F1 Micro: 0.8224, F1 Macro: 0.7519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2417, Accuracy: 0.8684, F1 Micro: 0.8684, F1 Macro: 0.8144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1442, Accuracy: 0.8816, F1 Micro: 0.8816, F1 Macro: 0.8362\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.8684, F1 Micro: 0.8684, F1 Macro: 0.8144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0925, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.8906\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.9013, F1 Micro: 0.9013, F1 Macro: 0.8738\n",
      "\n",
      "Sentiment analysis accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.8906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.85      0.84        40\n",
      "    positive       0.95      0.94      0.94       112\n",
      "\n",
      "    accuracy                           0.91       152\n",
      "   macro avg       0.89      0.89      0.89       152\n",
      "weighted avg       0.92      0.91      0.91       152\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 135: Accuracy: 0.8642, F1 Micro: 0.8642, F1 Macro: 0.6267\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       1.00      0.67      0.80        24\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.97      0.68      0.76       216\n",
      "weighted avg       0.94      0.93      0.92       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.83      0.15      0.26        33\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.54      0.38      0.38       216\n",
      "weighted avg       0.74      0.79      0.72       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.50      0.57        12\n",
      "     neutral       0.84      0.89      0.87       152\n",
      "    positive       0.64      0.58      0.61        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.72      0.66      0.68       216\n",
      "weighted avg       0.78      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.74      0.77        23\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.87      0.63      0.73        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.78      0.81       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.08      0.14        13\n",
      "     neutral       0.88      1.00      0.93       186\n",
      "    positive       0.67      0.12      0.20        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.40      0.43       216\n",
      "weighted avg       0.87      0.88      0.83       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.60        14\n",
      "     neutral       0.92      1.00      0.96       185\n",
      "    positive       0.78      0.41      0.54        17\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.61      0.70       216\n",
      "weighted avg       0.91      0.92      0.90       216\n",
      "\n",
      "Total train time: 74.92915534973145 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 13.499886989593506 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5791, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5077, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4855, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4425, Accuracy: 0.8073, F1 Micro: 0.8907, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4026, Accuracy: 0.8452, F1 Micro: 0.9099, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3609, Accuracy: 0.8519, F1 Micro: 0.9127, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2962, Accuracy: 0.8817, F1 Micro: 0.929, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.242, Accuracy: 0.8936, F1 Micro: 0.9351, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1938, Accuracy: 0.91, F1 Micro: 0.945, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1635, Accuracy: 0.9122, F1 Micro: 0.9457, F1 Macro: 0.9435\n",
      "\n",
      "Aspect detection accuracy: 0.9122, F1 Micro: 0.9457, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      1.00      0.98       187\n",
      "     machine       0.83      0.98      0.90       175\n",
      "      others       0.88      0.90      0.89       158\n",
      "        part       0.95      0.92      0.93       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.95      1061\n",
      "   macro avg       0.92      0.97      0.94      1061\n",
      "weighted avg       0.93      0.97      0.95      1061\n",
      " samples avg       0.93      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6377, Accuracy: 0.7072, F1 Micro: 0.7072, F1 Macro: 0.4142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4991, Accuracy: 0.7072, F1 Micro: 0.7072, F1 Macro: 0.4142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4556, Accuracy: 0.7207, F1 Micro: 0.7207, F1 Macro: 0.4617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3448, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1927, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8869\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.125, Accuracy: 0.9054, F1 Micro: 0.9054, F1 Macro: 0.89\n",
      "Epoch 8/10, Train Loss: 0.16, Accuracy: 0.8739, F1 Micro: 0.8739, F1 Macro: 0.8449\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8843\n",
      "Epoch 10/10, Train Loss: 0.0894, Accuracy: 0.8739, F1 Micro: 0.8739, F1 Macro: 0.8433\n",
      "\n",
      "Sentiment analysis accuracy: 0.9054, F1 Micro: 0.9054, F1 Macro: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.91      0.85        65\n",
      "    positive       0.96      0.90      0.93       157\n",
      "\n",
      "    accuracy                           0.91       222\n",
      "   macro avg       0.88      0.91      0.89       222\n",
      "weighted avg       0.91      0.91      0.91       222\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 208: Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.7968\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.97      1.00      0.98       181\n",
      "    positive       0.91      0.83      0.87        24\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.82      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.25      0.40        16\n",
      "     neutral       0.82      0.98      0.90       167\n",
      "    positive       0.69      0.27      0.39        33\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.84      0.50      0.56       216\n",
      "weighted avg       0.82      0.82      0.78       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.88      0.89      0.89       152\n",
      "    positive       0.73      0.67      0.70        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.77      0.77      0.77       216\n",
      "weighted avg       0.83      0.83      0.83       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.87      0.74        23\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.86      0.83       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.82      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.80      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 80.38610506057739 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 14.948939085006714 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5556, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.488, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4846, Accuracy: 0.7991, F1 Micro: 0.8871, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4078, Accuracy: 0.8318, F1 Micro: 0.9029, F1 Macro: 0.9019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3587, Accuracy: 0.8542, F1 Micro: 0.9141, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2958, Accuracy: 0.8884, F1 Micro: 0.9323, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2392, Accuracy: 0.9189, F1 Micro: 0.9501, F1 Macro: 0.9483\n",
      "Epoch 8/10, Train Loss: 0.1925, Accuracy: 0.9174, F1 Micro: 0.9492, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.149, Accuracy: 0.9249, F1 Micro: 0.9534, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1294, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "\n",
      "Aspect detection accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.88      0.91      0.89       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6348, Accuracy: 0.6809, F1 Micro: 0.6809, F1 Macro: 0.4051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5196, Accuracy: 0.8468, F1 Micro: 0.8468, F1 Macro: 0.8153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.331, Accuracy: 0.8936, F1 Micro: 0.8936, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9362, F1 Micro: 0.9362, F1 Macro: 0.9287\n",
      "Epoch 5/10, Train Loss: 0.1369, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9159\n",
      "Epoch 6/10, Train Loss: 0.1464, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.118, Accuracy: 0.9404, F1 Micro: 0.9404, F1 Macro: 0.9324\n",
      "Epoch 8/10, Train Loss: 0.0676, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9108\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9362, F1 Micro: 0.9362, F1 Macro: 0.9278\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9362, F1 Micro: 0.9362, F1 Macro: 0.9283\n",
      "\n",
      "Sentiment analysis accuracy: 0.9404, F1 Micro: 0.9404, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        75\n",
      "    positive       0.97      0.94      0.96       160\n",
      "\n",
      "    accuracy                           0.94       235\n",
      "   macro avg       0.93      0.94      0.93       235\n",
      "weighted avg       0.94      0.94      0.94       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 274: Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.8714\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.92      0.99      0.95       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.78      0.83       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.88      0.91      0.90       152\n",
      "    positive       0.77      0.71      0.74        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.80      0.79      0.80       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.96      0.66      0.78        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 82.01774525642395 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 13.58928918838501 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5509, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4823, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4662, Accuracy: 0.8192, F1 Micro: 0.8969, F1 Macro: 0.8957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4062, Accuracy: 0.8557, F1 Micro: 0.9151, F1 Macro: 0.9147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3241, Accuracy: 0.8936, F1 Micro: 0.9355, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2502, Accuracy: 0.9256, F1 Micro: 0.9538, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1907, Accuracy: 0.9397, F1 Micro: 0.9627, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1549, Accuracy: 0.9472, F1 Micro: 0.9673, F1 Macro: 0.966\n",
      "Epoch 9/10, Train Loss: 0.1344, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1028, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9661\n",
      "\n",
      "Aspect detection accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.89      0.96      0.93       158\n",
      "        part       0.94      0.97      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5807, Accuracy: 0.6851, F1 Micro: 0.6851, F1 Macro: 0.4066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4908, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2875, Accuracy: 0.9149, F1 Micro: 0.9149, F1 Macro: 0.9052\n",
      "Epoch 4/10, Train Loss: 0.1981, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9149, F1 Micro: 0.9149, F1 Macro: 0.9058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9136\n",
      "Epoch 7/10, Train Loss: 0.0607, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8926\n",
      "Epoch 8/10, Train Loss: 0.1137, Accuracy: 0.9149, F1 Micro: 0.9149, F1 Macro: 0.9047\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8951\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9091\n",
      "\n",
      "Sentiment analysis accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.93      0.88        74\n",
      "    positive       0.97      0.92      0.94       161\n",
      "\n",
      "    accuracy                           0.92       235\n",
      "   macro avg       0.90      0.93      0.91       235\n",
      "weighted avg       0.93      0.92      0.92       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 333: Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.8746\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.78      0.82       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.92      0.81      0.86       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.91      0.79        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.97      0.68      0.80        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.85      0.85       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 89.52478623390198 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 12.388311386108398 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5574, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4891, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4522, Accuracy: 0.8415, F1 Micro: 0.9081, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3735, Accuracy: 0.8936, F1 Micro: 0.9363, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.273, Accuracy: 0.9271, F1 Micro: 0.9555, F1 Macro: 0.9546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2156, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1721, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "Epoch 8/10, Train Loss: 0.1279, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1032, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0835, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.968\n",
      "\n",
      "Aspect detection accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.602, Accuracy: 0.6996, F1 Micro: 0.6996, F1 Macro: 0.4367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.414, Accuracy: 0.9053, F1 Micro: 0.9053, F1 Macro: 0.8945\n",
      "Epoch 3/10, Train Loss: 0.2746, Accuracy: 0.8807, F1 Micro: 0.8807, F1 Macro: 0.8699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9342, F1 Micro: 0.9342, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1634, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1329, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.933\n",
      "Epoch 8/10, Train Loss: 0.1052, Accuracy: 0.9342, F1 Micro: 0.9342, F1 Macro: 0.9245\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.92\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.929\n",
      "\n",
      "Sentiment analysis accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.933\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.91        74\n",
      "    positive       0.97      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.94       243\n",
      "   macro avg       0.93      0.94      0.93       243\n",
      "weighted avg       0.94      0.94      0.94       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 387: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8859\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.92      0.99      0.96       167\n",
      "    positive       0.91      0.64      0.75        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.79      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.82      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 97.24354648590088 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.158166885375977 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5528, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4815, Accuracy: 0.8051, F1 Micro: 0.8899, F1 Macro: 0.8884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4304, Accuracy: 0.8423, F1 Micro: 0.908, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3382, Accuracy: 0.9048, F1 Micro: 0.9423, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.252, Accuracy: 0.9479, F1 Micro: 0.9678, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1935, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9664\n",
      "Epoch 7/10, Train Loss: 0.1448, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1106, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.974\n",
      "\n",
      "Aspect detection accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6001, Accuracy: 0.7984, F1 Micro: 0.7984, F1 Macro: 0.7368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3661, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9556, F1 Micro: 0.9556, F1 Macro: 0.9494\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9398\n",
      "Epoch 7/10, Train Loss: 0.1162, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.911\n",
      "Epoch 8/10, Train Loss: 0.1287, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9237\n",
      "Epoch 9/10, Train Loss: 0.0936, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9358\n",
      "\n",
      "Sentiment analysis accuracy: 0.9556, F1 Micro: 0.9556, F1 Macro: 0.9494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        79\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       248\n",
      "   macro avg       0.95      0.95      0.95       248\n",
      "weighted avg       0.96      0.96      0.96       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 435: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.912\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 98.64683938026428 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 10.203371524810791 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.549, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4795, Accuracy: 0.8058, F1 Micro: 0.8903, F1 Macro: 0.889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4143, Accuracy: 0.8571, F1 Micro: 0.9161, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3217, Accuracy: 0.9129, F1 Micro: 0.9462, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.23, Accuracy: 0.9449, F1 Micro: 0.9659, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1818, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.971\n",
      "Epoch 7/10, Train Loss: 0.1315, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9685\n",
      "Epoch 8/10, Train Loss: 0.1092, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0884, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.078, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9722\n",
      "\n",
      "Aspect detection accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5671, Accuracy: 0.8287, F1 Micro: 0.8287, F1 Macro: 0.7685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3462, Accuracy: 0.8924, F1 Micro: 0.8924, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2469, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1349, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.123, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1342, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9417\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9314\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9242\n",
      "Epoch 10/10, Train Loss: 0.0817, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9242\n",
      "\n",
      "Sentiment analysis accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        79\n",
      "    positive       0.99      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.93      0.96      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 478: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9057\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.92      0.91      0.92       152\n",
      "    positive       0.78      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.78      0.81      0.80       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 104.6650140285492 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.332647800445557 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5293, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4809, Accuracy: 0.8051, F1 Micro: 0.8898, F1 Macro: 0.8885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4145, Accuracy: 0.872, F1 Micro: 0.924, F1 Macro: 0.9232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2997, Accuracy: 0.9293, F1 Micro: 0.9565, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2128, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1563, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Epoch 8/10, Train Loss: 0.093, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9718\n",
      "\n",
      "Aspect detection accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5623, Accuracy: 0.8697, F1 Micro: 0.8697, F1 Macro: 0.8534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2929, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9189\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9261\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9252\n",
      "\n",
      "Sentiment analysis accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        79\n",
      "    positive       0.98      0.93      0.95       182\n",
      "\n",
      "    accuracy                           0.93       261\n",
      "   macro avg       0.91      0.94      0.93       261\n",
      "weighted avg       0.94      0.93      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 517: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9035\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.91      0.82        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.88       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 113.9195339679718 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.49856448173523 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4891, Accuracy: 0.8051, F1 Micro: 0.8885, F1 Macro: 0.8867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.411, Accuracy: 0.8787, F1 Micro: 0.9281, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2884, Accuracy: 0.9397, F1 Micro: 0.9626, F1 Macro: 0.9609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1533, Accuracy: 0.9487, F1 Micro: 0.9679, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1173, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9735\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Aspect detection accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5753, Accuracy: 0.8566, F1 Micro: 0.8566, F1 Macro: 0.839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3164, Accuracy: 0.8924, F1 Micro: 0.8924, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2014, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9296\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.912\n",
      "Epoch 5/10, Train Loss: 0.1565, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9381\n",
      "Epoch 7/10, Train Loss: 0.1068, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1291, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9409\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9217\n",
      "\n",
      "Sentiment analysis accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.91      0.92        82\n",
      "    positive       0.96      0.96      0.96       169\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.94      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 540: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9106\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 110.05447244644165 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.039291143417358 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.538, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4815, Accuracy: 0.8296, F1 Micro: 0.9024, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.37, Accuracy: 0.8943, F1 Micro: 0.9367, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2692, Accuracy: 0.9516, F1 Micro: 0.9701, F1 Macro: 0.9692\n",
      "Epoch 5/10, Train Loss: 0.1841, Accuracy: 0.9516, F1 Micro: 0.9701, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9732\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9731\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9729\n",
      "\n",
      "Aspect detection accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5768, Accuracy: 0.8612, F1 Micro: 0.8612, F1 Macro: 0.8533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3267, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1922, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9275\n",
      "Epoch 4/10, Train Loss: 0.1465, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1749, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9323\n",
      "Epoch 6/10, Train Loss: 0.1312, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.8997\n",
      "Epoch 7/10, Train Loss: 0.1167, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9151\n",
      "Epoch 8/10, Train Loss: 0.0808, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.918\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9118\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9109\n",
      "\n",
      "Sentiment analysis accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        82\n",
      "    positive       0.97      0.94      0.95       163\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.94      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 573: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9064\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.90      0.68      0.78        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 108.5024676322937 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.303596496582031 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5359, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4679, Accuracy: 0.8333, F1 Micro: 0.9041, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3546, Accuracy: 0.9241, F1 Micro: 0.9536, F1 Macro: 0.952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2536, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Epoch 5/10, Train Loss: 0.1757, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0826, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "\n",
      "Aspect detection accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5661, Accuracy: 0.8281, F1 Micro: 0.8281, F1 Macro: 0.7874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2977, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2107, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1995, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1217, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9345\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.915\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.918\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Epoch 10/10, Train Loss: 0.0745, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9216\n",
      "\n",
      "Sentiment analysis accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        84\n",
      "    positive       0.97      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.93      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 603: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9161\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.82      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 116.2240252494812 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.595546245574951 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4566, Accuracy: 0.8311, F1 Micro: 0.9029, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3426, Accuracy: 0.9174, F1 Micro: 0.9487, F1 Macro: 0.9457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.231, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1616, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1163, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Epoch 7/10, Train Loss: 0.0931, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0783, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "\n",
      "Aspect detection accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.8819, F1 Micro: 0.8819, F1 Macro: 0.8716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2325, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Epoch 4/10, Train Loss: 0.1745, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 8/10, Train Loss: 0.0793, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9016\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.909\n",
      "\n",
      "Sentiment analysis accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        83\n",
      "    positive       0.98      0.93      0.95       171\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.92      0.94      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 630: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 116.98635363578796 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.061865329742432 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5264, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.462, Accuracy: 0.8378, F1 Micro: 0.9058, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3339, Accuracy: 0.9397, F1 Micro: 0.9626, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2209, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1564, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0754, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "\n",
      "Aspect detection accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5359, Accuracy: 0.8805, F1 Micro: 0.8805, F1 Macro: 0.8713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2164, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1697, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.928\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9223\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9342\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9125\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9217\n",
      "\n",
      "Sentiment analysis accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        83\n",
      "    positive       0.98      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       251\n",
      "   macro avg       0.93      0.95      0.93       251\n",
      "weighted avg       0.94      0.94      0.94       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 648: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9125\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.20906162261963 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.7045934200286865 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4568, Accuracy: 0.8504, F1 Micro: 0.9116, F1 Macro: 0.91\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3195, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2136, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1502, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "\n",
      "Aspect detection accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5232, Accuracy: 0.8633, F1 Micro: 0.8633, F1 Macro: 0.8551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2974, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9129\n",
      "Epoch 3/10, Train Loss: 0.1697, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1418, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0707, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9307\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9184\n",
      "Epoch 8/10, Train Loss: 0.0479, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.927\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.918\n",
      "Epoch 10/10, Train Loss: 0.0816, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9266\n",
      "\n",
      "Sentiment analysis accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       172\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.92      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9171\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 126.89085841178894 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.137051820755005 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.536, Accuracy: 0.8006, F1 Micro: 0.8877, F1 Macro: 0.8862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4399, Accuracy: 0.8631, F1 Micro: 0.9197, F1 Macro: 0.9194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3232, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2226, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1606, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1148, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "\n",
      "Aspect detection accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5859, Accuracy: 0.8549, F1 Micro: 0.8549, F1 Macro: 0.826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3257, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1852, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.156, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9137\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9141\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9128\n",
      "\n",
      "Sentiment analysis accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.96      0.90        84\n",
      "    positive       0.98      0.91      0.95       171\n",
      "\n",
      "    accuracy                           0.93       255\n",
      "   macro avg       0.91      0.94      0.92       255\n",
      "weighted avg       0.94      0.93      0.93       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 698: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9052\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.82      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.6679711341858 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.653476238250732 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5286, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4563, Accuracy: 0.8609, F1 Micro: 0.9177, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3133, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1126, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9775\n",
      "\n",
      "Aspect detection accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.95      0.93      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5354, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9176\n",
      "Epoch 4/10, Train Loss: 0.1428, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9127\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.9111, F1 Micro: 0.9111, F1 Macro: 0.9016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0589, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9263\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9246\n",
      "Epoch 10/10, Train Loss: 0.0324, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9132\n",
      "\n",
      "Sentiment analysis accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.89      0.90        84\n",
      "    positive       0.95      0.96      0.95       186\n",
      "\n",
      "    accuracy                           0.94       270\n",
      "   macro avg       0.93      0.92      0.93       270\n",
      "weighted avg       0.94      0.94      0.94       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 723: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9116\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.82      0.90      0.86        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.0487027168274 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.027179956436157 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5307, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4347, Accuracy: 0.875, F1 Micro: 0.9258, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2926, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1837, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5251, Accuracy: 0.8968, F1 Micro: 0.8968, F1 Macro: 0.8871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2768, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Epoch 3/10, Train Loss: 0.1492, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1527, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Epoch 5/10, Train Loss: 0.1252, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9389\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8878\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9084\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9054\n",
      "\n",
      "Sentiment analysis accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 748: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.71943712234497 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.5015251636505127 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5312, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4337, Accuracy: 0.8698, F1 Micro: 0.9225, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2942, Accuracy: 0.9375, F1 Micro: 0.9609, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.196, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Epoch 6/10, Train Loss: 0.1032, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "\n",
      "Aspect detection accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5252, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9253\n",
      "Epoch 3/10, Train Loss: 0.2157, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9228\n",
      "Epoch 4/10, Train Loss: 0.153, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1394, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9426\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9348\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9348\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9387\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9067\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.931\n",
      "\n",
      "Sentiment analysis accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.94       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 756: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9225\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.59218835830688 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.300245523452759 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5281, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4216, Accuracy: 0.8862, F1 Micro: 0.932, F1 Macro: 0.9312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2742, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0746, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5085, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9049\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1723, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1433, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9436\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9312\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9308\n",
      "Epoch 8/10, Train Loss: 0.0494, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 9/10, Train Loss: 0.0495, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.919\n",
      "\n",
      "Sentiment analysis accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       170\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.93      0.96      0.94       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9217\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.66151976585388 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.8255016803741455 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5316, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4149, Accuracy: 0.8936, F1 Micro: 0.9358, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2534, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1706, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9768\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.973\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9824\n",
      "\n",
      "Aspect detection accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.99      0.96       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4899, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9193\n",
      "Epoch 2/10, Train Loss: 0.2342, Accuracy: 0.8842, F1 Micro: 0.8842, F1 Macro: 0.8769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.145, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9295\n",
      "Epoch 4/10, Train Loss: 0.1135, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 6/10, Train Loss: 0.0764, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9161\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8899\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9099\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8924\n",
      "\n",
      "Sentiment analysis accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        85\n",
      "    positive       0.97      0.94      0.95       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.92      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9214\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.95      0.79      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.1428656578064 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.1666152477264404 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4103, Accuracy: 0.9129, F1 Micro: 0.9472, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2533, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1697, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1152, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 6/10, Train Loss: 0.0881, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "\n",
      "Aspect detection accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.94      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4932, Accuracy: 0.8914, F1 Micro: 0.8914, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2358, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.133, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9282\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9202\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 10/10, Train Loss: 0.0369, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9063\n",
      "\n",
      "Sentiment analysis accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.94      0.93       267\n",
      "weighted avg       0.94      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 831: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9169\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.94      0.95       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.54922819137573 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.7104463577270508 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5299, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4119, Accuracy: 0.9137, F1 Micro: 0.9477, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2628, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1637, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Epoch 10/10, Train Loss: 0.0406, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4276, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2164, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1628, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Epoch 4/10, Train Loss: 0.127, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9384\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 7/10, Train Loss: 0.0855, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0756, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9388\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.911\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "\n",
      "Sentiment analysis accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        84\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.94      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 856: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9187\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.20792937278748 s\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 0.9738912582397461 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5221, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4057, Accuracy: 0.9137, F1 Micro: 0.9478, F1 Macro: 0.9466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1596, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Aspect detection accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5107, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1719, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "Epoch 4/10, Train Loss: 0.1475, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.098, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.913\n",
      "Epoch 6/10, Train Loss: 0.1156, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9204\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.913\n",
      "Epoch 8/10, Train Loss: 0.0557, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.924\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9101\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9313\n",
      "\n",
      "Sentiment analysis accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.98      0.92        85\n",
      "    positive       0.99      0.93      0.96       181\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 864: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9233\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.57168436050415 s\n",
      "Total runtime: 3036.4902267456055 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6175447,
     "sourceId": 10843157,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6273.138556,
   "end_time": "2025-04-01T06:44:36.722534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-01T05:00:03.583978",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "022f8f9be23444469bdd019f19f674d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f43c0b52a84478c8c51ab0620a22b08",
       "placeholder": "​",
       "style": "IPY_MODEL_77b1fca0f4e84e7aa26fedb7e710f864",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/229k [00:00&lt;00:00, 5.28MB/s]"
      }
     },
     "0ce40ae5e9c245e3864942a0774a2b65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18d926fe355142d6add516889cce91ed",
        "IPY_MODEL_d29d95d554db4dfebb6051f2fdcdcfd6",
        "IPY_MODEL_929b65d8a9554853b8720f9f92d2d0f7"
       ],
       "layout": "IPY_MODEL_e60c00c9ac054adc9f3011f6cf401ef8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0dddb19a8eb64f97b0f7e5fab124b20f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0e6e8d858c364d5f846accf2120ca2f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_54208eb0cada442499b8b6d751c575b4",
        "IPY_MODEL_f6ecb5521f7844149e3dc030797efb61",
        "IPY_MODEL_5ded0fe9423d4188928884765a2dab45"
       ],
       "layout": "IPY_MODEL_88fe0700b27c40c4a7f2581d8d53cf46",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0f43c0b52a84478c8c51ab0620a22b08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "150970e1b9264ab69424e616b2090a2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "16dbfb54c9904384bce1d272bab46f0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b47b1d9af12545db82401550d317d2ec",
       "max": 497787752,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3bc4143551e743c79822ebfb39a357b8",
       "tabbable": null,
       "tooltip": null,
       "value": 497787752
      }
     },
     "1800f24164664bdc85da4beb5f7666c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18d926fe355142d6add516889cce91ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_924f55bf2c92449e9e20e12e995bb6fe",
       "placeholder": "​",
       "style": "IPY_MODEL_bb3913ff00fa40398c260935b25de3fd",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "30db1f44cc7d4ac7913a1698c0dd506f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31fd80b096bc49b8acdccc2deb4ce321": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "32bb5f43b1d1477ebbb7e41b38ab277e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3bc4143551e743c79822ebfb39a357b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3c42b5e157cf448eae7449dfc22cb989": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d244752bec14715b654e786b5bd596f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d4b3e10c14743bbb44889d407fe5c0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3d7aff9f6d1d4ed18b4ac4ed6f833f73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "467ba3fc0b0f4aedb1b1cbde82578462": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fea1150c0524a39b95a7f1f41886384": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4fec72c00a3143109c5016bd33f0df21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a6e639e691964316b7782ccc28c0c216",
       "placeholder": "​",
       "style": "IPY_MODEL_1800f24164664bdc85da4beb5f7666c7",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "54208eb0cada442499b8b6d751c575b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_30db1f44cc7d4ac7913a1698c0dd506f",
       "placeholder": "​",
       "style": "IPY_MODEL_150970e1b9264ab69424e616b2090a2a",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "58022b72bae54742b9299c060bdd99a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_467ba3fc0b0f4aedb1b1cbde82578462",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_31fd80b096bc49b8acdccc2deb4ce321",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "5ded0fe9423d4188928884765a2dab45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_813f326408d34fb0bbfc8974eebd5708",
       "placeholder": "​",
       "style": "IPY_MODEL_7d31476c4f5546789b6ed11afb7002a4",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 124B/s]"
      }
     },
     "62a4d90356a64ca79556e7390f49e421": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a2dd2abed9284a979e5d9b7ab620cb0e",
        "IPY_MODEL_58022b72bae54742b9299c060bdd99a3",
        "IPY_MODEL_022f8f9be23444469bdd019f19f674d1"
       ],
       "layout": "IPY_MODEL_a0eeb8a75691493593591ea525c302c0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "62decb9584954e2991306da8f3a6dcdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b6d95d286b44d06bd74871f2d140ae9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c9d86ce48c4478ea21a91f99c86aaa1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d154a3d026b41c497495af5307447b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "75d6806e75d34e2f9c0d4b6c0aa90ca7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4fec72c00a3143109c5016bd33f0df21",
        "IPY_MODEL_16dbfb54c9904384bce1d272bab46f0c",
        "IPY_MODEL_cc146629da724f90ab1829ec73b0023e"
       ],
       "layout": "IPY_MODEL_6b6d95d286b44d06bd74871f2d140ae9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "76d04946dce74a9d86e0bfcd4e37dfad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cb78b2fea736402d9d862be6297d81c1",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d99903f29ad14a59aa0ad4e47bb216c8",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "77b1fca0f4e84e7aa26fedb7e710f864": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7c32c5e10bcc493b91b8db1b51e08a71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d31476c4f5546789b6ed11afb7002a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "813f326408d34fb0bbfc8974eebd5708": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "860b720aa7464753b8ed56f2d8ca07e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b05c7a9f5d18468395ceed122b7b8f49",
       "placeholder": "​",
       "style": "IPY_MODEL_62decb9584954e2991306da8f3a6dcdc",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "88fe0700b27c40c4a7f2581d8d53cf46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "924f55bf2c92449e9e20e12e995bb6fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "929b65d8a9554853b8720f9f92d2d0f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9c83a07ae449468b9ad1585d2e5f6dc3",
       "placeholder": "​",
       "style": "IPY_MODEL_3d4b3e10c14743bbb44889d407fe5c0d",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 11.1kB/s]"
      }
     },
     "9c83a07ae449468b9ad1585d2e5f6dc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a0eeb8a75691493593591ea525c302c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2dd2abed9284a979e5d9b7ab620cb0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3c42b5e157cf448eae7449dfc22cb989",
       "placeholder": "​",
       "style": "IPY_MODEL_0dddb19a8eb64f97b0f7e5fab124b20f",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: 100%"
      }
     },
     "a6a8724d80bb4d2e99ff85124945b7cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32bb5f43b1d1477ebbb7e41b38ab277e",
       "placeholder": "​",
       "style": "IPY_MODEL_6d154a3d026b41c497495af5307447b2",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/1.53k [00:00&lt;00:00, 147kB/s]"
      }
     },
     "a6e639e691964316b7782ccc28c0c216": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b05c7a9f5d18468395ceed122b7b8f49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b47b1d9af12545db82401550d317d2ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb3913ff00fa40398c260935b25de3fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c065437c63c14c7aac1e32d64a76f736": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c07915f5fe034634bbca15a0f6986e47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb78b2fea736402d9d862be6297d81c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc146629da724f90ab1829ec73b0023e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6c9d86ce48c4478ea21a91f99c86aaa1",
       "placeholder": "​",
       "style": "IPY_MODEL_c065437c63c14c7aac1e32d64a76f736",
       "tabbable": null,
       "tooltip": null,
       "value": " 498M/498M [00:02&lt;00:00, 220MB/s]"
      }
     },
     "d29d95d554db4dfebb6051f2fdcdcfd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c32c5e10bcc493b91b8db1b51e08a71",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c07915f5fe034634bbca15a0f6986e47",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "d99903f29ad14a59aa0ad4e47bb216c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e60c00c9ac054adc9f3011f6cf401ef8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eaef4551401a454fa836cbe819c24711": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_860b720aa7464753b8ed56f2d8ca07e2",
        "IPY_MODEL_76d04946dce74a9d86e0bfcd4e37dfad",
        "IPY_MODEL_a6a8724d80bb4d2e99ff85124945b7cc"
       ],
       "layout": "IPY_MODEL_3d7aff9f6d1d4ed18b4ac4ed6f833f73",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f6ecb5521f7844149e3dc030797efb61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3d244752bec14715b654e786b5bd596f",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4fea1150c0524a39b95a7f1f41886384",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
