{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10000921,"sourceType":"datasetVersion","datasetId":6155806}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport wandb\nimport torch\nimport itertools\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification, BertPreTrainedModel, Trainer, TrainingArguments\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:33:36.771955Z","iopub.execute_input":"2024-12-30T08:33:36.772255Z","iopub.status.idle":"2024-12-30T08:33:38.120651Z","shell.execute_reply.started":"2024-12-30T08:33:36.772231Z","shell.execute_reply":"2024-12-30T08:33:38.120008Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb-key\")","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:33:38.121621Z","iopub.execute_input":"2024-12-30T08:33:38.121820Z","iopub.status.idle":"2024-12-30T08:33:38.275285Z","shell.execute_reply.started":"2024-12-30T08:33:38.121802Z","shell.execute_reply":"2024-12-30T08:33:38.274461Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"wandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:33:38.277019Z","iopub.execute_input":"2024-12-30T08:33:38.277238Z","iopub.status.idle":"2024-12-30T08:33:44.864814Z","shell.execute_reply.started":"2024-12-30T08:33:38.277219Z","shell.execute_reply":"2024-12-30T08:33:44.864156Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicost918\u001b[0m (\u001b[33mnicost918-petra-christian-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:33:44.866224Z","iopub.execute_input":"2024-12-30T08:33:44.866743Z","iopub.status.idle":"2024-12-30T08:33:44.874566Z","shell.execute_reply.started":"2024-12-30T08:33:44.866721Z","shell.execute_reply":"2024-12-30T08:33:44.873734Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/doctors-answer-text-dataset/Indo-Online Health Consultation-Medical Interview-Clean.csv', encoding='latin-1')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:33:44.875359Z","iopub.execute_input":"2024-12-30T08:33:44.875647Z","iopub.status.idle":"2024-12-30T08:33:44.992834Z","shell.execute_reply.started":"2024-12-30T08:33:44.875626Z","shell.execute_reply":"2024-12-30T08:33:44.992209Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   No                                             answer  1-FR  2-GI  3-PI  \\\n0   1  Halo Rizal,Radang tenggorokan umunya disebabka...     1     0     1   \n1   2  Halo Hellas,Cacar air merupakan suatu penyakit...     1     0     1   \n2   3  Halo Rory.......Terimakasih atas pertanyaan An...     1     0     1   \n3   4  Alo AfriYani, Terimakasih atas pertanyaannya. ...     1     0     1   \n4   5  Halo,Telinga berdenging atau  tinitus  merupak...     1     0     1   \n\n   4-DM  5-EDTRB  6-RE                                         Text_Clean  \\\n0     1        1     0  halo rizal radang tenggorokan umunya disebabka...   \n1     1        1     0  halo hellas cacar air merupakan suatu penyakit...   \n2     1        1     0  halo rory terimakasih atas pertanyaan anda per...   \n3     1        1     0  alo afriyani terimakasih atas pertanyaannya ku...   \n4     1        1     0  halo telinga berdenging atau tinitus merupakan...   \n\n                                       filtered_text  \\\n0  halo rizal radang tenggorokan umunya disebabka...   \n1  halo hellas cacar air penyakit disebabkan viru...   \n2  halo rory terimakasih ketahui gangguan kulit s...   \n3  alo afriyani terimakasih pertanyaannya kuku ja...   \n4  halo telinga berdenging tinitus sensasi penden...   \n\n                                               token  \\\n0  ['halo', 'rizal', 'radang', 'tenggorokan', 'um...   \n1  ['halo', 'hellas', 'cacar', 'air', 'penyakit',...   \n2  ['halo', 'rory', 'terimakasih', 'ketahui', 'ga...   \n3  ['alo', 'afriyani', 'terimakasih', 'pertanyaan...   \n4  ['halo', 'telinga', 'berdenging', 'tinitus', '...   \n\n                                      tokens_stemmed  \\\n0  ['halo', 'rizal', 'radang', 'tenggorok', 'umu'...   \n1  ['halo', 'hellas', 'cacar', 'air', 'sakit', 's...   \n2  ['halo', 'rory', 'terimakasih', 'tahu', 'gangg...   \n3  ['alo', 'afriyani', 'terimakasih', 'tanya', 'k...   \n4  ['halo', 'telinga', 'denging', 'tinitus', 'sen...   \n\n                                        Process_Data  \n0  halo rizal radang tenggorok umu sebab infeksi ...  \n1  halo hellas cacar air sakit sebab virus varise...  \n2  halo rory terimakasih tahu ganggu kulit rangka...  \n3  alo afriyani terimakasih tanya kuku jari kaki ...  \n4  halo telinga denging tinitus sensasi dengar de...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No</th>\n      <th>answer</th>\n      <th>1-FR</th>\n      <th>2-GI</th>\n      <th>3-PI</th>\n      <th>4-DM</th>\n      <th>5-EDTRB</th>\n      <th>6-RE</th>\n      <th>Text_Clean</th>\n      <th>filtered_text</th>\n      <th>token</th>\n      <th>tokens_stemmed</th>\n      <th>Process_Data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Halo Rizal,Radang tenggorokan umunya disebabka...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo rizal radang tenggorokan umunya disebabka...</td>\n      <td>halo rizal radang tenggorokan umunya disebabka...</td>\n      <td>['halo', 'rizal', 'radang', 'tenggorokan', 'um...</td>\n      <td>['halo', 'rizal', 'radang', 'tenggorok', 'umu'...</td>\n      <td>halo rizal radang tenggorok umu sebab infeksi ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Halo Hellas,Cacar air merupakan suatu penyakit...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo hellas cacar air merupakan suatu penyakit...</td>\n      <td>halo hellas cacar air penyakit disebabkan viru...</td>\n      <td>['halo', 'hellas', 'cacar', 'air', 'penyakit',...</td>\n      <td>['halo', 'hellas', 'cacar', 'air', 'sakit', 's...</td>\n      <td>halo hellas cacar air sakit sebab virus varise...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Halo Rory.......Terimakasih atas pertanyaan An...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo rory terimakasih atas pertanyaan anda per...</td>\n      <td>halo rory terimakasih ketahui gangguan kulit s...</td>\n      <td>['halo', 'rory', 'terimakasih', 'ketahui', 'ga...</td>\n      <td>['halo', 'rory', 'terimakasih', 'tahu', 'gangg...</td>\n      <td>halo rory terimakasih tahu ganggu kulit rangka...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Alo AfriYani, Terimakasih atas pertanyaannya. ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>alo afriyani terimakasih atas pertanyaannya ku...</td>\n      <td>alo afriyani terimakasih pertanyaannya kuku ja...</td>\n      <td>['alo', 'afriyani', 'terimakasih', 'pertanyaan...</td>\n      <td>['alo', 'afriyani', 'terimakasih', 'tanya', 'k...</td>\n      <td>alo afriyani terimakasih tanya kuku jari kaki ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Halo,Telinga berdenging atau  tinitus  merupak...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>halo telinga berdenging atau tinitus merupakan...</td>\n      <td>halo telinga berdenging tinitus sensasi penden...</td>\n      <td>['halo', 'telinga', 'berdenging', 'tinitus', '...</td>\n      <td>['halo', 'telinga', 'denging', 'tinitus', 'sen...</td>\n      <td>halo telinga denging tinitus sensasi dengar de...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\nlabels = data.columns[2:8]\n# Extract features and labels for training and validation\nX_train = train_data['Text_Clean'].values\ny_train = train_data[labels].values\nX_val = val_data['Text_Clean'].values\ny_val = val_data[labels].values\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:33:44.993652Z","iopub.execute_input":"2024-12-30T08:33:44.993883Z","iopub.status.idle":"2024-12-30T08:33:45.009190Z","shell.execute_reply.started":"2024-12-30T08:33:44.993840Z","shell.execute_reply":"2024-12-30T08:33:45.008373Z"}},"outputs":[{"name":"stdout","text":"(400,) (400, 6)\n(100,) (100, 6)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 8\nLEARNING_RATE = 2e-5","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:33:45.009997Z","iopub.execute_input":"2024-12-30T08:33:45.010269Z","iopub.status.idle":"2024-12-30T08:33:45.023430Z","shell.execute_reply.started":"2024-12-30T08:33:45.010249Z","shell.execute_reply":"2024-12-30T08:33:45.022645Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torch\n\n# Define custom Dataset class\nclass DoctorAnswerDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=96, use_float=True):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.use_float = use_float\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        labels = self.labels[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n        item = {key: val.squeeze() for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n        return item\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-12-30T08:36:54.172755Z","iopub.execute_input":"2024-12-30T08:36:54.173134Z","iopub.status.idle":"2024-12-30T08:36:54.616985Z","shell.execute_reply.started":"2024-12-30T08:36:54.173105Z","shell.execute_reply":"2024-12-30T08:36:54.616069Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class BertForMultiLabelClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = [2,2,2,2,2,2]\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, i) for i in self.num_labels])\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        subword_to_word_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        sequence_output = self.dropout(outputs[1])\n        logits = []\n        for classifier in self.classifiers:\n            logit = classifier(sequence_output)\n            logits.append(logit)\n\n        \n        logits = [torch.sigmoid(logit) for logit in logits]\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        \n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            total_loss = 0\n            for i, (logit, num_label) in enumerate(zip(logits, self.num_labels)):\n                label = labels[:, i]\n                loss = loss_fct(logit.view(-1, num_label), label.view(-1))\n                total_loss += loss\n\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:33:45.167927Z","iopub.execute_input":"2024-12-30T08:33:45.168131Z","iopub.status.idle":"2024-12-30T08:33:45.174976Z","shell.execute_reply.started":"2024-12-30T08:33:45.168114Z","shell.execute_reply":"2024-12-30T08:33:45.174083Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define compute metrics for evaluation\ndef compute_metrics_multi(p):\n    logits = p.predictions # logits list<tensor(bs, num_label)> ~ list of batch prediction per class \n    label_batch = p.label_ids\n\n    # print(p.predictions)\n    # generate prediction & label list\n    list_hyp = []\n    list_label = []\n    hyp = [torch.topk(torch.tensor(logit, dtype=torch.float), 1)[1] for logit in logits] # list<tensor(bs)>\n    batch_size = label_batch.shape[0]\n    num_label = len(hyp)\n    for i in range(batch_size):\n        hyps = []\n        labels = torch.tensor(label_batch[i,:], dtype=torch.float)\n        for j in range(num_label):\n            hyps.append(hyp[j][i].item())\n\n        hyps = torch.tensor(hyps, dtype=torch.float)\n        list_hyp.append(hyps)\n        list_label.append(labels)\n    \n    accuracy = accuracy_score(list_label, list_hyp)\n    # print(accuracy)\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(list_label, list_hyp, average='micro', zero_division=0)\n    f1_macro = f1_score(list_label, list_hyp, average='macro', zero_division=0)\n\n    # print(classification_report(list_label, list_hyp, zero_division=0, target_names=['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik']))\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro\n    }","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:33:45.175780Z","iopub.execute_input":"2024-12-30T08:33:45.176093Z","iopub.status.idle":"2024-12-30T08:33:45.191292Z","shell.execute_reply.started":"2024-12-30T08:33:45.176063Z","shell.execute_reply":"2024-12-30T08:33:45.190496Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define compute metrics for evaluation\ndef compute_metrics_single(p):\n    preds = torch.sigmoid(torch.tensor(p.predictions)).round()  # Sigmoid and threshold for multi-label\n    labels = torch.tensor(p.label_ids)\n    \n    accuracy = accuracy_score(labels, preds)\n\n    # Standard multi-label precision, recall, and F1 metrics\n    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n    a, b, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:33:45.192142Z","iopub.execute_input":"2024-12-30T08:33:45.192397Z","iopub.status.idle":"2024-12-30T08:33:45.207734Z","shell.execute_reply.started":"2024-12-30T08:33:45.192366Z","shell.execute_reply":"2024-12-30T08:33:45.206940Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_model(sequence_length, model_name, seed=42, multi_classifier=False, layers_freezed=6):\n    if multi_classifier:\n        config = BertConfig.from_pretrained(model_name)\n        config.num_labels = 6\n        config.num_labels_list = [2,2,2,2,2,2]\n        model = BertForMultiLabelClassification.from_pretrained(\n            model_name,\n            config=config\n        )\n        train_dataset = DoctorAnswerDataset(X_train, y_train, tokenizer, max_length=sequence_length, use_float=False)\n        val_dataset = DoctorAnswerDataset(X_val, y_val, tokenizer, max_length=sequence_length, use_float=False)\n    else:\n        model = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=6,\n            problem_type=\"multi_label_classification\"\n        )\n        train_dataset = DoctorAnswerDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n        val_dataset = DoctorAnswerDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n\n    # Freeze the first few layers of the encoder\n    for name, param in model.named_parameters():\n        # Specify the layers you want to freeze (e.g., first 6 layers)\n        if \"encoder.layer\" in name:\n            # Extract the layer number safely\n            layer_num = name.split(\".\")[3]\n            try:\n                # Freeze only the first 6 layers\n                if int(layer_num) < layers_freezed:\n                    param.requires_grad = False\n            except ValueError:\n                # Skip any parameter names that donâ€™t follow the expected format\n                continue\n\n    model.to(device)\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir='./results/dat-passive',\n        eval_strategy=\"epoch\",                    # Evaluate after every epoch\n        save_strategy=\"epoch\",                    # Save model after every epoch\n        learning_rate=LEARNING_RATE,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=EPOCHS,\n        load_best_model_at_end=True,\n        metric_for_best_model='f1_micro',\n        save_total_limit=1,\n        seed=seed\n    )\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics_multi if multi_classifier else compute_metrics_single\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate after training\n    eval_results = trainer.evaluate()\n    \n    print(eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:40:30.152347Z","iopub.execute_input":"2024-12-30T08:40:30.152806Z","iopub.status.idle":"2024-12-30T08:40:30.162032Z","shell.execute_reply.started":"2024-12-30T08:40:30.152768Z","shell.execute_reply":"2024-12-30T08:40:30.161001Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# ABLATION: CLASSIFIER TYPE","metadata":{}},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:36:57.767723Z","iopub.execute_input":"2024-12-30T08:36:57.768052Z","iopub.status.idle":"2024-12-30T08:39:37.648737Z","shell.execute_reply.started":"2024-12-30T08:36:57.768024Z","shell.execute_reply":"2024-12-30T08:39:37.647961Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForMultiLabelClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight', 'classifiers.5.bias', 'classifiers.5.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241230_083659-cepnie6o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nicost918-petra-christian-university/huggingface/runs/cepnie6o' target=\"_blank\">./results/dat-passive</a></strong> to <a href='https://wandb.ai/nicost918-petra-christian-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nicost918-petra-christian-university/huggingface' target=\"_blank\">https://wandb.ai/nicost918-petra-christian-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nicost918-petra-christian-university/huggingface/runs/cepnie6o' target=\"_blank\">https://wandb.ai/nicost918-petra-christian-university/huggingface/runs/cepnie6o</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:28, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.258182</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>2.165038</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>2.141862</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>2.131219</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>2.125198</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>2.121506</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>2.119169</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>2.117689</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>2.116830</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.234500</td>\n      <td>2.116560</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 2.2581820487976074, 'eval_accuracy': 0.81, 'eval_precision': 0.9625, 'eval_recall': 0.9821428571428571, 'eval_f1_micro': 0.9722222222222222, 'eval_f1_macro': 0.6532843428612597, 'eval_runtime': 1.3022, 'eval_samples_per_second': 76.793, 'eval_steps_per_second': 9.983, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:40:32.215174Z","iopub.execute_input":"2024-12-30T08:40:32.215513Z","iopub.status.idle":"2024-12-30T08:43:01.055557Z","shell.execute_reply.started":"2024-12-30T08:40:32.215483Z","shell.execute_reply":"2024-12-30T08:43:01.054708Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:26, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.153289</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.133968</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.119680</td>\n      <td>0.830000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.655811</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.110883</td>\n      <td>0.800000</td>\n      <td>0.974293</td>\n      <td>0.966837</td>\n      <td>0.970551</td>\n      <td>0.651543</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.102934</td>\n      <td>0.800000</td>\n      <td>0.974293</td>\n      <td>0.966837</td>\n      <td>0.970551</td>\n      <td>0.774260</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.114418</td>\n      <td>0.830000</td>\n      <td>0.958025</td>\n      <td>0.989796</td>\n      <td>0.973651</td>\n      <td>0.765101</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.101405</td>\n      <td>0.880000</td>\n      <td>0.974811</td>\n      <td>0.987245</td>\n      <td>0.980989</td>\n      <td>0.804614</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.093723</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.799323</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.095886</td>\n      <td>0.880000</td>\n      <td>0.974811</td>\n      <td>0.987245</td>\n      <td>0.980989</td>\n      <td>0.804614</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.105100</td>\n      <td>0.094677</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.799323</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.10140484571456909, 'eval_accuracy': 0.88, 'eval_precision': 0.9748110831234257, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.9809885931558936, 'eval_f1_macro': 0.8046139979966416, 'eval_runtime': 1.2667, 'eval_samples_per_second': 78.946, 'eval_steps_per_second': 10.263, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# ABLATION - FREEZE LAYERS","metadata":{}},{"cell_type":"markdown","source":"## NO FREEZE","metadata":{}},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=False, layers_freezed=0) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:44:12.626798Z","iopub.execute_input":"2024-12-30T08:44:12.627170Z","iopub.status.idle":"2024-12-30T08:47:02.515165Z","shell.execute_reply.started":"2024-12-30T08:44:12.627141Z","shell.execute_reply":"2024-12-30T08:47:02.514404Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:47, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.157151</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.128832</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.109180</td>\n      <td>0.840000</td>\n      <td>0.972222</td>\n      <td>0.982143</td>\n      <td>0.977157</td>\n      <td>0.656672</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.094492</td>\n      <td>0.820000</td>\n      <td>0.976923</td>\n      <td>0.971939</td>\n      <td>0.974425</td>\n      <td>0.794235</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.095794</td>\n      <td>0.840000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.766219</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.092389</td>\n      <td>0.840000</td>\n      <td>0.965174</td>\n      <td>0.989796</td>\n      <td>0.977330</td>\n      <td>0.776912</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.082709</td>\n      <td>0.860000</td>\n      <td>0.977157</td>\n      <td>0.982143</td>\n      <td>0.979644</td>\n      <td>0.781359</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.084181</td>\n      <td>0.890000</td>\n      <td>0.979747</td>\n      <td>0.987245</td>\n      <td>0.983482</td>\n      <td>0.824021</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.083140</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.782355</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.093500</td>\n      <td>0.083338</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.782355</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.08418140560388565, 'eval_accuracy': 0.89, 'eval_precision': 0.979746835443038, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.9834815756035578, 'eval_f1_macro': 0.8240212361241235, 'eval_runtime': 1.2393, 'eval_samples_per_second': 80.69, 'eval_steps_per_second': 10.49, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## 6 LAYERS","metadata":{}},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=False, layers_freezed=6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:47:02.516200Z","iopub.execute_input":"2024-12-30T08:47:02.516402Z","iopub.status.idle":"2024-12-30T08:49:28.399098Z","shell.execute_reply.started":"2024-12-30T08:47:02.516383Z","shell.execute_reply":"2024-12-30T08:49:28.398384Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:23, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.153289</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.133968</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.119680</td>\n      <td>0.830000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.655811</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.110883</td>\n      <td>0.800000</td>\n      <td>0.974293</td>\n      <td>0.966837</td>\n      <td>0.970551</td>\n      <td>0.651543</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.102934</td>\n      <td>0.800000</td>\n      <td>0.974293</td>\n      <td>0.966837</td>\n      <td>0.970551</td>\n      <td>0.774260</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.114418</td>\n      <td>0.830000</td>\n      <td>0.958025</td>\n      <td>0.989796</td>\n      <td>0.973651</td>\n      <td>0.765101</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.101405</td>\n      <td>0.880000</td>\n      <td>0.974811</td>\n      <td>0.987245</td>\n      <td>0.980989</td>\n      <td>0.804614</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.093723</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.799323</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.095886</td>\n      <td>0.880000</td>\n      <td>0.974811</td>\n      <td>0.987245</td>\n      <td>0.980989</td>\n      <td>0.804614</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.105100</td>\n      <td>0.094677</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.799323</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.10140484571456909, 'eval_accuracy': 0.88, 'eval_precision': 0.9748110831234257, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.9809885931558936, 'eval_f1_macro': 0.8046139979966416, 'eval_runtime': 1.2729, 'eval_samples_per_second': 78.561, 'eval_steps_per_second': 10.213, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## 8 LAYERS","metadata":{}},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=False, layers_freezed=8) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:49:41.097421Z","iopub.execute_input":"2024-12-30T08:49:41.097718Z","iopub.status.idle":"2024-12-30T08:51:59.698957Z","shell.execute_reply.started":"2024-12-30T08:49:41.097695Z","shell.execute_reply":"2024-12-30T08:51:59.698186Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:15, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.152985</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.132651</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.123681</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.111540</td>\n      <td>0.820000</td>\n      <td>0.972010</td>\n      <td>0.974490</td>\n      <td>0.973248</td>\n      <td>0.653725</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.099186</td>\n      <td>0.830000</td>\n      <td>0.979381</td>\n      <td>0.969388</td>\n      <td>0.974359</td>\n      <td>0.763374</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.099705</td>\n      <td>0.860000</td>\n      <td>0.970000</td>\n      <td>0.989796</td>\n      <td>0.979798</td>\n      <td>0.803848</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.097034</td>\n      <td>0.860000</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.779335</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.095565</td>\n      <td>0.870000</td>\n      <td>0.977099</td>\n      <td>0.979592</td>\n      <td>0.978344</td>\n      <td>0.780353</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.096155</td>\n      <td>0.860000</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.779335</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.114700</td>\n      <td>0.094778</td>\n      <td>0.860000</td>\n      <td>0.977099</td>\n      <td>0.979592</td>\n      <td>0.978344</td>\n      <td>0.780353</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.09970502555370331, 'eval_accuracy': 0.86, 'eval_precision': 0.97, 'eval_recall': 0.9897959183673469, 'eval_f1_micro': 0.9797979797979798, 'eval_f1_macro': 0.8038483804329607, 'eval_runtime': 1.2789, 'eval_samples_per_second': 78.191, 'eval_steps_per_second': 10.165, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 10 LAYERS","metadata":{}},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=False, layers_freezed=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:51:59.700017Z","iopub.execute_input":"2024-12-30T08:51:59.700346Z","iopub.status.idle":"2024-12-30T08:54:10.574119Z","shell.execute_reply.started":"2024-12-30T08:51:59.700321Z","shell.execute_reply":"2024-12-30T08:54:10.573381Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:08, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.148364</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.131279</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.127156</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.118766</td>\n      <td>0.830000</td>\n      <td>0.967337</td>\n      <td>0.982143</td>\n      <td>0.974684</td>\n      <td>0.654960</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.111589</td>\n      <td>0.850000</td>\n      <td>0.974684</td>\n      <td>0.982143</td>\n      <td>0.978399</td>\n      <td>0.657542</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.109423</td>\n      <td>0.840000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.655811</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.106947</td>\n      <td>0.840000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.655587</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.107434</td>\n      <td>0.830000</td>\n      <td>0.979381</td>\n      <td>0.969388</td>\n      <td>0.974359</td>\n      <td>0.654335</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.106637</td>\n      <td>0.850000</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.656466</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.140100</td>\n      <td>0.106721</td>\n      <td>0.860000</td>\n      <td>0.979540</td>\n      <td>0.977041</td>\n      <td>0.978289</td>\n      <td>0.657355</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.11158884316682816, 'eval_accuracy': 0.85, 'eval_precision': 0.9746835443037974, 'eval_recall': 0.9821428571428571, 'eval_f1_micro': 0.9783989834815755, 'eval_f1_macro': 0.6575418529109637, 'eval_runtime': 1.2378, 'eval_samples_per_second': 80.785, 'eval_steps_per_second': 10.502, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## FULLY FREEZED","metadata":{}},{"cell_type":"code","source":"train_model(256, 'indobenchmark/indobert-base-p2', multi_classifier=False, layers_freezed=12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:54:10.575241Z","iopub.execute_input":"2024-12-30T08:54:10.575519Z","iopub.status.idle":"2024-12-30T08:56:13.954804Z","shell.execute_reply.started":"2024-12-30T08:54:10.575496Z","shell.execute_reply":"2024-12-30T08:56:13.954027Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:00, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.209542</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.145771</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.135573</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.131824</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.128772</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.127095</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.126248</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.125449</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.124987</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.182300</td>\n      <td>0.124862</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.20954178273677826, 'eval_accuracy': 0.81, 'eval_precision': 0.9625, 'eval_recall': 0.9821428571428571, 'eval_f1_micro': 0.9722222222222222, 'eval_f1_macro': 0.6532843428612597, 'eval_runtime': 1.2544, 'eval_samples_per_second': 79.72, 'eval_steps_per_second': 10.364, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# ABLATION - SEQUENCE LENGTH","metadata":{}},{"cell_type":"code","source":"# Tokenize each text and calculate their lengths\ntoken_lengths = [len(tokenizer.tokenize(text)) for text in X_train]\n\n# Calculate the average length\naverage_length = sum(token_lengths) / len(token_lengths)\nmax_length = max(token_lengths)\n\nprint(\"Average length of tokenized text:\", average_length)\nprint(\"Max token length:\", max_length)","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:35:25.777493Z","iopub.execute_input":"2024-12-30T08:35:25.777817Z","iopub.status.idle":"2024-12-30T08:35:27.609705Z","shell.execute_reply.started":"2024-12-30T08:35:25.777788Z","shell.execute_reply":"2024-12-30T08:35:27.608744Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Average length of tokenized text: 223.215\nMax token length: 868\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncounts, bins = np.histogram(token_lengths, range=(0, 500))\nplt.stairs(counts, bins)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:35:28.382114Z","iopub.execute_input":"2024-12-30T08:35:28.382415Z","iopub.status.idle":"2024-12-30T08:35:28.599504Z","shell.execute_reply.started":"2024-12-30T08:35:28.382390Z","shell.execute_reply":"2024-12-30T08:35:28.598415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## MAX LENGTH 300","metadata":{}},{"cell_type":"code","source":"train_model(300, 'indobenchmark/indobert-base-p2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T08:56:52.882848Z","iopub.execute_input":"2024-12-30T08:56:52.883205Z","iopub.status.idle":"2024-12-30T08:59:51.157205Z","shell.execute_reply.started":"2024-12-30T08:56:52.883178Z","shell.execute_reply":"2024-12-30T08:59:51.156442Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:55, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.153134</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.131524</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.116037</td>\n      <td>0.840000</td>\n      <td>0.972222</td>\n      <td>0.982143</td>\n      <td>0.977157</td>\n      <td>0.656672</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.107444</td>\n      <td>0.820000</td>\n      <td>0.976982</td>\n      <td>0.974490</td>\n      <td>0.975734</td>\n      <td>0.764559</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.105234</td>\n      <td>0.820000</td>\n      <td>0.974425</td>\n      <td>0.971939</td>\n      <td>0.973180</td>\n      <td>0.793361</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.113187</td>\n      <td>0.850000</td>\n      <td>0.962779</td>\n      <td>0.989796</td>\n      <td>0.976101</td>\n      <td>0.766811</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.105177</td>\n      <td>0.840000</td>\n      <td>0.974619</td>\n      <td>0.979592</td>\n      <td>0.977099</td>\n      <td>0.801596</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.100343</td>\n      <td>0.840000</td>\n      <td>0.972222</td>\n      <td>0.982143</td>\n      <td>0.977157</td>\n      <td>0.796582</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.102309</td>\n      <td>0.820000</td>\n      <td>0.972081</td>\n      <td>0.977041</td>\n      <td>0.974555</td>\n      <td>0.777572</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.102000</td>\n      <td>0.100214</td>\n      <td>0.830000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.795429</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.11603716015815735, 'eval_accuracy': 0.84, 'eval_precision': 0.9722222222222222, 'eval_recall': 0.9821428571428571, 'eval_f1_micro': 0.9771573604060914, 'eval_f1_macro': 0.6566716365711341, 'eval_runtime': 1.4683, 'eval_samples_per_second': 68.104, 'eval_steps_per_second': 8.854, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## MAX LENGTH 360","metadata":{}},{"cell_type":"code","source":"train_model(360, 'indobenchmark/indobert-base-p2')","metadata":{"execution":{"iopub.status.busy":"2024-12-30T08:59:51.158260Z","iopub.execute_input":"2024-12-30T08:59:51.158561Z","iopub.status.idle":"2024-12-30T09:03:11.539763Z","shell.execute_reply.started":"2024-12-30T08:59:51.158537Z","shell.execute_reply":"2024-12-30T09:03:11.538938Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.153928</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.134430</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.119926</td>\n      <td>0.830000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.655811</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.109754</td>\n      <td>0.820000</td>\n      <td>0.974490</td>\n      <td>0.974490</td>\n      <td>0.974490</td>\n      <td>0.654593</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.103272</td>\n      <td>0.820000</td>\n      <td>0.976864</td>\n      <td>0.969388</td>\n      <td>0.973111</td>\n      <td>0.793182</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.110966</td>\n      <td>0.860000</td>\n      <td>0.965174</td>\n      <td>0.989796</td>\n      <td>0.977330</td>\n      <td>0.788173</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.102744</td>\n      <td>0.880000</td>\n      <td>0.977273</td>\n      <td>0.987245</td>\n      <td>0.982234</td>\n      <td>0.800306</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.099426</td>\n      <td>0.870000</td>\n      <td>0.974811</td>\n      <td>0.987245</td>\n      <td>0.980989</td>\n      <td>0.799427</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.100458</td>\n      <td>0.860000</td>\n      <td>0.974747</td>\n      <td>0.984694</td>\n      <td>0.979695</td>\n      <td>0.798444</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.104500</td>\n      <td>0.099566</td>\n      <td>0.860000</td>\n      <td>0.974747</td>\n      <td>0.984694</td>\n      <td>0.979695</td>\n      <td>0.798444</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.10274364799261093, 'eval_accuracy': 0.88, 'eval_precision': 0.9772727272727273, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.9822335025380711, 'eval_f1_macro': 0.8003064003176297, 'eval_runtime': 1.6806, 'eval_samples_per_second': 59.501, 'eval_steps_per_second': 7.735, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## MAX LENGTH 420","metadata":{}},{"cell_type":"code","source":"train_model(420, 'indobenchmark/indobert-base-p2')","metadata":{"execution":{"iopub.status.busy":"2024-12-30T09:03:11.541181Z","iopub.execute_input":"2024-12-30T09:03:11.541430Z","iopub.status.idle":"2024-12-30T09:06:59.309662Z","shell.execute_reply.started":"2024-12-30T09:03:11.541407Z","shell.execute_reply":"2024-12-30T09:06:59.308927Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:44, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.156908</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.135876</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.121796</td>\n      <td>0.830000</td>\n      <td>0.967337</td>\n      <td>0.982143</td>\n      <td>0.974684</td>\n      <td>0.654960</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.106797</td>\n      <td>0.840000</td>\n      <td>0.982005</td>\n      <td>0.974490</td>\n      <td>0.978233</td>\n      <td>0.766343</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.103253</td>\n      <td>0.810000</td>\n      <td>0.974359</td>\n      <td>0.969388</td>\n      <td>0.971867</td>\n      <td>0.775325</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.100239</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.798557</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.098259</td>\n      <td>0.870000</td>\n      <td>0.977215</td>\n      <td>0.984694</td>\n      <td>0.980940</td>\n      <td>0.799323</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.097328</td>\n      <td>0.860000</td>\n      <td>0.974747</td>\n      <td>0.984694</td>\n      <td>0.979695</td>\n      <td>0.798444</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.096836</td>\n      <td>0.840000</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.977041</td>\n      <td>0.796305</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.104000</td>\n      <td>0.094777</td>\n      <td>0.860000</td>\n      <td>0.977157</td>\n      <td>0.982143</td>\n      <td>0.979644</td>\n      <td>0.798328</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.09825873374938965, 'eval_accuracy': 0.87, 'eval_precision': 0.9772151898734177, 'eval_recall': 0.9846938775510204, 'eval_f1_micro': 0.9809402795425667, 'eval_f1_macro': 0.7993229927056363, 'eval_runtime': 1.8653, 'eval_samples_per_second': 53.612, 'eval_steps_per_second': 6.97, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# ABLATION - BATCH SIZE","metadata":{}},{"cell_type":"markdown","source":"## BATCH SIZE 4","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE=4\n\ntrain_model(360, 'indobenchmark/indobert-base-p2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:07:35.912384Z","iopub.execute_input":"2024-12-30T09:07:35.912692Z","iopub.status.idle":"2024-12-30T09:11:03.223309Z","shell.execute_reply.started":"2024-12-30T09:07:35.912668Z","shell.execute_reply":"2024-12-30T09:11:03.222439Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 03:24, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.145134</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.121676</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.107339</td>\n      <td>0.830000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.655587</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.097914</td>\n      <td>0.840000</td>\n      <td>0.974619</td>\n      <td>0.979592</td>\n      <td>0.977099</td>\n      <td>0.796445</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.129900</td>\n      <td>0.108746</td>\n      <td>0.810000</td>\n      <td>0.974359</td>\n      <td>0.969388</td>\n      <td>0.971867</td>\n      <td>0.775325</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.129900</td>\n      <td>0.111375</td>\n      <td>0.820000</td>\n      <td>0.965000</td>\n      <td>0.984694</td>\n      <td>0.974747</td>\n      <td>0.754718</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.129900</td>\n      <td>0.105964</td>\n      <td>0.840000</td>\n      <td>0.974619</td>\n      <td>0.979592</td>\n      <td>0.977099</td>\n      <td>0.779466</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.129900</td>\n      <td>0.104648</td>\n      <td>0.820000</td>\n      <td>0.969697</td>\n      <td>0.979592</td>\n      <td>0.974619</td>\n      <td>0.777721</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.129900</td>\n      <td>0.105113</td>\n      <td>0.830000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.778725</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.033200</td>\n      <td>0.103379</td>\n      <td>0.820000</td>\n      <td>0.972081</td>\n      <td>0.977041</td>\n      <td>0.974555</td>\n      <td>0.777572</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.09791357815265656, 'eval_accuracy': 0.84, 'eval_precision': 0.9746192893401016, 'eval_recall': 0.9795918367346939, 'eval_f1_micro': 0.9770992366412214, 'eval_f1_macro': 0.7964452339718501, 'eval_runtime': 1.637, 'eval_samples_per_second': 61.087, 'eval_steps_per_second': 15.272, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# USE BEST CONFIG","metadata":{}},{"cell_type":"code","source":"seeds = [50, 81, 14, 3, 94]\nBATCH_SIZE=8\n\nfor seed in seeds:\n    set_seed(seed)\n    print(\"SEED:\", seed)\n    train_model(360, 'indobenchmark/indobert-base-p2', seed=seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T09:11:24.758013Z","iopub.execute_input":"2024-12-30T09:11:24.758323Z","iopub.status.idle":"2024-12-30T09:28:04.283418Z","shell.execute_reply.started":"2024-12-30T09:11:24.758300Z","shell.execute_reply":"2024-12-30T09:28:04.282603Z"}},"outputs":[{"name":"stdout","text":"SEED: 50\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.142204</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.127398</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.115961</td>\n      <td>0.840000</td>\n      <td>0.974619</td>\n      <td>0.979592</td>\n      <td>0.977099</td>\n      <td>0.656570</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.109016</td>\n      <td>0.840000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.765704</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.112200</td>\n      <td>0.840000</td>\n      <td>0.965087</td>\n      <td>0.987245</td>\n      <td>0.976040</td>\n      <td>0.766698</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.099398</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.101897</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.101619</td>\n      <td>0.850000</td>\n      <td>0.969925</td>\n      <td>0.987245</td>\n      <td>0.978508</td>\n      <td>0.767681</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.098322</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.103300</td>\n      <td>0.100682</td>\n      <td>0.850000</td>\n      <td>0.969925</td>\n      <td>0.987245</td>\n      <td>0.978508</td>\n      <td>0.767681</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.09939751029014587, 'eval_accuracy': 0.86, 'eval_precision': 0.9723618090452262, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.979746835443038, 'eval_f1_macro': 0.781569886469384, 'eval_runtime': 1.6662, 'eval_samples_per_second': 60.017, 'eval_steps_per_second': 7.802, 'epoch': 10.0}\nSEED: 81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.142945</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.126140</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.109187</td>\n      <td>0.830000</td>\n      <td>0.976923</td>\n      <td>0.971939</td>\n      <td>0.974425</td>\n      <td>0.654466</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.104657</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.092427</td>\n      <td>0.850000</td>\n      <td>0.979592</td>\n      <td>0.979592</td>\n      <td>0.979592</td>\n      <td>0.798210</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.094833</td>\n      <td>0.860000</td>\n      <td>0.974747</td>\n      <td>0.984694</td>\n      <td>0.979695</td>\n      <td>0.798444</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.103743</td>\n      <td>0.850000</td>\n      <td>0.965174</td>\n      <td>0.989796</td>\n      <td>0.977330</td>\n      <td>0.767681</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.096606</td>\n      <td>0.850000</td>\n      <td>0.972292</td>\n      <td>0.984694</td>\n      <td>0.978454</td>\n      <td>0.780587</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.095834</td>\n      <td>0.850000</td>\n      <td>0.972292</td>\n      <td>0.984694</td>\n      <td>0.978454</td>\n      <td>0.780587</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.098100</td>\n      <td>0.095709</td>\n      <td>0.860000</td>\n      <td>0.974747</td>\n      <td>0.984694</td>\n      <td>0.979695</td>\n      <td>0.798444</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.1046573743224144, 'eval_accuracy': 0.86, 'eval_precision': 0.9723618090452262, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.979746835443038, 'eval_f1_macro': 0.781569886469384, 'eval_runtime': 1.6597, 'eval_samples_per_second': 60.252, 'eval_steps_per_second': 7.833, 'epoch': 10.0}\nSEED: 14\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.147084</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.136137</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.123061</td>\n      <td>0.840000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.655587</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.108249</td>\n      <td>0.830000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.655587</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.114226</td>\n      <td>0.850000</td>\n      <td>0.974684</td>\n      <td>0.982143</td>\n      <td>0.978399</td>\n      <td>0.780471</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.105466</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.098899</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.101636</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.100574</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.101100</td>\n      <td>0.099854</td>\n      <td>0.860000</td>\n      <td>0.972362</td>\n      <td>0.987245</td>\n      <td>0.979747</td>\n      <td>0.781570</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.10546613484621048, 'eval_accuracy': 0.86, 'eval_precision': 0.9723618090452262, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.979746835443038, 'eval_f1_macro': 0.781569886469384, 'eval_runtime': 1.6847, 'eval_samples_per_second': 59.358, 'eval_steps_per_second': 7.716, 'epoch': 10.0}\nSEED: 3\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.140445</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.130863</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.130041</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.114028</td>\n      <td>0.830000</td>\n      <td>0.967337</td>\n      <td>0.982143</td>\n      <td>0.974684</td>\n      <td>0.654960</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.110303</td>\n      <td>0.880000</td>\n      <td>0.974811</td>\n      <td>0.987245</td>\n      <td>0.980989</td>\n      <td>0.822254</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.105492</td>\n      <td>0.840000</td>\n      <td>0.974555</td>\n      <td>0.977041</td>\n      <td>0.975796</td>\n      <td>0.764699</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.107217</td>\n      <td>0.830000</td>\n      <td>0.974490</td>\n      <td>0.974490</td>\n      <td>0.974490</td>\n      <td>0.817170</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.106375</td>\n      <td>0.860000</td>\n      <td>0.974747</td>\n      <td>0.984694</td>\n      <td>0.979695</td>\n      <td>0.821260</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.104728</td>\n      <td>0.850000</td>\n      <td>0.974684</td>\n      <td>0.982143</td>\n      <td>0.978399</td>\n      <td>0.797450</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.105700</td>\n      <td>0.105377</td>\n      <td>0.860000</td>\n      <td>0.974684</td>\n      <td>0.982143</td>\n      <td>0.978399</td>\n      <td>0.820255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.11030309647321701, 'eval_accuracy': 0.88, 'eval_precision': 0.9748110831234257, 'eval_recall': 0.9872448979591837, 'eval_f1_micro': 0.9809885931558936, 'eval_f1_macro': 0.8222537267342297, 'eval_runtime': 1.6611, 'eval_samples_per_second': 60.201, 'eval_steps_per_second': 7.826, 'epoch': 10.0}\nSEED: 94\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.147507</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.135386</td>\n      <td>0.810000</td>\n      <td>0.962500</td>\n      <td>0.982143</td>\n      <td>0.972222</td>\n      <td>0.653284</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.117667</td>\n      <td>0.840000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.655811</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.114920</td>\n      <td>0.830000</td>\n      <td>0.969620</td>\n      <td>0.977041</td>\n      <td>0.973316</td>\n      <td>0.653859</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.106099</td>\n      <td>0.860000</td>\n      <td>0.972292</td>\n      <td>0.984694</td>\n      <td>0.978454</td>\n      <td>0.780587</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.108590</td>\n      <td>0.850000</td>\n      <td>0.967500</td>\n      <td>0.987245</td>\n      <td>0.977273</td>\n      <td>0.776799</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.102087</td>\n      <td>0.840000</td>\n      <td>0.969773</td>\n      <td>0.982143</td>\n      <td>0.975919</td>\n      <td>0.765704</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.101693</td>\n      <td>0.850000</td>\n      <td>0.969849</td>\n      <td>0.984694</td>\n      <td>0.977215</td>\n      <td>0.766698</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.102166</td>\n      <td>0.860000</td>\n      <td>0.972292</td>\n      <td>0.984694</td>\n      <td>0.978454</td>\n      <td>0.780587</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.105800</td>\n      <td>0.101570</td>\n      <td>0.850000</td>\n      <td>0.969849</td>\n      <td>0.984694</td>\n      <td>0.977215</td>\n      <td>0.766698</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.1060992181301117, 'eval_accuracy': 0.86, 'eval_precision': 0.9722921914357683, 'eval_recall': 0.9846938775510204, 'eval_f1_micro': 0.9784537389100126, 'eval_f1_macro': 0.7805870600675631, 'eval_runtime': 1.6641, 'eval_samples_per_second': 60.094, 'eval_steps_per_second': 7.812, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":35}]}