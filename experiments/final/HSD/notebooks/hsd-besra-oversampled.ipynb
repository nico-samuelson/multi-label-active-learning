{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6dd2b2",
   "metadata": {
    "papermill": {
     "duration": 0.011559,
     "end_time": "2025-05-10T04:16:16.712084",
     "exception": false,
     "start_time": "2025-05-10T04:16:16.700525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6a57f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:16.735035Z",
     "iopub.status.busy": "2025-05-10T04:16:16.734769Z",
     "iopub.status.idle": "2025-05-10T04:16:38.756946Z",
     "shell.execute_reply": "2025-05-10T04:16:38.756262Z"
    },
    "papermill": {
     "duration": 22.035508,
     "end_time": "2025-05-10T04:16:38.758409",
     "exception": false,
     "start_time": "2025-05-10T04:16:16.722901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8df07",
   "metadata": {
    "papermill": {
     "duration": 0.010114,
     "end_time": "2025-05-10T04:16:38.779265",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.769151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f014885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:38.801200Z",
     "iopub.status.busy": "2025-05-10T04:16:38.800694Z",
     "iopub.status.idle": "2025-05-10T04:16:38.804091Z",
     "shell.execute_reply": "2025-05-10T04:16:38.803460Z"
    },
    "papermill": {
     "duration": 0.015524,
     "end_time": "2025-05-10T04:16:38.805271",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.789747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95b59cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:38.826367Z",
     "iopub.status.busy": "2025-05-10T04:16:38.826164Z",
     "iopub.status.idle": "2025-05-10T04:16:38.829511Z",
     "shell.execute_reply": "2025-05-10T04:16:38.828933Z"
    },
    "papermill": {
     "duration": 0.015312,
     "end_time": "2025-05-10T04:16:38.830682",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.815370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1afd093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:38.851789Z",
     "iopub.status.busy": "2025-05-10T04:16:38.851589Z",
     "iopub.status.idle": "2025-05-10T04:16:38.859151Z",
     "shell.execute_reply": "2025-05-10T04:16:38.858577Z"
    },
    "papermill": {
     "duration": 0.019215,
     "end_time": "2025-05-10T04:16:38.860350",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.841135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e154e8e",
   "metadata": {
    "papermill": {
     "duration": 0.009874,
     "end_time": "2025-05-10T04:16:38.880286",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.870412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a6837d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:38.901286Z",
     "iopub.status.busy": "2025-05-10T04:16:38.901085Z",
     "iopub.status.idle": "2025-05-10T04:16:38.952854Z",
     "shell.execute_reply": "2025-05-10T04:16:38.951520Z"
    },
    "papermill": {
     "duration": 0.064215,
     "end_time": "2025-05-10T04:16:38.954511",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.890296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-besra'\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "sequence_length = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48441387",
   "metadata": {
    "papermill": {
     "duration": 0.009908,
     "end_time": "2025-05-10T04:16:38.974664",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.964756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82a919f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:38.995992Z",
     "iopub.status.busy": "2025-05-10T04:16:38.995696Z",
     "iopub.status.idle": "2025-05-10T04:16:39.140642Z",
     "shell.execute_reply": "2025-05-10T04:16:39.139730Z"
    },
    "papermill": {
     "duration": 0.157495,
     "end_time": "2025-05-10T04:16:39.142241",
     "exception": false,
     "start_time": "2025-05-10T04:16:38.984746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (14169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deklarasi pilkada 2018 aman dan anti hoax warg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "1  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "2  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "3  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "4  deklarasi pilkada 2018 aman dan anti hoax warg...   0        0   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              0         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         1            1        0            0          0   \n",
       "4              0         0            0        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         0        0            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            1          0  \n",
       "4         0        0            0          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech/oversampled-2.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad67151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.164379Z",
     "iopub.status.busy": "2025-05-10T04:16:39.164156Z",
     "iopub.status.idle": "2025-05-10T04:16:39.173639Z",
     "shell.execute_reply": "2025-05-10T04:16:39.172982Z"
    },
    "papermill": {
     "duration": 0.02201,
     "end_time": "2025-05-10T04:16:39.175052",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.153042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deklarasi pilkada 2018 aman dan anti hoax warg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "1  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "2  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "3  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "4  deklarasi pilkada 2018 aman dan anti hoax warg...   0        0   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              0         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         1            1        0            0          0   \n",
       "4              0         0            0        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         0        0            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            1          0  \n",
       "4         0        0            0          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af04fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.198426Z",
     "iopub.status.busy": "2025-05-10T04:16:39.198157Z",
     "iopub.status.idle": "2025-05-10T04:16:39.209088Z",
     "shell.execute_reply": "2025-05-10T04:16:39.208412Z"
    },
    "papermill": {
     "duration": 0.023222,
     "end_time": "2025-05-10T04:16:39.210272",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.187050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    8084\n",
       "1    6085\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19aaa3c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.233151Z",
     "iopub.status.busy": "2025-05-10T04:16:39.232930Z",
     "iopub.status.idle": "2025-05-10T04:16:39.238473Z",
     "shell.execute_reply": "2025-05-10T04:16:39.237661Z"
    },
    "papermill": {
     "duration": 0.018317,
     "end_time": "2025-05-10T04:16:39.239698",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.221381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8084\n",
       "1    6085\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813d153d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.262097Z",
     "iopub.status.busy": "2025-05-10T04:16:39.261852Z",
     "iopub.status.idle": "2025-05-10T04:16:39.273169Z",
     "shell.execute_reply": "2025-05-10T04:16:39.272426Z"
    },
    "papermill": {
     "duration": 0.023753,
     "end_time": "2025-05-10T04:16:39.274314",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.250561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic shape:  (8309, 13)\n",
      "Non-toxic shape:  (5860, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623d15d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.296941Z",
     "iopub.status.busy": "2025-05-10T04:16:39.296692Z",
     "iopub.status.idle": "2025-05-10T04:16:39.304777Z",
     "shell.execute_reply": "2025-05-10T04:16:39.304101Z"
    },
    "papermill": {
     "duration": 0.020849,
     "end_time": "2025-05-10T04:16:39.306062",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.285213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64869d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.329834Z",
     "iopub.status.busy": "2025-05-10T04:16:39.329609Z",
     "iopub.status.idle": "2025-05-10T04:16:39.342410Z",
     "shell.execute_reply": "2025-05-10T04:16:39.341725Z"
    },
    "papermill": {
     "duration": 0.026327,
     "end_time": "2025-05-10T04:16:39.343611",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.317284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb750ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.366993Z",
     "iopub.status.busy": "2025-05-10T04:16:39.366748Z",
     "iopub.status.idle": "2025-05-10T04:16:39.370242Z",
     "shell.execute_reply": "2025-05-10T04:16:39.369505Z"
    },
    "papermill": {
     "duration": 0.016546,
     "end_time": "2025-05-10T04:16:39.371521",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.354975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be83f1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.394452Z",
     "iopub.status.busy": "2025-05-10T04:16:39.394255Z",
     "iopub.status.idle": "2025-05-10T04:16:39.776125Z",
     "shell.execute_reply": "2025-05-10T04:16:39.775301Z"
    },
    "papermill": {
     "duration": 0.394687,
     "end_time": "2025-05-10T04:16:39.777475",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.382788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11335,) (11335, 12)\n",
      "(2834,) (2834, 12)\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "\n",
    "# Define the labels columns for multi-label classification\n",
    "label_columns = data.columns[1:]  # Assuming label columns start from the third column\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['Tweet'].values\n",
    "y_train = train_data[label_columns].values\n",
    "X_val = val_data['Tweet'].values\n",
    "y_val = val_data[label_columns].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b07b1",
   "metadata": {
    "papermill": {
     "duration": 0.011177,
     "end_time": "2025-05-10T04:16:39.800224",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.789047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e5598f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:39.823558Z",
     "iopub.status.busy": "2025-05-10T04:16:39.823297Z",
     "iopub.status.idle": "2025-05-10T04:16:40.622020Z",
     "shell.execute_reply": "2025-05-10T04:16:40.621316Z"
    },
    "papermill": {
     "duration": 0.812143,
     "end_time": "2025-05-10T04:16:40.623539",
     "exception": false,
     "start_time": "2025-05-10T04:16:39.811396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb9252b6aae408dbdc2bd9fea2df346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e60993ad80c4ea4ab9f5ccf404a69e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3684a500cf34f1788548951b2c2ba83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9aede2fe7247a78aa5d0726a372053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d758f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.647894Z",
     "iopub.status.busy": "2025-05-10T04:16:40.647626Z",
     "iopub.status.idle": "2025-05-10T04:16:40.652057Z",
     "shell.execute_reply": "2025-05-10T04:16:40.651227Z"
    },
    "papermill": {
     "duration": 0.017767,
     "end_time": "2025-05-10T04:16:40.653340",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.635573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=64, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eeb06f",
   "metadata": {
    "papermill": {
     "duration": 0.011157,
     "end_time": "2025-05-10T04:16:40.676160",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.665003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccd7da43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.699736Z",
     "iopub.status.busy": "2025-05-10T04:16:40.699504Z",
     "iopub.status.idle": "2025-05-10T04:16:40.703213Z",
     "shell.execute_reply": "2025-05-10T04:16:40.702431Z"
    },
    "papermill": {
     "duration": 0.016964,
     "end_time": "2025-05-10T04:16:40.704443",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.687479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6419b04b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.729498Z",
     "iopub.status.busy": "2025-05-10T04:16:40.729269Z",
     "iopub.status.idle": "2025-05-10T04:16:40.734076Z",
     "shell.execute_reply": "2025-05-10T04:16:40.733425Z"
    },
    "papermill": {
     "duration": 0.019029,
     "end_time": "2025-05-10T04:16:40.735253",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.716224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "546d6fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.759984Z",
     "iopub.status.busy": "2025-05-10T04:16:40.759766Z",
     "iopub.status.idle": "2025-05-10T04:16:40.771567Z",
     "shell.execute_reply": "2025-05-10T04:16:40.770966Z"
    },
    "papermill": {
     "duration": 0.025092,
     "end_time": "2025-05-10T04:16:40.772719",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.747627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "\n",
    "            nearest_cp = current_train_size\n",
    "            if nearest_cp not in checkpoints:\n",
    "                for cp in checkpoints:\n",
    "                    if cp > current_train_size:\n",
    "                        nearest_cp = cp\n",
    "                        break\n",
    "            percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}-{percentage}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0059f8",
   "metadata": {
    "papermill": {
     "duration": 0.011703,
     "end_time": "2025-05-10T04:16:40.796122",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.784419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48db85ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.820100Z",
     "iopub.status.busy": "2025-05-10T04:16:40.819879Z",
     "iopub.status.idle": "2025-05-10T04:16:40.824921Z",
     "shell.execute_reply": "2025-05-10T04:16:40.824297Z"
    },
    "papermill": {
     "duration": 0.018184,
     "end_time": "2025-05-10T04:16:40.826002",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.807818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6786aa",
   "metadata": {
    "papermill": {
     "duration": 0.011396,
     "end_time": "2025-05-10T04:16:40.848991",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.837595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6a4562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.873192Z",
     "iopub.status.busy": "2025-05-10T04:16:40.872980Z",
     "iopub.status.idle": "2025-05-10T04:16:40.894002Z",
     "shell.execute_reply": "2025-05-10T04:16:40.893384Z"
    },
    "papermill": {
     "duration": 0.0345,
     "end_time": "2025-05-10T04:16:40.895201",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.860701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = HateSpeechDataset(X_pool, np.zeros((len(X_pool), 12)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = HateSpeechDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "    \n",
    "    accelerator.wait_for_everyone()    \n",
    "    if accelerator.is_local_main_process:\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'HS': [y_train[i][0] for i in temp],\n",
    "                'Abusive': [y_train[i][1] for i in temp],\n",
    "                'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                'HS_Group': [y_train[i][3] for i in temp],\n",
    "                'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                'HS_Race': [y_train[i][5] for i in temp],\n",
    "                'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                'HS_Other': [y_train[i][8] for i in temp],\n",
    "                'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                'HS_Strong': [y_train[i][11] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'HS': [y_train[i][0] for i in temp],\n",
    "                    'Abusive': [y_train[i][1] for i in temp],\n",
    "                    'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                    'HS_Group': [y_train[i][3] for i in temp],\n",
    "                    'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                    'HS_Race': [y_train[i][5] for i in temp],\n",
    "                    'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                    'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                    'HS_Other': [y_train[i][8] for i in temp],\n",
    "                    'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                    'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                    'HS_Strong': [y_train[i][11] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f2dc4",
   "metadata": {
    "papermill": {
     "duration": 0.011314,
     "end_time": "2025-05-10T04:16:40.918583",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.907269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46f0080b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:40.943152Z",
     "iopub.status.busy": "2025-05-10T04:16:40.942952Z",
     "iopub.status.idle": "2025-05-10T04:16:40.953117Z",
     "shell.execute_reply": "2025-05-10T04:16:40.952493Z"
    },
    "papermill": {
     "duration": 0.023889,
     "end_time": "2025-05-10T04:16:40.954238",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.930349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "\n",
    "        nearest_cp = current_train_size\n",
    "        if nearest_cp not in checkpoints:\n",
    "            for cp in checkpoints:\n",
    "                if cp > current_train_size:\n",
    "                    nearest_cp = cp\n",
    "                    break\n",
    "        percentage = math.ceil(nearest_cp / total_data * 100)\n",
    "        \n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}-{percentage}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(model_accuracies), 4)}, F1 Micro: {round(np.mean(model_f1_micros), 4)}, F1 Macro: {round(np.mean(model_f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda4c5d",
   "metadata": {
    "papermill": {
     "duration": 0.011318,
     "end_time": "2025-05-10T04:16:40.976897",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.965579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RUN THE LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcc10adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:16:41.001064Z",
     "iopub.status.busy": "2025-05-10T04:16:41.000830Z",
     "iopub.status.idle": "2025-05-10T08:20:04.333374Z",
     "shell.execute_reply": "2025-05-10T08:20:04.332584Z"
    },
    "papermill": {
     "duration": 14603.34649,
     "end_time": "2025-05-10T08:20:04.334875",
     "exception": false,
     "start_time": "2025-05-10T04:16:40.988385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6193, Accuracy: 0.8179, F1 Micro: 0.1675, F1 Macro: 0.042\n",
      "Epoch 2/10, Train Loss: 0.4754, Accuracy: 0.8236, F1 Micro: 0.0569, F1 Macro: 0.0213\n",
      "Epoch 3/10, Train Loss: 0.4294, Accuracy: 0.8278, F1 Micro: 0.0973, F1 Macro: 0.0345\n",
      "Epoch 4/10, Train Loss: 0.4087, Accuracy: 0.8308, F1 Micro: 0.1389, F1 Macro: 0.0464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3988, Accuracy: 0.8447, F1 Micro: 0.311, F1 Macro: 0.1026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3953, Accuracy: 0.8496, F1 Micro: 0.3724, F1 Macro: 0.1355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3624, Accuracy: 0.8544, F1 Micro: 0.4227, F1 Macro: 0.1763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3528, Accuracy: 0.8635, F1 Micro: 0.4792, F1 Macro: 0.2524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3212, Accuracy: 0.872, F1 Micro: 0.5442, F1 Macro: 0.3354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.295, Accuracy: 0.877, F1 Micro: 0.5653, F1 Macro: 0.3907\n",
      "Model 1 - Iteration 708: Accuracy: 0.877, F1 Micro: 0.5653, F1 Macro: 0.3907\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.75      0.78      1245\n",
      "      Abusive       0.86      0.73      0.79      1228\n",
      "HS_Individual       0.62      0.26      0.37       591\n",
      "     HS_Group       0.71      0.50      0.59       654\n",
      "  HS_Religion       0.69      0.11      0.19       302\n",
      "      HS_Race       0.86      0.41      0.55       276\n",
      "  HS_Physical       0.57      0.03      0.06       262\n",
      "    HS_Gender       0.50      0.02      0.04       253\n",
      "     HS_Other       0.90      0.07      0.12       289\n",
      "      HS_Weak       0.60      0.26      0.36       478\n",
      "  HS_Moderate       0.56      0.24      0.33       530\n",
      "    HS_Strong       0.73      0.38      0.50       237\n",
      "\n",
      "    micro avg       0.77      0.45      0.57      6345\n",
      "    macro avg       0.70      0.31      0.39      6345\n",
      " weighted avg       0.73      0.45      0.52      6345\n",
      "  samples avg       0.41      0.28      0.31      6345\n",
      "\n",
      "Training completed in 51.9919548034668 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6334, Accuracy: 0.816, F1 Micro: 0.2121, F1 Macro: 0.0538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4873, Accuracy: 0.8345, F1 Micro: 0.3019, F1 Macro: 0.0931\n",
      "Epoch 3/10, Train Loss: 0.4333, Accuracy: 0.8367, F1 Micro: 0.2255, F1 Macro: 0.0747\n",
      "Epoch 4/10, Train Loss: 0.4097, Accuracy: 0.8386, F1 Micro: 0.235, F1 Macro: 0.0813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3991, Accuracy: 0.845, F1 Micro: 0.3172, F1 Macro: 0.1022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3952, Accuracy: 0.8469, F1 Micro: 0.3746, F1 Macro: 0.1168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3622, Accuracy: 0.8547, F1 Micro: 0.4134, F1 Macro: 0.1499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3503, Accuracy: 0.8612, F1 Micro: 0.448, F1 Macro: 0.2086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3256, Accuracy: 0.8692, F1 Micro: 0.5328, F1 Macro: 0.3081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2976, Accuracy: 0.8742, F1 Micro: 0.5595, F1 Macro: 0.3598\n",
      "Model 2 - Iteration 708: Accuracy: 0.8742, F1 Micro: 0.5595, F1 Macro: 0.3598\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.77      0.78      1245\n",
      "      Abusive       0.83      0.77      0.80      1228\n",
      "HS_Individual       0.58      0.29      0.39       591\n",
      "     HS_Group       0.65      0.53      0.59       654\n",
      "  HS_Religion       0.64      0.03      0.06       302\n",
      "      HS_Race       0.92      0.44      0.60       276\n",
      "  HS_Physical       0.00      0.00      0.00       262\n",
      "    HS_Gender       0.00      0.00      0.00       253\n",
      "     HS_Other       0.50      0.06      0.11       289\n",
      "      HS_Weak       0.68      0.14      0.23       478\n",
      "  HS_Moderate       0.55      0.20      0.30       530\n",
      "    HS_Strong       0.74      0.35      0.48       237\n",
      "\n",
      "    micro avg       0.75      0.44      0.56      6345\n",
      "    macro avg       0.58      0.30      0.36      6345\n",
      " weighted avg       0.66      0.44      0.50      6345\n",
      "  samples avg       0.43      0.29      0.32      6345\n",
      "\n",
      "Training completed in 53.89190173149109 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6503, Accuracy: 0.8023, F1 Micro: 0.3936, F1 Macro: 0.1097\n",
      "Epoch 2/10, Train Loss: 0.4856, Accuracy: 0.8299, F1 Micro: 0.1514, F1 Macro: 0.0483\n",
      "Epoch 3/10, Train Loss: 0.4273, Accuracy: 0.8314, F1 Micro: 0.1528, F1 Macro: 0.0539\n",
      "Epoch 4/10, Train Loss: 0.4026, Accuracy: 0.8316, F1 Micro: 0.156, F1 Macro: 0.0507\n",
      "Epoch 5/10, Train Loss: 0.3918, Accuracy: 0.8477, F1 Micro: 0.3471, F1 Macro: 0.1212\n",
      "Epoch 6/10, Train Loss: 0.3907, Accuracy: 0.8489, F1 Micro: 0.3857, F1 Macro: 0.1423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3598, Accuracy: 0.8528, F1 Micro: 0.4163, F1 Macro: 0.1829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.348, Accuracy: 0.8582, F1 Micro: 0.4491, F1 Macro: 0.2294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3243, Accuracy: 0.8669, F1 Micro: 0.5173, F1 Macro: 0.2945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3038, Accuracy: 0.8721, F1 Micro: 0.5301, F1 Macro: 0.3182\n",
      "Model 3 - Iteration 708: Accuracy: 0.8721, F1 Micro: 0.5301, F1 Macro: 0.3182\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.72      0.76      1245\n",
      "      Abusive       0.81      0.78      0.80      1228\n",
      "HS_Individual       0.65      0.19      0.29       591\n",
      "     HS_Group       0.69      0.43      0.53       654\n",
      "  HS_Religion       0.65      0.07      0.12       302\n",
      "      HS_Race       0.88      0.38      0.54       276\n",
      "  HS_Physical       0.00      0.00      0.00       262\n",
      "    HS_Gender       0.00      0.00      0.00       253\n",
      "     HS_Other       1.00      0.03      0.07       289\n",
      "      HS_Weak       0.82      0.06      0.11       478\n",
      "  HS_Moderate       0.61      0.14      0.22       530\n",
      "    HS_Strong       0.72      0.26      0.38       237\n",
      "\n",
      "    micro avg       0.78      0.40      0.53      6345\n",
      "    macro avg       0.64      0.26      0.32      6345\n",
      " weighted avg       0.70      0.40      0.46      6345\n",
      "  samples avg       0.46      0.28      0.32      6345\n",
      "\n",
      "Training completed in 49.42466711997986 s\n",
      "Averaged - Iteration 708: Accuracy: 0.8744, F1 Micro: 0.5516, F1 Macro: 0.3562\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 1063\n",
      "Sampling duration: 143.98849534988403 seconds\n",
      "New train size: 1771\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5327, Accuracy: 0.8288, F1 Micro: 0.1122, F1 Macro: 0.0433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4301, Accuracy: 0.8474, F1 Micro: 0.3441, F1 Macro: 0.1084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3896, Accuracy: 0.8573, F1 Micro: 0.4306, F1 Macro: 0.1917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.352, Accuracy: 0.8691, F1 Micro: 0.4904, F1 Macro: 0.2888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3032, Accuracy: 0.8861, F1 Micro: 0.6229, F1 Macro: 0.4702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2716, Accuracy: 0.8923, F1 Micro: 0.6393, F1 Macro: 0.5321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2369, Accuracy: 0.8987, F1 Micro: 0.6844, F1 Macro: 0.5909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2093, Accuracy: 0.9032, F1 Micro: 0.7034, F1 Macro: 0.6217\n",
      "Epoch 9/10, Train Loss: 0.1831, Accuracy: 0.9052, F1 Micro: 0.6937, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1667, Accuracy: 0.9096, F1 Micro: 0.7266, F1 Macro: 0.6527\n",
      "Model 1 - Iteration 1771: Accuracy: 0.9096, F1 Micro: 0.7266, F1 Macro: 0.6527\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.81      0.83      1245\n",
      "      Abusive       0.86      0.86      0.86      1228\n",
      "HS_Individual       0.72      0.61      0.66       591\n",
      "     HS_Group       0.80      0.70      0.75       654\n",
      "  HS_Religion       0.74      0.57      0.64       302\n",
      "      HS_Race       0.90      0.69      0.78       276\n",
      "  HS_Physical       0.66      0.17      0.27       262\n",
      "    HS_Gender       0.70      0.55      0.62       253\n",
      "     HS_Other       0.83      0.37      0.51       289\n",
      "      HS_Weak       0.68      0.58      0.62       478\n",
      "  HS_Moderate       0.70      0.56      0.62       530\n",
      "    HS_Strong       0.83      0.57      0.68       237\n",
      "\n",
      "    micro avg       0.80      0.67      0.73      6345\n",
      "    macro avg       0.77      0.59      0.65      6345\n",
      " weighted avg       0.79      0.67      0.71      6345\n",
      "  samples avg       0.44      0.40      0.40      6345\n",
      "\n",
      "Training completed in 72.72362756729126 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5462, Accuracy: 0.8386, F1 Micro: 0.3369, F1 Macro: 0.1022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4313, Accuracy: 0.845, F1 Micro: 0.3684, F1 Macro: 0.1105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3925, Accuracy: 0.8556, F1 Micro: 0.426, F1 Macro: 0.1599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3586, Accuracy: 0.8693, F1 Micro: 0.5093, F1 Macro: 0.2792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3067, Accuracy: 0.8779, F1 Micro: 0.5459, F1 Macro: 0.3766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2754, Accuracy: 0.8925, F1 Micro: 0.6676, F1 Macro: 0.5573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2442, Accuracy: 0.8987, F1 Micro: 0.676, F1 Macro: 0.5719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2126, Accuracy: 0.9015, F1 Micro: 0.6814, F1 Macro: 0.5896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1869, Accuracy: 0.9061, F1 Micro: 0.7114, F1 Macro: 0.6249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1728, Accuracy: 0.9088, F1 Micro: 0.7168, F1 Macro: 0.6308\n",
      "Model 2 - Iteration 1771: Accuracy: 0.9088, F1 Micro: 0.7168, F1 Macro: 0.6308\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.80      0.82      1245\n",
      "      Abusive       0.88      0.82      0.85      1228\n",
      "HS_Individual       0.72      0.60      0.66       591\n",
      "     HS_Group       0.79      0.69      0.74       654\n",
      "  HS_Religion       0.82      0.43      0.56       302\n",
      "      HS_Race       0.89      0.71      0.79       276\n",
      "  HS_Physical       0.67      0.05      0.10       262\n",
      "    HS_Gender       0.73      0.53      0.62       253\n",
      "     HS_Other       0.81      0.37      0.51       289\n",
      "      HS_Weak       0.72      0.56      0.63       478\n",
      "  HS_Moderate       0.73      0.50      0.60       530\n",
      "    HS_Strong       0.78      0.64      0.70       237\n",
      "\n",
      "    micro avg       0.81      0.64      0.72      6345\n",
      "    macro avg       0.78      0.56      0.63      6345\n",
      " weighted avg       0.80      0.64      0.70      6345\n",
      "  samples avg       0.44      0.39      0.39      6345\n",
      "\n",
      "Training completed in 74.73013544082642 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5495, Accuracy: 0.8278, F1 Micro: 0.1141, F1 Macro: 0.039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4272, Accuracy: 0.8464, F1 Micro: 0.341, F1 Macro: 0.1077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3865, Accuracy: 0.8538, F1 Micro: 0.4104, F1 Macro: 0.1699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3573, Accuracy: 0.8692, F1 Micro: 0.5078, F1 Macro: 0.294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3062, Accuracy: 0.877, F1 Micro: 0.5472, F1 Macro: 0.3985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.277, Accuracy: 0.8907, F1 Micro: 0.6625, F1 Macro: 0.5482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2457, Accuracy: 0.8963, F1 Micro: 0.6665, F1 Macro: 0.5594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2171, Accuracy: 0.8985, F1 Micro: 0.6713, F1 Macro: 0.575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1891, Accuracy: 0.9027, F1 Micro: 0.6928, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.176, Accuracy: 0.9055, F1 Micro: 0.6965, F1 Macro: 0.6121\n",
      "Model 3 - Iteration 1771: Accuracy: 0.9055, F1 Micro: 0.6965, F1 Macro: 0.6121\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.74      0.80      1245\n",
      "      Abusive       0.89      0.76      0.82      1228\n",
      "HS_Individual       0.77      0.52      0.62       591\n",
      "     HS_Group       0.79      0.68      0.73       654\n",
      "  HS_Religion       0.79      0.50      0.61       302\n",
      "      HS_Race       0.88      0.72      0.80       276\n",
      "  HS_Physical       0.71      0.02      0.04       262\n",
      "    HS_Gender       0.72      0.45      0.55       253\n",
      "     HS_Other       0.79      0.36      0.49       289\n",
      "      HS_Weak       0.76      0.49      0.59       478\n",
      "  HS_Moderate       0.71      0.51      0.59       530\n",
      "    HS_Strong       0.78      0.63      0.70       237\n",
      "\n",
      "    micro avg       0.82      0.60      0.70      6345\n",
      "    macro avg       0.79      0.53      0.61      6345\n",
      " weighted avg       0.81      0.60      0.68      6345\n",
      "  samples avg       0.42      0.36      0.37      6345\n",
      "\n",
      "Training completed in 74.15161609649658 s\n",
      "Averaged - Iteration 1771: Accuracy: 0.908, F1 Micro: 0.7133, F1 Macro: 0.6319\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 957\n",
      "Sampling duration: 129.50106620788574 seconds\n",
      "New train size: 2728\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5005, Accuracy: 0.8365, F1 Micro: 0.2135, F1 Macro: 0.0705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3986, Accuracy: 0.8567, F1 Micro: 0.4185, F1 Macro: 0.167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3364, Accuracy: 0.8781, F1 Micro: 0.5725, F1 Macro: 0.3734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2759, Accuracy: 0.8919, F1 Micro: 0.637, F1 Macro: 0.5207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.232, Accuracy: 0.9032, F1 Micro: 0.6889, F1 Macro: 0.6089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2051, Accuracy: 0.907, F1 Micro: 0.7015, F1 Macro: 0.616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.177, Accuracy: 0.9139, F1 Micro: 0.7462, F1 Macro: 0.6842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1525, Accuracy: 0.9163, F1 Micro: 0.7502, F1 Macro: 0.6929\n",
      "Epoch 9/10, Train Loss: 0.1302, Accuracy: 0.9179, F1 Micro: 0.7449, F1 Macro: 0.682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1147, Accuracy: 0.92, F1 Micro: 0.7592, F1 Macro: 0.7151\n",
      "Model 1 - Iteration 2728: Accuracy: 0.92, F1 Micro: 0.7592, F1 Macro: 0.7151\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.82      0.84      1245\n",
      "      Abusive       0.90      0.82      0.86      1228\n",
      "HS_Individual       0.76      0.68      0.71       591\n",
      "     HS_Group       0.83      0.72      0.77       654\n",
      "  HS_Religion       0.80      0.60      0.68       302\n",
      "      HS_Race       0.90      0.74      0.81       276\n",
      "  HS_Physical       0.85      0.43      0.57       262\n",
      "    HS_Gender       0.77      0.61      0.68       253\n",
      "     HS_Other       0.73      0.54      0.62       289\n",
      "      HS_Weak       0.71      0.62      0.66       478\n",
      "  HS_Moderate       0.74      0.59      0.66       530\n",
      "    HS_Strong       0.84      0.62      0.71       237\n",
      "\n",
      "    micro avg       0.83      0.70      0.76      6345\n",
      "    macro avg       0.81      0.65      0.72      6345\n",
      " weighted avg       0.82      0.70      0.76      6345\n",
      "  samples avg       0.44      0.41      0.41      6345\n",
      "\n",
      "Training completed in 88.46130180358887 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5077, Accuracy: 0.8331, F1 Micro: 0.1651, F1 Macro: 0.0569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3997, Accuracy: 0.8532, F1 Micro: 0.3837, F1 Macro: 0.1317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3441, Accuracy: 0.8773, F1 Micro: 0.5858, F1 Macro: 0.3691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2817, Accuracy: 0.8919, F1 Micro: 0.6275, F1 Macro: 0.4993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.239, Accuracy: 0.9031, F1 Micro: 0.6909, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2137, Accuracy: 0.9104, F1 Micro: 0.724, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1815, Accuracy: 0.9123, F1 Micro: 0.7388, F1 Macro: 0.6726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1579, Accuracy: 0.9166, F1 Micro: 0.7492, F1 Macro: 0.6791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1349, Accuracy: 0.9173, F1 Micro: 0.7582, F1 Macro: 0.6935\n",
      "Epoch 10/10, Train Loss: 0.1205, Accuracy: 0.9194, F1 Micro: 0.7522, F1 Macro: 0.6941\n",
      "Model 2 - Iteration 2728: Accuracy: 0.9173, F1 Micro: 0.7582, F1 Macro: 0.6935\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1245\n",
      "      Abusive       0.88      0.86      0.87      1228\n",
      "HS_Individual       0.73      0.68      0.70       591\n",
      "     HS_Group       0.78      0.78      0.78       654\n",
      "  HS_Religion       0.73      0.62      0.67       302\n",
      "      HS_Race       0.86      0.77      0.81       276\n",
      "  HS_Physical       0.89      0.16      0.27       262\n",
      "    HS_Gender       0.83      0.60      0.69       253\n",
      "     HS_Other       0.71      0.52      0.60       289\n",
      "      HS_Weak       0.71      0.62      0.66       478\n",
      "  HS_Moderate       0.70      0.70      0.70       530\n",
      "    HS_Strong       0.83      0.66      0.73       237\n",
      "\n",
      "    micro avg       0.80      0.72      0.76      6345\n",
      "    macro avg       0.79      0.65      0.69      6345\n",
      " weighted avg       0.80      0.72      0.75      6345\n",
      "  samples avg       0.46      0.43      0.43      6345\n",
      "\n",
      "Training completed in 88.27979731559753 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5098, Accuracy: 0.8348, F1 Micro: 0.1996, F1 Macro: 0.0688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3937, Accuracy: 0.8535, F1 Micro: 0.4151, F1 Macro: 0.1583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.339, Accuracy: 0.878, F1 Micro: 0.5792, F1 Macro: 0.3882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2819, Accuracy: 0.8895, F1 Micro: 0.6181, F1 Macro: 0.4966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2378, Accuracy: 0.9011, F1 Micro: 0.6905, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2133, Accuracy: 0.9057, F1 Micro: 0.6983, F1 Macro: 0.6097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1856, Accuracy: 0.913, F1 Micro: 0.7407, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1621, Accuracy: 0.9154, F1 Micro: 0.7483, F1 Macro: 0.6751\n",
      "Epoch 9/10, Train Loss: 0.14, Accuracy: 0.9177, F1 Micro: 0.748, F1 Macro: 0.6757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1206, Accuracy: 0.9208, F1 Micro: 0.7637, F1 Macro: 0.7083\n",
      "Model 3 - Iteration 2728: Accuracy: 0.9208, F1 Micro: 0.7637, F1 Macro: 0.7083\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1245\n",
      "      Abusive       0.89      0.84      0.87      1228\n",
      "HS_Individual       0.73      0.70      0.72       591\n",
      "     HS_Group       0.84      0.73      0.78       654\n",
      "  HS_Religion       0.80      0.66      0.72       302\n",
      "      HS_Race       0.89      0.75      0.82       276\n",
      "  HS_Physical       0.89      0.25      0.39       262\n",
      "    HS_Gender       0.73      0.64      0.68       253\n",
      "     HS_Other       0.81      0.49      0.61       289\n",
      "      HS_Weak       0.69      0.65      0.67       478\n",
      "  HS_Moderate       0.75      0.62      0.68       530\n",
      "    HS_Strong       0.84      0.62      0.72       237\n",
      "\n",
      "    micro avg       0.82      0.71      0.76      6345\n",
      "    macro avg       0.81      0.65      0.71      6345\n",
      " weighted avg       0.82      0.71      0.76      6345\n",
      "  samples avg       0.45      0.42      0.42      6345\n",
      "\n",
      "Training completed in 88.14498448371887 s\n",
      "Averaged - Iteration 2728: Accuracy: 0.9193, F1 Micro: 0.7604, F1 Macro: 0.7056\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 861\n",
      "Sampling duration: 117.16611099243164 seconds\n",
      "New train size: 3589\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4801, Accuracy: 0.8393, F1 Micro: 0.2358, F1 Macro: 0.0802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3686, Accuracy: 0.876, F1 Micro: 0.5801, F1 Macro: 0.3609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2983, Accuracy: 0.8945, F1 Micro: 0.671, F1 Macro: 0.5722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2503, Accuracy: 0.9059, F1 Micro: 0.715, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.206, Accuracy: 0.9111, F1 Micro: 0.7449, F1 Macro: 0.6756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1767, Accuracy: 0.9143, F1 Micro: 0.7489, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1524, Accuracy: 0.9189, F1 Micro: 0.7579, F1 Macro: 0.701\n",
      "Epoch 8/10, Train Loss: 0.1355, Accuracy: 0.9211, F1 Micro: 0.7558, F1 Macro: 0.7064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1159, Accuracy: 0.9228, F1 Micro: 0.7707, F1 Macro: 0.7277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0993, Accuracy: 0.925, F1 Micro: 0.7755, F1 Macro: 0.7371\n",
      "Model 1 - Iteration 3589: Accuracy: 0.925, F1 Micro: 0.7755, F1 Macro: 0.7371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.81      0.84      1245\n",
      "      Abusive       0.90      0.85      0.87      1228\n",
      "HS_Individual       0.81      0.62      0.70       591\n",
      "     HS_Group       0.81      0.78      0.79       654\n",
      "  HS_Religion       0.79      0.68      0.73       302\n",
      "      HS_Race       0.89      0.79      0.84       276\n",
      "  HS_Physical       0.82      0.48      0.60       262\n",
      "    HS_Gender       0.92      0.56      0.69       253\n",
      "     HS_Other       0.76      0.57      0.65       289\n",
      "      HS_Weak       0.77      0.58      0.66       478\n",
      "  HS_Moderate       0.73      0.68      0.70       530\n",
      "    HS_Strong       0.85      0.69      0.76       237\n",
      "\n",
      "    micro avg       0.84      0.72      0.78      6345\n",
      "    macro avg       0.83      0.67      0.74      6345\n",
      " weighted avg       0.84      0.72      0.77      6345\n",
      "  samples avg       0.46      0.43      0.42      6345\n",
      "\n",
      "Training completed in 100.91645383834839 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.488, Accuracy: 0.8451, F1 Micro: 0.3142, F1 Macro: 0.1018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3741, Accuracy: 0.8696, F1 Micro: 0.5574, F1 Macro: 0.3222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3028, Accuracy: 0.8933, F1 Micro: 0.6572, F1 Macro: 0.5421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2508, Accuracy: 0.9059, F1 Micro: 0.7019, F1 Macro: 0.6132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2111, Accuracy: 0.9127, F1 Micro: 0.7464, F1 Macro: 0.6759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1765, Accuracy: 0.917, F1 Micro: 0.7577, F1 Macro: 0.7005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1547, Accuracy: 0.9191, F1 Micro: 0.7588, F1 Macro: 0.7047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1378, Accuracy: 0.9213, F1 Micro: 0.764, F1 Macro: 0.7164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1164, Accuracy: 0.9224, F1 Micro: 0.7737, F1 Macro: 0.7279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1014, Accuracy: 0.9251, F1 Micro: 0.7823, F1 Macro: 0.7413\n",
      "Model 2 - Iteration 3589: Accuracy: 0.9251, F1 Micro: 0.7823, F1 Macro: 0.7413\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1245\n",
      "      Abusive       0.88      0.88      0.88      1228\n",
      "HS_Individual       0.73      0.72      0.72       591\n",
      "     HS_Group       0.83      0.74      0.79       654\n",
      "  HS_Religion       0.80      0.64      0.71       302\n",
      "      HS_Race       0.88      0.78      0.82       276\n",
      "  HS_Physical       0.85      0.43      0.57       262\n",
      "    HS_Gender       0.79      0.70      0.74       253\n",
      "     HS_Other       0.73      0.56      0.63       289\n",
      "      HS_Weak       0.69      0.68      0.68       478\n",
      "  HS_Moderate       0.77      0.66      0.71       530\n",
      "    HS_Strong       0.88      0.68      0.77       237\n",
      "\n",
      "    micro avg       0.82      0.75      0.78      6345\n",
      "    macro avg       0.81      0.69      0.74      6345\n",
      " weighted avg       0.82      0.75      0.78      6345\n",
      "  samples avg       0.46      0.44      0.44      6345\n",
      "\n",
      "Training completed in 102.34291577339172 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4881, Accuracy: 0.8368, F1 Micro: 0.2213, F1 Macro: 0.0708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3709, Accuracy: 0.8693, F1 Micro: 0.5328, F1 Macro: 0.2858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3021, Accuracy: 0.8921, F1 Micro: 0.656, F1 Macro: 0.5468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2519, Accuracy: 0.9023, F1 Micro: 0.6882, F1 Macro: 0.5924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2097, Accuracy: 0.9107, F1 Micro: 0.7404, F1 Macro: 0.6621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1803, Accuracy: 0.9162, F1 Micro: 0.7459, F1 Macro: 0.6756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1541, Accuracy: 0.9177, F1 Micro: 0.762, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1413, Accuracy: 0.9215, F1 Micro: 0.7681, F1 Macro: 0.7189\n",
      "Epoch 9/10, Train Loss: 0.1166, Accuracy: 0.922, F1 Micro: 0.7678, F1 Macro: 0.718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1018, Accuracy: 0.9251, F1 Micro: 0.7781, F1 Macro: 0.7373\n",
      "Model 3 - Iteration 3589: Accuracy: 0.9251, F1 Micro: 0.7781, F1 Macro: 0.7373\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.85      1245\n",
      "      Abusive       0.90      0.84      0.87      1228\n",
      "HS_Individual       0.79      0.63      0.70       591\n",
      "     HS_Group       0.80      0.79      0.80       654\n",
      "  HS_Religion       0.76      0.75      0.76       302\n",
      "      HS_Race       0.89      0.83      0.86       276\n",
      "  HS_Physical       0.89      0.37      0.52       262\n",
      "    HS_Gender       0.86      0.57      0.69       253\n",
      "     HS_Other       0.71      0.59      0.64       289\n",
      "      HS_Weak       0.78      0.58      0.66       478\n",
      "  HS_Moderate       0.73      0.71      0.72       530\n",
      "    HS_Strong       0.85      0.72      0.78       237\n",
      "\n",
      "    micro avg       0.83      0.73      0.78      6345\n",
      "    macro avg       0.82      0.68      0.74      6345\n",
      " weighted avg       0.83      0.73      0.77      6345\n",
      "  samples avg       0.46      0.43      0.43      6345\n",
      "\n",
      "Training completed in 101.48429083824158 s\n",
      "Averaged - Iteration 3589: Accuracy: 0.9251, F1 Micro: 0.7786, F1 Macro: 0.7386\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 775\n",
      "Sampling duration: 104.61092686653137 seconds\n",
      "New train size: 4364\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4635, Accuracy: 0.8478, F1 Micro: 0.36, F1 Macro: 0.1135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3526, Accuracy: 0.8803, F1 Micro: 0.5804, F1 Macro: 0.4041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2707, Accuracy: 0.902, F1 Micro: 0.7039, F1 Macro: 0.6207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2251, Accuracy: 0.9106, F1 Micro: 0.7241, F1 Macro: 0.6501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1907, Accuracy: 0.9156, F1 Micro: 0.7429, F1 Macro: 0.6774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1589, Accuracy: 0.9225, F1 Micro: 0.7705, F1 Macro: 0.7099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1367, Accuracy: 0.924, F1 Micro: 0.7721, F1 Macro: 0.7241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1191, Accuracy: 0.923, F1 Micro: 0.7831, F1 Macro: 0.742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1019, Accuracy: 0.9261, F1 Micro: 0.7854, F1 Macro: 0.7485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0888, Accuracy: 0.9261, F1 Micro: 0.7861, F1 Macro: 0.7492\n",
      "Model 1 - Iteration 4364: Accuracy: 0.9261, F1 Micro: 0.7861, F1 Macro: 0.7492\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1245\n",
      "      Abusive       0.87      0.90      0.89      1228\n",
      "HS_Individual       0.76      0.66      0.71       591\n",
      "     HS_Group       0.81      0.78      0.80       654\n",
      "  HS_Religion       0.81      0.71      0.76       302\n",
      "      HS_Race       0.86      0.84      0.85       276\n",
      "  HS_Physical       0.86      0.46      0.60       262\n",
      "    HS_Gender       0.87      0.65      0.74       253\n",
      "     HS_Other       0.74      0.59      0.65       289\n",
      "      HS_Weak       0.68      0.63      0.66       478\n",
      "  HS_Moderate       0.73      0.69      0.71       530\n",
      "    HS_Strong       0.86      0.70      0.77       237\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      6345\n",
      "    macro avg       0.81      0.71      0.75      6345\n",
      " weighted avg       0.82      0.76      0.78      6345\n",
      "  samples avg       0.47      0.45      0.44      6345\n",
      "\n",
      "Training completed in 114.75325632095337 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4711, Accuracy: 0.8482, F1 Micro: 0.3698, F1 Macro: 0.1126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3579, Accuracy: 0.8764, F1 Micro: 0.5423, F1 Macro: 0.3394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.275, Accuracy: 0.9014, F1 Micro: 0.6874, F1 Macro: 0.586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2324, Accuracy: 0.9121, F1 Micro: 0.7276, F1 Macro: 0.6472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1953, Accuracy: 0.9169, F1 Micro: 0.7528, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1652, Accuracy: 0.9211, F1 Micro: 0.7691, F1 Macro: 0.705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1411, Accuracy: 0.9224, F1 Micro: 0.7802, F1 Macro: 0.7346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1234, Accuracy: 0.9236, F1 Micro: 0.7818, F1 Macro: 0.7359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1048, Accuracy: 0.9264, F1 Micro: 0.7858, F1 Macro: 0.7464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0945, Accuracy: 0.9251, F1 Micro: 0.7859, F1 Macro: 0.7507\n",
      "Model 2 - Iteration 4364: Accuracy: 0.9251, F1 Micro: 0.7859, F1 Macro: 0.7507\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1245\n",
      "      Abusive       0.88      0.89      0.89      1228\n",
      "HS_Individual       0.77      0.67      0.72       591\n",
      "     HS_Group       0.77      0.81      0.79       654\n",
      "  HS_Religion       0.77      0.72      0.74       302\n",
      "      HS_Race       0.84      0.86      0.85       276\n",
      "  HS_Physical       0.86      0.47      0.61       262\n",
      "    HS_Gender       0.87      0.62      0.72       253\n",
      "     HS_Other       0.69      0.62      0.65       289\n",
      "      HS_Weak       0.72      0.61      0.66       478\n",
      "  HS_Moderate       0.69      0.75      0.72       530\n",
      "    HS_Strong       0.86      0.75      0.80       237\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      6345\n",
      "    macro avg       0.80      0.72      0.75      6345\n",
      " weighted avg       0.81      0.77      0.78      6345\n",
      "  samples avg       0.47      0.45      0.44      6345\n",
      "\n",
      "Training completed in 114.19336295127869 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4682, Accuracy: 0.8468, F1 Micro: 0.3556, F1 Macro: 0.1144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3563, Accuracy: 0.8784, F1 Micro: 0.559, F1 Macro: 0.3662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.8992, F1 Micro: 0.6753, F1 Macro: 0.5685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2325, Accuracy: 0.9092, F1 Micro: 0.7173, F1 Macro: 0.6396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1955, Accuracy: 0.9148, F1 Micro: 0.7386, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1644, Accuracy: 0.9219, F1 Micro: 0.7684, F1 Macro: 0.7055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1414, Accuracy: 0.9209, F1 Micro: 0.7751, F1 Macro: 0.7254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1261, Accuracy: 0.9243, F1 Micro: 0.7808, F1 Macro: 0.7341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1082, Accuracy: 0.9243, F1 Micro: 0.7833, F1 Macro: 0.7436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.092, Accuracy: 0.9266, F1 Micro: 0.7885, F1 Macro: 0.7504\n",
      "Model 3 - Iteration 4364: Accuracy: 0.9266, F1 Micro: 0.7885, F1 Macro: 0.7504\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1245\n",
      "      Abusive       0.87      0.89      0.88      1228\n",
      "HS_Individual       0.76      0.67      0.71       591\n",
      "     HS_Group       0.80      0.80      0.80       654\n",
      "  HS_Religion       0.81      0.73      0.76       302\n",
      "      HS_Race       0.85      0.85      0.85       276\n",
      "  HS_Physical       0.85      0.42      0.57       262\n",
      "    HS_Gender       0.82      0.63      0.71       253\n",
      "     HS_Other       0.70      0.61      0.65       289\n",
      "      HS_Weak       0.73      0.63      0.68       478\n",
      "  HS_Moderate       0.73      0.74      0.74       530\n",
      "    HS_Strong       0.83      0.76      0.80       237\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      6345\n",
      "    macro avg       0.80      0.71      0.75      6345\n",
      " weighted avg       0.82      0.76      0.78      6345\n",
      "  samples avg       0.47      0.45      0.44      6345\n",
      "\n",
      "Training completed in 115.77992916107178 s\n",
      "Averaged - Iteration 4364: Accuracy: 0.9259, F1 Micro: 0.7869, F1 Macro: 0.7501\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 698\n",
      "Sampling duration: 94.84754347801208 seconds\n",
      "New train size: 5062\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4634, Accuracy: 0.8533, F1 Micro: 0.432, F1 Macro: 0.1616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3379, Accuracy: 0.887, F1 Micro: 0.6171, F1 Macro: 0.4467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2676, Accuracy: 0.9049, F1 Micro: 0.7182, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2195, Accuracy: 0.9153, F1 Micro: 0.7372, F1 Macro: 0.6713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1766, Accuracy: 0.9208, F1 Micro: 0.7748, F1 Macro: 0.7194\n",
      "Epoch 6/10, Train Loss: 0.1553, Accuracy: 0.9176, F1 Micro: 0.7617, F1 Macro: 0.7144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1273, Accuracy: 0.9272, F1 Micro: 0.7867, F1 Macro: 0.7432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1052, Accuracy: 0.9275, F1 Micro: 0.7919, F1 Macro: 0.7553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0941, Accuracy: 0.9285, F1 Micro: 0.7937, F1 Macro: 0.7618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0831, Accuracy: 0.9318, F1 Micro: 0.8022, F1 Macro: 0.7726\n",
      "Model 1 - Iteration 5062: Accuracy: 0.9318, F1 Micro: 0.8022, F1 Macro: 0.7726\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1245\n",
      "      Abusive       0.90      0.89      0.89      1228\n",
      "HS_Individual       0.77      0.73      0.75       591\n",
      "     HS_Group       0.83      0.76      0.79       654\n",
      "  HS_Religion       0.87      0.67      0.76       302\n",
      "      HS_Race       0.86      0.86      0.86       276\n",
      "  HS_Physical       0.86      0.57      0.69       262\n",
      "    HS_Gender       0.84      0.70      0.77       253\n",
      "     HS_Other       0.72      0.61      0.66       289\n",
      "      HS_Weak       0.73      0.69      0.71       478\n",
      "  HS_Moderate       0.77      0.67      0.72       530\n",
      "    HS_Strong       0.87      0.76      0.81       237\n",
      "\n",
      "    micro avg       0.84      0.77      0.80      6345\n",
      "    macro avg       0.83      0.73      0.77      6345\n",
      " weighted avg       0.84      0.77      0.80      6345\n",
      "  samples avg       0.47      0.45      0.45      6345\n",
      "\n",
      "Training completed in 122.98831558227539 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4686, Accuracy: 0.8488, F1 Micro: 0.3903, F1 Macro: 0.1185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3421, Accuracy: 0.8863, F1 Micro: 0.6051, F1 Macro: 0.417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2724, Accuracy: 0.9049, F1 Micro: 0.7186, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2242, Accuracy: 0.9159, F1 Micro: 0.7412, F1 Macro: 0.679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.182, Accuracy: 0.9225, F1 Micro: 0.7705, F1 Macro: 0.7122\n",
      "Epoch 6/10, Train Loss: 0.1586, Accuracy: 0.9195, F1 Micro: 0.7632, F1 Macro: 0.7152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1322, Accuracy: 0.9293, F1 Micro: 0.7906, F1 Macro: 0.748\n",
      "Epoch 8/10, Train Loss: 0.1118, Accuracy: 0.9261, F1 Micro: 0.7854, F1 Macro: 0.742\n",
      "Epoch 9/10, Train Loss: 0.0998, Accuracy: 0.9241, F1 Micro: 0.7865, F1 Macro: 0.7547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0847, Accuracy: 0.9323, F1 Micro: 0.8054, F1 Macro: 0.7756\n",
      "Model 2 - Iteration 5062: Accuracy: 0.9323, F1 Micro: 0.8054, F1 Macro: 0.7756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1245\n",
      "      Abusive       0.90      0.89      0.89      1228\n",
      "HS_Individual       0.75      0.74      0.75       591\n",
      "     HS_Group       0.85      0.78      0.81       654\n",
      "  HS_Religion       0.87      0.71      0.78       302\n",
      "      HS_Race       0.85      0.85      0.85       276\n",
      "  HS_Physical       0.86      0.53      0.65       262\n",
      "    HS_Gender       0.83      0.72      0.77       253\n",
      "     HS_Other       0.71      0.65      0.68       289\n",
      "      HS_Weak       0.72      0.69      0.70       478\n",
      "  HS_Moderate       0.78      0.69      0.73       530\n",
      "    HS_Strong       0.84      0.79      0.82       237\n",
      "\n",
      "    micro avg       0.83      0.78      0.81      6345\n",
      "    macro avg       0.82      0.74      0.78      6345\n",
      " weighted avg       0.83      0.78      0.80      6345\n",
      "  samples avg       0.48      0.46      0.45      6345\n",
      "\n",
      "Training completed in 120.46234059333801 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4668, Accuracy: 0.8505, F1 Micro: 0.4212, F1 Macro: 0.1636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3411, Accuracy: 0.8838, F1 Micro: 0.5912, F1 Macro: 0.4346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2747, Accuracy: 0.9025, F1 Micro: 0.714, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2261, Accuracy: 0.9156, F1 Micro: 0.7415, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1826, Accuracy: 0.9213, F1 Micro: 0.7647, F1 Macro: 0.6935\n",
      "Epoch 6/10, Train Loss: 0.1592, Accuracy: 0.9179, F1 Micro: 0.7602, F1 Macro: 0.707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1351, Accuracy: 0.929, F1 Micro: 0.789, F1 Macro: 0.7441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1101, Accuracy: 0.9283, F1 Micro: 0.7924, F1 Macro: 0.7502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9303, F1 Micro: 0.7994, F1 Macro: 0.7652\n",
      "Epoch 10/10, Train Loss: 0.0837, Accuracy: 0.9318, F1 Micro: 0.799, F1 Macro: 0.7668\n",
      "Model 3 - Iteration 5062: Accuracy: 0.9303, F1 Micro: 0.7994, F1 Macro: 0.7652\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1245\n",
      "      Abusive       0.90      0.88      0.89      1228\n",
      "HS_Individual       0.72      0.76      0.74       591\n",
      "     HS_Group       0.86      0.76      0.81       654\n",
      "  HS_Religion       0.82      0.75      0.78       302\n",
      "      HS_Race       0.86      0.86      0.86       276\n",
      "  HS_Physical       0.87      0.45      0.59       262\n",
      "    HS_Gender       0.79      0.71      0.75       253\n",
      "     HS_Other       0.74      0.62      0.68       289\n",
      "      HS_Weak       0.67      0.71      0.69       478\n",
      "  HS_Moderate       0.80      0.65      0.71       530\n",
      "    HS_Strong       0.83      0.78      0.81       237\n",
      "\n",
      "    micro avg       0.83      0.77      0.80      6345\n",
      "    macro avg       0.81      0.73      0.77      6345\n",
      " weighted avg       0.83      0.77      0.80      6345\n",
      "  samples avg       0.47      0.45      0.45      6345\n",
      "\n",
      "Training completed in 120.90859317779541 s\n",
      "Averaged - Iteration 5062: Accuracy: 0.9315, F1 Micro: 0.8023, F1 Macro: 0.7711\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 628\n",
      "Sampling duration: 85.42062711715698 seconds\n",
      "New train size: 5690\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4522, Accuracy: 0.854, F1 Micro: 0.4295, F1 Macro: 0.174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3241, Accuracy: 0.8945, F1 Micro: 0.664, F1 Macro: 0.5534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2531, Accuracy: 0.9115, F1 Micro: 0.7231, F1 Macro: 0.6365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2103, Accuracy: 0.9196, F1 Micro: 0.7518, F1 Macro: 0.692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1726, Accuracy: 0.9262, F1 Micro: 0.7802, F1 Macro: 0.7325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1419, Accuracy: 0.9277, F1 Micro: 0.7896, F1 Macro: 0.7491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1231, Accuracy: 0.9301, F1 Micro: 0.7953, F1 Macro: 0.7615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9304, F1 Micro: 0.801, F1 Macro: 0.7711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.934, F1 Micro: 0.8069, F1 Macro: 0.7778\n",
      "Epoch 10/10, Train Loss: 0.0786, Accuracy: 0.9332, F1 Micro: 0.8046, F1 Macro: 0.7797\n",
      "Model 1 - Iteration 5690: Accuracy: 0.934, F1 Micro: 0.8069, F1 Macro: 0.7778\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.85      0.87      1245\n",
      "      Abusive       0.91      0.87      0.89      1228\n",
      "HS_Individual       0.79      0.73      0.76       591\n",
      "     HS_Group       0.84      0.77      0.80       654\n",
      "  HS_Religion       0.84      0.71      0.77       302\n",
      "      HS_Race       0.87      0.86      0.86       276\n",
      "  HS_Physical       0.89      0.54      0.67       262\n",
      "    HS_Gender       0.88      0.70      0.78       253\n",
      "     HS_Other       0.74      0.62      0.67       289\n",
      "      HS_Weak       0.77      0.67      0.72       478\n",
      "  HS_Moderate       0.78      0.66      0.72       530\n",
      "    HS_Strong       0.82      0.81      0.82       237\n",
      "\n",
      "    micro avg       0.85      0.77      0.81      6345\n",
      "    macro avg       0.84      0.73      0.78      6345\n",
      " weighted avg       0.85      0.77      0.80      6345\n",
      "  samples avg       0.48      0.45      0.45      6345\n",
      "\n",
      "Training completed in 131.72384428977966 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4575, Accuracy: 0.8541, F1 Micro: 0.4244, F1 Macro: 0.1473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3273, Accuracy: 0.8939, F1 Micro: 0.6587, F1 Macro: 0.5469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2577, Accuracy: 0.9112, F1 Micro: 0.7186, F1 Macro: 0.6333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2128, Accuracy: 0.9197, F1 Micro: 0.7469, F1 Macro: 0.6784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.178, Accuracy: 0.9259, F1 Micro: 0.7813, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1476, Accuracy: 0.9292, F1 Micro: 0.7897, F1 Macro: 0.7488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1294, Accuracy: 0.9282, F1 Micro: 0.7962, F1 Macro: 0.7591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1105, Accuracy: 0.9305, F1 Micro: 0.8044, F1 Macro: 0.7662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0912, Accuracy: 0.933, F1 Micro: 0.8064, F1 Macro: 0.7749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9333, F1 Micro: 0.8068, F1 Macro: 0.7786\n",
      "Model 2 - Iteration 5690: Accuracy: 0.9333, F1 Micro: 0.8068, F1 Macro: 0.7786\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.86      0.87      1245\n",
      "      Abusive       0.92      0.86      0.89      1228\n",
      "HS_Individual       0.75      0.75      0.75       591\n",
      "     HS_Group       0.86      0.77      0.81       654\n",
      "  HS_Religion       0.83      0.76      0.80       302\n",
      "      HS_Race       0.88      0.81      0.84       276\n",
      "  HS_Physical       0.88      0.52      0.66       262\n",
      "    HS_Gender       0.82      0.76      0.79       253\n",
      "     HS_Other       0.72      0.63      0.67       289\n",
      "      HS_Weak       0.73      0.70      0.71       478\n",
      "  HS_Moderate       0.78      0.71      0.74       530\n",
      "    HS_Strong       0.89      0.76      0.82       237\n",
      "\n",
      "    micro avg       0.84      0.78      0.81      6345\n",
      "    macro avg       0.83      0.74      0.78      6345\n",
      " weighted avg       0.84      0.78      0.81      6345\n",
      "  samples avg       0.47      0.45      0.45      6345\n",
      "\n",
      "Training completed in 132.70999217033386 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4536, Accuracy: 0.8522, F1 Micro: 0.4031, F1 Macro: 0.1618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3267, Accuracy: 0.8918, F1 Micro: 0.6544, F1 Macro: 0.5436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2588, Accuracy: 0.9082, F1 Micro: 0.7098, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2161, Accuracy: 0.9184, F1 Micro: 0.7511, F1 Macro: 0.6818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1801, Accuracy: 0.9252, F1 Micro: 0.7777, F1 Macro: 0.7238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1475, Accuracy: 0.9291, F1 Micro: 0.7877, F1 Macro: 0.7419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1253, Accuracy: 0.9302, F1 Micro: 0.7978, F1 Macro: 0.758\n",
      "Epoch 8/10, Train Loss: 0.1087, Accuracy: 0.9317, F1 Micro: 0.793, F1 Macro: 0.7563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 0.9324, F1 Micro: 0.8028, F1 Macro: 0.7672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0794, Accuracy: 0.9329, F1 Micro: 0.8055, F1 Macro: 0.7776\n",
      "Model 3 - Iteration 5690: Accuracy: 0.9329, F1 Micro: 0.8055, F1 Macro: 0.7776\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.86      0.87      1245\n",
      "      Abusive       0.91      0.87      0.89      1228\n",
      "HS_Individual       0.75      0.76      0.75       591\n",
      "     HS_Group       0.87      0.75      0.80       654\n",
      "  HS_Religion       0.86      0.70      0.77       302\n",
      "      HS_Race       0.90      0.81      0.85       276\n",
      "  HS_Physical       0.88      0.59      0.71       262\n",
      "    HS_Gender       0.81      0.75      0.78       253\n",
      "     HS_Other       0.71      0.64      0.67       289\n",
      "      HS_Weak       0.70      0.71      0.71       478\n",
      "  HS_Moderate       0.79      0.66      0.72       530\n",
      "    HS_Strong       0.86      0.76      0.81       237\n",
      "\n",
      "    micro avg       0.84      0.77      0.81      6345\n",
      "    macro avg       0.83      0.74      0.78      6345\n",
      " weighted avg       0.84      0.77      0.80      6345\n",
      "  samples avg       0.47      0.45      0.45      6345\n",
      "\n",
      "Training completed in 132.46683073043823 s\n",
      "Averaged - Iteration 5690: Accuracy: 0.9334, F1 Micro: 0.8064, F1 Macro: 0.778\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 565\n",
      "Sampling duration: 77.09936785697937 seconds\n",
      "New train size: 6255\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4477, Accuracy: 0.862, F1 Micro: 0.456, F1 Macro: 0.2065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3102, Accuracy: 0.8965, F1 Micro: 0.6701, F1 Macro: 0.567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.913, F1 Micro: 0.7427, F1 Macro: 0.6773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2003, Accuracy: 0.9221, F1 Micro: 0.7615, F1 Macro: 0.706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1689, Accuracy: 0.9269, F1 Micro: 0.7742, F1 Macro: 0.728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.9302, F1 Micro: 0.7964, F1 Macro: 0.7583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1156, Accuracy: 0.9302, F1 Micro: 0.8024, F1 Macro: 0.7717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1004, Accuracy: 0.9337, F1 Micro: 0.8119, F1 Macro: 0.7851\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9361, F1 Micro: 0.8114, F1 Macro: 0.7813\n",
      "Epoch 10/10, Train Loss: 0.0752, Accuracy: 0.9352, F1 Micro: 0.811, F1 Macro: 0.785\n",
      "Model 1 - Iteration 6255: Accuracy: 0.9337, F1 Micro: 0.8119, F1 Macro: 0.7851\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1245\n",
      "      Abusive       0.89      0.91      0.90      1228\n",
      "HS_Individual       0.76      0.75      0.75       591\n",
      "     HS_Group       0.84      0.79      0.81       654\n",
      "  HS_Religion       0.83      0.77      0.80       302\n",
      "      HS_Race       0.86      0.82      0.84       276\n",
      "  HS_Physical       0.86      0.61      0.71       262\n",
      "    HS_Gender       0.84      0.73      0.78       253\n",
      "     HS_Other       0.71      0.65      0.68       289\n",
      "      HS_Weak       0.73      0.72      0.72       478\n",
      "  HS_Moderate       0.76      0.72      0.74       530\n",
      "    HS_Strong       0.87      0.78      0.82       237\n",
      "\n",
      "    micro avg       0.83      0.80      0.81      6345\n",
      "    macro avg       0.82      0.76      0.79      6345\n",
      " weighted avg       0.83      0.80      0.81      6345\n",
      "  samples avg       0.48      0.47      0.46      6345\n",
      "\n",
      "Training completed in 139.68327569961548 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4523, Accuracy: 0.8608, F1 Micro: 0.4535, F1 Macro: 0.1886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3159, Accuracy: 0.8968, F1 Micro: 0.6694, F1 Macro: 0.5568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2464, Accuracy: 0.9129, F1 Micro: 0.7249, F1 Macro: 0.6419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2058, Accuracy: 0.9228, F1 Micro: 0.7661, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.9273, F1 Micro: 0.7798, F1 Macro: 0.731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1432, Accuracy: 0.9287, F1 Micro: 0.7912, F1 Macro: 0.7471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9321, F1 Micro: 0.8024, F1 Macro: 0.768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.9323, F1 Micro: 0.8058, F1 Macro: 0.7752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.9356, F1 Micro: 0.8126, F1 Macro: 0.785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0759, Accuracy: 0.9352, F1 Micro: 0.8157, F1 Macro: 0.7897\n",
      "Model 2 - Iteration 6255: Accuracy: 0.9352, F1 Micro: 0.8157, F1 Macro: 0.7897\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1245\n",
      "      Abusive       0.90      0.91      0.90      1228\n",
      "HS_Individual       0.76      0.77      0.76       591\n",
      "     HS_Group       0.85      0.78      0.81       654\n",
      "  HS_Religion       0.84      0.78      0.81       302\n",
      "      HS_Race       0.87      0.80      0.84       276\n",
      "  HS_Physical       0.85      0.65      0.73       262\n",
      "    HS_Gender       0.85      0.76      0.80       253\n",
      "     HS_Other       0.76      0.63      0.69       289\n",
      "      HS_Weak       0.70      0.71      0.71       478\n",
      "  HS_Moderate       0.78      0.68      0.72       530\n",
      "    HS_Strong       0.85      0.78      0.82       237\n",
      "\n",
      "    micro avg       0.83      0.80      0.82      6345\n",
      "    macro avg       0.82      0.76      0.79      6345\n",
      " weighted avg       0.83      0.80      0.81      6345\n",
      "  samples avg       0.48      0.47      0.46      6345\n",
      "\n",
      "Training completed in 142.2427396774292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4491, Accuracy: 0.8594, F1 Micro: 0.4595, F1 Macro: 0.2069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3151, Accuracy: 0.8968, F1 Micro: 0.6728, F1 Macro: 0.5636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.25, Accuracy: 0.9111, F1 Micro: 0.727, F1 Macro: 0.6519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.9216, F1 Micro: 0.765, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1763, Accuracy: 0.9268, F1 Micro: 0.7743, F1 Macro: 0.72\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1472, Accuracy: 0.9283, F1 Micro: 0.7886, F1 Macro: 0.7446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.9309, F1 Micro: 0.7996, F1 Macro: 0.7642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.9342, F1 Micro: 0.808, F1 Macro: 0.7762\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9331, F1 Micro: 0.8049, F1 Macro: 0.7758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9332, F1 Micro: 0.8083, F1 Macro: 0.78\n",
      "Model 3 - Iteration 6255: Accuracy: 0.9332, F1 Micro: 0.8083, F1 Macro: 0.78\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1245\n",
      "      Abusive       0.89      0.91      0.90      1228\n",
      "HS_Individual       0.74      0.79      0.76       591\n",
      "     HS_Group       0.88      0.75      0.81       654\n",
      "  HS_Religion       0.89      0.67      0.76       302\n",
      "      HS_Race       0.89      0.82      0.85       276\n",
      "  HS_Physical       0.88      0.60      0.72       262\n",
      "    HS_Gender       0.78      0.79      0.79       253\n",
      "     HS_Other       0.72      0.62      0.67       289\n",
      "      HS_Weak       0.69      0.74      0.72       478\n",
      "  HS_Moderate       0.83      0.62      0.71       530\n",
      "    HS_Strong       0.83      0.81      0.82       237\n",
      "\n",
      "    micro avg       0.83      0.78      0.81      6345\n",
      "    macro avg       0.82      0.75      0.78      6345\n",
      " weighted avg       0.84      0.78      0.81      6345\n",
      "  samples avg       0.48      0.46      0.46      6345\n",
      "\n",
      "Training completed in 141.21422624588013 s\n",
      "Averaged - Iteration 6255: Accuracy: 0.934, F1 Micro: 0.8119, F1 Macro: 0.7849\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 508\n",
      "Sampling duration: 70.14618372917175 seconds\n",
      "New train size: 6763\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4365, Accuracy: 0.8685, F1 Micro: 0.4893, F1 Macro: 0.2443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3018, Accuracy: 0.9, F1 Micro: 0.6825, F1 Macro: 0.6011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2375, Accuracy: 0.9161, F1 Micro: 0.7433, F1 Macro: 0.6723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9227, F1 Micro: 0.7729, F1 Macro: 0.7215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.9256, F1 Micro: 0.7914, F1 Macro: 0.7476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1361, Accuracy: 0.9316, F1 Micro: 0.8032, F1 Macro: 0.7691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9335, F1 Micro: 0.8039, F1 Macro: 0.7721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9335, F1 Micro: 0.814, F1 Macro: 0.787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9359, F1 Micro: 0.818, F1 Macro: 0.7947\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.9364, F1 Micro: 0.8147, F1 Macro: 0.7954\n",
      "Model 1 - Iteration 6763: Accuracy: 0.9359, F1 Micro: 0.818, F1 Macro: 0.7947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1245\n",
      "      Abusive       0.88      0.92      0.90      1228\n",
      "HS_Individual       0.78      0.75      0.76       591\n",
      "     HS_Group       0.83      0.80      0.82       654\n",
      "  HS_Religion       0.84      0.74      0.79       302\n",
      "      HS_Race       0.90      0.83      0.86       276\n",
      "  HS_Physical       0.86      0.63      0.73       262\n",
      "    HS_Gender       0.90      0.73      0.80       253\n",
      "     HS_Other       0.71      0.68      0.70       289\n",
      "      HS_Weak       0.74      0.70      0.72       478\n",
      "  HS_Moderate       0.75      0.74      0.74       530\n",
      "    HS_Strong       0.90      0.79      0.84       237\n",
      "\n",
      "    micro avg       0.83      0.80      0.82      6345\n",
      "    macro avg       0.83      0.76      0.79      6345\n",
      " weighted avg       0.83      0.80      0.82      6345\n",
      "  samples avg       0.49      0.47      0.46      6345\n",
      "\n",
      "Training completed in 148.58017587661743 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4418, Accuracy: 0.8656, F1 Micro: 0.4846, F1 Macro: 0.2307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3085, Accuracy: 0.8999, F1 Micro: 0.6842, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.243, Accuracy: 0.9165, F1 Micro: 0.7529, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9224, F1 Micro: 0.7704, F1 Macro: 0.7128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1689, Accuracy: 0.9248, F1 Micro: 0.7909, F1 Macro: 0.7462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.9312, F1 Micro: 0.7972, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1155, Accuracy: 0.9344, F1 Micro: 0.8074, F1 Macro: 0.7757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.934, F1 Micro: 0.8108, F1 Macro: 0.7799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9363, F1 Micro: 0.8186, F1 Macro: 0.7914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0761, Accuracy: 0.9386, F1 Micro: 0.8197, F1 Macro: 0.7955\n",
      "Model 2 - Iteration 6763: Accuracy: 0.9386, F1 Micro: 0.8197, F1 Macro: 0.7955\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.85      0.87      1245\n",
      "      Abusive       0.93      0.87      0.90      1228\n",
      "HS_Individual       0.78      0.76      0.77       591\n",
      "     HS_Group       0.89      0.75      0.82       654\n",
      "  HS_Religion       0.88      0.75      0.81       302\n",
      "      HS_Race       0.90      0.84      0.87       276\n",
      "  HS_Physical       0.85      0.61      0.71       262\n",
      "    HS_Gender       0.89      0.73      0.80       253\n",
      "     HS_Other       0.77      0.63      0.69       289\n",
      "      HS_Weak       0.74      0.70      0.72       478\n",
      "  HS_Moderate       0.83      0.68      0.75       530\n",
      "    HS_Strong       0.90      0.79      0.84       237\n",
      "\n",
      "    micro avg       0.87      0.78      0.82      6345\n",
      "    macro avg       0.86      0.75      0.80      6345\n",
      " weighted avg       0.87      0.78      0.82      6345\n",
      "  samples avg       0.48      0.46      0.46      6345\n",
      "\n",
      "Training completed in 150.58369302749634 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4406, Accuracy: 0.8636, F1 Micro: 0.4642, F1 Macro: 0.2414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3079, Accuracy: 0.8983, F1 Micro: 0.6862, F1 Macro: 0.605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.243, Accuracy: 0.914, F1 Micro: 0.732, F1 Macro: 0.6543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9214, F1 Micro: 0.764, F1 Macro: 0.7045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.167, Accuracy: 0.9284, F1 Micro: 0.789, F1 Macro: 0.7365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1402, Accuracy: 0.9308, F1 Micro: 0.7972, F1 Macro: 0.7588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1175, Accuracy: 0.9324, F1 Micro: 0.7975, F1 Macro: 0.7649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.097, Accuracy: 0.9335, F1 Micro: 0.8115, F1 Macro: 0.7823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0826, Accuracy: 0.9334, F1 Micro: 0.8117, F1 Macro: 0.7846\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.9361, F1 Micro: 0.8051, F1 Macro: 0.7845\n",
      "Model 3 - Iteration 6763: Accuracy: 0.9334, F1 Micro: 0.8117, F1 Macro: 0.7846\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.88      0.87      1245\n",
      "      Abusive       0.87      0.90      0.89      1228\n",
      "HS_Individual       0.76      0.78      0.77       591\n",
      "     HS_Group       0.85      0.79      0.82       654\n",
      "  HS_Religion       0.83      0.73      0.77       302\n",
      "      HS_Race       0.87      0.84      0.85       276\n",
      "  HS_Physical       0.87      0.58      0.69       262\n",
      "    HS_Gender       0.82      0.76      0.79       253\n",
      "     HS_Other       0.70      0.66      0.68       289\n",
      "      HS_Weak       0.71      0.73      0.72       478\n",
      "  HS_Moderate       0.77      0.70      0.73       530\n",
      "    HS_Strong       0.86      0.80      0.83       237\n",
      "\n",
      "    micro avg       0.82      0.80      0.81      6345\n",
      "    macro avg       0.81      0.76      0.78      6345\n",
      " weighted avg       0.82      0.80      0.81      6345\n",
      "  samples avg       0.48      0.47      0.46      6345\n",
      "\n",
      "Training completed in 149.54299473762512 s\n",
      "Averaged - Iteration 6763: Accuracy: 0.936, F1 Micro: 0.8165, F1 Macro: 0.7916\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7084\n",
      "Acquired samples: 321\n",
      "Sampling duration: 63.504003047943115 seconds\n",
      "New train size: 7084\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4356, Accuracy: 0.8655, F1 Micro: 0.4745, F1 Macro: 0.2171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3002, Accuracy: 0.9043, F1 Micro: 0.6984, F1 Macro: 0.6224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2319, Accuracy: 0.9177, F1 Micro: 0.742, F1 Macro: 0.6663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1923, Accuracy: 0.9274, F1 Micro: 0.7779, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.929, F1 Micro: 0.7911, F1 Macro: 0.7509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1304, Accuracy: 0.9342, F1 Micro: 0.8077, F1 Macro: 0.775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1105, Accuracy: 0.9361, F1 Micro: 0.8086, F1 Macro: 0.777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9372, F1 Micro: 0.8141, F1 Macro: 0.7894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9372, F1 Micro: 0.8207, F1 Macro: 0.796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.9378, F1 Micro: 0.8216, F1 Macro: 0.7994\n",
      "Model 1 - Iteration 7084: Accuracy: 0.9378, F1 Micro: 0.8216, F1 Macro: 0.7994\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.86      0.87      1245\n",
      "      Abusive       0.90      0.90      0.90      1228\n",
      "HS_Individual       0.78      0.77      0.77       591\n",
      "     HS_Group       0.87      0.78      0.83       654\n",
      "  HS_Religion       0.85      0.77      0.81       302\n",
      "      HS_Race       0.88      0.84      0.86       276\n",
      "  HS_Physical       0.90      0.64      0.75       262\n",
      "    HS_Gender       0.88      0.77      0.82       253\n",
      "     HS_Other       0.72      0.66      0.69       289\n",
      "      HS_Weak       0.72      0.71      0.71       478\n",
      "  HS_Moderate       0.82      0.69      0.75       530\n",
      "    HS_Strong       0.85      0.82      0.84       237\n",
      "\n",
      "    micro avg       0.85      0.80      0.82      6345\n",
      "    macro avg       0.84      0.77      0.80      6345\n",
      " weighted avg       0.85      0.80      0.82      6345\n",
      "  samples avg       0.49      0.47      0.46      6345\n",
      "\n",
      "Training completed in 157.28503441810608 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4402, Accuracy: 0.8632, F1 Micro: 0.4525, F1 Macro: 0.2035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3026, Accuracy: 0.9019, F1 Micro: 0.69, F1 Macro: 0.6134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9159, F1 Micro: 0.7321, F1 Macro: 0.6478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1966, Accuracy: 0.9262, F1 Micro: 0.7748, F1 Macro: 0.7177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9287, F1 Micro: 0.7938, F1 Macro: 0.7521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9344, F1 Micro: 0.805, F1 Macro: 0.7691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.11, Accuracy: 0.938, F1 Micro: 0.8171, F1 Macro: 0.788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0962, Accuracy: 0.9387, F1 Micro: 0.8218, F1 Macro: 0.7968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0792, Accuracy: 0.9383, F1 Micro: 0.8236, F1 Macro: 0.8005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9372, F1 Micro: 0.8238, F1 Macro: 0.8022\n",
      "Model 2 - Iteration 7084: Accuracy: 0.9372, F1 Micro: 0.8238, F1 Macro: 0.8022\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.89      0.88      1245\n",
      "      Abusive       0.89      0.91      0.90      1228\n",
      "HS_Individual       0.73      0.81      0.77       591\n",
      "     HS_Group       0.86      0.79      0.82       654\n",
      "  HS_Religion       0.84      0.76      0.80       302\n",
      "      HS_Race       0.89      0.84      0.86       276\n",
      "  HS_Physical       0.84      0.69      0.76       262\n",
      "    HS_Gender       0.85      0.81      0.83       253\n",
      "     HS_Other       0.71      0.67      0.69       289\n",
      "      HS_Weak       0.70      0.76      0.73       478\n",
      "  HS_Moderate       0.82      0.69      0.75       530\n",
      "    HS_Strong       0.83      0.84      0.84       237\n",
      "\n",
      "    micro avg       0.83      0.82      0.82      6345\n",
      "    macro avg       0.82      0.79      0.80      6345\n",
      " weighted avg       0.83      0.82      0.82      6345\n",
      "  samples avg       0.49      0.48      0.47      6345\n",
      "\n",
      "Training completed in 157.4281735420227 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.437, Accuracy: 0.8638, F1 Micro: 0.4584, F1 Macro: 0.2185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3023, Accuracy: 0.9006, F1 Micro: 0.6779, F1 Macro: 0.5906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.9141, F1 Micro: 0.727, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.199, Accuracy: 0.9255, F1 Micro: 0.7782, F1 Macro: 0.7191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1627, Accuracy: 0.9292, F1 Micro: 0.7914, F1 Macro: 0.7477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1355, Accuracy: 0.9324, F1 Micro: 0.7953, F1 Macro: 0.7561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9341, F1 Micro: 0.7995, F1 Macro: 0.7671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9363, F1 Micro: 0.814, F1 Macro: 0.7913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9333, F1 Micro: 0.8177, F1 Macro: 0.7947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9363, F1 Micro: 0.8193, F1 Macro: 0.7981\n",
      "Model 3 - Iteration 7084: Accuracy: 0.9363, F1 Micro: 0.8193, F1 Macro: 0.7981\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1245\n",
      "      Abusive       0.90      0.89      0.89      1228\n",
      "HS_Individual       0.76      0.78      0.77       591\n",
      "     HS_Group       0.85      0.79      0.82       654\n",
      "  HS_Religion       0.83      0.76      0.79       302\n",
      "      HS_Race       0.87      0.87      0.87       276\n",
      "  HS_Physical       0.89      0.68      0.77       262\n",
      "    HS_Gender       0.82      0.77      0.80       253\n",
      "     HS_Other       0.75      0.67      0.71       289\n",
      "      HS_Weak       0.71      0.72      0.72       478\n",
      "  HS_Moderate       0.79      0.71      0.75       530\n",
      "    HS_Strong       0.83      0.81      0.82       237\n",
      "\n",
      "    micro avg       0.84      0.80      0.82      6345\n",
      "    macro avg       0.82      0.78      0.80      6345\n",
      " weighted avg       0.84      0.80      0.82      6345\n",
      "  samples avg       0.48      0.47      0.46      6345\n",
      "\n",
      "Training completed in 157.1409888267517 s\n",
      "Averaged - Iteration 7084: Accuracy: 0.9371, F1 Micro: 0.8216, F1 Macro: 0.7999\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 8501\n",
      "Acquired samples: 426\n",
      "Sampling duration: 59.38037586212158 seconds\n",
      "New train size: 7510\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4269, Accuracy: 0.8726, F1 Micro: 0.5243, F1 Macro: 0.3043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2888, Accuracy: 0.9055, F1 Micro: 0.7285, F1 Macro: 0.6542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2273, Accuracy: 0.92, F1 Micro: 0.7542, F1 Macro: 0.6944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9303, F1 Micro: 0.7973, F1 Macro: 0.7554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.928, F1 Micro: 0.8016, F1 Macro: 0.7674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1264, Accuracy: 0.9358, F1 Micro: 0.8137, F1 Macro: 0.7823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.938, F1 Micro: 0.8204, F1 Macro: 0.7957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.939, F1 Micro: 0.8215, F1 Macro: 0.7956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.938, F1 Micro: 0.8272, F1 Macro: 0.8043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0674, Accuracy: 0.9414, F1 Micro: 0.8314, F1 Macro: 0.8116\n",
      "Model 1 - Iteration 7510: Accuracy: 0.9414, F1 Micro: 0.8314, F1 Macro: 0.8116\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.86      0.88      1245\n",
      "      Abusive       0.91      0.90      0.91      1228\n",
      "HS_Individual       0.81      0.74      0.78       591\n",
      "     HS_Group       0.86      0.81      0.83       654\n",
      "  HS_Religion       0.86      0.78      0.82       302\n",
      "      HS_Race       0.88      0.87      0.87       276\n",
      "  HS_Physical       0.89      0.69      0.78       262\n",
      "    HS_Gender       0.87      0.79      0.83       253\n",
      "     HS_Other       0.80      0.63      0.71       289\n",
      "      HS_Weak       0.75      0.71      0.73       478\n",
      "  HS_Moderate       0.79      0.73      0.76       530\n",
      "    HS_Strong       0.89      0.82      0.86       237\n",
      "\n",
      "    micro avg       0.86      0.80      0.83      6345\n",
      "    macro avg       0.85      0.78      0.81      6345\n",
      " weighted avg       0.86      0.80      0.83      6345\n",
      "  samples avg       0.49      0.47      0.47      6345\n",
      "\n",
      "Training completed in 161.8680214881897 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4321, Accuracy: 0.8696, F1 Micro: 0.5024, F1 Macro: 0.2776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2919, Accuracy: 0.9079, F1 Micro: 0.7141, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.92, F1 Micro: 0.767, F1 Macro: 0.7103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1874, Accuracy: 0.9298, F1 Micro: 0.7952, F1 Macro: 0.7517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1564, Accuracy: 0.9296, F1 Micro: 0.8039, F1 Macro: 0.7689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1303, Accuracy: 0.9356, F1 Micro: 0.8157, F1 Macro: 0.7815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.938, F1 Micro: 0.8233, F1 Macro: 0.7978\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9396, F1 Micro: 0.8224, F1 Macro: 0.7957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9418, F1 Micro: 0.8307, F1 Macro: 0.8084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9418, F1 Micro: 0.8332, F1 Macro: 0.8133\n",
      "Model 2 - Iteration 7510: Accuracy: 0.9418, F1 Micro: 0.8332, F1 Macro: 0.8133\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.87      0.88      1245\n",
      "      Abusive       0.91      0.90      0.91      1228\n",
      "HS_Individual       0.80      0.77      0.78       591\n",
      "     HS_Group       0.87      0.81      0.84       654\n",
      "  HS_Religion       0.87      0.77      0.82       302\n",
      "      HS_Race       0.90      0.87      0.88       276\n",
      "  HS_Physical       0.89      0.71      0.79       262\n",
      "    HS_Gender       0.85      0.79      0.82       253\n",
      "     HS_Other       0.77      0.64      0.70       289\n",
      "      HS_Weak       0.74      0.71      0.73       478\n",
      "  HS_Moderate       0.81      0.74      0.77       530\n",
      "    HS_Strong       0.90      0.81      0.85       237\n",
      "\n",
      "    micro avg       0.86      0.81      0.83      6345\n",
      "    macro avg       0.85      0.78      0.81      6345\n",
      " weighted avg       0.86      0.81      0.83      6345\n",
      "  samples avg       0.49      0.47      0.47      6345\n",
      "\n",
      "Training completed in 161.807231426239 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4303, Accuracy: 0.8691, F1 Micro: 0.492, F1 Macro: 0.2724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2933, Accuracy: 0.9056, F1 Micro: 0.709, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2314, Accuracy: 0.9203, F1 Micro: 0.7538, F1 Macro: 0.6816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9294, F1 Micro: 0.7975, F1 Macro: 0.753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1563, Accuracy: 0.9325, F1 Micro: 0.8061, F1 Macro: 0.7692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9355, F1 Micro: 0.8149, F1 Macro: 0.7806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.937, F1 Micro: 0.8207, F1 Macro: 0.7933\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9383, F1 Micro: 0.8206, F1 Macro: 0.7971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.941, F1 Micro: 0.8271, F1 Macro: 0.8058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9413, F1 Micro: 0.8303, F1 Macro: 0.8113\n",
      "Model 3 - Iteration 7510: Accuracy: 0.9413, F1 Micro: 0.8303, F1 Macro: 0.8113\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.85      0.87      1245\n",
      "      Abusive       0.91      0.89      0.90      1228\n",
      "HS_Individual       0.80      0.76      0.78       591\n",
      "     HS_Group       0.88      0.78      0.83       654\n",
      "  HS_Religion       0.85      0.78      0.81       302\n",
      "      HS_Race       0.89      0.87      0.88       276\n",
      "  HS_Physical       0.89      0.68      0.77       262\n",
      "    HS_Gender       0.89      0.79      0.83       253\n",
      "     HS_Other       0.77      0.66      0.71       289\n",
      "      HS_Weak       0.75      0.71      0.73       478\n",
      "  HS_Moderate       0.82      0.73      0.77       530\n",
      "    HS_Strong       0.90      0.81      0.85       237\n",
      "\n",
      "    micro avg       0.86      0.80      0.83      6345\n",
      "    macro avg       0.85      0.78      0.81      6345\n",
      " weighted avg       0.86      0.80      0.83      6345\n",
      "  samples avg       0.49      0.47      0.47      6345\n",
      "\n",
      "Training completed in 160.4470546245575 s\n",
      "Averaged - Iteration 7510: Accuracy: 0.9415, F1 Micro: 0.8316, F1 Macro: 0.812\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 8501\n",
      "Acquired samples: 383\n",
      "Sampling duration: 53.0177538394928 seconds\n",
      "New train size: 7893\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4277, Accuracy: 0.8712, F1 Micro: 0.4991, F1 Macro: 0.2915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.285, Accuracy: 0.907, F1 Micro: 0.6951, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2228, Accuracy: 0.9242, F1 Micro: 0.7791, F1 Macro: 0.7308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1768, Accuracy: 0.9334, F1 Micro: 0.806, F1 Macro: 0.7649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.9357, F1 Micro: 0.8143, F1 Macro: 0.7807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.124, Accuracy: 0.9393, F1 Micro: 0.8221, F1 Macro: 0.7954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1011, Accuracy: 0.9411, F1 Micro: 0.8299, F1 Macro: 0.8051\n",
      "Epoch 8/10, Train Loss: 0.0844, Accuracy: 0.94, F1 Micro: 0.8289, F1 Macro: 0.8098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9426, F1 Micro: 0.8323, F1 Macro: 0.8109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9402, F1 Micro: 0.8341, F1 Macro: 0.8157\n",
      "Model 1 - Iteration 7893: Accuracy: 0.9402, F1 Micro: 0.8341, F1 Macro: 0.8157\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.90      0.88      1245\n",
      "      Abusive       0.90      0.92      0.91      1228\n",
      "HS_Individual       0.79      0.78      0.79       591\n",
      "     HS_Group       0.81      0.85      0.83       654\n",
      "  HS_Religion       0.82      0.84      0.83       302\n",
      "      HS_Race       0.87      0.89      0.88       276\n",
      "  HS_Physical       0.89      0.73      0.80       262\n",
      "    HS_Gender       0.88      0.80      0.84       253\n",
      "     HS_Other       0.72      0.69      0.70       289\n",
      "      HS_Weak       0.74      0.74      0.74       478\n",
      "  HS_Moderate       0.76      0.76      0.76       530\n",
      "    HS_Strong       0.82      0.85      0.84       237\n",
      "\n",
      "    micro avg       0.83      0.84      0.83      6345\n",
      "    macro avg       0.82      0.81      0.82      6345\n",
      " weighted avg       0.83      0.84      0.83      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 167.46635222434998 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4337, Accuracy: 0.8694, F1 Micro: 0.4865, F1 Macro: 0.2609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2893, Accuracy: 0.9057, F1 Micro: 0.6897, F1 Macro: 0.5871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2264, Accuracy: 0.9237, F1 Micro: 0.7674, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1831, Accuracy: 0.932, F1 Micro: 0.8001, F1 Macro: 0.7563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9342, F1 Micro: 0.8087, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9376, F1 Micro: 0.8179, F1 Macro: 0.793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1044, Accuracy: 0.9397, F1 Micro: 0.8303, F1 Macro: 0.8059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9427, F1 Micro: 0.8366, F1 Macro: 0.8169\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.9417, F1 Micro: 0.8331, F1 Macro: 0.8113\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.9407, F1 Micro: 0.8332, F1 Macro: 0.8133\n",
      "Model 2 - Iteration 7893: Accuracy: 0.9427, F1 Micro: 0.8366, F1 Macro: 0.8169\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.88      0.88      1245\n",
      "      Abusive       0.92      0.90      0.91      1228\n",
      "HS_Individual       0.85      0.74      0.79       591\n",
      "     HS_Group       0.82      0.85      0.84       654\n",
      "  HS_Religion       0.84      0.79      0.81       302\n",
      "      HS_Race       0.90      0.86      0.88       276\n",
      "  HS_Physical       0.85      0.75      0.80       262\n",
      "    HS_Gender       0.88      0.78      0.83       253\n",
      "     HS_Other       0.76      0.64      0.70       289\n",
      "      HS_Weak       0.81      0.70      0.75       478\n",
      "  HS_Moderate       0.75      0.78      0.77       530\n",
      "    HS_Strong       0.88      0.83      0.85       237\n",
      "\n",
      "    micro avg       0.86      0.82      0.84      6345\n",
      "    macro avg       0.85      0.79      0.82      6345\n",
      " weighted avg       0.86      0.82      0.84      6345\n",
      "  samples avg       0.48      0.47      0.47      6345\n",
      "\n",
      "Training completed in 164.72362756729126 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4307, Accuracy: 0.8708, F1 Micro: 0.4983, F1 Macro: 0.2685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2899, Accuracy: 0.9048, F1 Micro: 0.6882, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2287, Accuracy: 0.923, F1 Micro: 0.7743, F1 Macro: 0.7161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9308, F1 Micro: 0.8029, F1 Macro: 0.7633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9352, F1 Micro: 0.8043, F1 Macro: 0.768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1269, Accuracy: 0.9372, F1 Micro: 0.8181, F1 Macro: 0.7902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9412, F1 Micro: 0.8266, F1 Macro: 0.8024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9409, F1 Micro: 0.8309, F1 Macro: 0.8107\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9414, F1 Micro: 0.8283, F1 Macro: 0.8083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9427, F1 Micro: 0.8376, F1 Macro: 0.8172\n",
      "Model 3 - Iteration 7893: Accuracy: 0.9427, F1 Micro: 0.8376, F1 Macro: 0.8172\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.88      0.88      1245\n",
      "      Abusive       0.90      0.92      0.91      1228\n",
      "HS_Individual       0.80      0.78      0.79       591\n",
      "     HS_Group       0.86      0.81      0.83       654\n",
      "  HS_Religion       0.86      0.79      0.82       302\n",
      "      HS_Race       0.89      0.86      0.87       276\n",
      "  HS_Physical       0.88      0.75      0.81       262\n",
      "    HS_Gender       0.86      0.80      0.83       253\n",
      "     HS_Other       0.75      0.63      0.68       289\n",
      "      HS_Weak       0.76      0.74      0.75       478\n",
      "  HS_Moderate       0.80      0.75      0.77       530\n",
      "    HS_Strong       0.86      0.83      0.84       237\n",
      "\n",
      "    micro avg       0.85      0.82      0.84      6345\n",
      "    macro avg       0.84      0.79      0.82      6345\n",
      " weighted avg       0.85      0.82      0.84      6345\n",
      "  samples avg       0.49      0.48      0.47      6345\n",
      "\n",
      "Training completed in 167.49337935447693 s\n",
      "Averaged - Iteration 7893: Accuracy: 0.9418, F1 Micro: 0.8361, F1 Macro: 0.8166\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 8501\n",
      "Acquired samples: 345\n",
      "Sampling duration: 48.864248514175415 seconds\n",
      "New train size: 8238\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4225, Accuracy: 0.8775, F1 Micro: 0.5573, F1 Macro: 0.3607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.286, Accuracy: 0.912, F1 Micro: 0.722, F1 Macro: 0.6357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2153, Accuracy: 0.9261, F1 Micro: 0.7745, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9343, F1 Micro: 0.8112, F1 Macro: 0.7774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.9382, F1 Micro: 0.818, F1 Macro: 0.7886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.937, F1 Micro: 0.8239, F1 Macro: 0.7977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0975, Accuracy: 0.9372, F1 Micro: 0.8271, F1 Macro: 0.806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0836, Accuracy: 0.9434, F1 Micro: 0.8384, F1 Macro: 0.8169\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9433, F1 Micro: 0.8376, F1 Macro: 0.8216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9457, F1 Micro: 0.843, F1 Macro: 0.8262\n",
      "Model 1 - Iteration 8238: Accuracy: 0.9457, F1 Micro: 0.843, F1 Macro: 0.8262\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.86      0.89      1245\n",
      "      Abusive       0.91      0.90      0.91      1228\n",
      "HS_Individual       0.83      0.77      0.80       591\n",
      "     HS_Group       0.88      0.81      0.84       654\n",
      "  HS_Religion       0.89      0.77      0.83       302\n",
      "      HS_Race       0.90      0.87      0.88       276\n",
      "  HS_Physical       0.89      0.74      0.81       262\n",
      "    HS_Gender       0.89      0.84      0.86       253\n",
      "     HS_Other       0.80      0.65      0.72       289\n",
      "      HS_Weak       0.78      0.72      0.75       478\n",
      "  HS_Moderate       0.85      0.72      0.78       530\n",
      "    HS_Strong       0.87      0.84      0.86       237\n",
      "\n",
      "    micro avg       0.88      0.81      0.84      6345\n",
      "    macro avg       0.87      0.79      0.83      6345\n",
      " weighted avg       0.88      0.81      0.84      6345\n",
      "  samples avg       0.49      0.47      0.47      6345\n",
      "\n",
      "Training completed in 173.09737420082092 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4279, Accuracy: 0.8742, F1 Micro: 0.5339, F1 Macro: 0.3109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2917, Accuracy: 0.9082, F1 Micro: 0.6982, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.9269, F1 Micro: 0.7792, F1 Macro: 0.7252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.9334, F1 Micro: 0.8049, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1468, Accuracy: 0.9361, F1 Micro: 0.8102, F1 Macro: 0.7728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9409, F1 Micro: 0.8301, F1 Macro: 0.8029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9394, F1 Micro: 0.8325, F1 Macro: 0.8114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.087, Accuracy: 0.9418, F1 Micro: 0.8362, F1 Macro: 0.8138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9436, F1 Micro: 0.8412, F1 Macro: 0.8224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9467, F1 Micro: 0.847, F1 Macro: 0.8292\n",
      "Model 2 - Iteration 8238: Accuracy: 0.9467, F1 Micro: 0.847, F1 Macro: 0.8292\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.88      0.89      1245\n",
      "      Abusive       0.93      0.90      0.91      1228\n",
      "HS_Individual       0.82      0.79      0.80       591\n",
      "     HS_Group       0.88      0.82      0.85       654\n",
      "  HS_Religion       0.87      0.75      0.81       302\n",
      "      HS_Race       0.91      0.89      0.90       276\n",
      "  HS_Physical       0.88      0.76      0.82       262\n",
      "    HS_Gender       0.89      0.84      0.87       253\n",
      "     HS_Other       0.79      0.65      0.71       289\n",
      "      HS_Weak       0.77      0.74      0.76       478\n",
      "  HS_Moderate       0.85      0.72      0.78       530\n",
      "    HS_Strong       0.85      0.86      0.85       237\n",
      "\n",
      "    micro avg       0.87      0.82      0.85      6345\n",
      "    macro avg       0.86      0.80      0.83      6345\n",
      " weighted avg       0.87      0.82      0.85      6345\n",
      "  samples avg       0.49      0.48      0.47      6345\n",
      "\n",
      "Training completed in 176.6705379486084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.423, Accuracy: 0.8781, F1 Micro: 0.5743, F1 Macro: 0.375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.29, Accuracy: 0.9069, F1 Micro: 0.6947, F1 Macro: 0.5974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2226, Accuracy: 0.9238, F1 Micro: 0.7659, F1 Macro: 0.6993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.9321, F1 Micro: 0.8094, F1 Macro: 0.7778\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.9354, F1 Micro: 0.809, F1 Macro: 0.772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.9379, F1 Micro: 0.8222, F1 Macro: 0.7931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9354, F1 Micro: 0.8226, F1 Macro: 0.8012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9435, F1 Micro: 0.8365, F1 Macro: 0.8173\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9428, F1 Micro: 0.8361, F1 Macro: 0.8189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0607, Accuracy: 0.9438, F1 Micro: 0.8401, F1 Macro: 0.8237\n",
      "Model 3 - Iteration 8238: Accuracy: 0.9438, F1 Micro: 0.8401, F1 Macro: 0.8237\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.88      0.88      1245\n",
      "      Abusive       0.91      0.90      0.90      1228\n",
      "HS_Individual       0.80      0.80      0.80       591\n",
      "     HS_Group       0.88      0.80      0.84       654\n",
      "  HS_Religion       0.84      0.79      0.81       302\n",
      "      HS_Race       0.89      0.89      0.89       276\n",
      "  HS_Physical       0.88      0.78      0.83       262\n",
      "    HS_Gender       0.89      0.82      0.85       253\n",
      "     HS_Other       0.79      0.63      0.70       289\n",
      "      HS_Weak       0.75      0.75      0.75       478\n",
      "  HS_Moderate       0.83      0.71      0.76       530\n",
      "    HS_Strong       0.86      0.86      0.86       237\n",
      "\n",
      "    micro avg       0.86      0.82      0.84      6345\n",
      "    macro avg       0.85      0.80      0.82      6345\n",
      " weighted avg       0.86      0.82      0.84      6345\n",
      "  samples avg       0.49      0.48      0.48      6345\n",
      "\n",
      "Training completed in 172.12951159477234 s\n",
      "Averaged - Iteration 8238: Accuracy: 0.9454, F1 Micro: 0.8434, F1 Macro: 0.8264\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 8501\n",
      "Acquired samples: 263\n",
      "Sampling duration: 43.13458013534546 seconds\n",
      "New train size: 8501\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4169, Accuracy: 0.8794, F1 Micro: 0.5677, F1 Macro: 0.3833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.9106, F1 Micro: 0.7019, F1 Macro: 0.6217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2146, Accuracy: 0.9283, F1 Micro: 0.7829, F1 Macro: 0.7405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9332, F1 Micro: 0.8088, F1 Macro: 0.7768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9392, F1 Micro: 0.8211, F1 Macro: 0.7982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9406, F1 Micro: 0.8297, F1 Macro: 0.8067\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.9386, F1 Micro: 0.8259, F1 Macro: 0.8025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0811, Accuracy: 0.944, F1 Micro: 0.8408, F1 Macro: 0.8222\n",
      "Epoch 9/10, Train Loss: 0.0696, Accuracy: 0.9442, F1 Micro: 0.8399, F1 Macro: 0.8224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.945, F1 Micro: 0.845, F1 Macro: 0.8282\n",
      "Model 1 - Iteration 8501: Accuracy: 0.945, F1 Micro: 0.845, F1 Macro: 0.8282\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.88      0.89      1245\n",
      "      Abusive       0.89      0.93      0.91      1228\n",
      "HS_Individual       0.81      0.80      0.80       591\n",
      "     HS_Group       0.87      0.81      0.84       654\n",
      "  HS_Religion       0.87      0.80      0.84       302\n",
      "      HS_Race       0.91      0.83      0.87       276\n",
      "  HS_Physical       0.89      0.77      0.82       262\n",
      "    HS_Gender       0.89      0.85      0.87       253\n",
      "     HS_Other       0.75      0.68      0.71       289\n",
      "      HS_Weak       0.75      0.76      0.76       478\n",
      "  HS_Moderate       0.81      0.76      0.79       530\n",
      "    HS_Strong       0.90      0.80      0.85       237\n",
      "\n",
      "    micro avg       0.86      0.83      0.85      6345\n",
      "    macro avg       0.85      0.81      0.83      6345\n",
      " weighted avg       0.86      0.83      0.84      6345\n",
      "  samples avg       0.50      0.49      0.48      6345\n",
      "\n",
      "Training completed in 174.96720504760742 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4226, Accuracy: 0.8777, F1 Micro: 0.5588, F1 Macro: 0.3573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2805, Accuracy: 0.9092, F1 Micro: 0.7004, F1 Macro: 0.6215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9272, F1 Micro: 0.7772, F1 Macro: 0.7233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9317, F1 Micro: 0.8025, F1 Macro: 0.7654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9391, F1 Micro: 0.823, F1 Macro: 0.797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.941, F1 Micro: 0.8274, F1 Macro: 0.8039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1016, Accuracy: 0.9408, F1 Micro: 0.8333, F1 Macro: 0.8115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9447, F1 Micro: 0.8385, F1 Macro: 0.8212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9444, F1 Micro: 0.8408, F1 Macro: 0.822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9472, F1 Micro: 0.8468, F1 Macro: 0.829\n",
      "Model 2 - Iteration 8501: Accuracy: 0.9472, F1 Micro: 0.8468, F1 Macro: 0.829\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.92      0.86      0.89      1245\n",
      "      Abusive       0.91      0.91      0.91      1228\n",
      "HS_Individual       0.84      0.76      0.80       591\n",
      "     HS_Group       0.87      0.81      0.84       654\n",
      "  HS_Religion       0.86      0.81      0.83       302\n",
      "      HS_Race       0.93      0.83      0.88       276\n",
      "  HS_Physical       0.92      0.76      0.84       262\n",
      "    HS_Gender       0.89      0.84      0.86       253\n",
      "     HS_Other       0.83      0.59      0.69       289\n",
      "      HS_Weak       0.80      0.72      0.76       478\n",
      "  HS_Moderate       0.83      0.74      0.78       530\n",
      "    HS_Strong       0.92      0.82      0.87       237\n",
      "\n",
      "    micro avg       0.88      0.81      0.85      6345\n",
      "    macro avg       0.88      0.79      0.83      6345\n",
      " weighted avg       0.88      0.81      0.85      6345\n",
      "  samples avg       0.50      0.47      0.47      6345\n",
      "\n",
      "Training completed in 178.8370280265808 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4221, Accuracy: 0.8762, F1 Micro: 0.5555, F1 Macro: 0.368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2828, Accuracy: 0.9057, F1 Micro: 0.6811, F1 Macro: 0.5879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9279, F1 Micro: 0.7814, F1 Macro: 0.722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9326, F1 Micro: 0.8057, F1 Macro: 0.7684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1465, Accuracy: 0.9387, F1 Micro: 0.8188, F1 Macro: 0.7948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1233, Accuracy: 0.9379, F1 Micro: 0.8257, F1 Macro: 0.8006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.9401, F1 Micro: 0.8315, F1 Macro: 0.8087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9397, F1 Micro: 0.8339, F1 Macro: 0.8151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9453, F1 Micro: 0.8426, F1 Macro: 0.8259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9468, F1 Micro: 0.8465, F1 Macro: 0.8314\n",
      "Model 3 - Iteration 8501: Accuracy: 0.9468, F1 Micro: 0.8465, F1 Macro: 0.8314\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.86      0.89      1245\n",
      "      Abusive       0.92      0.89      0.90      1228\n",
      "HS_Individual       0.85      0.76      0.80       591\n",
      "     HS_Group       0.87      0.82      0.84       654\n",
      "  HS_Religion       0.86      0.80      0.83       302\n",
      "      HS_Race       0.90      0.84      0.87       276\n",
      "  HS_Physical       0.89      0.77      0.83       262\n",
      "    HS_Gender       0.90      0.85      0.87       253\n",
      "     HS_Other       0.81      0.65      0.72       289\n",
      "      HS_Weak       0.82      0.72      0.77       478\n",
      "  HS_Moderate       0.83      0.75      0.78       530\n",
      "    HS_Strong       0.86      0.86      0.86       237\n",
      "\n",
      "    micro avg       0.88      0.82      0.85      6345\n",
      "    macro avg       0.87      0.80      0.83      6345\n",
      " weighted avg       0.88      0.82      0.85      6345\n",
      "  samples avg       0.49      0.47      0.47      6345\n",
      "\n",
      "Training completed in 178.53763341903687 s\n",
      "Averaged - Iteration 8501: Accuracy: 0.9463, F1 Micro: 0.8461, F1 Macro: 0.8295\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9918\n",
      "Acquired samples: 284\n",
      "Sampling duration: 40.0572714805603 seconds\n",
      "New train size: 8785\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4174, Accuracy: 0.8806, F1 Micro: 0.588, F1 Macro: 0.3869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2727, Accuracy: 0.9146, F1 Micro: 0.7524, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2127, Accuracy: 0.9297, F1 Micro: 0.7909, F1 Macro: 0.7479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1651, Accuracy: 0.9363, F1 Micro: 0.8149, F1 Macro: 0.7856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9391, F1 Micro: 0.826, F1 Macro: 0.7983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9432, F1 Micro: 0.8346, F1 Macro: 0.8135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0958, Accuracy: 0.9416, F1 Micro: 0.8354, F1 Macro: 0.8127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9464, F1 Micro: 0.8454, F1 Macro: 0.8283\n",
      "Epoch 9/10, Train Loss: 0.0678, Accuracy: 0.9442, F1 Micro: 0.8403, F1 Macro: 0.8234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.058, Accuracy: 0.9449, F1 Micro: 0.8476, F1 Macro: 0.8335\n",
      "Model 1 - Iteration 8785: Accuracy: 0.9449, F1 Micro: 0.8476, F1 Macro: 0.8335\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.91      0.89      1245\n",
      "      Abusive       0.91      0.91      0.91      1228\n",
      "HS_Individual       0.76      0.84      0.80       591\n",
      "     HS_Group       0.86      0.83      0.84       654\n",
      "  HS_Religion       0.84      0.83      0.83       302\n",
      "      HS_Race       0.89      0.89      0.89       276\n",
      "  HS_Physical       0.88      0.83      0.85       262\n",
      "    HS_Gender       0.84      0.89      0.86       253\n",
      "     HS_Other       0.74      0.69      0.71       289\n",
      "      HS_Weak       0.72      0.79      0.75       478\n",
      "  HS_Moderate       0.81      0.76      0.79       530\n",
      "    HS_Strong       0.88      0.85      0.86       237\n",
      "\n",
      "    micro avg       0.84      0.85      0.85      6345\n",
      "    macro avg       0.83      0.83      0.83      6345\n",
      " weighted avg       0.84      0.85      0.85      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 181.3030788898468 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4224, Accuracy: 0.8781, F1 Micro: 0.5576, F1 Macro: 0.336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.9127, F1 Micro: 0.7478, F1 Macro: 0.6795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9303, F1 Micro: 0.7914, F1 Macro: 0.7433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9371, F1 Micro: 0.8149, F1 Macro: 0.7798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9401, F1 Micro: 0.8298, F1 Macro: 0.8016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.9423, F1 Micro: 0.838, F1 Macro: 0.8147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9445, F1 Micro: 0.842, F1 Macro: 0.8197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9462, F1 Micro: 0.8472, F1 Macro: 0.8296\n",
      "Epoch 9/10, Train Loss: 0.069, Accuracy: 0.9452, F1 Micro: 0.8451, F1 Macro: 0.8274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0586, Accuracy: 0.9477, F1 Micro: 0.8519, F1 Macro: 0.8367\n",
      "Model 2 - Iteration 8785: Accuracy: 0.9477, F1 Micro: 0.8519, F1 Macro: 0.8367\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.90      0.89      1245\n",
      "      Abusive       0.93      0.89      0.91      1228\n",
      "HS_Individual       0.82      0.79      0.80       591\n",
      "     HS_Group       0.85      0.86      0.85       654\n",
      "  HS_Religion       0.84      0.81      0.83       302\n",
      "      HS_Race       0.90      0.87      0.88       276\n",
      "  HS_Physical       0.90      0.81      0.85       262\n",
      "    HS_Gender       0.90      0.82      0.86       253\n",
      "     HS_Other       0.80      0.67      0.73       289\n",
      "      HS_Weak       0.78      0.74      0.76       478\n",
      "  HS_Moderate       0.79      0.80      0.79       530\n",
      "    HS_Strong       0.90      0.83      0.86       237\n",
      "\n",
      "    micro avg       0.87      0.84      0.85      6345\n",
      "    macro avg       0.86      0.82      0.84      6345\n",
      " weighted avg       0.87      0.84      0.85      6345\n",
      "  samples avg       0.49      0.48      0.47      6345\n",
      "\n",
      "Training completed in 182.36988878250122 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4226, Accuracy: 0.8787, F1 Micro: 0.5611, F1 Macro: 0.3444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.9113, F1 Micro: 0.7389, F1 Macro: 0.6632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2187, Accuracy: 0.9286, F1 Micro: 0.7867, F1 Macro: 0.7394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9359, F1 Micro: 0.8088, F1 Macro: 0.7698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.9398, F1 Micro: 0.8272, F1 Macro: 0.7967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9438, F1 Micro: 0.8379, F1 Macro: 0.8147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.9446, F1 Micro: 0.842, F1 Macro: 0.8213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9453, F1 Micro: 0.8442, F1 Macro: 0.8259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0684, Accuracy: 0.9485, F1 Micro: 0.8508, F1 Macro: 0.8348\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9468, F1 Micro: 0.8503, F1 Macro: 0.8347\n",
      "Model 3 - Iteration 8785: Accuracy: 0.9485, F1 Micro: 0.8508, F1 Macro: 0.8348\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.87      0.89      1245\n",
      "      Abusive       0.91      0.90      0.91      1228\n",
      "HS_Individual       0.87      0.78      0.82       591\n",
      "     HS_Group       0.89      0.81      0.85       654\n",
      "  HS_Religion       0.88      0.79      0.83       302\n",
      "      HS_Race       0.92      0.82      0.87       276\n",
      "  HS_Physical       0.92      0.78      0.84       262\n",
      "    HS_Gender       0.93      0.83      0.88       253\n",
      "     HS_Other       0.81      0.63      0.71       289\n",
      "      HS_Weak       0.81      0.72      0.77       478\n",
      "  HS_Moderate       0.83      0.77      0.80       530\n",
      "    HS_Strong       0.90      0.83      0.86       237\n",
      "\n",
      "    micro avg       0.89      0.82      0.85      6345\n",
      "    macro avg       0.88      0.79      0.83      6345\n",
      " weighted avg       0.89      0.82      0.85      6345\n",
      "  samples avg       0.50      0.48      0.48      6345\n",
      "\n",
      "Training completed in 180.841796875 s\n",
      "Averaged - Iteration 8785: Accuracy: 0.947, F1 Micro: 0.8501, F1 Macro: 0.835\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9918\n",
      "Acquired samples: 255\n",
      "Sampling duration: 35.82251191139221 seconds\n",
      "New train size: 9040\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4221, Accuracy: 0.8799, F1 Micro: 0.5562, F1 Macro: 0.3756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.277, Accuracy: 0.9182, F1 Micro: 0.7461, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2072, Accuracy: 0.9279, F1 Micro: 0.7759, F1 Macro: 0.743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9372, F1 Micro: 0.8113, F1 Macro: 0.7729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9382, F1 Micro: 0.827, F1 Macro: 0.8016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9411, F1 Micro: 0.8358, F1 Macro: 0.8144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9435, F1 Micro: 0.841, F1 Macro: 0.8217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.9456, F1 Micro: 0.8472, F1 Macro: 0.8307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.947, F1 Micro: 0.8487, F1 Macro: 0.8315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9474, F1 Micro: 0.8517, F1 Macro: 0.8358\n",
      "Model 1 - Iteration 9040: Accuracy: 0.9474, F1 Micro: 0.8517, F1 Macro: 0.8358\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.89      0.89      1245\n",
      "      Abusive       0.91      0.91      0.91      1228\n",
      "HS_Individual       0.82      0.78      0.80       591\n",
      "     HS_Group       0.86      0.87      0.86       654\n",
      "  HS_Religion       0.86      0.79      0.83       302\n",
      "      HS_Race       0.88      0.88      0.88       276\n",
      "  HS_Physical       0.90      0.81      0.85       262\n",
      "    HS_Gender       0.88      0.87      0.87       253\n",
      "     HS_Other       0.78      0.67      0.72       289\n",
      "      HS_Weak       0.77      0.74      0.76       478\n",
      "  HS_Moderate       0.81      0.78      0.80       530\n",
      "    HS_Strong       0.87      0.85      0.86       237\n",
      "\n",
      "    micro avg       0.86      0.84      0.85      6345\n",
      "    macro avg       0.85      0.82      0.84      6345\n",
      " weighted avg       0.86      0.84      0.85      6345\n",
      "  samples avg       0.50      0.49      0.48      6345\n",
      "\n",
      "Training completed in 187.13422894477844 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4269, Accuracy: 0.8792, F1 Micro: 0.5575, F1 Macro: 0.3483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2827, Accuracy: 0.9169, F1 Micro: 0.742, F1 Macro: 0.6635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2123, Accuracy: 0.9284, F1 Micro: 0.7772, F1 Macro: 0.7363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.9378, F1 Micro: 0.8171, F1 Macro: 0.7816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.94, F1 Micro: 0.8265, F1 Macro: 0.7996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9427, F1 Micro: 0.8387, F1 Macro: 0.8135\n",
      "Epoch 7/10, Train Loss: 0.0939, Accuracy: 0.9403, F1 Micro: 0.8358, F1 Macro: 0.8149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9467, F1 Micro: 0.8477, F1 Macro: 0.8303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0684, Accuracy: 0.9481, F1 Micro: 0.8511, F1 Macro: 0.8343\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9455, F1 Micro: 0.8481, F1 Macro: 0.8317\n",
      "Model 2 - Iteration 9040: Accuracy: 0.9481, F1 Micro: 0.8511, F1 Macro: 0.8343\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.88      0.89      1245\n",
      "      Abusive       0.91      0.90      0.91      1228\n",
      "HS_Individual       0.81      0.81      0.81       591\n",
      "     HS_Group       0.90      0.82      0.86       654\n",
      "  HS_Religion       0.88      0.76      0.82       302\n",
      "      HS_Race       0.94      0.84      0.89       276\n",
      "  HS_Physical       0.92      0.76      0.83       262\n",
      "    HS_Gender       0.87      0.86      0.86       253\n",
      "     HS_Other       0.78      0.66      0.72       289\n",
      "      HS_Weak       0.77      0.76      0.76       478\n",
      "  HS_Moderate       0.88      0.73      0.79       530\n",
      "    HS_Strong       0.88      0.86      0.87       237\n",
      "\n",
      "    micro avg       0.88      0.83      0.85      6345\n",
      "    macro avg       0.87      0.80      0.83      6345\n",
      " weighted avg       0.88      0.83      0.85      6345\n",
      "  samples avg       0.49      0.48      0.48      6345\n",
      "\n",
      "Training completed in 184.3380208015442 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4246, Accuracy: 0.8741, F1 Micro: 0.5103, F1 Macro: 0.3203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.9144, F1 Micro: 0.7362, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2157, Accuracy: 0.9271, F1 Micro: 0.7714, F1 Macro: 0.7266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9365, F1 Micro: 0.8144, F1 Macro: 0.7821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.9402, F1 Micro: 0.8278, F1 Macro: 0.7997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1166, Accuracy: 0.9423, F1 Micro: 0.8375, F1 Macro: 0.8159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9436, F1 Micro: 0.8418, F1 Macro: 0.8235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9452, F1 Micro: 0.846, F1 Macro: 0.8286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0688, Accuracy: 0.9485, F1 Micro: 0.8511, F1 Macro: 0.8342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0586, Accuracy: 0.9481, F1 Micro: 0.8526, F1 Macro: 0.8379\n",
      "Model 3 - Iteration 9040: Accuracy: 0.9481, F1 Micro: 0.8526, F1 Macro: 0.8379\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.89      0.89      1245\n",
      "      Abusive       0.92      0.90      0.91      1228\n",
      "HS_Individual       0.81      0.83      0.82       591\n",
      "     HS_Group       0.90      0.81      0.85       654\n",
      "  HS_Religion       0.85      0.80      0.83       302\n",
      "      HS_Race       0.91      0.84      0.87       276\n",
      "  HS_Physical       0.91      0.81      0.86       262\n",
      "    HS_Gender       0.90      0.86      0.88       253\n",
      "     HS_Other       0.78      0.69      0.73       289\n",
      "      HS_Weak       0.77      0.78      0.77       478\n",
      "  HS_Moderate       0.85      0.73      0.79       530\n",
      "    HS_Strong       0.84      0.87      0.86       237\n",
      "\n",
      "    micro avg       0.87      0.84      0.85      6345\n",
      "    macro avg       0.86      0.82      0.84      6345\n",
      " weighted avg       0.87      0.84      0.85      6345\n",
      "  samples avg       0.50      0.48      0.48      6345\n",
      "\n",
      "Training completed in 187.2292091846466 s\n",
      "Averaged - Iteration 9040: Accuracy: 0.9479, F1 Micro: 0.8518, F1 Macro: 0.836\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9918\n",
      "Acquired samples: 250\n",
      "Sampling duration: 33.416465759277344 seconds\n",
      "New train size: 9290\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4141, Accuracy: 0.8867, F1 Micro: 0.6134, F1 Macro: 0.4361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2713, Accuracy: 0.9197, F1 Micro: 0.7568, F1 Macro: 0.6909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2105, Accuracy: 0.9319, F1 Micro: 0.7992, F1 Macro: 0.761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.9375, F1 Micro: 0.8198, F1 Macro: 0.786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.9412, F1 Micro: 0.8314, F1 Macro: 0.8035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1148, Accuracy: 0.9425, F1 Micro: 0.8388, F1 Macro: 0.8168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.946, F1 Micro: 0.8457, F1 Macro: 0.8258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9471, F1 Micro: 0.8504, F1 Macro: 0.8334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9473, F1 Micro: 0.8512, F1 Macro: 0.8351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.9483, F1 Micro: 0.8556, F1 Macro: 0.842\n",
      "Model 1 - Iteration 9290: Accuracy: 0.9483, F1 Micro: 0.8556, F1 Macro: 0.842\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.90      0.89      1245\n",
      "      Abusive       0.91      0.93      0.92      1228\n",
      "HS_Individual       0.81      0.81      0.81       591\n",
      "     HS_Group       0.84      0.86      0.85       654\n",
      "  HS_Religion       0.84      0.83      0.84       302\n",
      "      HS_Race       0.90      0.88      0.89       276\n",
      "  HS_Physical       0.91      0.82      0.87       262\n",
      "    HS_Gender       0.86      0.88      0.87       253\n",
      "     HS_Other       0.80      0.69      0.74       289\n",
      "      HS_Weak       0.77      0.77      0.77       478\n",
      "  HS_Moderate       0.79      0.79      0.79       530\n",
      "    HS_Strong       0.91      0.84      0.87       237\n",
      "\n",
      "    micro avg       0.86      0.85      0.86      6345\n",
      "    macro avg       0.85      0.83      0.84      6345\n",
      " weighted avg       0.86      0.85      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 191.28952646255493 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4203, Accuracy: 0.8849, F1 Micro: 0.6184, F1 Macro: 0.4301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.278, Accuracy: 0.9179, F1 Micro: 0.7489, F1 Macro: 0.6783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.9317, F1 Micro: 0.7959, F1 Macro: 0.7528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1704, Accuracy: 0.9357, F1 Micro: 0.8153, F1 Macro: 0.7819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9402, F1 Micro: 0.8296, F1 Macro: 0.8004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9442, F1 Micro: 0.8404, F1 Macro: 0.8174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.947, F1 Micro: 0.8462, F1 Macro: 0.8286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9474, F1 Micro: 0.8479, F1 Macro: 0.8301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0698, Accuracy: 0.9453, F1 Micro: 0.8483, F1 Macro: 0.8298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.949, F1 Micro: 0.8569, F1 Macro: 0.8426\n",
      "Model 2 - Iteration 9290: Accuracy: 0.949, F1 Micro: 0.8569, F1 Macro: 0.8426\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.90      0.90      1245\n",
      "      Abusive       0.90      0.93      0.91      1228\n",
      "HS_Individual       0.81      0.82      0.82       591\n",
      "     HS_Group       0.88      0.84      0.86       654\n",
      "  HS_Religion       0.86      0.81      0.83       302\n",
      "      HS_Race       0.90      0.89      0.89       276\n",
      "  HS_Physical       0.90      0.84      0.87       262\n",
      "    HS_Gender       0.86      0.89      0.87       253\n",
      "     HS_Other       0.79      0.65      0.71       289\n",
      "      HS_Weak       0.76      0.78      0.77       478\n",
      "  HS_Moderate       0.83      0.76      0.80       530\n",
      "    HS_Strong       0.88      0.86      0.87       237\n",
      "\n",
      "    micro avg       0.86      0.85      0.86      6345\n",
      "    macro avg       0.86      0.83      0.84      6345\n",
      " weighted avg       0.86      0.85      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 190.87146854400635 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4198, Accuracy: 0.8834, F1 Micro: 0.6064, F1 Macro: 0.432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2782, Accuracy: 0.916, F1 Micro: 0.7418, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2176, Accuracy: 0.9308, F1 Micro: 0.7941, F1 Macro: 0.7505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1723, Accuracy: 0.9364, F1 Micro: 0.8158, F1 Macro: 0.7805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.9401, F1 Micro: 0.8309, F1 Macro: 0.8038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1154, Accuracy: 0.9442, F1 Micro: 0.8399, F1 Macro: 0.8194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.945, F1 Micro: 0.8426, F1 Macro: 0.8232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.946, F1 Micro: 0.8449, F1 Macro: 0.8285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.9468, F1 Micro: 0.8498, F1 Macro: 0.833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0592, Accuracy: 0.9466, F1 Micro: 0.8509, F1 Macro: 0.8351\n",
      "Model 3 - Iteration 9290: Accuracy: 0.9466, F1 Micro: 0.8509, F1 Macro: 0.8351\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.90      0.89      1245\n",
      "      Abusive       0.91      0.92      0.91      1228\n",
      "HS_Individual       0.80      0.82      0.81       591\n",
      "     HS_Group       0.86      0.84      0.85       654\n",
      "  HS_Religion       0.86      0.81      0.84       302\n",
      "      HS_Race       0.88      0.87      0.88       276\n",
      "  HS_Physical       0.88      0.85      0.87       262\n",
      "    HS_Gender       0.85      0.88      0.86       253\n",
      "     HS_Other       0.76      0.65      0.70       289\n",
      "      HS_Weak       0.75      0.77      0.76       478\n",
      "  HS_Moderate       0.81      0.78      0.80       530\n",
      "    HS_Strong       0.87      0.85      0.86       237\n",
      "\n",
      "    micro avg       0.85      0.85      0.85      6345\n",
      "    macro avg       0.84      0.83      0.84      6345\n",
      " weighted avg       0.85      0.85      0.85      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 190.83248376846313 s\n",
      "Averaged - Iteration 9290: Accuracy: 0.9479, F1 Micro: 0.8545, F1 Macro: 0.8399\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9918\n",
      "Acquired samples: 250\n",
      "Sampling duration: 28.717551469802856 seconds\n",
      "New train size: 9540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4093, Accuracy: 0.8889, F1 Micro: 0.6351, F1 Macro: 0.4997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9222, F1 Micro: 0.772, F1 Macro: 0.7238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2031, Accuracy: 0.9318, F1 Micro: 0.8013, F1 Macro: 0.7627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1647, Accuracy: 0.9377, F1 Micro: 0.8245, F1 Macro: 0.7959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9429, F1 Micro: 0.835, F1 Macro: 0.8135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9451, F1 Micro: 0.8427, F1 Macro: 0.8208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9452, F1 Micro: 0.8468, F1 Macro: 0.829\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9455, F1 Micro: 0.8441, F1 Macro: 0.8261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.9475, F1 Micro: 0.8528, F1 Macro: 0.8377\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.9475, F1 Micro: 0.8496, F1 Macro: 0.8372\n",
      "Model 1 - Iteration 9540: Accuracy: 0.9475, F1 Micro: 0.8528, F1 Macro: 0.8377\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.89      0.89      1245\n",
      "      Abusive       0.91      0.92      0.91      1228\n",
      "HS_Individual       0.81      0.81      0.81       591\n",
      "     HS_Group       0.87      0.85      0.86       654\n",
      "  HS_Religion       0.87      0.78      0.83       302\n",
      "      HS_Race       0.88      0.88      0.88       276\n",
      "  HS_Physical       0.87      0.82      0.85       262\n",
      "    HS_Gender       0.87      0.88      0.87       253\n",
      "     HS_Other       0.77      0.68      0.72       289\n",
      "      HS_Weak       0.77      0.77      0.77       478\n",
      "  HS_Moderate       0.82      0.76      0.79       530\n",
      "    HS_Strong       0.86      0.88      0.87       237\n",
      "\n",
      "    micro avg       0.86      0.85      0.85      6345\n",
      "    macro avg       0.85      0.83      0.84      6345\n",
      " weighted avg       0.86      0.85      0.85      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 191.92409372329712 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4154, Accuracy: 0.8871, F1 Micro: 0.6378, F1 Macro: 0.4906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2694, Accuracy: 0.9206, F1 Micro: 0.7583, F1 Macro: 0.6935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.9323, F1 Micro: 0.8028, F1 Macro: 0.7644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9359, F1 Micro: 0.8191, F1 Macro: 0.7847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9426, F1 Micro: 0.8329, F1 Macro: 0.8069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1139, Accuracy: 0.9456, F1 Micro: 0.8428, F1 Macro: 0.8176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0933, Accuracy: 0.9472, F1 Micro: 0.8491, F1 Macro: 0.8299\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9464, F1 Micro: 0.8474, F1 Macro: 0.8292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9483, F1 Micro: 0.8542, F1 Macro: 0.8381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9497, F1 Micro: 0.8555, F1 Macro: 0.8412\n",
      "Model 2 - Iteration 9540: Accuracy: 0.9497, F1 Micro: 0.8555, F1 Macro: 0.8412\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.88      0.90      1245\n",
      "      Abusive       0.91      0.91      0.91      1228\n",
      "HS_Individual       0.81      0.83      0.82       591\n",
      "     HS_Group       0.92      0.80      0.86       654\n",
      "  HS_Religion       0.91      0.73      0.81       302\n",
      "      HS_Race       0.94      0.85      0.89       276\n",
      "  HS_Physical       0.90      0.81      0.85       262\n",
      "    HS_Gender       0.87      0.89      0.88       253\n",
      "     HS_Other       0.80      0.69      0.74       289\n",
      "      HS_Weak       0.79      0.78      0.79       478\n",
      "  HS_Moderate       0.89      0.70      0.78       530\n",
      "    HS_Strong       0.88      0.89      0.88       237\n",
      "\n",
      "    micro avg       0.88      0.83      0.86      6345\n",
      "    macro avg       0.88      0.81      0.84      6345\n",
      " weighted avg       0.88      0.83      0.85      6345\n",
      "  samples avg       0.50      0.48      0.48      6345\n",
      "\n",
      "Training completed in 193.54431438446045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4143, Accuracy: 0.8856, F1 Micro: 0.6313, F1 Macro: 0.4836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2706, Accuracy: 0.9194, F1 Micro: 0.7515, F1 Macro: 0.6873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2091, Accuracy: 0.9302, F1 Micro: 0.7986, F1 Macro: 0.7536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1694, Accuracy: 0.9381, F1 Micro: 0.8242, F1 Macro: 0.7944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9417, F1 Micro: 0.8345, F1 Macro: 0.8087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1113, Accuracy: 0.9454, F1 Micro: 0.8436, F1 Macro: 0.8181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9467, F1 Micro: 0.8483, F1 Macro: 0.8278\n",
      "Epoch 8/10, Train Loss: 0.0797, Accuracy: 0.9451, F1 Micro: 0.8452, F1 Macro: 0.8282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.9467, F1 Micro: 0.851, F1 Macro: 0.8369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9474, F1 Micro: 0.853, F1 Macro: 0.837\n",
      "Model 3 - Iteration 9540: Accuracy: 0.9474, F1 Micro: 0.853, F1 Macro: 0.837\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.91      0.89      1245\n",
      "      Abusive       0.90      0.92      0.91      1228\n",
      "HS_Individual       0.77      0.86      0.81       591\n",
      "     HS_Group       0.91      0.81      0.86       654\n",
      "  HS_Religion       0.88      0.78      0.83       302\n",
      "      HS_Race       0.92      0.86      0.89       276\n",
      "  HS_Physical       0.90      0.84      0.87       262\n",
      "    HS_Gender       0.78      0.89      0.84       253\n",
      "     HS_Other       0.80      0.65      0.72       289\n",
      "      HS_Weak       0.73      0.81      0.77       478\n",
      "  HS_Moderate       0.86      0.73      0.79       530\n",
      "    HS_Strong       0.87      0.86      0.87       237\n",
      "\n",
      "    micro avg       0.86      0.85      0.85      6345\n",
      "    macro avg       0.85      0.83      0.84      6345\n",
      " weighted avg       0.86      0.85      0.85      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 193.28774762153625 s\n",
      "Averaged - Iteration 9540: Accuracy: 0.9482, F1 Micro: 0.8538, F1 Macro: 0.8386\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9918\n",
      "Acquired samples: 250\n",
      "Sampling duration: 24.95816707611084 seconds\n",
      "New train size: 9790\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4107, Accuracy: 0.891, F1 Micro: 0.6528, F1 Macro: 0.4934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2618, Accuracy: 0.9234, F1 Micro: 0.7685, F1 Macro: 0.7245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1999, Accuracy: 0.9337, F1 Micro: 0.8097, F1 Macro: 0.7764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1579, Accuracy: 0.9382, F1 Micro: 0.8287, F1 Macro: 0.8014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.944, F1 Micro: 0.8397, F1 Macro: 0.8134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9423, F1 Micro: 0.8399, F1 Macro: 0.8213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9477, F1 Micro: 0.8524, F1 Macro: 0.8359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9485, F1 Micro: 0.8542, F1 Macro: 0.838\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.9472, F1 Micro: 0.8541, F1 Macro: 0.8403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9504, F1 Micro: 0.8611, F1 Macro: 0.85\n",
      "Model 1 - Iteration 9790: Accuracy: 0.9504, F1 Micro: 0.8611, F1 Macro: 0.85\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.91      0.90      1245\n",
      "      Abusive       0.93      0.90      0.91      1228\n",
      "HS_Individual       0.82      0.82      0.82       591\n",
      "     HS_Group       0.85      0.86      0.85       654\n",
      "  HS_Religion       0.85      0.83      0.84       302\n",
      "      HS_Race       0.89      0.89      0.89       276\n",
      "  HS_Physical       0.92      0.86      0.89       262\n",
      "    HS_Gender       0.90      0.92      0.91       253\n",
      "     HS_Other       0.76      0.70      0.73       289\n",
      "      HS_Weak       0.80      0.77      0.78       478\n",
      "  HS_Moderate       0.80      0.80      0.80       530\n",
      "    HS_Strong       0.86      0.88      0.87       237\n",
      "\n",
      "    micro avg       0.87      0.86      0.86      6345\n",
      "    macro avg       0.86      0.84      0.85      6345\n",
      " weighted avg       0.86      0.86      0.86      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 197.80481243133545 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4156, Accuracy: 0.8878, F1 Micro: 0.6528, F1 Macro: 0.4975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9202, F1 Micro: 0.757, F1 Macro: 0.6996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.9329, F1 Micro: 0.8012, F1 Macro: 0.7631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1652, Accuracy: 0.9404, F1 Micro: 0.828, F1 Macro: 0.7954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1399, Accuracy: 0.9445, F1 Micro: 0.841, F1 Macro: 0.8159\n",
      "Epoch 6/10, Train Loss: 0.1096, Accuracy: 0.9407, F1 Micro: 0.8371, F1 Macro: 0.817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.9444, F1 Micro: 0.8456, F1 Macro: 0.8282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0767, Accuracy: 0.9495, F1 Micro: 0.857, F1 Macro: 0.8399\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9486, F1 Micro: 0.8564, F1 Macro: 0.8395\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.9481, F1 Micro: 0.8544, F1 Macro: 0.8393\n",
      "Model 2 - Iteration 9790: Accuracy: 0.9495, F1 Micro: 0.857, F1 Macro: 0.8399\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.90      0.90      1245\n",
      "      Abusive       0.92      0.91      0.92      1228\n",
      "HS_Individual       0.83      0.80      0.81       591\n",
      "     HS_Group       0.86      0.85      0.86       654\n",
      "  HS_Religion       0.87      0.81      0.84       302\n",
      "      HS_Race       0.91      0.84      0.87       276\n",
      "  HS_Physical       0.88      0.82      0.85       262\n",
      "    HS_Gender       0.88      0.90      0.89       253\n",
      "     HS_Other       0.81      0.62      0.70       289\n",
      "      HS_Weak       0.80      0.75      0.78       478\n",
      "  HS_Moderate       0.81      0.79      0.80       530\n",
      "    HS_Strong       0.88      0.84      0.86       237\n",
      "\n",
      "    micro avg       0.87      0.84      0.86      6345\n",
      "    macro avg       0.86      0.82      0.84      6345\n",
      " weighted avg       0.87      0.84      0.86      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 194.39959049224854 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4154, Accuracy: 0.8875, F1 Micro: 0.6305, F1 Macro: 0.4494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2683, Accuracy: 0.9179, F1 Micro: 0.7553, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2057, Accuracy: 0.9314, F1 Micro: 0.8028, F1 Macro: 0.7643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1644, Accuracy: 0.9385, F1 Micro: 0.8266, F1 Macro: 0.7947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9441, F1 Micro: 0.8409, F1 Macro: 0.8163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.9458, F1 Micro: 0.8441, F1 Macro: 0.8257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0907, Accuracy: 0.9467, F1 Micro: 0.8491, F1 Macro: 0.8325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9504, F1 Micro: 0.8572, F1 Macro: 0.8439\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.9489, F1 Micro: 0.8559, F1 Macro: 0.8426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0553, Accuracy: 0.949, F1 Micro: 0.8575, F1 Macro: 0.8441\n",
      "Model 3 - Iteration 9790: Accuracy: 0.949, F1 Micro: 0.8575, F1 Macro: 0.8441\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.91      0.90      1245\n",
      "      Abusive       0.93      0.90      0.92      1228\n",
      "HS_Individual       0.83      0.80      0.81       591\n",
      "     HS_Group       0.82      0.87      0.84       654\n",
      "  HS_Religion       0.85      0.83      0.84       302\n",
      "      HS_Race       0.90      0.90      0.90       276\n",
      "  HS_Physical       0.90      0.87      0.89       262\n",
      "    HS_Gender       0.87      0.91      0.89       253\n",
      "     HS_Other       0.77      0.65      0.71       289\n",
      "      HS_Weak       0.81      0.75      0.78       478\n",
      "  HS_Moderate       0.80      0.79      0.79       530\n",
      "    HS_Strong       0.83      0.90      0.86       237\n",
      "\n",
      "    micro avg       0.86      0.85      0.86      6345\n",
      "    macro avg       0.85      0.84      0.84      6345\n",
      " weighted avg       0.86      0.85      0.86      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 197.87006402015686 s\n",
      "Averaged - Iteration 9790: Accuracy: 0.9496, F1 Micro: 0.8585, F1 Macro: 0.8447\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9918\n",
      "Acquired samples: 128\n",
      "Sampling duration: 22.566660165786743 seconds\n",
      "New train size: 9918\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4077, Accuracy: 0.8935, F1 Micro: 0.6567, F1 Macro: 0.529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2609, Accuracy: 0.9226, F1 Micro: 0.7564, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1963, Accuracy: 0.9346, F1 Micro: 0.8056, F1 Macro: 0.7727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1615, Accuracy: 0.9405, F1 Micro: 0.8236, F1 Macro: 0.7957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9427, F1 Micro: 0.8319, F1 Macro: 0.8095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9415, F1 Micro: 0.8399, F1 Macro: 0.8204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9482, F1 Micro: 0.8532, F1 Macro: 0.836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.948, F1 Micro: 0.8532, F1 Macro: 0.8342\n",
      "Epoch 9/10, Train Loss: 0.0633, Accuracy: 0.9462, F1 Micro: 0.8528, F1 Macro: 0.8396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9488, F1 Micro: 0.8586, F1 Macro: 0.8453\n",
      "Model 1 - Iteration 9918: Accuracy: 0.9488, F1 Micro: 0.8586, F1 Macro: 0.8453\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.92      0.90      1245\n",
      "      Abusive       0.91      0.92      0.91      1228\n",
      "HS_Individual       0.82      0.83      0.82       591\n",
      "     HS_Group       0.83      0.87      0.85       654\n",
      "  HS_Religion       0.83      0.82      0.83       302\n",
      "      HS_Race       0.89      0.90      0.89       276\n",
      "  HS_Physical       0.87      0.86      0.87       262\n",
      "    HS_Gender       0.86      0.92      0.89       253\n",
      "     HS_Other       0.79      0.66      0.72       289\n",
      "      HS_Weak       0.77      0.79      0.78       478\n",
      "  HS_Moderate       0.78      0.81      0.80       530\n",
      "    HS_Strong       0.90      0.87      0.88       237\n",
      "\n",
      "    micro avg       0.85      0.86      0.86      6345\n",
      "    macro avg       0.84      0.85      0.85      6345\n",
      " weighted avg       0.85      0.86      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 199.52550554275513 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4143, Accuracy: 0.889, F1 Micro: 0.6538, F1 Macro: 0.4968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2683, Accuracy: 0.9186, F1 Micro: 0.7423, F1 Macro: 0.6742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9335, F1 Micro: 0.7988, F1 Macro: 0.7649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1667, Accuracy: 0.9398, F1 Micro: 0.8197, F1 Macro: 0.7867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.945, F1 Micro: 0.8384, F1 Macro: 0.8139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1107, Accuracy: 0.944, F1 Micro: 0.8434, F1 Macro: 0.8218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9483, F1 Micro: 0.8544, F1 Macro: 0.8358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0764, Accuracy: 0.9497, F1 Micro: 0.8549, F1 Macro: 0.8365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9485, F1 Micro: 0.8563, F1 Macro: 0.8412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9494, F1 Micro: 0.8597, F1 Macro: 0.8452\n",
      "Model 2 - Iteration 9918: Accuracy: 0.9494, F1 Micro: 0.8597, F1 Macro: 0.8452\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.92      0.90      1245\n",
      "      Abusive       0.92      0.92      0.92      1228\n",
      "HS_Individual       0.79      0.83      0.81       591\n",
      "     HS_Group       0.86      0.86      0.86       654\n",
      "  HS_Religion       0.85      0.81      0.83       302\n",
      "      HS_Race       0.87      0.89      0.88       276\n",
      "  HS_Physical       0.89      0.85      0.87       262\n",
      "    HS_Gender       0.88      0.91      0.89       253\n",
      "     HS_Other       0.80      0.67      0.73       289\n",
      "      HS_Weak       0.76      0.79      0.77       478\n",
      "  HS_Moderate       0.82      0.78      0.80       530\n",
      "    HS_Strong       0.86      0.88      0.87       237\n",
      "\n",
      "    micro avg       0.86      0.86      0.86      6345\n",
      "    macro avg       0.85      0.85      0.85      6345\n",
      " weighted avg       0.86      0.86      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 201.02657103538513 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.413, Accuracy: 0.8901, F1 Micro: 0.6517, F1 Macro: 0.5113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9192, F1 Micro: 0.7456, F1 Macro: 0.6791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2024, Accuracy: 0.9333, F1 Micro: 0.8024, F1 Macro: 0.7661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9408, F1 Micro: 0.8228, F1 Macro: 0.7933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1332, Accuracy: 0.9446, F1 Micro: 0.842, F1 Macro: 0.8209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9435, F1 Micro: 0.8443, F1 Macro: 0.8247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0904, Accuracy: 0.9488, F1 Micro: 0.8554, F1 Macro: 0.8373\n",
      "Epoch 8/10, Train Loss: 0.0744, Accuracy: 0.946, F1 Micro: 0.8514, F1 Macro: 0.8338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9502, F1 Micro: 0.8606, F1 Macro: 0.8472\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9482, F1 Micro: 0.8567, F1 Macro: 0.8436\n",
      "Model 3 - Iteration 9918: Accuracy: 0.9502, F1 Micro: 0.8606, F1 Macro: 0.8472\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.90      0.90      1245\n",
      "      Abusive       0.89      0.93      0.91      1228\n",
      "HS_Individual       0.86      0.80      0.83       591\n",
      "     HS_Group       0.85      0.87      0.86       654\n",
      "  HS_Religion       0.82      0.84      0.83       302\n",
      "      HS_Race       0.91      0.84      0.88       276\n",
      "  HS_Physical       0.89      0.85      0.87       262\n",
      "    HS_Gender       0.89      0.90      0.89       253\n",
      "     HS_Other       0.81      0.64      0.72       289\n",
      "      HS_Weak       0.81      0.76      0.79       478\n",
      "  HS_Moderate       0.80      0.83      0.81       530\n",
      "    HS_Strong       0.92      0.86      0.89       237\n",
      "\n",
      "    micro avg       0.87      0.86      0.86      6345\n",
      "    macro avg       0.86      0.83      0.85      6345\n",
      " weighted avg       0.87      0.86      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 198.80080461502075 s\n",
      "Averaged - Iteration 9918: Accuracy: 0.9495, F1 Micro: 0.8596, F1 Macro: 0.8459\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: \n",
      "11335Acquired samples: 250\n",
      "Sampling duration: 20.20791244506836 seconds\n",
      "New train size: 10168\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4037, Accuracy: 0.8921, F1 Micro: 0.6494, F1 Macro: 0.5172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2569, Accuracy: 0.9226, F1 Micro: 0.7836, F1 Macro: 0.7447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9342, F1 Micro: 0.8075, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1574, Accuracy: 0.9412, F1 Micro: 0.8306, F1 Macro: 0.8039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.9436, F1 Micro: 0.8393, F1 Macro: 0.813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9434, F1 Micro: 0.8417, F1 Macro: 0.8228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.086, Accuracy: 0.9484, F1 Micro: 0.8536, F1 Macro: 0.8352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.95, F1 Micro: 0.8582, F1 Macro: 0.8425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9517, F1 Micro: 0.8633, F1 Macro: 0.8499\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.9509, F1 Micro: 0.8602, F1 Macro: 0.848\n",
      "Model 1 - Iteration 10168: Accuracy: 0.9517, F1 Micro: 0.8633, F1 Macro: 0.8499\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.89      0.90      1245\n",
      "      Abusive       0.90      0.92      0.91      1228\n",
      "HS_Individual       0.83      0.83      0.83       591\n",
      "     HS_Group       0.89      0.84      0.87       654\n",
      "  HS_Religion       0.89      0.78      0.84       302\n",
      "      HS_Race       0.93      0.82      0.87       276\n",
      "  HS_Physical       0.87      0.89      0.88       262\n",
      "    HS_Gender       0.90      0.91      0.90       253\n",
      "     HS_Other       0.84      0.63      0.72       289\n",
      "      HS_Weak       0.80      0.81      0.80       478\n",
      "  HS_Moderate       0.84      0.78      0.81       530\n",
      "    HS_Strong       0.92      0.83      0.87       237\n",
      "\n",
      "    micro avg       0.88      0.85      0.86      6345\n",
      "    macro avg       0.88      0.83      0.85      6345\n",
      " weighted avg       0.88      0.85      0.86      6345\n",
      "  samples avg       0.50      0.49      0.49      6345\n",
      "\n",
      "Training completed in 204.6541759967804 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4106, Accuracy: 0.8925, F1 Micro: 0.6523, F1 Macro: 0.5102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2618, Accuracy: 0.9234, F1 Micro: 0.7808, F1 Macro: 0.7348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2015, Accuracy: 0.9335, F1 Micro: 0.8089, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1635, Accuracy: 0.941, F1 Micro: 0.8275, F1 Macro: 0.7963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1316, Accuracy: 0.9421, F1 Micro: 0.8339, F1 Macro: 0.8109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9465, F1 Micro: 0.8502, F1 Macro: 0.8321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0896, Accuracy: 0.9461, F1 Micro: 0.8513, F1 Macro: 0.8356\n",
      "Epoch 8/10, Train Loss: 0.0753, Accuracy: 0.9453, F1 Micro: 0.8501, F1 Macro: 0.8355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9511, F1 Micro: 0.861, F1 Macro: 0.8478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.952, F1 Micro: 0.8649, F1 Macro: 0.8522\n",
      "Model 2 - Iteration 10168: Accuracy: 0.952, F1 Micro: 0.8649, F1 Macro: 0.8522\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.90      0.90      1245\n",
      "      Abusive       0.91      0.92      0.91      1228\n",
      "HS_Individual       0.83      0.82      0.83       591\n",
      "     HS_Group       0.88      0.86      0.87       654\n",
      "  HS_Religion       0.88      0.79      0.83       302\n",
      "      HS_Race       0.90      0.89      0.90       276\n",
      "  HS_Physical       0.91      0.85      0.88       262\n",
      "    HS_Gender       0.89      0.91      0.90       253\n",
      "     HS_Other       0.81      0.67      0.73       289\n",
      "      HS_Weak       0.79      0.78      0.79       478\n",
      "  HS_Moderate       0.83      0.78      0.80       530\n",
      "    HS_Strong       0.88      0.87      0.88       237\n",
      "\n",
      "    micro avg       0.87      0.86      0.86      6345\n",
      "    macro avg       0.87      0.84      0.85      6345\n",
      " weighted avg       0.87      0.86      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 204.88757753372192 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4079, Accuracy: 0.8916, F1 Micro: 0.66, F1 Macro: 0.5332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.263, Accuracy: 0.9222, F1 Micro: 0.773, F1 Macro: 0.7117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.9344, F1 Micro: 0.8107, F1 Macro: 0.7738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.9418, F1 Micro: 0.833, F1 Macro: 0.8058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1303, Accuracy: 0.9444, F1 Micro: 0.8421, F1 Macro: 0.8148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1063, Accuracy: 0.9439, F1 Micro: 0.8439, F1 Macro: 0.8255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.9467, F1 Micro: 0.8515, F1 Macro: 0.8345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0751, Accuracy: 0.9479, F1 Micro: 0.8547, F1 Macro: 0.841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0621, Accuracy: 0.9497, F1 Micro: 0.8585, F1 Macro: 0.8454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9514, F1 Micro: 0.8633, F1 Macro: 0.8511\n",
      "Model 3 - Iteration 10168: Accuracy: 0.9514, F1 Micro: 0.8633, F1 Macro: 0.8511\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.90      0.90      1245\n",
      "      Abusive       0.91      0.92      0.91      1228\n",
      "HS_Individual       0.84      0.80      0.82       591\n",
      "     HS_Group       0.85      0.87      0.86       654\n",
      "  HS_Religion       0.85      0.82      0.84       302\n",
      "      HS_Race       0.88      0.90      0.89       276\n",
      "  HS_Physical       0.92      0.84      0.88       262\n",
      "    HS_Gender       0.92      0.90      0.91       253\n",
      "     HS_Other       0.83      0.67      0.74       289\n",
      "      HS_Weak       0.81      0.77      0.79       478\n",
      "  HS_Moderate       0.81      0.80      0.81       530\n",
      "    HS_Strong       0.87      0.85      0.86       237\n",
      "\n",
      "    micro avg       0.87      0.85      0.86      6345\n",
      "    macro avg       0.87      0.84      0.85      6345\n",
      " weighted avg       0.87      0.85      0.86      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 206.69278812408447 s\n",
      "Averaged - Iteration 10168: Accuracy: 0.9517, F1 Micro: 0.8638, F1 Macro: 0.851\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 11335\n",
      "Acquired samples: 250\n",
      "Sampling duration: 15.834604024887085 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.8939, F1 Micro: 0.6546, F1 Macro: 0.5341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2507, Accuracy: 0.9254, F1 Micro: 0.7712, F1 Macro: 0.7214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.9336, F1 Micro: 0.8096, F1 Macro: 0.7822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1553, Accuracy: 0.943, F1 Micro: 0.836, F1 Macro: 0.812\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.939, F1 Micro: 0.834, F1 Macro: 0.8132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9441, F1 Micro: 0.846, F1 Macro: 0.8267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9462, F1 Micro: 0.8506, F1 Macro: 0.8329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.946, F1 Micro: 0.8525, F1 Macro: 0.8373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9506, F1 Micro: 0.8592, F1 Macro: 0.8457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.9487, F1 Micro: 0.8597, F1 Macro: 0.8449\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9487, F1 Micro: 0.8597, F1 Macro: 0.8449\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.92      0.90      1245\n",
      "      Abusive       0.88      0.95      0.91      1228\n",
      "HS_Individual       0.78      0.87      0.82       591\n",
      "     HS_Group       0.88      0.85      0.87       654\n",
      "  HS_Religion       0.81      0.86      0.83       302\n",
      "      HS_Race       0.89      0.87      0.88       276\n",
      "  HS_Physical       0.88      0.88      0.88       262\n",
      "    HS_Gender       0.84      0.94      0.89       253\n",
      "     HS_Other       0.80      0.63      0.70       289\n",
      "      HS_Weak       0.72      0.84      0.78       478\n",
      "  HS_Moderate       0.82      0.79      0.80       530\n",
      "    HS_Strong       0.90      0.85      0.87       237\n",
      "\n",
      "    micro avg       0.84      0.88      0.86      6345\n",
      "    macro avg       0.84      0.85      0.84      6345\n",
      " weighted avg       0.85      0.88      0.86      6345\n",
      "  samples avg       0.51      0.51      0.50      6345\n",
      "\n",
      "Training completed in 209.33493876457214 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4068, Accuracy: 0.8905, F1 Micro: 0.6289, F1 Macro: 0.5023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2576, Accuracy: 0.923, F1 Micro: 0.7596, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2006, Accuracy: 0.9341, F1 Micro: 0.8022, F1 Macro: 0.7701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1605, Accuracy: 0.9418, F1 Micro: 0.8316, F1 Macro: 0.8034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9415, F1 Micro: 0.8386, F1 Macro: 0.817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1092, Accuracy: 0.9477, F1 Micro: 0.8492, F1 Macro: 0.8304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.9477, F1 Micro: 0.8556, F1 Macro: 0.8378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.9518, F1 Micro: 0.8629, F1 Macro: 0.8467\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9482, F1 Micro: 0.856, F1 Macro: 0.8426\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9474, F1 Micro: 0.8572, F1 Macro: 0.8426\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9518, F1 Micro: 0.8629, F1 Macro: 0.8467\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.90      0.90      1245\n",
      "      Abusive       0.91      0.91      0.91      1228\n",
      "HS_Individual       0.84      0.83      0.83       591\n",
      "     HS_Group       0.89      0.85      0.87       654\n",
      "  HS_Religion       0.86      0.78      0.82       302\n",
      "      HS_Race       0.94      0.81      0.87       276\n",
      "  HS_Physical       0.91      0.82      0.86       262\n",
      "    HS_Gender       0.89      0.91      0.90       253\n",
      "     HS_Other       0.81      0.66      0.72       289\n",
      "      HS_Weak       0.80      0.77      0.78       478\n",
      "  HS_Moderate       0.85      0.77      0.81       530\n",
      "    HS_Strong       0.89      0.85      0.87       237\n",
      "\n",
      "    micro avg       0.88      0.85      0.86      6345\n",
      "    macro avg       0.87      0.82      0.85      6345\n",
      " weighted avg       0.88      0.85      0.86      6345\n",
      "  samples avg       0.50      0.49      0.49      6345\n",
      "\n",
      "Training completed in 207.76242184638977 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4045, Accuracy: 0.8918, F1 Micro: 0.6499, F1 Macro: 0.53\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2568, Accuracy: 0.9207, F1 Micro: 0.7496, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.9337, F1 Micro: 0.8044, F1 Macro: 0.7705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1576, Accuracy: 0.9428, F1 Micro: 0.8335, F1 Macro: 0.8052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1267, Accuracy: 0.941, F1 Micro: 0.8376, F1 Macro: 0.8149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.107, Accuracy: 0.9488, F1 Micro: 0.8527, F1 Macro: 0.8333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9502, F1 Micro: 0.857, F1 Macro: 0.8377\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.9459, F1 Micro: 0.8514, F1 Macro: 0.8373\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.9457, F1 Micro: 0.8499, F1 Macro: 0.8368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9501, F1 Micro: 0.8624, F1 Macro: 0.849\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9501, F1 Micro: 0.8624, F1 Macro: 0.849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.92      0.90      1245\n",
      "      Abusive       0.90      0.93      0.92      1228\n",
      "HS_Individual       0.80      0.85      0.82       591\n",
      "     HS_Group       0.88      0.85      0.87       654\n",
      "  HS_Religion       0.83      0.84      0.83       302\n",
      "      HS_Race       0.89      0.89      0.89       276\n",
      "  HS_Physical       0.86      0.90      0.88       262\n",
      "    HS_Gender       0.89      0.91      0.90       253\n",
      "     HS_Other       0.78      0.64      0.70       289\n",
      "      HS_Weak       0.75      0.82      0.78       478\n",
      "  HS_Moderate       0.83      0.79      0.81       530\n",
      "    HS_Strong       0.89      0.88      0.88       237\n",
      "\n",
      "    micro avg       0.85      0.87      0.86      6345\n",
      "    macro avg       0.85      0.85      0.85      6345\n",
      " weighted avg       0.85      0.87      0.86      6345\n",
      "  samples avg       0.51      0.51      0.49      6345\n",
      "\n",
      "Training completed in 208.01260590553284 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9502, F1 Micro: 0.8617, F1 Macro: 0.8468\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 11335\n",
      "Acquired samples: 250\n",
      "Sampling duration: 13.977734804153442 seconds\n",
      "New train size: 10668\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3981, Accuracy: 0.8949, F1 Micro: 0.6464, F1 Macro: 0.5061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.9271, F1 Micro: 0.7755, F1 Macro: 0.7266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1864, Accuracy: 0.9342, F1 Micro: 0.7981, F1 Macro: 0.7598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1527, Accuracy: 0.9417, F1 Micro: 0.8297, F1 Macro: 0.8018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9445, F1 Micro: 0.844, F1 Macro: 0.8206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9433, F1 Micro: 0.8444, F1 Macro: 0.8277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.9453, F1 Micro: 0.8497, F1 Macro: 0.8335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9506, F1 Micro: 0.8604, F1 Macro: 0.8463\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9466, F1 Micro: 0.8547, F1 Macro: 0.8409\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9455, F1 Micro: 0.8518, F1 Macro: 0.8386\n",
      "Model 1 - Iteration 10668: Accuracy: 0.9506, F1 Micro: 0.8604, F1 Macro: 0.8463\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.89      0.90      1245\n",
      "      Abusive       0.91      0.92      0.91      1228\n",
      "HS_Individual       0.84      0.80      0.82       591\n",
      "     HS_Group       0.86      0.86      0.86       654\n",
      "  HS_Religion       0.84      0.81      0.83       302\n",
      "      HS_Race       0.90      0.86      0.88       276\n",
      "  HS_Physical       0.89      0.83      0.86       262\n",
      "    HS_Gender       0.92      0.89      0.91       253\n",
      "     HS_Other       0.84      0.64      0.72       289\n",
      "      HS_Weak       0.79      0.77      0.78       478\n",
      "  HS_Moderate       0.82      0.80      0.81       530\n",
      "    HS_Strong       0.91      0.86      0.88       237\n",
      "\n",
      "    micro avg       0.87      0.85      0.86      6345\n",
      "    macro avg       0.87      0.83      0.85      6345\n",
      " weighted avg       0.87      0.85      0.86      6345\n",
      "  samples avg       0.50      0.49      0.49      6345\n",
      "\n",
      "Training completed in 212.14562892913818 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4054, Accuracy: 0.8878, F1 Micro: 0.6036, F1 Macro: 0.4411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.922, F1 Micro: 0.7535, F1 Macro: 0.6867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1932, Accuracy: 0.9346, F1 Micro: 0.8036, F1 Macro: 0.7626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1577, Accuracy: 0.9401, F1 Micro: 0.8315, F1 Macro: 0.8027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9385, F1 Micro: 0.8325, F1 Macro: 0.8076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9452, F1 Micro: 0.8459, F1 Macro: 0.8276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0839, Accuracy: 0.9469, F1 Micro: 0.851, F1 Macro: 0.8331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.95, F1 Micro: 0.8582, F1 Macro: 0.8419\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.947, F1 Micro: 0.8537, F1 Macro: 0.8398\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9488, F1 Micro: 0.8575, F1 Macro: 0.8448\n",
      "Model 2 - Iteration 10668: Accuracy: 0.95, F1 Micro: 0.8582, F1 Macro: 0.8419\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.90      0.90      1245\n",
      "      Abusive       0.92      0.91      0.91      1228\n",
      "HS_Individual       0.85      0.79      0.82       591\n",
      "     HS_Group       0.85      0.87      0.86       654\n",
      "  HS_Religion       0.85      0.79      0.82       302\n",
      "      HS_Race       0.92      0.84      0.88       276\n",
      "  HS_Physical       0.88      0.83      0.85       262\n",
      "    HS_Gender       0.91      0.89      0.90       253\n",
      "     HS_Other       0.81      0.61      0.69       289\n",
      "      HS_Weak       0.81      0.75      0.78       478\n",
      "  HS_Moderate       0.79      0.81      0.80       530\n",
      "    HS_Strong       0.93      0.84      0.88       237\n",
      "\n",
      "    micro avg       0.87      0.84      0.86      6345\n",
      "    macro avg       0.87      0.82      0.84      6345\n",
      " weighted avg       0.87      0.84      0.86      6345\n",
      "  samples avg       0.50      0.49      0.48      6345\n",
      "\n",
      "Training completed in 211.51965594291687 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4038, Accuracy: 0.8896, F1 Micro: 0.6305, F1 Macro: 0.4753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2556, Accuracy: 0.9215, F1 Micro: 0.7549, F1 Macro: 0.695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1925, Accuracy: 0.9341, F1 Micro: 0.7989, F1 Macro: 0.7587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9406, F1 Micro: 0.8308, F1 Macro: 0.804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.944, F1 Micro: 0.8426, F1 Macro: 0.8172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0995, Accuracy: 0.9465, F1 Micro: 0.8478, F1 Macro: 0.8327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9472, F1 Micro: 0.8529, F1 Macro: 0.8377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0676, Accuracy: 0.9512, F1 Micro: 0.8628, F1 Macro: 0.8487\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.95, F1 Micro: 0.8581, F1 Macro: 0.8457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9519, F1 Micro: 0.8643, F1 Macro: 0.8542\n",
      "Model 3 - Iteration 10668: Accuracy: 0.9519, F1 Micro: 0.8643, F1 Macro: 0.8542\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.90      0.90      1245\n",
      "      Abusive       0.92      0.89      0.91      1228\n",
      "HS_Individual       0.82      0.84      0.83       591\n",
      "     HS_Group       0.89      0.84      0.86       654\n",
      "  HS_Religion       0.87      0.81      0.84       302\n",
      "      HS_Race       0.91      0.89      0.90       276\n",
      "  HS_Physical       0.93      0.85      0.89       262\n",
      "    HS_Gender       0.92      0.89      0.91       253\n",
      "     HS_Other       0.78      0.69      0.73       289\n",
      "      HS_Weak       0.78      0.80      0.79       478\n",
      "  HS_Moderate       0.86      0.77      0.81       530\n",
      "    HS_Strong       0.88      0.89      0.88       237\n",
      "\n",
      "    micro avg       0.88      0.85      0.86      6345\n",
      "    macro avg       0.87      0.84      0.85      6345\n",
      " weighted avg       0.88      0.85      0.86      6345\n",
      "  samples avg       0.49      0.49      0.48      6345\n",
      "\n",
      "Training completed in 213.2947120666504 s\n",
      "Averaged - Iteration 10668: Accuracy: 0.9508, F1 Micro: 0.861, F1 Macro: 0.8475\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 11335\n",
      "Acquired samples: 250\n",
      "Sampling duration: 10.492603063583374 seconds\n",
      "New train size: 10918\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.896, F1 Micro: 0.6478, F1 Macro: 0.4982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2407, Accuracy: 0.9243, F1 Micro: 0.7806, F1 Macro: 0.7301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9371, F1 Micro: 0.8127, F1 Macro: 0.7797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9403, F1 Micro: 0.8331, F1 Macro: 0.8063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.944, F1 Micro: 0.8424, F1 Macro: 0.8176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9435, F1 Micro: 0.8436, F1 Macro: 0.8199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0808, Accuracy: 0.95, F1 Micro: 0.8566, F1 Macro: 0.8411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9498, F1 Micro: 0.8576, F1 Macro: 0.844\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.948, F1 Micro: 0.8552, F1 Macro: 0.8406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9516, F1 Micro: 0.863, F1 Macro: 0.8504\n",
      "Model 1 - Iteration 10918: Accuracy: 0.9516, F1 Micro: 0.863, F1 Macro: 0.8504\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.89      0.90      1245\n",
      "      Abusive       0.92      0.91      0.92      1228\n",
      "HS_Individual       0.83      0.82      0.83       591\n",
      "     HS_Group       0.87      0.84      0.86       654\n",
      "  HS_Religion       0.85      0.83      0.84       302\n",
      "      HS_Race       0.92      0.86      0.89       276\n",
      "  HS_Physical       0.93      0.81      0.87       262\n",
      "    HS_Gender       0.90      0.92      0.91       253\n",
      "     HS_Other       0.82      0.66      0.73       289\n",
      "      HS_Weak       0.79      0.77      0.78       478\n",
      "  HS_Moderate       0.83      0.78      0.80       530\n",
      "    HS_Strong       0.90      0.88      0.89       237\n",
      "\n",
      "    micro avg       0.88      0.85      0.86      6345\n",
      "    macro avg       0.87      0.83      0.85      6345\n",
      " weighted avg       0.88      0.85      0.86      6345\n",
      "  samples avg       0.50      0.49      0.48      6345\n",
      "\n",
      "Training completed in 216.6054549217224 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3986, Accuracy: 0.8967, F1 Micro: 0.6579, F1 Macro: 0.5214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2459, Accuracy: 0.9254, F1 Micro: 0.7789, F1 Macro: 0.7251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.19, Accuracy: 0.9372, F1 Micro: 0.8117, F1 Macro: 0.7751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9418, F1 Micro: 0.8362, F1 Macro: 0.8113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9456, F1 Micro: 0.846, F1 Macro: 0.8231\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9464, F1 Micro: 0.8456, F1 Macro: 0.8225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9489, F1 Micro: 0.8536, F1 Macro: 0.8347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9515, F1 Micro: 0.8614, F1 Macro: 0.8469\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9485, F1 Micro: 0.8569, F1 Macro: 0.8405\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9494, F1 Micro: 0.8592, F1 Macro: 0.8429\n",
      "Model 2 - Iteration 10918: Accuracy: 0.9515, F1 Micro: 0.8614, F1 Macro: 0.8469\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.90      0.90      1245\n",
      "      Abusive       0.93      0.89      0.91      1228\n",
      "HS_Individual       0.82      0.82      0.82       591\n",
      "     HS_Group       0.89      0.83      0.86       654\n",
      "  HS_Religion       0.87      0.81      0.84       302\n",
      "      HS_Race       0.93      0.83      0.88       276\n",
      "  HS_Physical       0.92      0.83      0.87       262\n",
      "    HS_Gender       0.91      0.89      0.90       253\n",
      "     HS_Other       0.79      0.63      0.70       289\n",
      "      HS_Weak       0.79      0.78      0.79       478\n",
      "  HS_Moderate       0.86      0.76      0.81       530\n",
      "    HS_Strong       0.89      0.86      0.88       237\n",
      "\n",
      "    micro avg       0.88      0.84      0.86      6345\n",
      "    macro avg       0.88      0.82      0.85      6345\n",
      " weighted avg       0.88      0.84      0.86      6345\n",
      "  samples avg       0.50      0.49      0.48      6345\n",
      "\n",
      "Training completed in 214.89724254608154 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3979, Accuracy: 0.8906, F1 Micro: 0.6246, F1 Macro: 0.4852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9237, F1 Micro: 0.7718, F1 Macro: 0.7131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1913, Accuracy: 0.9367, F1 Micro: 0.811, F1 Macro: 0.7738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1494, Accuracy: 0.94, F1 Micro: 0.8315, F1 Macro: 0.8062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1234, Accuracy: 0.9414, F1 Micro: 0.8382, F1 Macro: 0.8145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9451, F1 Micro: 0.8463, F1 Macro: 0.8205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.9485, F1 Micro: 0.8509, F1 Macro: 0.8338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9501, F1 Micro: 0.8582, F1 Macro: 0.8433\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9483, F1 Micro: 0.8568, F1 Macro: 0.8434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9525, F1 Micro: 0.8659, F1 Macro: 0.8534\n",
      "Model 3 - Iteration 10918: Accuracy: 0.9525, F1 Micro: 0.8659, F1 Macro: 0.8534\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.90      0.90      1245\n",
      "      Abusive       0.93      0.91      0.92      1228\n",
      "HS_Individual       0.81      0.84      0.82       591\n",
      "     HS_Group       0.90      0.84      0.87       654\n",
      "  HS_Religion       0.87      0.82      0.85       302\n",
      "      HS_Race       0.92      0.87      0.89       276\n",
      "  HS_Physical       0.92      0.84      0.88       262\n",
      "    HS_Gender       0.89      0.92      0.91       253\n",
      "     HS_Other       0.79      0.69      0.73       289\n",
      "      HS_Weak       0.77      0.80      0.79       478\n",
      "  HS_Moderate       0.86      0.78      0.82       530\n",
      "    HS_Strong       0.88      0.87      0.88       237\n",
      "\n",
      "    micro avg       0.88      0.85      0.87      6345\n",
      "    macro avg       0.87      0.84      0.85      6345\n",
      " weighted avg       0.88      0.85      0.87      6345\n",
      "  samples avg       0.50      0.49      0.49      6345\n",
      "\n",
      "Training completed in 216.82444620132446 s\n",
      "Averaged - Iteration 10918: Accuracy: 0.9518, F1 Micro: 0.8634, F1 Macro: 0.8503\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 11335\n",
      "Acquired samples: 250\n",
      "Sampling duration: 6.045680284500122 seconds\n",
      "New train size: 11168\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3869, Accuracy: 0.897, F1 Micro: 0.664, F1 Macro: 0.5425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2378, Accuracy: 0.9265, F1 Micro: 0.7725, F1 Macro: 0.7225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1808, Accuracy: 0.9342, F1 Micro: 0.8005, F1 Macro: 0.7625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1459, Accuracy: 0.9416, F1 Micro: 0.8255, F1 Macro: 0.8024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1166, Accuracy: 0.9456, F1 Micro: 0.8457, F1 Macro: 0.8234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0942, Accuracy: 0.9489, F1 Micro: 0.8552, F1 Macro: 0.8341\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9481, F1 Micro: 0.8551, F1 Macro: 0.8375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.9492, F1 Micro: 0.8589, F1 Macro: 0.8447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9519, F1 Micro: 0.8649, F1 Macro: 0.8507\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.9497, F1 Micro: 0.8604, F1 Macro: 0.8473\n",
      "Model 1 - Iteration 11168: Accuracy: 0.9519, F1 Micro: 0.8649, F1 Macro: 0.8507\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.90      0.91      0.90      1245\n",
      "      Abusive       0.91      0.93      0.92      1228\n",
      "HS_Individual       0.84      0.81      0.82       591\n",
      "     HS_Group       0.86      0.87      0.86       654\n",
      "  HS_Religion       0.84      0.84      0.84       302\n",
      "      HS_Race       0.91      0.86      0.88       276\n",
      "  HS_Physical       0.93      0.85      0.89       262\n",
      "    HS_Gender       0.90      0.90      0.90       253\n",
      "     HS_Other       0.79      0.64      0.71       289\n",
      "      HS_Weak       0.79      0.78      0.79       478\n",
      "  HS_Moderate       0.82      0.79      0.81       530\n",
      "    HS_Strong       0.89      0.88      0.88       237\n",
      "\n",
      "    micro avg       0.87      0.86      0.86      6345\n",
      "    macro avg       0.86      0.84      0.85      6345\n",
      " weighted avg       0.87      0.86      0.86      6345\n",
      "  samples avg       0.51      0.50      0.49      6345\n",
      "\n",
      "Training completed in 219.30939984321594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8946, F1 Micro: 0.6453, F1 Macro: 0.5089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2421, Accuracy: 0.9261, F1 Micro: 0.7763, F1 Macro: 0.7238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1846, Accuracy: 0.934, F1 Micro: 0.8006, F1 Macro: 0.7628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9412, F1 Micro: 0.8207, F1 Macro: 0.7962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1194, Accuracy: 0.9469, F1 Micro: 0.8452, F1 Macro: 0.8201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.9499, F1 Micro: 0.8574, F1 Macro: 0.8371\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9484, F1 Micro: 0.8554, F1 Macro: 0.8377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9495, F1 Micro: 0.8592, F1 Macro: 0.8438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9544, F1 Micro: 0.87, F1 Macro: 0.8572\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9491, F1 Micro: 0.8607, F1 Macro: 0.8489\n",
      "Model 2 - Iteration 11168: Accuracy: 0.9544, F1 Micro: 0.87, F1 Macro: 0.8572\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.91      0.90      0.90      1245\n",
      "      Abusive       0.92      0.91      0.92      1228\n",
      "HS_Individual       0.84      0.82      0.83       591\n",
      "     HS_Group       0.91      0.85      0.88       654\n",
      "  HS_Religion       0.88      0.82      0.85       302\n",
      "      HS_Race       0.92      0.87      0.89       276\n",
      "  HS_Physical       0.92      0.86      0.89       262\n",
      "    HS_Gender       0.89      0.92      0.91       253\n",
      "     HS_Other       0.82      0.65      0.73       289\n",
      "      HS_Weak       0.82      0.77      0.79       478\n",
      "  HS_Moderate       0.86      0.77      0.82       530\n",
      "    HS_Strong       0.89      0.88      0.88       237\n",
      "\n",
      "    micro avg       0.89      0.85      0.87      6345\n",
      "    macro avg       0.88      0.83      0.86      6345\n",
      " weighted avg       0.89      0.85      0.87      6345\n",
      "  samples avg       0.50      0.49      0.49      6345\n",
      "\n",
      "Training completed in 219.46955490112305 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3918, Accuracy: 0.8939, F1 Micro: 0.6525, F1 Macro: 0.5078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2446, Accuracy: 0.9243, F1 Micro: 0.7685, F1 Macro: 0.7056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9346, F1 Micro: 0.7992, F1 Macro: 0.7606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1495, Accuracy: 0.9418, F1 Micro: 0.824, F1 Macro: 0.7994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1178, Accuracy: 0.9472, F1 Micro: 0.8499, F1 Macro: 0.8248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9491, F1 Micro: 0.8546, F1 Macro: 0.8329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0808, Accuracy: 0.9508, F1 Micro: 0.8602, F1 Macro: 0.8446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9513, F1 Micro: 0.8614, F1 Macro: 0.848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9511, F1 Micro: 0.8633, F1 Macro: 0.8504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9524, F1 Micro: 0.8666, F1 Macro: 0.854\n",
      "Model 3 - Iteration 11168: Accuracy: 0.9524, F1 Micro: 0.8666, F1 Macro: 0.854\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.91      0.90      1245\n",
      "      Abusive       0.92      0.92      0.92      1228\n",
      "HS_Individual       0.80      0.84      0.82       591\n",
      "     HS_Group       0.89      0.84      0.86       654\n",
      "  HS_Religion       0.89      0.80      0.84       302\n",
      "      HS_Race       0.92      0.89      0.91       276\n",
      "  HS_Physical       0.89      0.89      0.89       262\n",
      "    HS_Gender       0.87      0.92      0.89       253\n",
      "     HS_Other       0.79      0.66      0.72       289\n",
      "      HS_Weak       0.75      0.81      0.78       478\n",
      "  HS_Moderate       0.85      0.79      0.82       530\n",
      "    HS_Strong       0.93      0.85      0.89       237\n",
      "\n",
      "    micro avg       0.87      0.86      0.87      6345\n",
      "    macro avg       0.87      0.84      0.85      6345\n",
      " weighted avg       0.87      0.86      0.87      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 222.49879574775696 s\n",
      "Averaged - Iteration 11168: Accuracy: 0.9529, F1 Micro: 0.8672, F1 Macro: 0.854\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 11335\n",
      "Acquired samples: 167\n",
      "Sampling duration: 3.772599458694458 seconds\n",
      "New train size: 11335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.8917, F1 Micro: 0.6207, F1 Macro: 0.4641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2383, Accuracy: 0.9262, F1 Micro: 0.7799, F1 Macro: 0.7307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1787, Accuracy: 0.9339, F1 Micro: 0.8148, F1 Macro: 0.7829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9412, F1 Micro: 0.8289, F1 Macro: 0.8011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1138, Accuracy: 0.944, F1 Micro: 0.8418, F1 Macro: 0.8197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9457, F1 Micro: 0.8491, F1 Macro: 0.8298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9471, F1 Micro: 0.8553, F1 Macro: 0.8392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9492, F1 Micro: 0.8597, F1 Macro: 0.8443\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9479, F1 Micro: 0.8591, F1 Macro: 0.8472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.9517, F1 Micro: 0.8656, F1 Macro: 0.8549\n",
      "Model 1 - Iteration 11335: Accuracy: 0.9517, F1 Micro: 0.8656, F1 Macro: 0.8549\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.92      0.90      1245\n",
      "      Abusive       0.92      0.92      0.92      1228\n",
      "HS_Individual       0.79      0.84      0.82       591\n",
      "     HS_Group       0.88      0.85      0.87       654\n",
      "  HS_Religion       0.87      0.84      0.85       302\n",
      "      HS_Race       0.90      0.88      0.89       276\n",
      "  HS_Physical       0.89      0.89      0.89       262\n",
      "    HS_Gender       0.89      0.91      0.90       253\n",
      "     HS_Other       0.79      0.67      0.73       289\n",
      "      HS_Weak       0.74      0.81      0.77       478\n",
      "  HS_Moderate       0.84      0.80      0.82       530\n",
      "    HS_Strong       0.92      0.89      0.90       237\n",
      "\n",
      "    micro avg       0.87      0.87      0.87      6345\n",
      "    macro avg       0.86      0.85      0.85      6345\n",
      " weighted avg       0.87      0.87      0.87      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 222.8378450870514 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3886, Accuracy: 0.8909, F1 Micro: 0.6145, F1 Macro: 0.4536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2437, Accuracy: 0.9258, F1 Micro: 0.7782, F1 Macro: 0.7254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.935, F1 Micro: 0.8146, F1 Macro: 0.7804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1451, Accuracy: 0.9415, F1 Micro: 0.8265, F1 Macro: 0.7991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9456, F1 Micro: 0.8416, F1 Macro: 0.8159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9449, F1 Micro: 0.8473, F1 Macro: 0.8268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9483, F1 Micro: 0.8551, F1 Macro: 0.8368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9507, F1 Micro: 0.8615, F1 Macro: 0.8464\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9485, F1 Micro: 0.8556, F1 Macro: 0.8411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9527, F1 Micro: 0.8674, F1 Macro: 0.8563\n",
      "Model 2 - Iteration 11335: Accuracy: 0.9527, F1 Micro: 0.8674, F1 Macro: 0.8563\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.91      0.90      1245\n",
      "      Abusive       0.93      0.91      0.92      1228\n",
      "HS_Individual       0.81      0.83      0.82       591\n",
      "     HS_Group       0.87      0.85      0.86       654\n",
      "  HS_Religion       0.86      0.81      0.84       302\n",
      "      HS_Race       0.90      0.89      0.90       276\n",
      "  HS_Physical       0.90      0.89      0.89       262\n",
      "    HS_Gender       0.90      0.92      0.91       253\n",
      "     HS_Other       0.78      0.68      0.73       289\n",
      "      HS_Weak       0.78      0.80      0.79       478\n",
      "  HS_Moderate       0.84      0.79      0.81       530\n",
      "    HS_Strong       0.92      0.87      0.90       237\n",
      "\n",
      "    micro avg       0.87      0.86      0.87      6345\n",
      "    macro avg       0.87      0.85      0.86      6345\n",
      " weighted avg       0.87      0.86      0.87      6345\n",
      "  samples avg       0.50      0.50      0.49      6345\n",
      "\n",
      "Training completed in 223.13217306137085 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8843, F1 Micro: 0.5716, F1 Macro: 0.3946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2452, Accuracy: 0.9233, F1 Micro: 0.7706, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1839, Accuracy: 0.9353, F1 Micro: 0.8155, F1 Macro: 0.7821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1432, Accuracy: 0.941, F1 Micro: 0.8235, F1 Macro: 0.7916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9459, F1 Micro: 0.843, F1 Macro: 0.8199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9469, F1 Micro: 0.8526, F1 Macro: 0.8353\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.9438, F1 Micro: 0.8484, F1 Macro: 0.8317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9496, F1 Micro: 0.859, F1 Macro: 0.8432\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9475, F1 Micro: 0.8568, F1 Macro: 0.8445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9539, F1 Micro: 0.8688, F1 Macro: 0.8576\n",
      "Model 3 - Iteration 11335: Accuracy: 0.9539, F1 Micro: 0.8688, F1 Macro: 0.8576\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.92      0.89      0.90      1245\n",
      "      Abusive       0.93      0.92      0.92      1228\n",
      "HS_Individual       0.84      0.80      0.82       591\n",
      "     HS_Group       0.89      0.85      0.87       654\n",
      "  HS_Religion       0.85      0.82      0.83       302\n",
      "      HS_Race       0.93      0.87      0.90       276\n",
      "  HS_Physical       0.91      0.88      0.89       262\n",
      "    HS_Gender       0.91      0.92      0.92       253\n",
      "     HS_Other       0.84      0.64      0.73       289\n",
      "      HS_Weak       0.80      0.77      0.79       478\n",
      "  HS_Moderate       0.84      0.79      0.81       530\n",
      "    HS_Strong       0.93      0.89      0.91       237\n",
      "\n",
      "    micro avg       0.89      0.85      0.87      6345\n",
      "    macro avg       0.88      0.84      0.86      6345\n",
      " weighted avg       0.89      0.85      0.87      6345\n",
      "  samples avg       0.50      0.49      0.49      6345\n",
      "\n",
      "Training completed in 221.13796830177307 s\n",
      "Averaged - Iteration 11335: Accuracy: 0.9528, F1 Micro: 0.8673, F1 Macro: 0.8563\n",
      "Total sampling time: 1346.55 seconds\n",
      "Total runtime: 14602.4646525383 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning([50, 67, 42], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7fa56",
   "metadata": {
    "papermill": {
     "duration": 0.179343,
     "end_time": "2025-05-10T08:20:04.700667",
     "exception": false,
     "start_time": "2025-05-10T08:20:04.521324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6647971,
     "sourceId": 11748149,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14633.759978,
   "end_time": "2025-05-10T08:20:07.976205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-10T04:16:14.216227",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "048e0eac03664b2b8d516b0e97fb2468": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "076aff6c3744402298826f787f1ad395": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "078959aac27f4730a70554650c11151f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0afad0a0d09f48fda5683d889454095f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ebb9c63494e94361bc8b4574141f4e91",
       "placeholder": "",
       "style": "IPY_MODEL_076aff6c3744402298826f787f1ad395",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:100%"
      }
     },
     "0bb9115b069a4a4fbb431ca6ff602a61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e60993ad80c4ea4ab9f5ccf404a69e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_adde0c9564b54cd3841b2d61b9264ee3",
        "IPY_MODEL_c92eaf4040ae46d58ea715fd75e709e8",
        "IPY_MODEL_3deb3439f9a2466aa8c548e0190fdf31"
       ],
       "layout": "IPY_MODEL_acc19a90e96d41c6b3e9f909f84008eb",
       "tabbable": null,
       "tooltip": null
      }
     },
     "13d572f940714808a46763eb18577359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "23ca09f4e3d547acac8f4df5377bac17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "37af39dc99f74acbb511f3186356c9d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3deb3439f9a2466aa8c548e0190fdf31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d071352928ac432992807c906d95a02e",
       "placeholder": "",
       "style": "IPY_MODEL_e8d028bc1c9a42fe929e69d22d05eb76",
       "tabbable": null,
       "tooltip": null,
       "value": "229k/229k[00:00&lt;00:00,6.30MB/s]"
      }
     },
     "499c1eedb2f44fa09d2f9464ff634434": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4b850e79cc3e49c08114447afb6b0f47",
       "max": 1534.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_13d572f940714808a46763eb18577359",
       "tabbable": null,
       "tooltip": null,
       "value": 1534.0
      }
     },
     "4a1de1e627864e388876700261baf8ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b850e79cc3e49c08114447afb6b0f47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c58136db2c941d1888742d0258aca77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5050b20d207e4678877f1102a181341f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7d2542ff81974c0383d8c619f3552594",
       "placeholder": "",
       "style": "IPY_MODEL_4c58136db2c941d1888742d0258aca77",
       "tabbable": null,
       "tooltip": null,
       "value": "2.00/2.00[00:00&lt;00:00,163B/s]"
      }
     },
     "5284dae7c2ed4cc6a6e5b65235550813": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5295545b507c4b829a92c550685206cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f5d28774eae144d4bd40647f6b60e560",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_048e0eac03664b2b8d516b0e97fb2468",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "52a10f461a3447878af4b031f47d4db0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54743efb182643f09975a05864a1e997": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "55712bcef87f4290bc58b4766dc9c299": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_acc18a7991594cff8dc59ee137daa314",
       "placeholder": "",
       "style": "IPY_MODEL_b0394ead54fc4109882b08ceb7d9fa11",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:100%"
      }
     },
     "5d5c8a6979d24a158d73f3935ca69f10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b7e6a9f14c924ba18a2606288137cba2",
       "max": 112.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eb761a5d47444fc6a1b17d649a20f0f4",
       "tabbable": null,
       "tooltip": null,
       "value": 112.0
      }
     },
     "76223836fd7a4d37a467879615ffa2f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e601e777087243a387bd432ada296cea",
       "placeholder": "",
       "style": "IPY_MODEL_83f6e471c54b498c8682adffa5d5b885",
       "tabbable": null,
       "tooltip": null,
       "value": "1.53k/1.53k[00:00&lt;00:00,117kB/s]"
      }
     },
     "7855819a13ea4d0ab84bad893d732b7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_078959aac27f4730a70554650c11151f",
       "placeholder": "",
       "style": "IPY_MODEL_54743efb182643f09975a05864a1e997",
       "tabbable": null,
       "tooltip": null,
       "value": "112/112[00:00&lt;00:00,12.0kB/s]"
      }
     },
     "7d2542ff81974c0383d8c619f3552594": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83f6e471c54b498c8682adffa5d5b885": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a3684a500cf34f1788548951b2c2ba83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0afad0a0d09f48fda5683d889454095f",
        "IPY_MODEL_5d5c8a6979d24a158d73f3935ca69f10",
        "IPY_MODEL_7855819a13ea4d0ab84bad893d732b7b"
       ],
       "layout": "IPY_MODEL_5284dae7c2ed4cc6a6e5b65235550813",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a472907ab35146c08cccd519f3a18da0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fd16ff43509f4d63adfd42fb97f2727b",
       "placeholder": "",
       "style": "IPY_MODEL_ecfd3db91ad741aea5df326f1266b020",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "acc18a7991594cff8dc59ee137daa314": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "acc19a90e96d41c6b3e9f909f84008eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "adde0c9564b54cd3841b2d61b9264ee3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4a1de1e627864e388876700261baf8ef",
       "placeholder": "",
       "style": "IPY_MODEL_37af39dc99f74acbb511f3186356c9d0",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:100%"
      }
     },
     "b0394ead54fc4109882b08ceb7d9fa11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b7e6a9f14c924ba18a2606288137cba2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd9aede2fe7247a78aa5d0726a372053": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a472907ab35146c08cccd519f3a18da0",
        "IPY_MODEL_499c1eedb2f44fa09d2f9464ff634434",
        "IPY_MODEL_76223836fd7a4d37a467879615ffa2f1"
       ],
       "layout": "IPY_MODEL_0bb9115b069a4a4fbb431ca6ff602a61",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c92eaf4040ae46d58ea715fd75e709e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eec5f228d25c4a03b4c4bcb57c9554d2",
       "max": 229167.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_23ca09f4e3d547acac8f4df5377bac17",
       "tabbable": null,
       "tooltip": null,
       "value": 229167.0
      }
     },
     "d071352928ac432992807c906d95a02e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e601e777087243a387bd432ada296cea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8d028bc1c9a42fe929e69d22d05eb76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eb761a5d47444fc6a1b17d649a20f0f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ebb9c63494e94361bc8b4574141f4e91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ecfd3db91ad741aea5df326f1266b020": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eeb9252b6aae408dbdc2bd9fea2df346": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_55712bcef87f4290bc58b4766dc9c299",
        "IPY_MODEL_5295545b507c4b829a92c550685206cf",
        "IPY_MODEL_5050b20d207e4678877f1102a181341f"
       ],
       "layout": "IPY_MODEL_52a10f461a3447878af4b031f47d4db0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "eec5f228d25c4a03b4c4bcb57c9554d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5d28774eae144d4bd40647f6b60e560": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd16ff43509f4d63adfd42fb97f2727b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
