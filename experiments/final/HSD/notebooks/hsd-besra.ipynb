{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9cb777",
   "metadata": {
    "papermill": {
     "duration": 0.011565,
     "end_time": "2025-04-13T07:00:57.000278",
     "exception": false,
     "start_time": "2025-04-13T07:00:56.988713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1813b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:57.023097Z",
     "iopub.status.busy": "2025-04-13T07:00:57.022845Z",
     "iopub.status.idle": "2025-04-13T07:01:21.080016Z",
     "shell.execute_reply": "2025-04-13T07:01:21.079298Z"
    },
    "papermill": {
     "duration": 24.070952,
     "end_time": "2025-04-13T07:01:21.081547",
     "exception": false,
     "start_time": "2025-04-13T07:00:57.010595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abea063",
   "metadata": {
    "papermill": {
     "duration": 0.009895,
     "end_time": "2025-04-13T07:01:21.102299",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.092404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b021ebe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.123338Z",
     "iopub.status.busy": "2025-04-13T07:01:21.122867Z",
     "iopub.status.idle": "2025-04-13T07:01:21.126399Z",
     "shell.execute_reply": "2025-04-13T07:01:21.125600Z"
    },
    "papermill": {
     "duration": 0.015482,
     "end_time": "2025-04-13T07:01:21.127747",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.112265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3daf1d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.148836Z",
     "iopub.status.busy": "2025-04-13T07:01:21.148484Z",
     "iopub.status.idle": "2025-04-13T07:01:21.152177Z",
     "shell.execute_reply": "2025-04-13T07:01:21.151557Z"
    },
    "papermill": {
     "duration": 0.015388,
     "end_time": "2025-04-13T07:01:21.153255",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.137867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3454b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.174741Z",
     "iopub.status.busy": "2025-04-13T07:01:21.174483Z",
     "iopub.status.idle": "2025-04-13T07:01:21.183313Z",
     "shell.execute_reply": "2025-04-13T07:01:21.182749Z"
    },
    "papermill": {
     "duration": 0.020861,
     "end_time": "2025-04-13T07:01:21.184484",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.163623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930af54a",
   "metadata": {
    "papermill": {
     "duration": 0.010131,
     "end_time": "2025-04-13T07:01:21.205192",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.195061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd536665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.226890Z",
     "iopub.status.busy": "2025-04-13T07:01:21.226651Z",
     "iopub.status.idle": "2025-04-13T07:01:21.279756Z",
     "shell.execute_reply": "2025-04-13T07:01:21.278331Z"
    },
    "papermill": {
     "duration": 0.065908,
     "end_time": "2025-04-13T07:01:21.281518",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.215610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-besra'\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "sequence_length = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ab4d0",
   "metadata": {
    "papermill": {
     "duration": 0.010231,
     "end_time": "2025-04-13T07:01:21.302479",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.292248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6665981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.324762Z",
     "iopub.status.busy": "2025-04-13T07:01:21.324450Z",
     "iopub.status.idle": "2025-04-13T07:01:21.488577Z",
     "shell.execute_reply": "2025-04-13T07:01:21.487498Z"
    },
    "papermill": {
     "duration": 0.176715,
     "end_time": "2025-04-13T07:01:21.489786",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.313071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (13169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech/re_dataset.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661dff05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.513041Z",
     "iopub.status.busy": "2025-04-13T07:01:21.512690Z",
     "iopub.status.idle": "2025-04-13T07:01:21.523129Z",
     "shell.execute_reply": "2025-04-13T07:01:21.522286Z"
    },
    "papermill": {
     "duration": 0.023162,
     "end_time": "2025-04-13T07:01:21.524283",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.501121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24717841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.551471Z",
     "iopub.status.busy": "2025-04-13T07:01:21.551154Z",
     "iopub.status.idle": "2025-04-13T07:01:21.565193Z",
     "shell.execute_reply": "2025-04-13T07:01:21.564438Z"
    },
    "papermill": {
     "duration": 0.031231,
     "end_time": "2025-04-13T07:01:21.566478",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.535247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    7608\n",
       "1    5561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5799dd8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.598466Z",
     "iopub.status.busy": "2025-04-13T07:01:21.598206Z",
     "iopub.status.idle": "2025-04-13T07:01:21.605774Z",
     "shell.execute_reply": "2025-04-13T07:01:21.604589Z"
    },
    "papermill": {
     "duration": 0.023031,
     "end_time": "2025-04-13T07:01:21.607802",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.584771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8126\n",
       "1    5043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b1d39c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.634725Z",
     "iopub.status.busy": "2025-04-13T07:01:21.634423Z",
     "iopub.status.idle": "2025-04-13T07:01:21.647865Z",
     "shell.execute_reply": "2025-04-13T07:01:21.647060Z"
    },
    "papermill": {
     "duration": 0.026609,
     "end_time": "2025-04-13T07:01:21.649290",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.622681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic shape:  (7309, 13)\n",
      "Non-toxic shape:  (5860, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b9c8a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.672504Z",
     "iopub.status.busy": "2025-04-13T07:01:21.672207Z",
     "iopub.status.idle": "2025-04-13T07:01:21.682363Z",
     "shell.execute_reply": "2025-04-13T07:01:21.681594Z"
    },
    "papermill": {
     "duration": 0.023503,
     "end_time": "2025-04-13T07:01:21.683896",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.660393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602c8388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.724013Z",
     "iopub.status.busy": "2025-04-13T07:01:21.723703Z",
     "iopub.status.idle": "2025-04-13T07:01:21.742124Z",
     "shell.execute_reply": "2025-04-13T07:01:21.741336Z"
    },
    "papermill": {
     "duration": 0.039006,
     "end_time": "2025-04-13T07:01:21.743493",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.704487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04c3e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.767318Z",
     "iopub.status.busy": "2025-04-13T07:01:21.767093Z",
     "iopub.status.idle": "2025-04-13T07:01:21.770583Z",
     "shell.execute_reply": "2025-04-13T07:01:21.769821Z"
    },
    "papermill": {
     "duration": 0.016647,
     "end_time": "2025-04-13T07:01:21.771788",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.755141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e03998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:21.795938Z",
     "iopub.status.busy": "2025-04-13T07:01:21.795647Z",
     "iopub.status.idle": "2025-04-13T07:01:22.208089Z",
     "shell.execute_reply": "2025-04-13T07:01:22.206935Z"
    },
    "papermill": {
     "duration": 0.426376,
     "end_time": "2025-04-13T07:01:22.209927",
     "exception": false,
     "start_time": "2025-04-13T07:01:21.783551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535,) (10535, 12)\n",
      "(2634,) (2634, 12)\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "\n",
    "# Define the labels columns for multi-label classification\n",
    "label_columns = data.columns[1:]  # Assuming label columns start from the third column\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['Tweet'].values\n",
    "y_train = train_data[label_columns].values\n",
    "X_val = val_data['Tweet'].values\n",
    "y_val = val_data[label_columns].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539af9f",
   "metadata": {
    "papermill": {
     "duration": 0.015122,
     "end_time": "2025-04-13T07:01:22.240750",
     "exception": false,
     "start_time": "2025-04-13T07:01:22.225628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55ba627d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:22.272429Z",
     "iopub.status.busy": "2025-04-13T07:01:22.272132Z",
     "iopub.status.idle": "2025-04-13T07:01:22.976712Z",
     "shell.execute_reply": "2025-04-13T07:01:22.976086Z"
    },
    "papermill": {
     "duration": 0.722117,
     "end_time": "2025-04-13T07:01:22.978132",
     "exception": false,
     "start_time": "2025-04-13T07:01:22.256015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945ab7322ece4e2bb8fb294313854646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b4d5e7ab4442feb68c89916d6ad084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738eb5541fce464cabd5e866bd01b5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fc8fd52c284f7e8337696d84110cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09634847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.005716Z",
     "iopub.status.busy": "2025-04-13T07:01:23.005448Z",
     "iopub.status.idle": "2025-04-13T07:01:23.009589Z",
     "shell.execute_reply": "2025-04-13T07:01:23.008982Z"
    },
    "papermill": {
     "duration": 0.018957,
     "end_time": "2025-04-13T07:01:23.010831",
     "exception": false,
     "start_time": "2025-04-13T07:01:22.991874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=64, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181010c",
   "metadata": {
    "papermill": {
     "duration": 0.012058,
     "end_time": "2025-04-13T07:01:23.034901",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.022843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d80f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.060519Z",
     "iopub.status.busy": "2025-04-13T07:01:23.060313Z",
     "iopub.status.idle": "2025-04-13T07:01:23.064004Z",
     "shell.execute_reply": "2025-04-13T07:01:23.063001Z"
    },
    "papermill": {
     "duration": 0.017409,
     "end_time": "2025-04-13T07:01:23.065290",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.047881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4858ca91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.090670Z",
     "iopub.status.busy": "2025-04-13T07:01:23.090438Z",
     "iopub.status.idle": "2025-04-13T07:01:23.095142Z",
     "shell.execute_reply": "2025-04-13T07:01:23.094348Z"
    },
    "papermill": {
     "duration": 0.018491,
     "end_time": "2025-04-13T07:01:23.096279",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.077788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a560b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.121371Z",
     "iopub.status.busy": "2025-04-13T07:01:23.121144Z",
     "iopub.status.idle": "2025-04-13T07:01:23.132724Z",
     "shell.execute_reply": "2025-04-13T07:01:23.132079Z"
    },
    "papermill": {
     "duration": 0.025159,
     "end_time": "2025-04-13T07:01:23.133785",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.108626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0dea0",
   "metadata": {
    "papermill": {
     "duration": 0.011706,
     "end_time": "2025-04-13T07:01:23.157369",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.145663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7401ba51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.182204Z",
     "iopub.status.busy": "2025-04-13T07:01:23.181959Z",
     "iopub.status.idle": "2025-04-13T07:01:23.187317Z",
     "shell.execute_reply": "2025-04-13T07:01:23.186518Z"
    },
    "papermill": {
     "duration": 0.019161,
     "end_time": "2025-04-13T07:01:23.188450",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.169289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a8ac5",
   "metadata": {
    "papermill": {
     "duration": 0.012258,
     "end_time": "2025-04-13T07:01:23.212746",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.200488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73300e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.238032Z",
     "iopub.status.busy": "2025-04-13T07:01:23.237800Z",
     "iopub.status.idle": "2025-04-13T07:01:23.259371Z",
     "shell.execute_reply": "2025-04-13T07:01:23.258588Z"
    },
    "papermill": {
     "duration": 0.035824,
     "end_time": "2025-04-13T07:01:23.260706",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.224882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = HateSpeechDataset(X_pool, np.zeros((len(X_pool), 12)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = HateSpeechDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "    \n",
    "    accelerator.wait_for_everyone()    \n",
    "    if accelerator.is_local_main_process:\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'HS': [y_train[i][0] for i in temp],\n",
    "                'Abusive': [y_train[i][1] for i in temp],\n",
    "                'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                'HS_Group': [y_train[i][3] for i in temp],\n",
    "                'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                'HS_Race': [y_train[i][5] for i in temp],\n",
    "                'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                'HS_Other': [y_train[i][8] for i in temp],\n",
    "                'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                'HS_Strong': [y_train[i][11] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'HS': [y_train[i][0] for i in temp],\n",
    "                    'Abusive': [y_train[i][1] for i in temp],\n",
    "                    'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                    'HS_Group': [y_train[i][3] for i in temp],\n",
    "                    'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                    'HS_Race': [y_train[i][5] for i in temp],\n",
    "                    'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                    'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                    'HS_Other': [y_train[i][8] for i in temp],\n",
    "                    'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                    'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                    'HS_Strong': [y_train[i][11] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459066e",
   "metadata": {
    "papermill": {
     "duration": 0.011979,
     "end_time": "2025-04-13T07:01:23.284855",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.272876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3462f506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.310143Z",
     "iopub.status.busy": "2025-04-13T07:01:23.309916Z",
     "iopub.status.idle": "2025-04-13T07:01:23.319795Z",
     "shell.execute_reply": "2025-04-13T07:01:23.318999Z"
    },
    "papermill": {
     "duration": 0.023984,
     "end_time": "2025-04-13T07:01:23.321106",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.297122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15290d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.346275Z",
     "iopub.status.busy": "2025-04-13T07:01:23.346043Z",
     "iopub.status.idle": "2025-04-13T07:01:23.349603Z",
     "shell.execute_reply": "2025-04-13T07:01:23.348785Z"
    },
    "papermill": {
     "duration": 0.017375,
     "end_time": "2025-04-13T07:01:23.350792",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.333417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757a2ad",
   "metadata": {
    "papermill": {
     "duration": 0.011768,
     "end_time": "2025-04-13T07:01:23.374652",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.362884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b7070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6072, Accuracy: 0.8196, F1 Micro: 0.158, F1 Macro: 0.056\n",
      "Epoch 2/10, Train Loss: 0.4725, Accuracy: 0.8281, F1 Micro: 0.0079, F1 Macro: 0.0031\n",
      "Epoch 3/10, Train Loss: 0.4011, Accuracy: 0.8324, F1 Micro: 0.0621, F1 Macro: 0.0249\n",
      "Epoch 4/10, Train Loss: 0.4044, Accuracy: 0.833, F1 Micro: 0.0698, F1 Macro: 0.0275\n",
      "Epoch 5/10, Train Loss: 0.39, Accuracy: 0.835, F1 Micro: 0.0933, F1 Macro: 0.0364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3774, Accuracy: 0.8482, F1 Micro: 0.2492, F1 Macro: 0.0877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3646, Accuracy: 0.8569, F1 Micro: 0.3507, F1 Macro: 0.1183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3408, Accuracy: 0.871, F1 Micro: 0.473, F1 Macro: 0.208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3045, Accuracy: 0.8761, F1 Micro: 0.5665, F1 Macro: 0.2665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.272, Accuracy: 0.8786, F1 Micro: 0.5727, F1 Macro: 0.2739\n",
      "Model 1 - Iteration 658: Accuracy: 0.8786, F1 Micro: 0.5727, F1 Macro: 0.2739\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.70      0.75      1134\n",
      "      Abusive       0.81      0.74      0.77       992\n",
      "HS_Individual       0.65      0.48      0.55       732\n",
      "     HS_Group       0.49      0.05      0.10       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.55      0.59       762\n",
      "      HS_Weak       0.63      0.44      0.52       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.47      0.57      5556\n",
      "    macro avg       0.34      0.25      0.27      5556\n",
      " weighted avg       0.60      0.47      0.52      5556\n",
      "  samples avg       0.36      0.27      0.28      5556\n",
      "\n",
      "Training completed in 49.04898381233215 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6268, Accuracy: 0.8269, F1 Micro: 0.1933, F1 Macro: 0.0535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4844, Accuracy: 0.8431, F1 Micro: 0.2905, F1 Macro: 0.0901\n",
      "Epoch 3/10, Train Loss: 0.4084, Accuracy: 0.8367, F1 Micro: 0.1258, F1 Macro: 0.0434\n",
      "Epoch 4/10, Train Loss: 0.4073, Accuracy: 0.8361, F1 Micro: 0.1149, F1 Macro: 0.0411\n",
      "Epoch 5/10, Train Loss: 0.3883, Accuracy: 0.8442, F1 Micro: 0.2237, F1 Macro: 0.0783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3744, Accuracy: 0.8537, F1 Micro: 0.3359, F1 Macro: 0.1086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.361, Accuracy: 0.8638, F1 Micro: 0.4298, F1 Macro: 0.1602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3381, Accuracy: 0.872, F1 Micro: 0.4844, F1 Macro: 0.2089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3027, Accuracy: 0.8754, F1 Micro: 0.5488, F1 Macro: 0.2523\n",
      "Epoch 10/10, Train Loss: 0.2788, Accuracy: 0.877, F1 Micro: 0.5471, F1 Macro: 0.2563\n",
      "Model 2 - Iteration 658: Accuracy: 0.8754, F1 Micro: 0.5488, F1 Macro: 0.2523\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.72      0.76      1134\n",
      "      Abusive       0.80      0.69      0.74       992\n",
      "HS_Individual       0.64      0.44      0.52       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.51      0.57       762\n",
      "      HS_Weak       0.64      0.34      0.44       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.44      0.55      5556\n",
      "    macro avg       0.29      0.22      0.25      5556\n",
      " weighted avg       0.56      0.44      0.49      5556\n",
      "  samples avg       0.35      0.25      0.27      5556\n",
      "\n",
      "Training completed in 48.675028800964355 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6425, Accuracy: 0.7948, F1 Micro: 0.3968, F1 Macro: 0.1198\n",
      "Epoch 2/10, Train Loss: 0.4902, Accuracy: 0.8356, F1 Micro: 0.1886, F1 Macro: 0.0538\n",
      "Epoch 3/10, Train Loss: 0.4047, Accuracy: 0.8326, F1 Micro: 0.0804, F1 Macro: 0.0306\n",
      "Epoch 4/10, Train Loss: 0.4031, Accuracy: 0.8344, F1 Micro: 0.1031, F1 Macro: 0.0375\n",
      "Epoch 5/10, Train Loss: 0.3878, Accuracy: 0.8367, F1 Micro: 0.1276, F1 Macro: 0.047\n",
      "Epoch 6/10, Train Loss: 0.3748, Accuracy: 0.8493, F1 Micro: 0.2727, F1 Macro: 0.0935\n",
      "Epoch 7/10, Train Loss: 0.3632, Accuracy: 0.8552, F1 Micro: 0.3382, F1 Macro: 0.1118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3437, Accuracy: 0.8634, F1 Micro: 0.4097, F1 Macro: 0.1654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3109, Accuracy: 0.8763, F1 Micro: 0.5497, F1 Macro: 0.2516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2794, Accuracy: 0.8776, F1 Micro: 0.5601, F1 Macro: 0.2635\n",
      "Model 3 - Iteration 658: Accuracy: 0.8776, F1 Micro: 0.5601, F1 Macro: 0.2635\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.69      0.75      1134\n",
      "      Abusive       0.82      0.70      0.76       992\n",
      "HS_Individual       0.67      0.46      0.54       732\n",
      "     HS_Group       0.44      0.02      0.03       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.53      0.58       762\n",
      "      HS_Weak       0.62      0.42      0.50       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.74      0.45      0.56      5556\n",
      "    macro avg       0.33      0.23      0.26      5556\n",
      " weighted avg       0.60      0.45      0.50      5556\n",
      "  samples avg       0.38      0.27      0.29      5556\n",
      "\n",
      "Training completed in 45.0896372795105 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8772, F1 Micro: 0.5606, F1 Macro: 0.2632\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 131.39720940589905 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5354, Accuracy: 0.8331, F1 Micro: 0.0915, F1 Macro: 0.0337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4137, Accuracy: 0.8374, F1 Micro: 0.124, F1 Macro: 0.0486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3875, Accuracy: 0.8565, F1 Micro: 0.3952, F1 Macro: 0.1321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3557, Accuracy: 0.8768, F1 Micro: 0.5613, F1 Macro: 0.2589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3113, Accuracy: 0.8819, F1 Micro: 0.6126, F1 Macro: 0.2996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2718, Accuracy: 0.8856, F1 Micro: 0.6261, F1 Macro: 0.329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2411, Accuracy: 0.8894, F1 Micro: 0.636, F1 Macro: 0.3515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2167, Accuracy: 0.8884, F1 Micro: 0.67, F1 Macro: 0.4222\n",
      "Epoch 9/10, Train Loss: 0.2038, Accuracy: 0.8903, F1 Micro: 0.6623, F1 Macro: 0.4586\n",
      "Epoch 10/10, Train Loss: 0.1723, Accuracy: 0.8928, F1 Micro: 0.6568, F1 Macro: 0.4459\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8884, F1 Micro: 0.67, F1 Macro: 0.4222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.86      0.81      1134\n",
      "      Abusive       0.85      0.81      0.83       992\n",
      "HS_Individual       0.63      0.66      0.65       732\n",
      "     HS_Group       0.57      0.53      0.55       402\n",
      "  HS_Religion       0.56      0.14      0.22       157\n",
      "      HS_Race       0.89      0.13      0.23       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.76      0.69       762\n",
      "      HS_Weak       0.61      0.63      0.62       689\n",
      "  HS_Moderate       0.43      0.38      0.40       331\n",
      "    HS_Strong       1.00      0.04      0.07       114\n",
      "\n",
      "    micro avg       0.68      0.66      0.67      5556\n",
      "    macro avg       0.58      0.41      0.42      5556\n",
      " weighted avg       0.67      0.66      0.65      5556\n",
      "  samples avg       0.38      0.36      0.35      5556\n",
      "\n",
      "Training completed in 68.15820646286011 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5528, Accuracy: 0.842, F1 Micro: 0.3679, F1 Macro: 0.1051\n",
      "Epoch 2/10, Train Loss: 0.4153, Accuracy: 0.8428, F1 Micro: 0.2195, F1 Macro: 0.0779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3848, Accuracy: 0.8584, F1 Micro: 0.4309, F1 Macro: 0.1449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3533, Accuracy: 0.8774, F1 Micro: 0.5442, F1 Macro: 0.2453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3165, Accuracy: 0.8801, F1 Micro: 0.6108, F1 Macro: 0.2951\n",
      "Epoch 6/10, Train Loss: 0.2768, Accuracy: 0.8852, F1 Micro: 0.6103, F1 Macro: 0.3176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2461, Accuracy: 0.8878, F1 Micro: 0.645, F1 Macro: 0.3628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2235, Accuracy: 0.887, F1 Micro: 0.6709, F1 Macro: 0.4061\n",
      "Epoch 9/10, Train Loss: 0.2059, Accuracy: 0.8909, F1 Micro: 0.6474, F1 Macro: 0.4278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1785, Accuracy: 0.893, F1 Micro: 0.671, F1 Macro: 0.4633\n",
      "Model 2 - Iteration 1646: Accuracy: 0.893, F1 Micro: 0.671, F1 Macro: 0.4633\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.82      0.81      1134\n",
      "      Abusive       0.87      0.77      0.82       992\n",
      "HS_Individual       0.69      0.60      0.64       732\n",
      "     HS_Group       0.57      0.56      0.56       402\n",
      "  HS_Religion       0.51      0.22      0.30       157\n",
      "      HS_Race       0.74      0.47      0.58       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.69      0.69       762\n",
      "      HS_Weak       0.67      0.57      0.61       689\n",
      "  HS_Moderate       0.42      0.44      0.43       331\n",
      "    HS_Strong       1.00      0.06      0.12       114\n",
      "\n",
      "    micro avg       0.71      0.63      0.67      5556\n",
      "    macro avg       0.58      0.43      0.46      5556\n",
      " weighted avg       0.70      0.63      0.66      5556\n",
      "  samples avg       0.37      0.35      0.34      5556\n",
      "\n",
      "Training completed in 66.72559094429016 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5595, Accuracy: 0.8347, F1 Micro: 0.1705, F1 Macro: 0.0543\n",
      "Epoch 2/10, Train Loss: 0.4152, Accuracy: 0.8368, F1 Micro: 0.1334, F1 Macro: 0.05\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3858, Accuracy: 0.8548, F1 Micro: 0.3437, F1 Macro: 0.1103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3581, Accuracy: 0.8734, F1 Micro: 0.5056, F1 Macro: 0.2195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3173, Accuracy: 0.88, F1 Micro: 0.6159, F1 Macro: 0.2923\n",
      "Epoch 6/10, Train Loss: 0.2781, Accuracy: 0.8849, F1 Micro: 0.6041, F1 Macro: 0.3077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2503, Accuracy: 0.8892, F1 Micro: 0.6374, F1 Macro: 0.3601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2268, Accuracy: 0.8836, F1 Micro: 0.6739, F1 Macro: 0.4302\n",
      "Epoch 9/10, Train Loss: 0.2108, Accuracy: 0.8901, F1 Micro: 0.6168, F1 Macro: 0.3987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1847, Accuracy: 0.8923, F1 Micro: 0.6812, F1 Macro: 0.4721\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8923, F1 Micro: 0.6812, F1 Macro: 0.4721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.85      0.81      1134\n",
      "      Abusive       0.84      0.83      0.83       992\n",
      "HS_Individual       0.65      0.65      0.65       732\n",
      "     HS_Group       0.56      0.57      0.57       402\n",
      "  HS_Religion       0.58      0.23      0.33       157\n",
      "      HS_Race       0.73      0.42      0.54       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.73      0.70       762\n",
      "      HS_Weak       0.63      0.62      0.62       689\n",
      "  HS_Moderate       0.44      0.45      0.45       331\n",
      "    HS_Strong       1.00      0.10      0.18       114\n",
      "\n",
      "    micro avg       0.69      0.67      0.68      5556\n",
      "    macro avg       0.57      0.45      0.47      5556\n",
      " weighted avg       0.68      0.67      0.66      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 67.0218780040741 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8842, F1 Micro: 0.6173, F1 Macro: 0.3579\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 118.58892178535461 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4945, Accuracy: 0.8349, F1 Micro: 0.0942, F1 Macro: 0.0369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3857, Accuracy: 0.8521, F1 Micro: 0.3048, F1 Macro: 0.1021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3366, Accuracy: 0.8792, F1 Micro: 0.5635, F1 Macro: 0.2656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2992, Accuracy: 0.8868, F1 Micro: 0.6381, F1 Macro: 0.346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2538, Accuracy: 0.8914, F1 Micro: 0.6506, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.231, Accuracy: 0.8964, F1 Micro: 0.6649, F1 Macro: 0.4286\n",
      "Epoch 7/10, Train Loss: 0.2007, Accuracy: 0.898, F1 Micro: 0.6603, F1 Macro: 0.4418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1765, Accuracy: 0.896, F1 Micro: 0.7012, F1 Macro: 0.5012\n",
      "Epoch 9/10, Train Loss: 0.1591, Accuracy: 0.8986, F1 Micro: 0.7006, F1 Macro: 0.5053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1305, Accuracy: 0.9003, F1 Micro: 0.7043, F1 Macro: 0.4994\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9003, F1 Micro: 0.7043, F1 Macro: 0.4994\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1134\n",
      "      Abusive       0.86      0.83      0.85       992\n",
      "HS_Individual       0.64      0.74      0.69       732\n",
      "     HS_Group       0.67      0.49      0.57       402\n",
      "  HS_Religion       0.68      0.31      0.43       157\n",
      "      HS_Race       0.85      0.43      0.57       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.76      0.72       762\n",
      "      HS_Weak       0.62      0.70      0.66       689\n",
      "  HS_Moderate       0.52      0.39      0.45       331\n",
      "    HS_Strong       0.94      0.14      0.24       114\n",
      "\n",
      "    micro avg       0.72      0.69      0.70      5556\n",
      "    macro avg       0.61      0.47      0.50      5556\n",
      " weighted avg       0.71      0.69      0.69      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 82.21046757698059 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.508, Accuracy: 0.8385, F1 Micro: 0.1528, F1 Macro: 0.0541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.384, Accuracy: 0.857, F1 Micro: 0.3592, F1 Macro: 0.1182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.336, Accuracy: 0.8792, F1 Micro: 0.5867, F1 Macro: 0.2739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3014, Accuracy: 0.8851, F1 Micro: 0.6452, F1 Macro: 0.3427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2559, Accuracy: 0.8916, F1 Micro: 0.6501, F1 Macro: 0.4317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2387, Accuracy: 0.8952, F1 Micro: 0.6761, F1 Macro: 0.4233\n",
      "Epoch 7/10, Train Loss: 0.2095, Accuracy: 0.8989, F1 Micro: 0.6688, F1 Macro: 0.4607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1803, Accuracy: 0.8971, F1 Micro: 0.6959, F1 Macro: 0.4882\n",
      "Epoch 9/10, Train Loss: 0.1664, Accuracy: 0.901, F1 Micro: 0.6938, F1 Macro: 0.4942\n",
      "Epoch 10/10, Train Loss: 0.1376, Accuracy: 0.8974, F1 Micro: 0.6957, F1 Macro: 0.5043\n",
      "Model 2 - Iteration 2535: Accuracy: 0.8971, F1 Micro: 0.6959, F1 Macro: 0.4882\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.84      0.81      1134\n",
      "      Abusive       0.82      0.88      0.85       992\n",
      "HS_Individual       0.65      0.68      0.66       732\n",
      "     HS_Group       0.63      0.53      0.58       402\n",
      "  HS_Religion       0.65      0.24      0.35       157\n",
      "      HS_Race       0.75      0.50      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.76      0.72       762\n",
      "      HS_Weak       0.63      0.64      0.63       689\n",
      "  HS_Moderate       0.47      0.42      0.44       331\n",
      "    HS_Strong       1.00      0.12      0.22       114\n",
      "\n",
      "    micro avg       0.71      0.68      0.70      5556\n",
      "    macro avg       0.59      0.47      0.49      5556\n",
      " weighted avg       0.69      0.68      0.68      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 79.06765079498291 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5114, Accuracy: 0.8375, F1 Micro: 0.1591, F1 Macro: 0.059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3841, Accuracy: 0.848, F1 Micro: 0.2595, F1 Macro: 0.0882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3404, Accuracy: 0.8789, F1 Micro: 0.5558, F1 Macro: 0.2574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.305, Accuracy: 0.8841, F1 Micro: 0.6399, F1 Macro: 0.333\n",
      "Epoch 5/10, Train Loss: 0.2585, Accuracy: 0.8913, F1 Micro: 0.6376, F1 Macro: 0.4058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2385, Accuracy: 0.8962, F1 Micro: 0.6742, F1 Macro: 0.4237\n",
      "Epoch 7/10, Train Loss: 0.2093, Accuracy: 0.8987, F1 Micro: 0.6699, F1 Macro: 0.4693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1822, Accuracy: 0.8944, F1 Micro: 0.6945, F1 Macro: 0.5065\n",
      "Epoch 9/10, Train Loss: 0.1658, Accuracy: 0.9017, F1 Micro: 0.6931, F1 Macro: 0.5086\n",
      "Epoch 10/10, Train Loss: 0.1415, Accuracy: 0.9027, F1 Micro: 0.6929, F1 Macro: 0.5102\n",
      "Model 3 - Iteration 2535: Accuracy: 0.8944, F1 Micro: 0.6945, F1 Macro: 0.5065\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.86      0.81      1134\n",
      "      Abusive       0.81      0.86      0.84       992\n",
      "HS_Individual       0.64      0.67      0.66       732\n",
      "     HS_Group       0.56      0.58      0.57       402\n",
      "  HS_Religion       0.72      0.28      0.40       157\n",
      "      HS_Race       0.76      0.57      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.77      0.72       762\n",
      "      HS_Weak       0.63      0.63      0.63       689\n",
      "  HS_Moderate       0.44      0.50      0.47       331\n",
      "    HS_Strong       0.96      0.20      0.33       114\n",
      "\n",
      "    micro avg       0.69      0.70      0.69      5556\n",
      "    macro avg       0.58      0.49      0.51      5556\n",
      " weighted avg       0.68      0.70      0.68      5556\n",
      "  samples avg       0.40      0.39      0.38      5556\n",
      "\n",
      "Training completed in 77.76078605651855 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8886, F1 Micro: 0.6443, F1 Macro: 0.4046\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 106.50207686424255 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4709, Accuracy: 0.8383, F1 Micro: 0.1362, F1 Macro: 0.0513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3674, Accuracy: 0.8763, F1 Micro: 0.5235, F1 Macro: 0.2437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3107, Accuracy: 0.8848, F1 Micro: 0.5864, F1 Macro: 0.3038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2696, Accuracy: 0.8906, F1 Micro: 0.6061, F1 Macro: 0.3946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2351, Accuracy: 0.8991, F1 Micro: 0.6883, F1 Macro: 0.4863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1963, Accuracy: 0.9011, F1 Micro: 0.6951, F1 Macro: 0.4869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1631, Accuracy: 0.9043, F1 Micro: 0.7177, F1 Macro: 0.5258\n",
      "Epoch 8/10, Train Loss: 0.1507, Accuracy: 0.9075, F1 Micro: 0.7125, F1 Macro: 0.5413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1247, Accuracy: 0.9086, F1 Micro: 0.7196, F1 Macro: 0.5559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1057, Accuracy: 0.9072, F1 Micro: 0.7204, F1 Macro: 0.5824\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9072, F1 Micro: 0.7204, F1 Macro: 0.5824\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.82      0.82      1134\n",
      "      Abusive       0.86      0.86      0.86       992\n",
      "HS_Individual       0.70      0.65      0.67       732\n",
      "     HS_Group       0.62      0.59      0.61       402\n",
      "  HS_Religion       0.73      0.52      0.61       157\n",
      "      HS_Race       0.80      0.53      0.64       120\n",
      "  HS_Physical       0.72      0.18      0.29        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.75      0.73      0.74       762\n",
      "      HS_Weak       0.67      0.62      0.64       689\n",
      "  HS_Moderate       0.52      0.50      0.51       331\n",
      "    HS_Strong       0.91      0.45      0.60       114\n",
      "\n",
      "    micro avg       0.75      0.69      0.72      5556\n",
      "    macro avg       0.67      0.54      0.58      5556\n",
      " weighted avg       0.74      0.69      0.71      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 96.66230940818787 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4797, Accuracy: 0.8459, F1 Micro: 0.2448, F1 Macro: 0.0832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3642, Accuracy: 0.8773, F1 Micro: 0.5666, F1 Macro: 0.2581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3126, Accuracy: 0.8871, F1 Micro: 0.6218, F1 Macro: 0.3271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2696, Accuracy: 0.8955, F1 Micro: 0.6457, F1 Macro: 0.4469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2383, Accuracy: 0.9009, F1 Micro: 0.6812, F1 Macro: 0.4759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1976, Accuracy: 0.9031, F1 Micro: 0.6997, F1 Macro: 0.5048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1669, Accuracy: 0.9042, F1 Micro: 0.7221, F1 Macro: 0.5272\n",
      "Epoch 8/10, Train Loss: 0.1549, Accuracy: 0.9062, F1 Micro: 0.7031, F1 Macro: 0.5339\n",
      "Epoch 9/10, Train Loss: 0.1264, Accuracy: 0.9084, F1 Micro: 0.7106, F1 Macro: 0.533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1118, Accuracy: 0.91, F1 Micro: 0.7295, F1 Macro: 0.5952\n",
      "Model 2 - Iteration 3335: Accuracy: 0.91, F1 Micro: 0.7295, F1 Macro: 0.5952\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.86      0.86      0.86       992\n",
      "HS_Individual       0.70      0.70      0.70       732\n",
      "     HS_Group       0.67      0.55      0.61       402\n",
      "  HS_Religion       0.71      0.53      0.61       157\n",
      "      HS_Race       0.80      0.57      0.66       120\n",
      "  HS_Physical       0.64      0.12      0.21        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.77      0.72      0.74       762\n",
      "      HS_Weak       0.65      0.67      0.66       689\n",
      "  HS_Moderate       0.56      0.48      0.52       331\n",
      "    HS_Strong       0.93      0.48      0.64       114\n",
      "\n",
      "    micro avg       0.76      0.70      0.73      5556\n",
      "    macro avg       0.76      0.55      0.60      5556\n",
      " weighted avg       0.76      0.70      0.72      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 94.8313250541687 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4842, Accuracy: 0.8397, F1 Micro: 0.1651, F1 Macro: 0.0616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3675, Accuracy: 0.8732, F1 Micro: 0.5055, F1 Macro: 0.2195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3143, Accuracy: 0.8859, F1 Micro: 0.6021, F1 Macro: 0.3191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2712, Accuracy: 0.8954, F1 Micro: 0.6421, F1 Macro: 0.4484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2403, Accuracy: 0.9018, F1 Micro: 0.6869, F1 Macro: 0.4951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1998, Accuracy: 0.9039, F1 Micro: 0.7054, F1 Macro: 0.5249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1689, Accuracy: 0.9043, F1 Micro: 0.7187, F1 Macro: 0.535\n",
      "Epoch 8/10, Train Loss: 0.1557, Accuracy: 0.9074, F1 Micro: 0.6988, F1 Macro: 0.5519\n",
      "Epoch 9/10, Train Loss: 0.1321, Accuracy: 0.9082, F1 Micro: 0.7163, F1 Macro: 0.5502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1154, Accuracy: 0.9076, F1 Micro: 0.7271, F1 Macro: 0.5772\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9076, F1 Micro: 0.7271, F1 Macro: 0.5772\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.86      0.87      0.86       992\n",
      "HS_Individual       0.69      0.68      0.69       732\n",
      "     HS_Group       0.64      0.60      0.62       402\n",
      "  HS_Religion       0.76      0.53      0.62       157\n",
      "      HS_Race       0.75      0.62      0.68       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.76      0.74       762\n",
      "      HS_Weak       0.66      0.66      0.66       689\n",
      "  HS_Moderate       0.51      0.50      0.51       331\n",
      "    HS_Strong       0.90      0.54      0.67       114\n",
      "\n",
      "    micro avg       0.74      0.71      0.73      5556\n",
      "    macro avg       0.69      0.55      0.58      5556\n",
      " weighted avg       0.74      0.71      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 95.0150728225708 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8935, F1 Micro: 0.6646, F1 Macro: 0.4497\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 96.07104396820068 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4593, Accuracy: 0.8458, F1 Micro: 0.236, F1 Macro: 0.0817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3496, Accuracy: 0.8826, F1 Micro: 0.6164, F1 Macro: 0.3009\n",
      "Epoch 3/10, Train Loss: 0.2855, Accuracy: 0.8903, F1 Micro: 0.6085, F1 Macro: 0.3586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2482, Accuracy: 0.8992, F1 Micro: 0.6616, F1 Macro: 0.4222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2145, Accuracy: 0.9044, F1 Micro: 0.6956, F1 Macro: 0.484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1847, Accuracy: 0.9078, F1 Micro: 0.72, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1576, Accuracy: 0.9027, F1 Micro: 0.7241, F1 Macro: 0.5745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1312, Accuracy: 0.908, F1 Micro: 0.7258, F1 Macro: 0.5759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1154, Accuracy: 0.9027, F1 Micro: 0.7314, F1 Macro: 0.5898\n",
      "Epoch 10/10, Train Loss: 0.1041, Accuracy: 0.9109, F1 Micro: 0.7281, F1 Macro: 0.5989\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9027, F1 Micro: 0.7314, F1 Macro: 0.5898\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.90      0.82      1134\n",
      "      Abusive       0.83      0.90      0.86       992\n",
      "HS_Individual       0.64      0.78      0.70       732\n",
      "     HS_Group       0.62      0.60      0.61       402\n",
      "  HS_Religion       0.63      0.54      0.58       157\n",
      "      HS_Race       0.69      0.62      0.66       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.84      0.75       762\n",
      "      HS_Weak       0.62      0.74      0.67       689\n",
      "  HS_Moderate       0.52      0.50      0.51       331\n",
      "    HS_Strong       0.83      0.61      0.70       114\n",
      "\n",
      "    micro avg       0.70      0.77      0.73      5556\n",
      "    macro avg       0.63      0.60      0.59      5556\n",
      " weighted avg       0.69      0.77      0.72      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 104.1228883266449 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4658, Accuracy: 0.8517, F1 Micro: 0.3316, F1 Macro: 0.1065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3485, Accuracy: 0.88, F1 Micro: 0.6049, F1 Macro: 0.2941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2858, Accuracy: 0.8938, F1 Micro: 0.6427, F1 Macro: 0.3858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2502, Accuracy: 0.8998, F1 Micro: 0.6716, F1 Macro: 0.4145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.215, Accuracy: 0.9046, F1 Micro: 0.6986, F1 Macro: 0.502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1888, Accuracy: 0.9082, F1 Micro: 0.7097, F1 Macro: 0.5165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1617, Accuracy: 0.8992, F1 Micro: 0.7179, F1 Macro: 0.5565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.139, Accuracy: 0.9078, F1 Micro: 0.7349, F1 Macro: 0.5843\n",
      "Epoch 9/10, Train Loss: 0.1186, Accuracy: 0.9059, F1 Micro: 0.7313, F1 Macro: 0.5802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.106, Accuracy: 0.9132, F1 Micro: 0.7364, F1 Macro: 0.5966\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9132, F1 Micro: 0.7364, F1 Macro: 0.5966\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.89      0.84      0.86       992\n",
      "HS_Individual       0.71      0.67      0.69       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.77      0.54      0.64       157\n",
      "      HS_Race       0.85      0.60      0.70       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.75      0.75      0.75       762\n",
      "      HS_Weak       0.69      0.64      0.66       689\n",
      "  HS_Moderate       0.57      0.53      0.55       331\n",
      "    HS_Strong       0.90      0.58      0.71       114\n",
      "\n",
      "    micro avg       0.77      0.70      0.74      5556\n",
      "    macro avg       0.81      0.55      0.60      5556\n",
      " weighted avg       0.78      0.70      0.73      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 106.80954003334045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4685, Accuracy: 0.8456, F1 Micro: 0.2556, F1 Macro: 0.0852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3526, Accuracy: 0.8807, F1 Micro: 0.6167, F1 Macro: 0.3048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2871, Accuracy: 0.896, F1 Micro: 0.6532, F1 Macro: 0.3953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2483, Accuracy: 0.8997, F1 Micro: 0.6642, F1 Macro: 0.4488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2173, Accuracy: 0.9045, F1 Micro: 0.6941, F1 Macro: 0.4942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1888, Accuracy: 0.9062, F1 Micro: 0.7218, F1 Macro: 0.5531\n",
      "Epoch 7/10, Train Loss: 0.1607, Accuracy: 0.8937, F1 Micro: 0.7145, F1 Macro: 0.5605\n",
      "Epoch 8/10, Train Loss: 0.1374, Accuracy: 0.9073, F1 Micro: 0.7194, F1 Macro: 0.5786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1221, Accuracy: 0.9085, F1 Micro: 0.7413, F1 Macro: 0.5949\n",
      "Epoch 10/10, Train Loss: 0.1083, Accuracy: 0.9122, F1 Micro: 0.725, F1 Macro: 0.5855\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9085, F1 Micro: 0.7413, F1 Macro: 0.5949\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.88      0.83      1134\n",
      "      Abusive       0.84      0.88      0.86       992\n",
      "HS_Individual       0.67      0.75      0.71       732\n",
      "     HS_Group       0.65      0.65      0.65       402\n",
      "  HS_Religion       0.66      0.61      0.64       157\n",
      "      HS_Race       0.71      0.71      0.71       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.81      0.75       762\n",
      "      HS_Weak       0.64      0.72      0.68       689\n",
      "  HS_Moderate       0.54      0.54      0.54       331\n",
      "    HS_Strong       0.85      0.67      0.75       114\n",
      "\n",
      "    micro avg       0.72      0.76      0.74      5556\n",
      "    macro avg       0.67      0.60      0.59      5556\n",
      " weighted avg       0.72      0.76      0.73      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 102.40773367881775 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.8964, F1 Micro: 0.679, F1 Macro: 0.4785\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 86.53585290908813 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4487, Accuracy: 0.8524, F1 Micro: 0.3238, F1 Macro: 0.1049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3342, Accuracy: 0.8819, F1 Micro: 0.56, F1 Macro: 0.2757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2776, Accuracy: 0.897, F1 Micro: 0.6673, F1 Macro: 0.4095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2324, Accuracy: 0.9031, F1 Micro: 0.707, F1 Macro: 0.5107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1972, Accuracy: 0.9054, F1 Micro: 0.7203, F1 Macro: 0.5606\n",
      "Epoch 6/10, Train Loss: 0.1731, Accuracy: 0.9081, F1 Micro: 0.7161, F1 Macro: 0.5188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1476, Accuracy: 0.9063, F1 Micro: 0.7343, F1 Macro: 0.578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1261, Accuracy: 0.9137, F1 Micro: 0.7358, F1 Macro: 0.588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1058, Accuracy: 0.9081, F1 Micro: 0.7401, F1 Macro: 0.6251\n",
      "Epoch 10/10, Train Loss: 0.0978, Accuracy: 0.9154, F1 Micro: 0.7379, F1 Macro: 0.5983\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9081, F1 Micro: 0.7401, F1 Macro: 0.6251\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.88      0.83      1134\n",
      "      Abusive       0.84      0.90      0.87       992\n",
      "HS_Individual       0.70      0.68      0.69       732\n",
      "     HS_Group       0.58      0.73      0.64       402\n",
      "  HS_Religion       0.62      0.66      0.64       157\n",
      "      HS_Race       0.71      0.66      0.68       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.67      0.12      0.20        51\n",
      "     HS_Other       0.74      0.79      0.76       762\n",
      "      HS_Weak       0.67      0.65      0.66       689\n",
      "  HS_Moderate       0.50      0.65      0.56       331\n",
      "    HS_Strong       0.89      0.64      0.74       114\n",
      "\n",
      "    micro avg       0.72      0.76      0.74      5556\n",
      "    macro avg       0.70      0.62      0.63      5556\n",
      " weighted avg       0.73      0.76      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 114.18581438064575 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.454, Accuracy: 0.8539, F1 Micro: 0.3363, F1 Macro: 0.1103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.332, Accuracy: 0.8828, F1 Micro: 0.5746, F1 Macro: 0.2763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.28, Accuracy: 0.8973, F1 Micro: 0.6738, F1 Macro: 0.4288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2362, Accuracy: 0.9024, F1 Micro: 0.6969, F1 Macro: 0.4863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2013, Accuracy: 0.9088, F1 Micro: 0.7128, F1 Macro: 0.5432\n",
      "Epoch 6/10, Train Loss: 0.1803, Accuracy: 0.9102, F1 Micro: 0.7087, F1 Macro: 0.5349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1538, Accuracy: 0.9076, F1 Micro: 0.7346, F1 Macro: 0.5855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1337, Accuracy: 0.9108, F1 Micro: 0.739, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1132, Accuracy: 0.9071, F1 Micro: 0.741, F1 Macro: 0.6349\n",
      "Epoch 10/10, Train Loss: 0.0993, Accuracy: 0.9162, F1 Micro: 0.7393, F1 Macro: 0.6002\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9071, F1 Micro: 0.741, F1 Macro: 0.6349\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.89      0.82      1134\n",
      "      Abusive       0.85      0.90      0.87       992\n",
      "HS_Individual       0.65      0.75      0.70       732\n",
      "     HS_Group       0.61      0.65      0.63       402\n",
      "  HS_Religion       0.68      0.67      0.67       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.71      0.80      0.76       762\n",
      "      HS_Weak       0.63      0.71      0.67       689\n",
      "  HS_Moderate       0.51      0.56      0.54       331\n",
      "    HS_Strong       0.89      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.71      0.77      0.74      5556\n",
      "    macro avg       0.71      0.64      0.63      5556\n",
      " weighted avg       0.72      0.77      0.74      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 114.60613179206848 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4571, Accuracy: 0.8506, F1 Micro: 0.2926, F1 Macro: 0.0982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3374, Accuracy: 0.881, F1 Micro: 0.5561, F1 Macro: 0.2734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2815, Accuracy: 0.8973, F1 Micro: 0.6814, F1 Macro: 0.4465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2382, Accuracy: 0.9014, F1 Micro: 0.7077, F1 Macro: 0.5239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2018, Accuracy: 0.9085, F1 Micro: 0.7224, F1 Macro: 0.5634\n",
      "Epoch 6/10, Train Loss: 0.1784, Accuracy: 0.9101, F1 Micro: 0.7221, F1 Macro: 0.5544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1529, Accuracy: 0.9027, F1 Micro: 0.7327, F1 Macro: 0.5875\n",
      "Epoch 8/10, Train Loss: 0.1345, Accuracy: 0.9139, F1 Micro: 0.7256, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1116, Accuracy: 0.9109, F1 Micro: 0.7428, F1 Macro: 0.6152\n",
      "Epoch 10/10, Train Loss: 0.0997, Accuracy: 0.9145, F1 Micro: 0.7367, F1 Macro: 0.6024\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9109, F1 Micro: 0.7428, F1 Macro: 0.6152\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.87      0.83      1134\n",
      "      Abusive       0.87      0.88      0.87       992\n",
      "HS_Individual       0.71      0.67      0.69       732\n",
      "     HS_Group       0.60      0.70      0.65       402\n",
      "  HS_Religion       0.73      0.66      0.69       157\n",
      "      HS_Race       0.75      0.73      0.74       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.40      0.04      0.07        51\n",
      "     HS_Other       0.73      0.78      0.76       762\n",
      "      HS_Weak       0.69      0.64      0.66       689\n",
      "  HS_Moderate       0.50      0.64      0.56       331\n",
      "    HS_Strong       0.87      0.70      0.78       114\n",
      "\n",
      "    micro avg       0.74      0.75      0.74      5556\n",
      "    macro avg       0.72      0.61      0.62      5556\n",
      " weighted avg       0.74      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 112.77964782714844 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.8985, F1 Micro: 0.6894, F1 Macro: 0.5029\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 77.98534727096558 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4385, Accuracy: 0.861, F1 Micro: 0.4455, F1 Macro: 0.1619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3192, Accuracy: 0.8898, F1 Micro: 0.6221, F1 Macro: 0.328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.9023, F1 Micro: 0.6993, F1 Macro: 0.4753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2177, Accuracy: 0.9038, F1 Micro: 0.7041, F1 Macro: 0.4932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1858, Accuracy: 0.905, F1 Micro: 0.7311, F1 Macro: 0.5614\n",
      "Epoch 6/10, Train Loss: 0.1553, Accuracy: 0.9113, F1 Micro: 0.7295, F1 Macro: 0.5637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1358, Accuracy: 0.9156, F1 Micro: 0.7414, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.111, Accuracy: 0.9167, F1 Micro: 0.747, F1 Macro: 0.6135\n",
      "Epoch 9/10, Train Loss: 0.0989, Accuracy: 0.9159, F1 Micro: 0.746, F1 Macro: 0.6386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0897, Accuracy: 0.9173, F1 Micro: 0.7549, F1 Macro: 0.6395\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9173, F1 Micro: 0.7549, F1 Macro: 0.6395\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.75      0.67      0.71       732\n",
      "     HS_Group       0.64      0.71      0.67       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.79      0.57      0.66       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.73      0.63      0.68       689\n",
      "  HS_Moderate       0.56      0.63      0.59       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.78      0.61      0.64      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 124.45964002609253 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4439, Accuracy: 0.8634, F1 Micro: 0.4552, F1 Macro: 0.1651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3179, Accuracy: 0.8863, F1 Micro: 0.5869, F1 Macro: 0.3027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2575, Accuracy: 0.9005, F1 Micro: 0.6915, F1 Macro: 0.4674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2223, Accuracy: 0.9058, F1 Micro: 0.7006, F1 Macro: 0.4964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9093, F1 Micro: 0.7339, F1 Macro: 0.5541\n",
      "Epoch 6/10, Train Loss: 0.1628, Accuracy: 0.9097, F1 Micro: 0.7297, F1 Macro: 0.5434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1424, Accuracy: 0.9138, F1 Micro: 0.747, F1 Macro: 0.5949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1173, Accuracy: 0.9144, F1 Micro: 0.7484, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1042, Accuracy: 0.9154, F1 Micro: 0.7487, F1 Macro: 0.6397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.089, Accuracy: 0.9156, F1 Micro: 0.7523, F1 Macro: 0.6301\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9156, F1 Micro: 0.7523, F1 Macro: 0.6301\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.70      0.71      0.70       732\n",
      "     HS_Group       0.69      0.62      0.66       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.84      0.60      0.70       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.67      0.16      0.25        51\n",
      "     HS_Other       0.72      0.81      0.77       762\n",
      "      HS_Weak       0.68      0.67      0.67       689\n",
      "  HS_Moderate       0.59      0.55      0.57       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.77      0.60      0.63      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 126.51043558120728 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4472, Accuracy: 0.8556, F1 Micro: 0.3641, F1 Macro: 0.1143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3226, Accuracy: 0.8859, F1 Micro: 0.5912, F1 Macro: 0.3009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2602, Accuracy: 0.9019, F1 Micro: 0.6872, F1 Macro: 0.4805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2248, Accuracy: 0.9057, F1 Micro: 0.6998, F1 Macro: 0.5183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1921, Accuracy: 0.9085, F1 Micro: 0.7328, F1 Macro: 0.5658\n",
      "Epoch 6/10, Train Loss: 0.1652, Accuracy: 0.9111, F1 Micro: 0.7315, F1 Macro: 0.5605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1425, Accuracy: 0.9167, F1 Micro: 0.7419, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1185, Accuracy: 0.9158, F1 Micro: 0.7504, F1 Macro: 0.6201\n",
      "Epoch 9/10, Train Loss: 0.1042, Accuracy: 0.9167, F1 Micro: 0.7504, F1 Macro: 0.6311\n",
      "Epoch 10/10, Train Loss: 0.0918, Accuracy: 0.9157, F1 Micro: 0.7498, F1 Macro: 0.6434\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9158, F1 Micro: 0.7504, F1 Macro: 0.6201\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.83      1134\n",
      "      Abusive       0.89      0.86      0.87       992\n",
      "HS_Individual       0.72      0.71      0.71       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.75      0.63      0.69       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.76      0.75      0.76       762\n",
      "      HS_Weak       0.68      0.69      0.69       689\n",
      "  HS_Moderate       0.57      0.58      0.57       331\n",
      "    HS_Strong       0.93      0.61      0.74       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.77      0.59      0.62      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 122.81888437271118 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.901, F1 Micro: 0.6984, F1 Macro: 0.5211\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 70.8305070400238 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4346, Accuracy: 0.8623, F1 Micro: 0.4074, F1 Macro: 0.1493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.313, Accuracy: 0.8917, F1 Micro: 0.6257, F1 Macro: 0.344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2511, Accuracy: 0.9031, F1 Micro: 0.7049, F1 Macro: 0.4952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2218, Accuracy: 0.9092, F1 Micro: 0.7129, F1 Macro: 0.5398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1846, Accuracy: 0.9128, F1 Micro: 0.7261, F1 Macro: 0.5535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1577, Accuracy: 0.9138, F1 Micro: 0.7403, F1 Macro: 0.5944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.133, Accuracy: 0.915, F1 Micro: 0.7515, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1111, Accuracy: 0.916, F1 Micro: 0.753, F1 Macro: 0.6298\n",
      "Epoch 9/10, Train Loss: 0.0966, Accuracy: 0.9174, F1 Micro: 0.7524, F1 Macro: 0.6414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0822, Accuracy: 0.9182, F1 Micro: 0.754, F1 Macro: 0.654\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9182, F1 Micro: 0.754, F1 Macro: 0.654\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.76      0.65      0.70       732\n",
      "     HS_Group       0.66      0.67      0.67       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       0.62      0.14      0.23        72\n",
      "    HS_Gender       0.58      0.22      0.31        51\n",
      "     HS_Other       0.79      0.73      0.76       762\n",
      "      HS_Weak       0.74      0.62      0.68       689\n",
      "  HS_Moderate       0.58      0.63      0.60       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5556\n",
      "    macro avg       0.73      0.62      0.65      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 134.2121126651764 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4391, Accuracy: 0.8643, F1 Micro: 0.4421, F1 Macro: 0.1628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3139, Accuracy: 0.8916, F1 Micro: 0.653, F1 Macro: 0.3732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2534, Accuracy: 0.9018, F1 Micro: 0.6986, F1 Macro: 0.4773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2249, Accuracy: 0.9096, F1 Micro: 0.7149, F1 Macro: 0.5478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1894, Accuracy: 0.9129, F1 Micro: 0.7214, F1 Macro: 0.5376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1641, Accuracy: 0.9142, F1 Micro: 0.7228, F1 Macro: 0.5488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1385, Accuracy: 0.9147, F1 Micro: 0.7484, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1162, Accuracy: 0.9171, F1 Micro: 0.7532, F1 Macro: 0.6228\n",
      "Epoch 9/10, Train Loss: 0.1002, Accuracy: 0.916, F1 Micro: 0.7522, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9147, F1 Micro: 0.7551, F1 Macro: 0.6591\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9147, F1 Micro: 0.7551, F1 Macro: 0.6591\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1134\n",
      "      Abusive       0.84      0.93      0.88       992\n",
      "HS_Individual       0.70      0.69      0.70       732\n",
      "     HS_Group       0.64      0.70      0.67       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.76      0.70      0.73       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.69      0.65      0.67       689\n",
      "  HS_Moderate       0.56      0.66      0.60       331\n",
      "    HS_Strong       0.91      0.75      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.76      0.76      5556\n",
      "    macro avg       0.73      0.64      0.66      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 134.35518884658813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4413, Accuracy: 0.8586, F1 Micro: 0.3831, F1 Macro: 0.1288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3166, Accuracy: 0.8915, F1 Micro: 0.6278, F1 Macro: 0.3516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.9009, F1 Micro: 0.6961, F1 Macro: 0.4957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2263, Accuracy: 0.9082, F1 Micro: 0.716, F1 Macro: 0.546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1899, Accuracy: 0.9123, F1 Micro: 0.7239, F1 Macro: 0.5616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1649, Accuracy: 0.9142, F1 Micro: 0.7322, F1 Macro: 0.5865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1381, Accuracy: 0.9132, F1 Micro: 0.7483, F1 Macro: 0.5978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1173, Accuracy: 0.9172, F1 Micro: 0.7502, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1041, Accuracy: 0.9173, F1 Micro: 0.7526, F1 Macro: 0.6394\n",
      "Epoch 10/10, Train Loss: 0.0864, Accuracy: 0.9164, F1 Micro: 0.7487, F1 Macro: 0.6433\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9173, F1 Micro: 0.7526, F1 Macro: 0.6394\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.74      0.65      0.69       732\n",
      "     HS_Group       0.65      0.69      0.67       402\n",
      "  HS_Religion       0.76      0.64      0.70       157\n",
      "      HS_Race       0.79      0.67      0.72       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.62      0.16      0.25        51\n",
      "     HS_Other       0.76      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.62      0.67       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.85      0.78      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5556\n",
      "    macro avg       0.75      0.61      0.64      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 133.97337532043457 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.903, F1 Micro: 0.7053, F1 Macro: 0.5373\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: Acquired samples:\n",
      "6584 473\n",
      "Sampling duration: 64.11676025390625 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4236, Accuracy: 0.8728, F1 Micro: 0.5138, F1 Macro: 0.2212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3021, Accuracy: 0.8962, F1 Micro: 0.6665, F1 Macro: 0.4339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2501, Accuracy: 0.9026, F1 Micro: 0.7095, F1 Macro: 0.4957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2081, Accuracy: 0.9121, F1 Micro: 0.7301, F1 Macro: 0.5579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9136, F1 Micro: 0.7333, F1 Macro: 0.5828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1487, Accuracy: 0.9157, F1 Micro: 0.7461, F1 Macro: 0.5907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1254, Accuracy: 0.9178, F1 Micro: 0.7571, F1 Macro: 0.6235\n",
      "Epoch 8/10, Train Loss: 0.1098, Accuracy: 0.9186, F1 Micro: 0.7509, F1 Macro: 0.6286\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9205, F1 Micro: 0.757, F1 Macro: 0.6433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0796, Accuracy: 0.9184, F1 Micro: 0.7574, F1 Macro: 0.6455\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9184, F1 Micro: 0.7574, F1 Macro: 0.6455\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.71      0.70      0.71       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.80      0.60      0.69       120\n",
      "  HS_Physical       0.61      0.15      0.24        72\n",
      "    HS_Gender       0.64      0.14      0.23        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.67      0.68       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.84      0.80      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.74      0.61      0.65      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 139.84074807167053 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4274, Accuracy: 0.8722, F1 Micro: 0.5642, F1 Macro: 0.243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3045, Accuracy: 0.8946, F1 Micro: 0.645, F1 Macro: 0.3932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2522, Accuracy: 0.9028, F1 Micro: 0.7073, F1 Macro: 0.4904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2098, Accuracy: 0.9119, F1 Micro: 0.7267, F1 Macro: 0.5519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.916, F1 Micro: 0.7416, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.9145, F1 Micro: 0.7516, F1 Macro: 0.6153\n",
      "Epoch 7/10, Train Loss: 0.1275, Accuracy: 0.9166, F1 Micro: 0.7494, F1 Macro: 0.6181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1129, Accuracy: 0.9171, F1 Micro: 0.7539, F1 Macro: 0.6331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 0.9178, F1 Micro: 0.7565, F1 Macro: 0.6574\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9185, F1 Micro: 0.7557, F1 Macro: 0.6523\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9178, F1 Micro: 0.7565, F1 Macro: 0.6574\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.68      0.62      0.65       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.59      0.54      0.56       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5556\n",
      "    macro avg       0.74      0.62      0.66      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 140.0607099533081 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4301, Accuracy: 0.8683, F1 Micro: 0.4821, F1 Macro: 0.1896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3065, Accuracy: 0.894, F1 Micro: 0.6474, F1 Macro: 0.4099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2536, Accuracy: 0.902, F1 Micro: 0.7097, F1 Macro: 0.5037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2116, Accuracy: 0.9114, F1 Micro: 0.7289, F1 Macro: 0.5606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1781, Accuracy: 0.9148, F1 Micro: 0.7387, F1 Macro: 0.59\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1505, Accuracy: 0.9128, F1 Micro: 0.7492, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1285, Accuracy: 0.9169, F1 Micro: 0.7593, F1 Macro: 0.6287\n",
      "Epoch 8/10, Train Loss: 0.1122, Accuracy: 0.916, F1 Micro: 0.7507, F1 Macro: 0.6348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0926, Accuracy: 0.9163, F1 Micro: 0.7602, F1 Macro: 0.6431\n",
      "Epoch 10/10, Train Loss: 0.0833, Accuracy: 0.9177, F1 Micro: 0.7486, F1 Macro: 0.6355\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9163, F1 Micro: 0.7602, F1 Macro: 0.6431\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.71      0.72      0.71       732\n",
      "     HS_Group       0.64      0.69      0.66       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.72      0.74      0.73       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.50      0.16      0.24        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.55      0.62      0.58       331\n",
      "    HS_Strong       0.89      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.72      0.64      0.64      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 140.66777896881104 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9046, F1 Micro: 0.7112, F1 Macro: 0.5497\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 57.873140811920166 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4215, Accuracy: 0.8747, F1 Micro: 0.5146, F1 Macro: 0.2347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2998, Accuracy: 0.8955, F1 Micro: 0.6465, F1 Macro: 0.4183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2427, Accuracy: 0.9047, F1 Micro: 0.6992, F1 Macro: 0.4713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2051, Accuracy: 0.9135, F1 Micro: 0.7392, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.173, Accuracy: 0.9129, F1 Micro: 0.7435, F1 Macro: 0.5766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1488, Accuracy: 0.9178, F1 Micro: 0.7519, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1234, Accuracy: 0.9164, F1 Micro: 0.7582, F1 Macro: 0.634\n",
      "Epoch 8/10, Train Loss: 0.1054, Accuracy: 0.9196, F1 Micro: 0.7581, F1 Macro: 0.6593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.9188, F1 Micro: 0.763, F1 Macro: 0.6722\n",
      "Epoch 10/10, Train Loss: 0.0772, Accuracy: 0.917, F1 Micro: 0.7581, F1 Macro: 0.6555\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9188, F1 Micro: 0.763, F1 Macro: 0.6722\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.74      0.69      0.71       732\n",
      "     HS_Group       0.65      0.70      0.67       402\n",
      "  HS_Religion       0.71      0.65      0.68       157\n",
      "      HS_Race       0.74      0.62      0.68       120\n",
      "  HS_Physical       0.48      0.19      0.28        72\n",
      "    HS_Gender       0.59      0.33      0.42        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.66      0.69       689\n",
      "  HS_Moderate       0.58      0.63      0.61       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.71      0.65      0.67      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 144.41141724586487 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4246, Accuracy: 0.8743, F1 Micro: 0.506, F1 Macro: 0.2242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3014, Accuracy: 0.8921, F1 Micro: 0.6213, F1 Macro: 0.382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2469, Accuracy: 0.9054, F1 Micro: 0.6898, F1 Macro: 0.4771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9116, F1 Micro: 0.7252, F1 Macro: 0.5356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1775, Accuracy: 0.9142, F1 Micro: 0.7437, F1 Macro: 0.5869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1534, Accuracy: 0.9171, F1 Micro: 0.7516, F1 Macro: 0.6\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1277, Accuracy: 0.9176, F1 Micro: 0.7535, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9169, F1 Micro: 0.7571, F1 Macro: 0.6602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0929, Accuracy: 0.9161, F1 Micro: 0.7584, F1 Macro: 0.6575\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.919, F1 Micro: 0.7548, F1 Macro: 0.6509\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9161, F1 Micro: 0.7584, F1 Macro: 0.6575\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.70      0.72      0.71       732\n",
      "     HS_Group       0.65      0.66      0.66       402\n",
      "  HS_Religion       0.72      0.65      0.68       157\n",
      "      HS_Race       0.79      0.63      0.70       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.57      0.61      0.59       331\n",
      "    HS_Strong       0.92      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.75      0.76      0.76      5556\n",
      "    macro avg       0.73      0.63      0.66      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 145.90449142456055 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4274, Accuracy: 0.8679, F1 Micro: 0.4575, F1 Macro: 0.1905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3038, Accuracy: 0.8933, F1 Micro: 0.6331, F1 Macro: 0.3998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2485, Accuracy: 0.9047, F1 Micro: 0.6995, F1 Macro: 0.5014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9122, F1 Micro: 0.7262, F1 Macro: 0.5579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1762, Accuracy: 0.9134, F1 Micro: 0.746, F1 Macro: 0.5929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.156, Accuracy: 0.9122, F1 Micro: 0.7502, F1 Macro: 0.6065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1323, Accuracy: 0.9184, F1 Micro: 0.7581, F1 Macro: 0.6148\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9204, F1 Micro: 0.7537, F1 Macro: 0.6429\n",
      "Epoch 9/10, Train Loss: 0.0943, Accuracy: 0.9171, F1 Micro: 0.7574, F1 Macro: 0.6405\n",
      "Epoch 10/10, Train Loss: 0.082, Accuracy: 0.915, F1 Micro: 0.7478, F1 Macro: 0.6341\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9184, F1 Micro: 0.7581, F1 Macro: 0.6148\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.69      0.76      0.73       732\n",
      "     HS_Group       0.76      0.54      0.63       402\n",
      "  HS_Religion       0.80      0.54      0.64       157\n",
      "      HS_Race       0.82      0.59      0.69       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.65      0.46      0.54       331\n",
      "    HS_Strong       0.91      0.63      0.75       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5556\n",
      "    macro avg       0.79      0.58      0.61      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 142.33159232139587 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9059, F1 Micro: 0.7161, F1 Macro: 0.5595\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 54.19595241546631 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4173, Accuracy: 0.8745, F1 Micro: 0.5043, F1 Macro: 0.2298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2863, Accuracy: 0.8918, F1 Micro: 0.6098, F1 Macro: 0.3789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2457, Accuracy: 0.9009, F1 Micro: 0.7155, F1 Macro: 0.5156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2084, Accuracy: 0.9125, F1 Micro: 0.7373, F1 Macro: 0.5612\n",
      "Epoch 5/10, Train Loss: 0.1795, Accuracy: 0.9137, F1 Micro: 0.7373, F1 Macro: 0.5778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9174, F1 Micro: 0.749, F1 Macro: 0.6214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.122, Accuracy: 0.9196, F1 Micro: 0.7605, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.919, F1 Micro: 0.7633, F1 Macro: 0.6526\n",
      "Epoch 9/10, Train Loss: 0.0906, Accuracy: 0.9205, F1 Micro: 0.7581, F1 Macro: 0.6649\n",
      "Epoch 10/10, Train Loss: 0.0757, Accuracy: 0.918, F1 Micro: 0.7621, F1 Macro: 0.6518\n",
      "Model 1 - Iteration 6980: Accuracy: 0.919, F1 Micro: 0.7633, F1 Macro: 0.6526\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.84      0.59      0.69       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.62      0.49      0.55       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.76      0.62      0.65      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 148.85295248031616 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4197, Accuracy: 0.8768, F1 Micro: 0.525, F1 Macro: 0.235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2878, Accuracy: 0.8943, F1 Micro: 0.6263, F1 Macro: 0.372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.9021, F1 Micro: 0.7114, F1 Macro: 0.5012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.9113, F1 Micro: 0.7401, F1 Macro: 0.5773\n",
      "Epoch 5/10, Train Loss: 0.1799, Accuracy: 0.9159, F1 Micro: 0.739, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1426, Accuracy: 0.9184, F1 Micro: 0.7499, F1 Macro: 0.6142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1217, Accuracy: 0.92, F1 Micro: 0.7612, F1 Macro: 0.6517\n",
      "Epoch 8/10, Train Loss: 0.1054, Accuracy: 0.919, F1 Micro: 0.7579, F1 Macro: 0.6507\n",
      "Epoch 9/10, Train Loss: 0.0906, Accuracy: 0.9178, F1 Micro: 0.7579, F1 Macro: 0.6696\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.9178, F1 Micro: 0.7574, F1 Macro: 0.6534\n",
      "Model 2 - Iteration 6980: Accuracy: 0.92, F1 Micro: 0.7612, F1 Macro: 0.6517\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.92      0.84      0.87       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.82      0.65      0.73       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.71      0.24      0.35        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.63      0.50      0.56       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.76      0.61      0.65      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 147.48004412651062 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4229, Accuracy: 0.8715, F1 Micro: 0.4848, F1 Macro: 0.2099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2884, Accuracy: 0.8953, F1 Micro: 0.6384, F1 Macro: 0.3762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2471, Accuracy: 0.9021, F1 Micro: 0.7161, F1 Macro: 0.5241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9135, F1 Micro: 0.7339, F1 Macro: 0.5574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.9138, F1 Micro: 0.7418, F1 Macro: 0.5945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1471, Accuracy: 0.9181, F1 Micro: 0.7489, F1 Macro: 0.5952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1233, Accuracy: 0.9186, F1 Micro: 0.7616, F1 Macro: 0.6396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1094, Accuracy: 0.9173, F1 Micro: 0.7618, F1 Macro: 0.6528\n",
      "Epoch 9/10, Train Loss: 0.0924, Accuracy: 0.9185, F1 Micro: 0.744, F1 Macro: 0.6474\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9157, F1 Micro: 0.7602, F1 Macro: 0.6497\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9173, F1 Micro: 0.7618, F1 Macro: 0.6528\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.89      0.87       992\n",
      "HS_Individual       0.70      0.75      0.72       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.80      0.61      0.69       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.50      0.25      0.34        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.61      0.53      0.57       331\n",
      "    HS_Strong       0.84      0.80      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.73      0.63      0.65      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 149.94839024543762 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9071, F1 Micro: 0.7202, F1 Macro: 0.568\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 49.05651569366455 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4069, Accuracy: 0.8814, F1 Micro: 0.5804, F1 Macro: 0.2776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2878, Accuracy: 0.9012, F1 Micro: 0.6822, F1 Macro: 0.4415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2287, Accuracy: 0.9085, F1 Micro: 0.6977, F1 Macro: 0.4942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1974, Accuracy: 0.9156, F1 Micro: 0.739, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1662, Accuracy: 0.9175, F1 Micro: 0.749, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9177, F1 Micro: 0.7621, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1144, Accuracy: 0.9187, F1 Micro: 0.7661, F1 Macro: 0.6564\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.9189, F1 Micro: 0.7634, F1 Macro: 0.6504\n",
      "Epoch 9/10, Train Loss: 0.0851, Accuracy: 0.9204, F1 Micro: 0.7634, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0732, Accuracy: 0.9214, F1 Micro: 0.7671, F1 Macro: 0.6693\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9214, F1 Micro: 0.7671, F1 Macro: 0.6693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.67      0.66      0.67       402\n",
      "  HS_Religion       0.74      0.57      0.64       157\n",
      "      HS_Race       0.80      0.62      0.70       120\n",
      "  HS_Physical       0.71      0.17      0.27        72\n",
      "    HS_Gender       0.52      0.31      0.39        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.74      0.67      0.70       689\n",
      "  HS_Moderate       0.60      0.57      0.59       331\n",
      "    HS_Strong       0.84      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 156.06240320205688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4118, Accuracy: 0.879, F1 Micro: 0.5774, F1 Macro: 0.2643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2906, Accuracy: 0.8951, F1 Micro: 0.6313, F1 Macro: 0.3761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2344, Accuracy: 0.9096, F1 Micro: 0.7103, F1 Macro: 0.5276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2016, Accuracy: 0.9136, F1 Micro: 0.7402, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1695, Accuracy: 0.9182, F1 Micro: 0.7496, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1425, Accuracy: 0.9165, F1 Micro: 0.757, F1 Macro: 0.6319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1199, Accuracy: 0.9204, F1 Micro: 0.7584, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1014, Accuracy: 0.92, F1 Micro: 0.766, F1 Macro: 0.6649\n",
      "Epoch 9/10, Train Loss: 0.0865, Accuracy: 0.9178, F1 Micro: 0.7611, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9205, F1 Micro: 0.7664, F1 Macro: 0.6716\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9205, F1 Micro: 0.7664, F1 Macro: 0.6716\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.74      0.55      0.63       157\n",
      "      HS_Race       0.79      0.63      0.70       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 157.86895942687988 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4134, Accuracy: 0.8794, F1 Micro: 0.5698, F1 Macro: 0.2643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.292, Accuracy: 0.8973, F1 Micro: 0.6474, F1 Macro: 0.412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9084, F1 Micro: 0.7076, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2025, Accuracy: 0.9135, F1 Micro: 0.7337, F1 Macro: 0.5709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9163, F1 Micro: 0.7421, F1 Macro: 0.5884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9187, F1 Micro: 0.7567, F1 Macro: 0.6106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1201, Accuracy: 0.9178, F1 Micro: 0.767, F1 Macro: 0.6537\n",
      "Epoch 8/10, Train Loss: 0.1029, Accuracy: 0.9203, F1 Micro: 0.763, F1 Macro: 0.6448\n",
      "Epoch 9/10, Train Loss: 0.088, Accuracy: 0.9185, F1 Micro: 0.7603, F1 Macro: 0.6514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9224, F1 Micro: 0.7689, F1 Macro: 0.6633\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9224, F1 Micro: 0.7689, F1 Macro: 0.6633\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.76      0.69      0.73       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.76      0.55      0.64       157\n",
      "      HS_Race       0.77      0.63      0.69       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.54      0.25      0.35        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.75      0.65      0.70       689\n",
      "  HS_Moderate       0.60      0.62      0.61       331\n",
      "    HS_Strong       0.84      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 156.97006177902222 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9083, F1 Micro: 0.7242, F1 Macro: 0.5763\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 43.77269411087036 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4098, Accuracy: 0.8792, F1 Micro: 0.5962, F1 Macro: 0.2805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2833, Accuracy: 0.8989, F1 Micro: 0.673, F1 Macro: 0.4164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9089, F1 Micro: 0.7177, F1 Macro: 0.5371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.9102, F1 Micro: 0.7434, F1 Macro: 0.5842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.164, Accuracy: 0.9149, F1 Micro: 0.7511, F1 Macro: 0.5801\n",
      "Epoch 6/10, Train Loss: 0.145, Accuracy: 0.9183, F1 Micro: 0.7414, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1185, Accuracy: 0.9195, F1 Micro: 0.7654, F1 Macro: 0.641\n",
      "Epoch 8/10, Train Loss: 0.0996, Accuracy: 0.9212, F1 Micro: 0.7644, F1 Macro: 0.6616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.9183, F1 Micro: 0.7667, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9216, F1 Micro: 0.7676, F1 Macro: 0.6702\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9216, F1 Micro: 0.7676, F1 Macro: 0.6702\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.60      0.65       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.71      0.67      0.69       120\n",
      "  HS_Physical       0.54      0.18      0.27        72\n",
      "    HS_Gender       0.54      0.29      0.38        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.72      0.64      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 160.25389790534973 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4124, Accuracy: 0.88, F1 Micro: 0.5825, F1 Macro: 0.2692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2837, Accuracy: 0.8978, F1 Micro: 0.6646, F1 Macro: 0.4096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2353, Accuracy: 0.9088, F1 Micro: 0.7181, F1 Macro: 0.5361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2045, Accuracy: 0.9092, F1 Micro: 0.7416, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9133, F1 Micro: 0.7502, F1 Macro: 0.5877\n",
      "Epoch 6/10, Train Loss: 0.1469, Accuracy: 0.9187, F1 Micro: 0.7428, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1232, Accuracy: 0.9191, F1 Micro: 0.7648, F1 Macro: 0.6455\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9189, F1 Micro: 0.7638, F1 Macro: 0.6673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.92, F1 Micro: 0.7683, F1 Macro: 0.6808\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 0.9184, F1 Micro: 0.7643, F1 Macro: 0.6876\n",
      "Model 2 - Iteration 7656: Accuracy: 0.92, F1 Micro: 0.7683, F1 Macro: 0.6808\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.70      0.75      0.72       732\n",
      "     HS_Group       0.68      0.65      0.67       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.55      0.17      0.26        72\n",
      "    HS_Gender       0.61      0.37      0.46        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.66      0.68      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 158.82893633842468 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.416, Accuracy: 0.8792, F1 Micro: 0.5527, F1 Macro: 0.2563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2871, Accuracy: 0.8991, F1 Micro: 0.6686, F1 Macro: 0.4231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2344, Accuracy: 0.9089, F1 Micro: 0.7177, F1 Macro: 0.5539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2044, Accuracy: 0.9111, F1 Micro: 0.745, F1 Macro: 0.5796\n",
      "Epoch 5/10, Train Loss: 0.1659, Accuracy: 0.9158, F1 Micro: 0.7446, F1 Macro: 0.5695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1457, Accuracy: 0.9191, F1 Micro: 0.7557, F1 Macro: 0.5995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1236, Accuracy: 0.9203, F1 Micro: 0.7635, F1 Macro: 0.6465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1031, Accuracy: 0.9227, F1 Micro: 0.7651, F1 Macro: 0.6547\n",
      "Epoch 9/10, Train Loss: 0.0887, Accuracy: 0.9145, F1 Micro: 0.7602, F1 Macro: 0.6672\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9211, F1 Micro: 0.7621, F1 Macro: 0.6707\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9227, F1 Micro: 0.7651, F1 Macro: 0.6547\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.78      0.62      0.70       157\n",
      "      HS_Race       0.89      0.61      0.72       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.71      0.20      0.31        51\n",
      "     HS_Other       0.79      0.75      0.77       762\n",
      "      HS_Weak       0.72      0.67      0.69       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.73      0.77      5556\n",
      "    macro avg       0.79      0.61      0.65      5556\n",
      " weighted avg       0.80      0.73      0.76      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 158.83831477165222 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9093, F1 Micro: 0.7275, F1 Macro: 0.5834\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 39.4894061088562 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4034, Accuracy: 0.874, F1 Micro: 0.504, F1 Macro: 0.2383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2842, Accuracy: 0.9001, F1 Micro: 0.6762, F1 Macro: 0.4604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2299, Accuracy: 0.9088, F1 Micro: 0.7347, F1 Macro: 0.5495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1932, Accuracy: 0.9167, F1 Micro: 0.751, F1 Macro: 0.6018\n",
      "Epoch 5/10, Train Loss: 0.1605, Accuracy: 0.9185, F1 Micro: 0.7502, F1 Macro: 0.5919\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9207, F1 Micro: 0.7476, F1 Macro: 0.6291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1156, Accuracy: 0.9207, F1 Micro: 0.7679, F1 Macro: 0.6416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.9229, F1 Micro: 0.7718, F1 Macro: 0.6828\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9204, F1 Micro: 0.7688, F1 Macro: 0.6774\n",
      "Epoch 10/10, Train Loss: 0.0749, Accuracy: 0.9208, F1 Micro: 0.7709, F1 Macro: 0.676\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9229, F1 Micro: 0.7718, F1 Macro: 0.6828\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.74      0.64      0.69       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.57      0.17      0.26        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.80      0.77      0.78       762\n",
      "      HS_Weak       0.74      0.67      0.70       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.74      0.65      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 159.93657207489014 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4059, Accuracy: 0.8774, F1 Micro: 0.5391, F1 Macro: 0.2507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2862, Accuracy: 0.9005, F1 Micro: 0.6722, F1 Macro: 0.4536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2342, Accuracy: 0.9081, F1 Micro: 0.7241, F1 Macro: 0.5336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.9162, F1 Micro: 0.7392, F1 Macro: 0.5779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1652, Accuracy: 0.9177, F1 Micro: 0.75, F1 Macro: 0.6025\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9202, F1 Micro: 0.7472, F1 Macro: 0.6322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1186, Accuracy: 0.9194, F1 Micro: 0.763, F1 Macro: 0.6549\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.9204, F1 Micro: 0.7579, F1 Macro: 0.6622\n",
      "Epoch 9/10, Train Loss: 0.0906, Accuracy: 0.9221, F1 Micro: 0.762, F1 Macro: 0.6693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9195, F1 Micro: 0.7667, F1 Macro: 0.6763\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9195, F1 Micro: 0.7667, F1 Macro: 0.6763\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.83      0.61      0.70       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.73      0.37      0.49        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.62      0.57      0.59       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 161.54431533813477 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4093, Accuracy: 0.8736, F1 Micro: 0.5018, F1 Macro: 0.2333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2881, Accuracy: 0.9007, F1 Micro: 0.6815, F1 Macro: 0.4719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2337, Accuracy: 0.9058, F1 Micro: 0.7288, F1 Macro: 0.5511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1984, Accuracy: 0.9161, F1 Micro: 0.7404, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9181, F1 Micro: 0.7538, F1 Macro: 0.602\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.918, F1 Micro: 0.7365, F1 Macro: 0.6019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1214, Accuracy: 0.9204, F1 Micro: 0.7669, F1 Macro: 0.6473\n",
      "Epoch 8/10, Train Loss: 0.1001, Accuracy: 0.9219, F1 Micro: 0.7609, F1 Macro: 0.6587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0912, Accuracy: 0.9224, F1 Micro: 0.7683, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0775, Accuracy: 0.9225, F1 Micro: 0.7716, F1 Macro: 0.6718\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9225, F1 Micro: 0.7716, F1 Macro: 0.6718\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.71      0.70      0.70       402\n",
      "  HS_Religion       0.80      0.60      0.69       157\n",
      "      HS_Race       0.86      0.64      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.62      0.25      0.36        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.73      0.67      0.70       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.71742010116577 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9102, F1 Micro: 0.7305, F1 Macro: 0.5901\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.25369572639465 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4048, Accuracy: 0.8814, F1 Micro: 0.612, F1 Macro: 0.3051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2785, Accuracy: 0.9014, F1 Micro: 0.6851, F1 Macro: 0.4785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.9084, F1 Micro: 0.7283, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1901, Accuracy: 0.9154, F1 Micro: 0.7351, F1 Macro: 0.5666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.9173, F1 Micro: 0.7481, F1 Macro: 0.6055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1318, Accuracy: 0.9175, F1 Micro: 0.7619, F1 Macro: 0.6458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9225, F1 Micro: 0.7681, F1 Macro: 0.6573\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9191, F1 Micro: 0.7647, F1 Macro: 0.6594\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9207, F1 Micro: 0.7626, F1 Macro: 0.671\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9223, F1 Micro: 0.7662, F1 Macro: 0.6756\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9225, F1 Micro: 0.7681, F1 Macro: 0.6573\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.77      0.63      0.69       157\n",
      "      HS_Race       0.88      0.62      0.73       120\n",
      "  HS_Physical       0.60      0.08      0.15        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.76      0.62      0.66      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 165.18609189987183 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4083, Accuracy: 0.8793, F1 Micro: 0.6131, F1 Macro: 0.2895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.9003, F1 Micro: 0.6861, F1 Macro: 0.4639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.9098, F1 Micro: 0.7242, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.9164, F1 Micro: 0.7382, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.9196, F1 Micro: 0.7542, F1 Macro: 0.6197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1356, Accuracy: 0.9146, F1 Micro: 0.7603, F1 Macro: 0.6472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.9183, F1 Micro: 0.765, F1 Macro: 0.6601\n",
      "Epoch 8/10, Train Loss: 0.099, Accuracy: 0.9189, F1 Micro: 0.7618, F1 Macro: 0.656\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9175, F1 Micro: 0.7628, F1 Macro: 0.6744\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9228, F1 Micro: 0.7623, F1 Macro: 0.6657\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9183, F1 Micro: 0.765, F1 Macro: 0.6601\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.68      0.78      0.73       732\n",
      "     HS_Group       0.72      0.58      0.65       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.71      0.29      0.42        51\n",
      "     HS_Other       0.74      0.81      0.78       762\n",
      "      HS_Weak       0.66      0.77      0.71       689\n",
      "  HS_Moderate       0.64      0.51      0.57       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.76      0.63      0.66      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 166.2769799232483 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4106, Accuracy: 0.88, F1 Micro: 0.6162, F1 Macro: 0.3012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2817, Accuracy: 0.9012, F1 Micro: 0.6908, F1 Macro: 0.4909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2282, Accuracy: 0.9083, F1 Micro: 0.7167, F1 Macro: 0.5355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1967, Accuracy: 0.9145, F1 Micro: 0.7388, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9147, F1 Micro: 0.7495, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1373, Accuracy: 0.9148, F1 Micro: 0.7618, F1 Macro: 0.6331\n",
      "Epoch 7/10, Train Loss: 0.1168, Accuracy: 0.9192, F1 Micro: 0.7554, F1 Macro: 0.6336\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9199, F1 Micro: 0.7581, F1 Macro: 0.6469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0853, Accuracy: 0.9203, F1 Micro: 0.7657, F1 Macro: 0.6636\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.9227, F1 Micro: 0.7596, F1 Macro: 0.6609\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9203, F1 Micro: 0.7657, F1 Macro: 0.6636\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.73      0.61      0.66       402\n",
      "  HS_Religion       0.74      0.63      0.68       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.57      0.06      0.10        72\n",
      "    HS_Gender       0.50      0.41      0.45        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.86      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.73      0.64      0.66      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 165.49438500404358 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9108, F1 Micro: 0.7329, F1 Macro: 0.5948\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 32.68553113937378 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4005, Accuracy: 0.8828, F1 Micro: 0.6132, F1 Macro: 0.294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2737, Accuracy: 0.8996, F1 Micro: 0.7059, F1 Macro: 0.523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2274, Accuracy: 0.9102, F1 Micro: 0.7158, F1 Macro: 0.5269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.9152, F1 Micro: 0.7465, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.9203, F1 Micro: 0.7518, F1 Macro: 0.6225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1345, Accuracy: 0.9184, F1 Micro: 0.7627, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1106, Accuracy: 0.9229, F1 Micro: 0.7665, F1 Macro: 0.6638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.9222, F1 Micro: 0.7711, F1 Macro: 0.6782\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9226, F1 Micro: 0.7708, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9197, F1 Micro: 0.7724, F1 Macro: 0.6843\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9197, F1 Micro: 0.7724, F1 Macro: 0.6843\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.69      0.79      0.73       732\n",
      "     HS_Group       0.68      0.63      0.66       402\n",
      "  HS_Religion       0.69      0.62      0.65       157\n",
      "      HS_Race       0.74      0.62      0.68       120\n",
      "  HS_Physical       0.70      0.22      0.34        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.84      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.72      0.67      0.68      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 173.24075889587402 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4031, Accuracy: 0.8811, F1 Micro: 0.5942, F1 Macro: 0.2789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2755, Accuracy: 0.8997, F1 Micro: 0.703, F1 Macro: 0.5167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2294, Accuracy: 0.9098, F1 Micro: 0.7251, F1 Macro: 0.5533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1899, Accuracy: 0.9162, F1 Micro: 0.7484, F1 Macro: 0.5956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.921, F1 Micro: 0.7592, F1 Macro: 0.6198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1377, Accuracy: 0.9215, F1 Micro: 0.7646, F1 Macro: 0.6453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9199, F1 Micro: 0.7673, F1 Macro: 0.6742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.9208, F1 Micro: 0.7705, F1 Macro: 0.6856\n",
      "Epoch 9/10, Train Loss: 0.0812, Accuracy: 0.9219, F1 Micro: 0.769, F1 Macro: 0.6845\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9199, F1 Micro: 0.7676, F1 Macro: 0.6842\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9208, F1 Micro: 0.7705, F1 Macro: 0.6856\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.67      0.69      0.68       157\n",
      "      HS_Race       0.72      0.75      0.73       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 171.05604243278503 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4071, Accuracy: 0.8824, F1 Micro: 0.6066, F1 Macro: 0.2885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.9018, F1 Micro: 0.6981, F1 Macro: 0.5158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2327, Accuracy: 0.9101, F1 Micro: 0.7125, F1 Macro: 0.5454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1918, Accuracy: 0.9163, F1 Micro: 0.7462, F1 Macro: 0.5952\n",
      "Epoch 5/10, Train Loss: 0.1616, Accuracy: 0.9186, F1 Micro: 0.7418, F1 Macro: 0.5974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.92, F1 Micro: 0.7582, F1 Macro: 0.6205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9228, F1 Micro: 0.7685, F1 Macro: 0.673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.9221, F1 Micro: 0.7734, F1 Macro: 0.6766\n",
      "Epoch 9/10, Train Loss: 0.0853, Accuracy: 0.9225, F1 Micro: 0.7715, F1 Macro: 0.6701\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.9197, F1 Micro: 0.7702, F1 Macro: 0.6853\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9221, F1 Micro: 0.7734, F1 Macro: 0.6766\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.68      0.71      0.70       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.56      0.37      0.45        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.74      0.68      0.71       689\n",
      "  HS_Moderate       0.60      0.63      0.62       331\n",
      "    HS_Strong       0.83      0.84      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 170.18045854568481 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9115, F1 Micro: 0.7353, F1 Macro: 0.6002\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 29.300169706344604 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4025, Accuracy: 0.8843, F1 Micro: 0.6141, F1 Macro: 0.3092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2749, Accuracy: 0.8983, F1 Micro: 0.6385, F1 Macro: 0.4186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2183, Accuracy: 0.9123, F1 Micro: 0.7304, F1 Macro: 0.5497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1851, Accuracy: 0.9147, F1 Micro: 0.7526, F1 Macro: 0.5948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.158, Accuracy: 0.9191, F1 Micro: 0.7549, F1 Macro: 0.6306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.135, Accuracy: 0.9225, F1 Micro: 0.7667, F1 Macro: 0.648\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9205, F1 Micro: 0.7612, F1 Macro: 0.6595\n",
      "Epoch 8/10, Train Loss: 0.0971, Accuracy: 0.9228, F1 Micro: 0.764, F1 Macro: 0.6559\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9222, F1 Micro: 0.765, F1 Macro: 0.6659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0699, Accuracy: 0.9199, F1 Micro: 0.7707, F1 Macro: 0.6793\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9199, F1 Micro: 0.7707, F1 Macro: 0.6793\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.67      0.66      0.67       402\n",
      "  HS_Religion       0.72      0.58      0.64       157\n",
      "      HS_Race       0.81      0.59      0.68       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.71      0.33      0.45        51\n",
      "     HS_Other       0.75      0.84      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 175.42844128608704 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4039, Accuracy: 0.8825, F1 Micro: 0.5927, F1 Macro: 0.2885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2759, Accuracy: 0.8981, F1 Micro: 0.6409, F1 Macro: 0.4168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.221, Accuracy: 0.9125, F1 Micro: 0.7292, F1 Macro: 0.5561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1864, Accuracy: 0.914, F1 Micro: 0.7554, F1 Macro: 0.5922\n",
      "Epoch 5/10, Train Loss: 0.1587, Accuracy: 0.919, F1 Micro: 0.7553, F1 Macro: 0.6461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.9222, F1 Micro: 0.7597, F1 Macro: 0.656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9226, F1 Micro: 0.7688, F1 Macro: 0.6631\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9218, F1 Micro: 0.7665, F1 Macro: 0.6634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0818, Accuracy: 0.9226, F1 Micro: 0.7707, F1 Macro: 0.6749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9224, F1 Micro: 0.7717, F1 Macro: 0.6828\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9224, F1 Micro: 0.7717, F1 Macro: 0.6828\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.79      0.59      0.67       157\n",
      "      HS_Race       0.87      0.60      0.71       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.71      0.39      0.51        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.92      0.76      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.79      0.64      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 177.9956760406494 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4052, Accuracy: 0.8819, F1 Micro: 0.5944, F1 Macro: 0.2963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.8978, F1 Micro: 0.6348, F1 Macro: 0.4242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.912, F1 Micro: 0.7276, F1 Macro: 0.5562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1865, Accuracy: 0.9115, F1 Micro: 0.7532, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1597, Accuracy: 0.9178, F1 Micro: 0.7575, F1 Macro: 0.6274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9198, F1 Micro: 0.7641, F1 Macro: 0.6549\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9206, F1 Micro: 0.7636, F1 Macro: 0.6634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.098, Accuracy: 0.9218, F1 Micro: 0.7715, F1 Macro: 0.6691\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.9231, F1 Micro: 0.7605, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0738, Accuracy: 0.9225, F1 Micro: 0.7726, F1 Macro: 0.6672\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9225, F1 Micro: 0.7726, F1 Macro: 0.6672\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.75      0.64      0.69       157\n",
      "      HS_Race       0.78      0.64      0.70       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.67      0.27      0.39        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.59      0.61       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 178.30025005340576 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9121, F1 Micro: 0.7375, F1 Macro: 0.6047\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 26.4524827003479 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3983, Accuracy: 0.8807, F1 Micro: 0.6086, F1 Macro: 0.2972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2731, Accuracy: 0.9023, F1 Micro: 0.7049, F1 Macro: 0.5033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2235, Accuracy: 0.9129, F1 Micro: 0.7365, F1 Macro: 0.5539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.9185, F1 Micro: 0.7479, F1 Macro: 0.5984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9182, F1 Micro: 0.7601, F1 Macro: 0.613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1314, Accuracy: 0.9192, F1 Micro: 0.7662, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9231, F1 Micro: 0.7671, F1 Macro: 0.6682\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9184, F1 Micro: 0.7658, F1 Macro: 0.6619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.924, F1 Micro: 0.7725, F1 Macro: 0.686\n",
      "Epoch 10/10, Train Loss: 0.0728, Accuracy: 0.9206, F1 Micro: 0.7672, F1 Macro: 0.6884\n",
      "Model 1 - Iteration 8816: Accuracy: 0.924, F1 Micro: 0.7725, F1 Macro: 0.686\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.67      0.19      0.30        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.75      0.65      0.69      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 180.10238242149353 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4, Accuracy: 0.8817, F1 Micro: 0.6114, F1 Macro: 0.2849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2727, Accuracy: 0.9005, F1 Micro: 0.6997, F1 Macro: 0.4799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.9129, F1 Micro: 0.7251, F1 Macro: 0.5378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9178, F1 Micro: 0.7515, F1 Macro: 0.6074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.9207, F1 Micro: 0.7597, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1327, Accuracy: 0.9183, F1 Micro: 0.7685, F1 Macro: 0.6722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9232, F1 Micro: 0.7722, F1 Macro: 0.6752\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9172, F1 Micro: 0.767, F1 Macro: 0.6537\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9204, F1 Micro: 0.7699, F1 Macro: 0.684\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.9212, F1 Micro: 0.7705, F1 Macro: 0.6903\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9232, F1 Micro: 0.7722, F1 Macro: 0.6752\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.78      0.59      0.67       402\n",
      "  HS_Religion       0.80      0.59      0.68       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.82      0.35      0.49        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.68      0.50      0.57       331\n",
      "    HS_Strong       0.89      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.63      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 179.7057342529297 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4044, Accuracy: 0.881, F1 Micro: 0.5932, F1 Macro: 0.2791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2761, Accuracy: 0.9014, F1 Micro: 0.6982, F1 Macro: 0.4819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.9124, F1 Micro: 0.7299, F1 Macro: 0.564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1866, Accuracy: 0.9165, F1 Micro: 0.7376, F1 Macro: 0.592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1585, Accuracy: 0.9162, F1 Micro: 0.7602, F1 Macro: 0.6118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9217, F1 Micro: 0.7652, F1 Macro: 0.6425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9229, F1 Micro: 0.7677, F1 Macro: 0.6544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9203, F1 Micro: 0.7738, F1 Macro: 0.6647\n",
      "Epoch 9/10, Train Loss: 0.0835, Accuracy: 0.921, F1 Micro: 0.772, F1 Macro: 0.6829\n",
      "Epoch 10/10, Train Loss: 0.0756, Accuracy: 0.9187, F1 Micro: 0.7693, F1 Macro: 0.6809\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9203, F1 Micro: 0.7738, F1 Macro: 0.6647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.74      0.84      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.87      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.73      0.66      0.66      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 180.10752534866333 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9126, F1 Micro: 0.7394, F1 Macro: 0.6086\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.95680260658264 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3967, Accuracy: 0.8842, F1 Micro: 0.6039, F1 Macro: 0.2958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9031, F1 Micro: 0.7096, F1 Macro: 0.5068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9097, F1 Micro: 0.7347, F1 Macro: 0.5723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9139, F1 Micro: 0.7397, F1 Macro: 0.5659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.9217, F1 Micro: 0.7571, F1 Macro: 0.6147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9217, F1 Micro: 0.7634, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9224, F1 Micro: 0.7647, F1 Macro: 0.6595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0974, Accuracy: 0.9208, F1 Micro: 0.7706, F1 Macro: 0.6784\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9215, F1 Micro: 0.7639, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0697, Accuracy: 0.9236, F1 Micro: 0.7742, F1 Macro: 0.694\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9236, F1 Micro: 0.7742, F1 Macro: 0.694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.77      0.24      0.36        72\n",
      "    HS_Gender       0.69      0.39      0.50        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.70      0.52      0.60       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 186.81318306922913 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3983, Accuracy: 0.8826, F1 Micro: 0.5857, F1 Macro: 0.278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2722, Accuracy: 0.9026, F1 Micro: 0.7041, F1 Macro: 0.495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2262, Accuracy: 0.9131, F1 Micro: 0.7366, F1 Macro: 0.5694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.9164, F1 Micro: 0.7455, F1 Macro: 0.5667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1615, Accuracy: 0.9209, F1 Micro: 0.7516, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1361, Accuracy: 0.9227, F1 Micro: 0.7646, F1 Macro: 0.6397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.924, F1 Micro: 0.7729, F1 Macro: 0.6749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9245, F1 Micro: 0.7751, F1 Macro: 0.6876\n",
      "Epoch 9/10, Train Loss: 0.082, Accuracy: 0.9243, F1 Micro: 0.7745, F1 Macro: 0.6943\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9224, F1 Micro: 0.7728, F1 Macro: 0.7017\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9245, F1 Micro: 0.7751, F1 Macro: 0.6876\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.79      0.66      0.72       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.75      0.35      0.48        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.74      0.68      0.71       689\n",
      "  HS_Moderate       0.62      0.63      0.62       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 185.09866547584534 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4014, Accuracy: 0.8823, F1 Micro: 0.5776, F1 Macro: 0.2779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2739, Accuracy: 0.9021, F1 Micro: 0.706, F1 Macro: 0.5139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2256, Accuracy: 0.9116, F1 Micro: 0.7344, F1 Macro: 0.578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9163, F1 Micro: 0.744, F1 Macro: 0.5727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9212, F1 Micro: 0.7544, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1367, Accuracy: 0.9225, F1 Micro: 0.7712, F1 Macro: 0.6465\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.9227, F1 Micro: 0.7663, F1 Macro: 0.6458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1023, Accuracy: 0.9208, F1 Micro: 0.7728, F1 Macro: 0.6783\n",
      "Epoch 9/10, Train Loss: 0.0815, Accuracy: 0.9199, F1 Micro: 0.7695, F1 Macro: 0.6783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9243, F1 Micro: 0.7742, F1 Macro: 0.6963\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9243, F1 Micro: 0.7742, F1 Macro: 0.6963\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.74      0.59      0.65       402\n",
      "  HS_Religion       0.85      0.53      0.65       157\n",
      "      HS_Race       0.85      0.62      0.72       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.69      0.51      0.59       331\n",
      "    HS_Strong       0.85      0.89      0.87       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.78      0.65      0.70      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 186.27894639968872 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9133, F1 Micro: 0.7413, F1 Macro: 0.6131\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.78992247581482 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3958, Accuracy: 0.8839, F1 Micro: 0.6185, F1 Macro: 0.3017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2693, Accuracy: 0.9039, F1 Micro: 0.6869, F1 Macro: 0.4561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2228, Accuracy: 0.911, F1 Micro: 0.7447, F1 Macro: 0.5688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9195, F1 Micro: 0.7533, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9216, F1 Micro: 0.7648, F1 Macro: 0.6344\n",
      "Epoch 6/10, Train Loss: 0.1334, Accuracy: 0.9194, F1 Micro: 0.7631, F1 Macro: 0.6535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9237, F1 Micro: 0.7765, F1 Macro: 0.68\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9235, F1 Micro: 0.7722, F1 Macro: 0.6853\n",
      "Epoch 9/10, Train Loss: 0.0818, Accuracy: 0.923, F1 Micro: 0.7735, F1 Macro: 0.6909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9227, F1 Micro: 0.777, F1 Macro: 0.7009\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9227, F1 Micro: 0.777, F1 Macro: 0.7009\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.68      0.67      0.68       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.60      0.36      0.45        72\n",
      "    HS_Gender       0.56      0.45      0.50        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.63      0.54      0.58       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.73      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 185.29569816589355 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3988, Accuracy: 0.8831, F1 Micro: 0.5966, F1 Macro: 0.2824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.9019, F1 Micro: 0.673, F1 Macro: 0.4527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2256, Accuracy: 0.9114, F1 Micro: 0.7407, F1 Macro: 0.5646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.9193, F1 Micro: 0.7597, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1542, Accuracy: 0.9213, F1 Micro: 0.7653, F1 Macro: 0.6385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.9222, F1 Micro: 0.7691, F1 Macro: 0.6621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9218, F1 Micro: 0.7697, F1 Macro: 0.6619\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9226, F1 Micro: 0.7589, F1 Macro: 0.6517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.9218, F1 Micro: 0.7717, F1 Macro: 0.6846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9245, F1 Micro: 0.7777, F1 Macro: 0.7074\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9245, F1 Micro: 0.7777, F1 Macro: 0.7074\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.74      0.62      0.68       402\n",
      "  HS_Religion       0.79      0.63      0.70       157\n",
      "      HS_Race       0.84      0.63      0.72       120\n",
      "  HS_Physical       0.80      0.28      0.41        72\n",
      "    HS_Gender       0.69      0.47      0.56        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 187.75205731391907 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4007, Accuracy: 0.8831, F1 Micro: 0.6196, F1 Macro: 0.3047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2716, Accuracy: 0.9042, F1 Micro: 0.6876, F1 Macro: 0.4776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2263, Accuracy: 0.9088, F1 Micro: 0.7394, F1 Macro: 0.5657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9153, F1 Micro: 0.7574, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.9219, F1 Micro: 0.7646, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1344, Accuracy: 0.9211, F1 Micro: 0.7667, F1 Macro: 0.6593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1157, Accuracy: 0.9209, F1 Micro: 0.7672, F1 Macro: 0.6495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9213, F1 Micro: 0.7741, F1 Macro: 0.6747\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9185, F1 Micro: 0.7681, F1 Macro: 0.6766\n",
      "Epoch 10/10, Train Loss: 0.0769, Accuracy: 0.923, F1 Micro: 0.7612, F1 Macro: 0.6786\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9213, F1 Micro: 0.7741, F1 Macro: 0.6747\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.66      0.71      0.68       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.59      0.62      0.60       331\n",
      "    HS_Strong       0.83      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 186.27009272575378 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9137, F1 Micro: 0.743, F1 Macro: 0.6171\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 18.39872717857361 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3933, Accuracy: 0.8842, F1 Micro: 0.5873, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2685, Accuracy: 0.9058, F1 Micro: 0.7146, F1 Macro: 0.5056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.911, F1 Micro: 0.7414, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1852, Accuracy: 0.9203, F1 Micro: 0.7527, F1 Macro: 0.6102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1516, Accuracy: 0.9223, F1 Micro: 0.7672, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.13, Accuracy: 0.9221, F1 Micro: 0.7677, F1 Macro: 0.674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9194, F1 Micro: 0.7715, F1 Macro: 0.6791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9225, F1 Micro: 0.7732, F1 Macro: 0.6875\n",
      "Epoch 9/10, Train Loss: 0.0808, Accuracy: 0.9241, F1 Micro: 0.7712, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9226, F1 Micro: 0.7757, F1 Macro: 0.6977\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9226, F1 Micro: 0.7757, F1 Macro: 0.6977\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.75      0.72      0.73       120\n",
      "  HS_Physical       0.77      0.24      0.36        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.84      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.74      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 190.25523042678833 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3952, Accuracy: 0.885, F1 Micro: 0.6028, F1 Macro: 0.2827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2695, Accuracy: 0.9047, F1 Micro: 0.711, F1 Macro: 0.514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2214, Accuracy: 0.9112, F1 Micro: 0.7388, F1 Macro: 0.5881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9201, F1 Micro: 0.7552, F1 Macro: 0.6149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9224, F1 Micro: 0.7686, F1 Macro: 0.6308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1323, Accuracy: 0.9237, F1 Micro: 0.7709, F1 Macro: 0.6675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1116, Accuracy: 0.9252, F1 Micro: 0.7773, F1 Macro: 0.6851\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9212, F1 Micro: 0.7725, F1 Macro: 0.6844\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9225, F1 Micro: 0.773, F1 Macro: 0.699\n",
      "Epoch 10/10, Train Loss: 0.066, Accuracy: 0.9223, F1 Micro: 0.7765, F1 Macro: 0.7067\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9252, F1 Micro: 0.7773, F1 Macro: 0.6851\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.75      0.63      0.69       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.82      0.64      0.72       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 186.3917670249939 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3971, Accuracy: 0.8847, F1 Micro: 0.6069, F1 Macro: 0.2974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.27, Accuracy: 0.9053, F1 Micro: 0.7015, F1 Macro: 0.5149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2212, Accuracy: 0.912, F1 Micro: 0.7417, F1 Macro: 0.5846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.9191, F1 Micro: 0.7469, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.9188, F1 Micro: 0.7643, F1 Macro: 0.6194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1305, Accuracy: 0.9214, F1 Micro: 0.765, F1 Macro: 0.6549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1118, Accuracy: 0.9222, F1 Micro: 0.7759, F1 Macro: 0.6797\n",
      "Epoch 8/10, Train Loss: 0.0958, Accuracy: 0.9211, F1 Micro: 0.7716, F1 Macro: 0.679\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9209, F1 Micro: 0.7698, F1 Macro: 0.6911\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.923, F1 Micro: 0.7743, F1 Macro: 0.6908\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9222, F1 Micro: 0.7759, F1 Macro: 0.6797\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.71      0.77      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.83      0.65      0.73       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.57      0.33      0.42        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.56      0.59       331\n",
      "    HS_Strong       0.86      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 186.21975326538086 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9142, F1 Micro: 0.7446, F1 Macro: 0.6205\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 18.22283148765564 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3911, Accuracy: 0.8873, F1 Micro: 0.6329, F1 Macro: 0.3199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.9057, F1 Micro: 0.7137, F1 Macro: 0.5008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2179, Accuracy: 0.9122, F1 Micro: 0.7348, F1 Macro: 0.5549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1849, Accuracy: 0.9188, F1 Micro: 0.759, F1 Macro: 0.6153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.9229, F1 Micro: 0.7664, F1 Macro: 0.6392\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9211, F1 Micro: 0.7606, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1051, Accuracy: 0.9255, F1 Micro: 0.7758, F1 Macro: 0.6698\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.924, F1 Micro: 0.7715, F1 Macro: 0.6876\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9236, F1 Micro: 0.7717, F1 Macro: 0.6851\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9241, F1 Micro: 0.77, F1 Macro: 0.6884\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9255, F1 Micro: 0.7758, F1 Macro: 0.6698\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.76      0.71      0.73       732\n",
      "     HS_Group       0.74      0.65      0.69       402\n",
      "  HS_Religion       0.80      0.54      0.65       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.75      0.24      0.36        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.74      0.70      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.80      0.62      0.67      5556\n",
      " weighted avg       0.81      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 185.75020098686218 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.393, Accuracy: 0.8834, F1 Micro: 0.6049, F1 Macro: 0.2973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.9049, F1 Micro: 0.7075, F1 Macro: 0.4825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.9142, F1 Micro: 0.7339, F1 Macro: 0.5575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1874, Accuracy: 0.9196, F1 Micro: 0.7596, F1 Macro: 0.6168\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.921, F1 Micro: 0.7505, F1 Macro: 0.6127\n",
      "Epoch 6/10, Train Loss: 0.1327, Accuracy: 0.9234, F1 Micro: 0.7595, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9218, F1 Micro: 0.7685, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9232, F1 Micro: 0.7702, F1 Macro: 0.6879\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.92, F1 Micro: 0.7661, F1 Macro: 0.6897\n",
      "Epoch 10/10, Train Loss: 0.0705, Accuracy: 0.9219, F1 Micro: 0.7679, F1 Macro: 0.6921\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9232, F1 Micro: 0.7702, F1 Macro: 0.6879\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.82      0.84      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.68      0.72       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.78      0.61      0.68       157\n",
      "      HS_Race       0.85      0.68      0.76       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.58      0.35      0.44        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.73      0.65      0.69       689\n",
      "  HS_Moderate       0.61      0.60      0.61       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 184.3683888912201 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.396, Accuracy: 0.8845, F1 Micro: 0.6125, F1 Macro: 0.3037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2709, Accuracy: 0.9031, F1 Micro: 0.7099, F1 Macro: 0.5049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.9145, F1 Micro: 0.7329, F1 Macro: 0.5638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9177, F1 Micro: 0.7545, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1544, Accuracy: 0.9225, F1 Micro: 0.7584, F1 Macro: 0.6202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9233, F1 Micro: 0.7606, F1 Macro: 0.6283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9222, F1 Micro: 0.7665, F1 Macro: 0.6543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.923, F1 Micro: 0.7767, F1 Macro: 0.6931\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9227, F1 Micro: 0.7702, F1 Macro: 0.6694\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9226, F1 Micro: 0.7685, F1 Macro: 0.6869\n",
      "Model 3 - Iteration 9418: Accuracy: 0.923, F1 Micro: 0.7767, F1 Macro: 0.6931\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.67      0.71      0.69       402\n",
      "  HS_Religion       0.67      0.66      0.67       157\n",
      "      HS_Race       0.83      0.66      0.73       120\n",
      "  HS_Physical       0.75      0.21      0.33        72\n",
      "    HS_Gender       0.54      0.41      0.47        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.58      0.65      0.61       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.20254921913147 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9146, F1 Micro: 0.746, F1 Macro: 0.6233\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.554982900619507 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3882, Accuracy: 0.8832, F1 Micro: 0.5723, F1 Macro: 0.2849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9031, F1 Micro: 0.6629, F1 Macro: 0.4465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2156, Accuracy: 0.9162, F1 Micro: 0.7441, F1 Macro: 0.5705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1782, Accuracy: 0.9188, F1 Micro: 0.763, F1 Macro: 0.6111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.9218, F1 Micro: 0.7719, F1 Macro: 0.6569\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.921, F1 Micro: 0.7704, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.9244, F1 Micro: 0.776, F1 Macro: 0.6876\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.9249, F1 Micro: 0.7752, F1 Macro: 0.6901\n",
      "Epoch 9/10, Train Loss: 0.0804, Accuracy: 0.9214, F1 Micro: 0.7714, F1 Macro: 0.6921\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9211, F1 Micro: 0.7739, F1 Macro: 0.7022\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9244, F1 Micro: 0.776, F1 Macro: 0.6876\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.79      0.59      0.67       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.61      0.15      0.24        72\n",
      "    HS_Gender       0.68      0.37      0.48        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.62      0.62      0.62       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.8964719772339 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3905, Accuracy: 0.8824, F1 Micro: 0.5705, F1 Macro: 0.2717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2657, Accuracy: 0.902, F1 Micro: 0.6631, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2197, Accuracy: 0.9152, F1 Micro: 0.7385, F1 Macro: 0.5598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1815, Accuracy: 0.9192, F1 Micro: 0.7607, F1 Macro: 0.6056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9213, F1 Micro: 0.7738, F1 Macro: 0.6811\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9187, F1 Micro: 0.7665, F1 Macro: 0.6625\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9219, F1 Micro: 0.7728, F1 Macro: 0.6748\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.9231, F1 Micro: 0.7613, F1 Macro: 0.6739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9214, F1 Micro: 0.7745, F1 Macro: 0.6958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.069, Accuracy: 0.9228, F1 Micro: 0.776, F1 Macro: 0.7031\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9228, F1 Micro: 0.776, F1 Macro: 0.7031\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.81      0.70      0.75       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.71      0.43      0.54        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.86      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 190.97407388687134 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.8828, F1 Micro: 0.5688, F1 Macro: 0.2824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.9019, F1 Micro: 0.6564, F1 Macro: 0.4739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2193, Accuracy: 0.9153, F1 Micro: 0.734, F1 Macro: 0.5608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9182, F1 Micro: 0.7578, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.9224, F1 Micro: 0.7725, F1 Macro: 0.6397\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9195, F1 Micro: 0.7674, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.106, Accuracy: 0.9213, F1 Micro: 0.7734, F1 Macro: 0.6766\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.9244, F1 Micro: 0.7723, F1 Macro: 0.6737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0812, Accuracy: 0.9204, F1 Micro: 0.7746, F1 Macro: 0.6869\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9211, F1 Micro: 0.7696, F1 Macro: 0.691\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9204, F1 Micro: 0.7746, F1 Macro: 0.6869\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.75      0.65      0.70       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.73      0.37      0.49        51\n",
      "     HS_Other       0.72      0.84      0.77       762\n",
      "      HS_Weak       0.67      0.78      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 190.89494037628174 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.915, F1 Micro: 0.7473, F1 Macro: 0.6263\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.216819286346436 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3881, Accuracy: 0.8869, F1 Micro: 0.6504, F1 Macro: 0.3349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2609, Accuracy: 0.9057, F1 Micro: 0.7239, F1 Macro: 0.5361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.9145, F1 Micro: 0.7342, F1 Macro: 0.5597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.92, F1 Micro: 0.7508, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9209, F1 Micro: 0.7692, F1 Macro: 0.6568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9229, F1 Micro: 0.7714, F1 Macro: 0.6564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1041, Accuracy: 0.9223, F1 Micro: 0.7723, F1 Macro: 0.6665\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.9252, F1 Micro: 0.772, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9248, F1 Micro: 0.7732, F1 Macro: 0.6919\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9233, F1 Micro: 0.773, F1 Macro: 0.6882\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9248, F1 Micro: 0.7732, F1 Macro: 0.6919\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.76      0.67      0.71       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.79      0.59      0.67       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.83      0.21      0.33        72\n",
      "    HS_Gender       0.74      0.33      0.46        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.74      0.65      0.69       689\n",
      "  HS_Moderate       0.63      0.59      0.61       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.79      0.65      0.69      5556\n",
      " weighted avg       0.80      0.74      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 194.35808849334717 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3916, Accuracy: 0.8857, F1 Micro: 0.6295, F1 Macro: 0.3185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.264, Accuracy: 0.9037, F1 Micro: 0.718, F1 Macro: 0.5298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2193, Accuracy: 0.9149, F1 Micro: 0.7339, F1 Macro: 0.5654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1883, Accuracy: 0.9206, F1 Micro: 0.7496, F1 Macro: 0.5993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9163, F1 Micro: 0.7663, F1 Macro: 0.6688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.9195, F1 Micro: 0.7668, F1 Macro: 0.6575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9225, F1 Micro: 0.7677, F1 Macro: 0.6623\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9239, F1 Micro: 0.7677, F1 Macro: 0.6787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9211, F1 Micro: 0.771, F1 Macro: 0.6866\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9225, F1 Micro: 0.77, F1 Macro: 0.6887\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9211, F1 Micro: 0.771, F1 Macro: 0.6866\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.76      0.68      0.71       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.72      0.35      0.47        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.38208889961243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3938, Accuracy: 0.8852, F1 Micro: 0.6351, F1 Macro: 0.328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.265, Accuracy: 0.9017, F1 Micro: 0.7204, F1 Macro: 0.5357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2192, Accuracy: 0.9153, F1 Micro: 0.7351, F1 Macro: 0.57\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1878, Accuracy: 0.9195, F1 Micro: 0.7467, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1558, Accuracy: 0.9209, F1 Micro: 0.7728, F1 Macro: 0.6547\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9209, F1 Micro: 0.7618, F1 Macro: 0.65\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.924, F1 Micro: 0.7732, F1 Macro: 0.6563\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9241, F1 Micro: 0.7688, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.9251, F1 Micro: 0.7756, F1 Macro: 0.6845\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9217, F1 Micro: 0.7704, F1 Macro: 0.685\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9251, F1 Micro: 0.7756, F1 Macro: 0.6845\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.73      0.66      0.69       402\n",
      "  HS_Religion       0.80      0.59      0.68       157\n",
      "      HS_Race       0.81      0.71      0.76       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.71      0.29      0.42        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.65      0.59      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.79      0.64      0.68      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 193.44945430755615 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9153, F1 Micro: 0.7483, F1 Macro: 0.6289\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.282498598098755 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.8885, F1 Micro: 0.6317, F1 Macro: 0.3313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2588, Accuracy: 0.9083, F1 Micro: 0.7082, F1 Macro: 0.5267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9178, F1 Micro: 0.7514, F1 Macro: 0.5946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.177, Accuracy: 0.9206, F1 Micro: 0.7632, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.9236, F1 Micro: 0.7668, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9238, F1 Micro: 0.7703, F1 Macro: 0.6645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1063, Accuracy: 0.9216, F1 Micro: 0.7736, F1 Macro: 0.6953\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9204, F1 Micro: 0.773, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9241, F1 Micro: 0.7737, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9267, F1 Micro: 0.7793, F1 Macro: 0.7079\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9267, F1 Micro: 0.7793, F1 Macro: 0.7079\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.74      0.59      0.65       157\n",
      "      HS_Race       0.83      0.65      0.73       120\n",
      "  HS_Physical       0.78      0.35      0.48        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.81      0.78      0.79       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.68      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.78      0.66      0.71      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 199.97485280036926 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3874, Accuracy: 0.8864, F1 Micro: 0.6156, F1 Macro: 0.3213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2603, Accuracy: 0.9054, F1 Micro: 0.6848, F1 Macro: 0.505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2162, Accuracy: 0.9162, F1 Micro: 0.7403, F1 Macro: 0.5754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1785, Accuracy: 0.9217, F1 Micro: 0.7604, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9222, F1 Micro: 0.7638, F1 Macro: 0.6382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.924, F1 Micro: 0.7701, F1 Macro: 0.6555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9245, F1 Micro: 0.7737, F1 Macro: 0.6892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.9226, F1 Micro: 0.7768, F1 Macro: 0.6842\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9239, F1 Micro: 0.77, F1 Macro: 0.6931\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9263, F1 Micro: 0.7739, F1 Macro: 0.6994\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9226, F1 Micro: 0.7768, F1 Macro: 0.6842\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.69      0.70      0.70       402\n",
      "  HS_Religion       0.76      0.62      0.68       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.73      0.31      0.44        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.61      0.62      0.61       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 198.07158875465393 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3883, Accuracy: 0.8863, F1 Micro: 0.6186, F1 Macro: 0.3161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2609, Accuracy: 0.9063, F1 Micro: 0.6997, F1 Macro: 0.5295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2168, Accuracy: 0.9168, F1 Micro: 0.7459, F1 Macro: 0.5862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1788, Accuracy: 0.9194, F1 Micro: 0.7615, F1 Macro: 0.6201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1508, Accuracy: 0.9237, F1 Micro: 0.7719, F1 Macro: 0.6388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1236, Accuracy: 0.9242, F1 Micro: 0.7745, F1 Macro: 0.6589\n",
      "Epoch 7/10, Train Loss: 0.1046, Accuracy: 0.9238, F1 Micro: 0.7689, F1 Macro: 0.675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0878, Accuracy: 0.9243, F1 Micro: 0.7791, F1 Macro: 0.6899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.924, F1 Micro: 0.7791, F1 Macro: 0.6912\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9252, F1 Micro: 0.7762, F1 Macro: 0.6947\n",
      "Model 3 - Iteration 10018: Accuracy: 0.924, F1 Micro: 0.7791, F1 Macro: 0.6912\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.68      0.65      0.66       157\n",
      "      HS_Race       0.79      0.72      0.75       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 198.28092980384827 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9157, F1 Micro: 0.7495, F1 Macro: 0.6315\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.689225912094116 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3807, Accuracy: 0.8887, F1 Micro: 0.6169, F1 Macro: 0.3125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2566, Accuracy: 0.9073, F1 Micro: 0.7205, F1 Macro: 0.5325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2109, Accuracy: 0.9148, F1 Micro: 0.7426, F1 Macro: 0.575\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.9191, F1 Micro: 0.7391, F1 Macro: 0.6027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9179, F1 Micro: 0.7697, F1 Macro: 0.6417\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9171, F1 Micro: 0.7664, F1 Macro: 0.67\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1013, Accuracy: 0.9253, F1 Micro: 0.7746, F1 Macro: 0.6858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9255, F1 Micro: 0.7766, F1 Macro: 0.6797\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9243, F1 Micro: 0.7715, F1 Macro: 0.7032\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9261, F1 Micro: 0.7764, F1 Macro: 0.703\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9255, F1 Micro: 0.7766, F1 Macro: 0.6797\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.85      0.69      0.76       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.76      0.25      0.38        51\n",
      "     HS_Other       0.79      0.80      0.80       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.90      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.79      0.63      0.68      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 197.74181962013245 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3836, Accuracy: 0.8864, F1 Micro: 0.6065, F1 Macro: 0.3136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.9044, F1 Micro: 0.7094, F1 Macro: 0.4828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.9137, F1 Micro: 0.7419, F1 Macro: 0.5851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9209, F1 Micro: 0.7507, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9212, F1 Micro: 0.7701, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9242, F1 Micro: 0.7708, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9241, F1 Micro: 0.7708, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9205, F1 Micro: 0.7722, F1 Macro: 0.6869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9248, F1 Micro: 0.7755, F1 Macro: 0.706\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9217, F1 Micro: 0.7746, F1 Macro: 0.7028\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9248, F1 Micro: 0.7755, F1 Macro: 0.706\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.77      0.67      0.72       732\n",
      "     HS_Group       0.69      0.70      0.69       402\n",
      "  HS_Religion       0.81      0.57      0.67       157\n",
      "      HS_Race       0.82      0.70      0.76       120\n",
      "  HS_Physical       0.83      0.28      0.42        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.75      0.65      0.70       689\n",
      "  HS_Moderate       0.61      0.63      0.62       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 203.47024774551392 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3866, Accuracy: 0.8861, F1 Micro: 0.6053, F1 Macro: 0.3025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2608, Accuracy: 0.9072, F1 Micro: 0.7099, F1 Macro: 0.4989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2156, Accuracy: 0.9138, F1 Micro: 0.743, F1 Macro: 0.5868\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.919, F1 Micro: 0.7429, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9171, F1 Micro: 0.7648, F1 Macro: 0.622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1238, Accuracy: 0.9187, F1 Micro: 0.766, F1 Macro: 0.6559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1028, Accuracy: 0.9227, F1 Micro: 0.7702, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9226, F1 Micro: 0.7738, F1 Macro: 0.677\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.924, F1 Micro: 0.7684, F1 Macro: 0.6907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9243, F1 Micro: 0.7776, F1 Macro: 0.7027\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9243, F1 Micro: 0.7776, F1 Macro: 0.7027\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.80      0.67      0.73       120\n",
      "  HS_Physical       0.95      0.29      0.45        72\n",
      "    HS_Gender       0.58      0.49      0.53        51\n",
      "     HS_Other       0.80      0.80      0.80       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.61      0.54      0.57       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 202.18756699562073 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.916, F1 Micro: 0.7506, F1 Macro: 0.634\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.90325140953064 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3764, Accuracy: 0.8896, F1 Micro: 0.6284, F1 Macro: 0.3422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2494, Accuracy: 0.909, F1 Micro: 0.7083, F1 Macro: 0.5082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9157, F1 Micro: 0.7446, F1 Macro: 0.569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9194, F1 Micro: 0.7623, F1 Macro: 0.6085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.9242, F1 Micro: 0.7724, F1 Macro: 0.6568\n",
      "Epoch 6/10, Train Loss: 0.1222, Accuracy: 0.9239, F1 Micro: 0.7705, F1 Macro: 0.6672\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9235, F1 Micro: 0.762, F1 Macro: 0.6598\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9203, F1 Micro: 0.7665, F1 Macro: 0.6729\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9224, F1 Micro: 0.7714, F1 Macro: 0.6906\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9224, F1 Micro: 0.7722, F1 Macro: 0.7027\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9242, F1 Micro: 0.7724, F1 Macro: 0.6568\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.83      0.52      0.64       157\n",
      "      HS_Race       0.82      0.71      0.76       120\n",
      "  HS_Physical       0.69      0.12      0.21        72\n",
      "    HS_Gender       0.58      0.14      0.22        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.66      0.51      0.58       331\n",
      "    HS_Strong       0.91      0.76      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.61      0.66      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 200.09825325012207 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.8872, F1 Micro: 0.6148, F1 Macro: 0.3288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2521, Accuracy: 0.9067, F1 Micro: 0.6999, F1 Macro: 0.4857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9135, F1 Micro: 0.7444, F1 Macro: 0.5777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9196, F1 Micro: 0.7661, F1 Macro: 0.6292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9234, F1 Micro: 0.7715, F1 Macro: 0.668\n",
      "Epoch 6/10, Train Loss: 0.124, Accuracy: 0.9215, F1 Micro: 0.7695, F1 Macro: 0.6717\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9216, F1 Micro: 0.7714, F1 Macro: 0.673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.9226, F1 Micro: 0.7751, F1 Macro: 0.6776\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.9194, F1 Micro: 0.7693, F1 Macro: 0.6921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9247, F1 Micro: 0.7761, F1 Macro: 0.7043\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9247, F1 Micro: 0.7761, F1 Macro: 0.7043\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.76      0.68      0.72       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.81      0.61      0.69       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.73      0.43      0.54        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.74      0.66      0.70       689\n",
      "  HS_Moderate       0.61      0.62      0.61       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.67      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 203.82058119773865 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3819, Accuracy: 0.8877, F1 Micro: 0.6315, F1 Macro: 0.3329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2513, Accuracy: 0.9078, F1 Micro: 0.7058, F1 Macro: 0.5003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2086, Accuracy: 0.915, F1 Micro: 0.7408, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1727, Accuracy: 0.9191, F1 Micro: 0.76, F1 Macro: 0.6011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.9244, F1 Micro: 0.7764, F1 Macro: 0.6593\n",
      "Epoch 6/10, Train Loss: 0.1235, Accuracy: 0.9183, F1 Micro: 0.7657, F1 Macro: 0.6607\n",
      "Epoch 7/10, Train Loss: 0.1002, Accuracy: 0.9239, F1 Micro: 0.7688, F1 Macro: 0.6442\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9223, F1 Micro: 0.7702, F1 Macro: 0.6715\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9214, F1 Micro: 0.7692, F1 Macro: 0.6798\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9223, F1 Micro: 0.7729, F1 Macro: 0.7\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9244, F1 Micro: 0.7764, F1 Macro: 0.6593\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.82      0.56      0.67       157\n",
      "      HS_Race       0.78      0.70      0.74       120\n",
      "  HS_Physical       0.62      0.07      0.12        72\n",
      "    HS_Gender       0.55      0.24      0.33        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.91      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.62      0.66      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.42      0.42      0.41      5556\n",
      "\n",
      "Training completed in 200.3819396495819 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9164, F1 Micro: 0.7515, F1 Macro: 0.6355\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.8063156604766846 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3765, Accuracy: 0.8807, F1 Micro: 0.5372, F1 Macro: 0.2613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.9051, F1 Micro: 0.7248, F1 Macro: 0.537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.201, Accuracy: 0.9165, F1 Micro: 0.7445, F1 Macro: 0.5761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9201, F1 Micro: 0.7654, F1 Macro: 0.6205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9208, F1 Micro: 0.7669, F1 Macro: 0.6494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1166, Accuracy: 0.9242, F1 Micro: 0.7777, F1 Macro: 0.6672\n",
      "Epoch 7/10, Train Loss: 0.0979, Accuracy: 0.9243, F1 Micro: 0.7648, F1 Macro: 0.6754\n",
      "Epoch 8/10, Train Loss: 0.081, Accuracy: 0.9207, F1 Micro: 0.7646, F1 Macro: 0.6613\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.923, F1 Micro: 0.7768, F1 Macro: 0.6927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.059, Accuracy: 0.9235, F1 Micro: 0.7782, F1 Macro: 0.703\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9235, F1 Micro: 0.7782, F1 Macro: 0.703\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.70      0.61      0.66       402\n",
      "  HS_Religion       0.66      0.62      0.64       157\n",
      "      HS_Race       0.76      0.69      0.72       120\n",
      "  HS_Physical       0.89      0.33      0.48        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.51      0.57       331\n",
      "    HS_Strong       0.83      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 206.09369921684265 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3781, Accuracy: 0.8819, F1 Micro: 0.5503, F1 Macro: 0.2548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2533, Accuracy: 0.9058, F1 Micro: 0.7183, F1 Macro: 0.5344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2048, Accuracy: 0.9172, F1 Micro: 0.7457, F1 Macro: 0.5786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9198, F1 Micro: 0.7614, F1 Macro: 0.6193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1451, Accuracy: 0.9184, F1 Micro: 0.7667, F1 Macro: 0.6477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.9244, F1 Micro: 0.7746, F1 Macro: 0.6724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1022, Accuracy: 0.9224, F1 Micro: 0.7755, F1 Macro: 0.6957\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9198, F1 Micro: 0.7658, F1 Macro: 0.6662\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9217, F1 Micro: 0.7693, F1 Macro: 0.6758\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9196, F1 Micro: 0.7712, F1 Macro: 0.6916\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9224, F1 Micro: 0.7755, F1 Macro: 0.6957\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.69      0.79      0.73       732\n",
      "     HS_Group       0.77      0.58      0.66       402\n",
      "  HS_Religion       0.81      0.56      0.66       157\n",
      "      HS_Race       0.81      0.66      0.73       120\n",
      "  HS_Physical       0.54      0.26      0.36        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.67      0.77      0.72       689\n",
      "  HS_Moderate       0.71      0.51      0.59       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.75      0.66      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 205.64262890815735 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3806, Accuracy: 0.8803, F1 Micro: 0.5437, F1 Macro: 0.2647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.9071, F1 Micro: 0.7143, F1 Macro: 0.5235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2047, Accuracy: 0.9166, F1 Micro: 0.7428, F1 Macro: 0.5767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1678, Accuracy: 0.9202, F1 Micro: 0.762, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1427, Accuracy: 0.9198, F1 Micro: 0.7669, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9223, F1 Micro: 0.7735, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1008, Accuracy: 0.9229, F1 Micro: 0.7761, F1 Macro: 0.6838\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9186, F1 Micro: 0.761, F1 Macro: 0.6575\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.923, F1 Micro: 0.7713, F1 Macro: 0.6859\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9157, F1 Micro: 0.7698, F1 Macro: 0.7002\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9229, F1 Micro: 0.7761, F1 Macro: 0.6838\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.80      0.68      0.73       120\n",
      "  HS_Physical       0.52      0.19      0.28        72\n",
      "    HS_Gender       0.56      0.35      0.43        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.73      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 205.29642724990845 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9166, F1 Micro: 0.7524, F1 Macro: 0.6376\n",
      "Total sampling time: 1259.93 seconds\n",
      "Total runtime: 14763.108313083649 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f146eb",
   "metadata": {
    "papermill": {
     "duration": 0.012271,
     "end_time": "2025-04-13T07:01:23.427995",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.415724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461855c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5809, Accuracy: 0.8245, F1 Micro: 0.3956, F1 Macro: 0.1034\n",
      "Epoch 2/10, Train Loss: 0.4658, Accuracy: 0.8348, F1 Micro: 0.2636, F1 Macro: 0.0592\n",
      "Epoch 3/10, Train Loss: 0.397, Accuracy: 0.8338, F1 Micro: 0.0955, F1 Macro: 0.0378\n",
      "Epoch 4/10, Train Loss: 0.3979, Accuracy: 0.8362, F1 Micro: 0.14, F1 Macro: 0.0465\n",
      "Epoch 5/10, Train Loss: 0.383, Accuracy: 0.8437, F1 Micro: 0.2102, F1 Macro: 0.0769\n",
      "Epoch 6/10, Train Loss: 0.3654, Accuracy: 0.8522, F1 Micro: 0.3188, F1 Macro: 0.1133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3505, Accuracy: 0.8651, F1 Micro: 0.4503, F1 Macro: 0.1947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3265, Accuracy: 0.8738, F1 Micro: 0.5242, F1 Macro: 0.2445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2968, Accuracy: 0.8777, F1 Micro: 0.5603, F1 Macro: 0.2621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2615, Accuracy: 0.8778, F1 Micro: 0.5617, F1 Macro: 0.2627\n",
      "Model 1 - Iteration 658: Accuracy: 0.8778, F1 Micro: 0.5617, F1 Macro: 0.2627\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.70      0.75      1134\n",
      "      Abusive       0.84      0.69      0.75       992\n",
      "HS_Individual       0.65      0.47      0.55       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.52      0.59       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.74      0.45      0.56      5556\n",
      "    macro avg       0.30      0.24      0.26      5556\n",
      " weighted avg       0.57      0.45      0.50      5556\n",
      "  samples avg       0.36      0.26      0.28      5556\n",
      "\n",
      "Training completed in 46.810333251953125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6069, Accuracy: 0.818, F1 Micro: 0.3417, F1 Macro: 0.0941\n",
      "Epoch 2/10, Train Loss: 0.4689, Accuracy: 0.8331, F1 Micro: 0.122, F1 Macro: 0.0467\n",
      "Epoch 3/10, Train Loss: 0.3944, Accuracy: 0.8344, F1 Micro: 0.0998, F1 Macro: 0.0364\n",
      "Epoch 4/10, Train Loss: 0.3974, Accuracy: 0.8374, F1 Micro: 0.1422, F1 Macro: 0.047\n",
      "Epoch 5/10, Train Loss: 0.3818, Accuracy: 0.8464, F1 Micro: 0.2441, F1 Macro: 0.0839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3661, Accuracy: 0.8562, F1 Micro: 0.35, F1 Macro: 0.1133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3507, Accuracy: 0.865, F1 Micro: 0.4447, F1 Macro: 0.1653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.324, Accuracy: 0.8743, F1 Micro: 0.5154, F1 Macro: 0.2311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2909, Accuracy: 0.8765, F1 Micro: 0.5649, F1 Macro: 0.2614\n",
      "Epoch 10/10, Train Loss: 0.2604, Accuracy: 0.8767, F1 Micro: 0.557, F1 Macro: 0.2676\n",
      "Model 2 - Iteration 658: Accuracy: 0.8765, F1 Micro: 0.5649, F1 Macro: 0.2614\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.72      0.76      1134\n",
      "      Abusive       0.81      0.73      0.77       992\n",
      "HS_Individual       0.64      0.45      0.53       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.62      0.55      0.58       762\n",
      "      HS_Weak       0.60      0.42      0.50       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.47      0.56      5556\n",
      "    macro avg       0.29      0.24      0.26      5556\n",
      " weighted avg       0.55      0.47      0.50      5556\n",
      "  samples avg       0.37      0.27      0.29      5556\n",
      "\n",
      "Training completed in 48.71611785888672 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6221, Accuracy: 0.801, F1 Micro: 0.2723, F1 Macro: 0.0864\n",
      "Epoch 2/10, Train Loss: 0.4752, Accuracy: 0.8292, F1 Micro: 0.0255, F1 Macro: 0.0111\n",
      "Epoch 3/10, Train Loss: 0.3966, Accuracy: 0.83, F1 Micro: 0.0308, F1 Macro: 0.0133\n",
      "Epoch 4/10, Train Loss: 0.4013, Accuracy: 0.8341, F1 Micro: 0.1023, F1 Macro: 0.0364\n",
      "Epoch 5/10, Train Loss: 0.3872, Accuracy: 0.8389, F1 Micro: 0.1576, F1 Macro: 0.0571\n",
      "Epoch 6/10, Train Loss: 0.3748, Accuracy: 0.8414, F1 Micro: 0.1864, F1 Macro: 0.0657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3672, Accuracy: 0.8496, F1 Micro: 0.2928, F1 Macro: 0.0947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3502, Accuracy: 0.8627, F1 Micro: 0.4235, F1 Macro: 0.1614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3159, Accuracy: 0.8761, F1 Micro: 0.5531, F1 Macro: 0.2496\n",
      "Epoch 10/10, Train Loss: 0.2849, Accuracy: 0.8769, F1 Micro: 0.5458, F1 Macro: 0.2515\n",
      "Model 3 - Iteration 658: Accuracy: 0.8761, F1 Micro: 0.5531, F1 Macro: 0.2496\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.73      0.75      1134\n",
      "      Abusive       0.77      0.76      0.77       992\n",
      "HS_Individual       0.69      0.37      0.48       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.55      0.59       762\n",
      "      HS_Weak       0.70      0.29      0.41       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.44      0.55      5556\n",
      "    macro avg       0.30      0.22      0.25      5556\n",
      " weighted avg       0.56      0.44      0.48      5556\n",
      "  samples avg       0.39      0.27      0.29      5556\n",
      "\n",
      "Training completed in 46.977036476135254 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8768, F1 Micro: 0.5599, F1 Macro: 0.2579\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 144.9790506362915 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.517, Accuracy: 0.8358, F1 Micro: 0.2598, F1 Macro: 0.06\n",
      "Epoch 2/10, Train Loss: 0.4025, Accuracy: 0.8368, F1 Micro: 0.1359, F1 Macro: 0.0509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3656, Accuracy: 0.859, F1 Micro: 0.4069, F1 Macro: 0.1484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3425, Accuracy: 0.878, F1 Micro: 0.562, F1 Macro: 0.2622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3036, Accuracy: 0.8815, F1 Micro: 0.5832, F1 Macro: 0.2765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2824, Accuracy: 0.8848, F1 Micro: 0.6496, F1 Macro: 0.3465\n",
      "Epoch 7/10, Train Loss: 0.2537, Accuracy: 0.886, F1 Micro: 0.5916, F1 Macro: 0.3106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2258, Accuracy: 0.8904, F1 Micro: 0.6655, F1 Macro: 0.4031\n",
      "Epoch 9/10, Train Loss: 0.1968, Accuracy: 0.8938, F1 Micro: 0.6507, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1879, Accuracy: 0.8938, F1 Micro: 0.688, F1 Macro: 0.4807\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8938, F1 Micro: 0.688, F1 Macro: 0.4807\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.86      0.81      1134\n",
      "      Abusive       0.84      0.83      0.84       992\n",
      "HS_Individual       0.63      0.69      0.66       732\n",
      "     HS_Group       0.59      0.53      0.56       402\n",
      "  HS_Religion       0.69      0.21      0.32       157\n",
      "      HS_Race       0.79      0.53      0.63       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.75      0.70       762\n",
      "      HS_Weak       0.61      0.66      0.63       689\n",
      "  HS_Moderate       0.46      0.36      0.41       331\n",
      "    HS_Strong       1.00      0.11      0.20       114\n",
      "\n",
      "    micro avg       0.70      0.68      0.69      5556\n",
      "    macro avg       0.59      0.46      0.48      5556\n",
      " weighted avg       0.68      0.68      0.67      5556\n",
      "  samples avg       0.39      0.38      0.37      5556\n",
      "\n",
      "Training completed in 70.03231525421143 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5283, Accuracy: 0.8338, F1 Micro: 0.117, F1 Macro: 0.045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4027, Accuracy: 0.84, F1 Micro: 0.1634, F1 Macro: 0.0587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3631, Accuracy: 0.86, F1 Micro: 0.4173, F1 Macro: 0.1326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3396, Accuracy: 0.8773, F1 Micro: 0.5445, F1 Macro: 0.25\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.299, Accuracy: 0.88, F1 Micro: 0.6021, F1 Macro: 0.2839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2787, Accuracy: 0.883, F1 Micro: 0.646, F1 Macro: 0.3313\n",
      "Epoch 7/10, Train Loss: 0.2582, Accuracy: 0.8877, F1 Micro: 0.6104, F1 Macro: 0.3111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2236, Accuracy: 0.8861, F1 Micro: 0.6661, F1 Macro: 0.411\n",
      "Epoch 9/10, Train Loss: 0.2004, Accuracy: 0.8872, F1 Micro: 0.594, F1 Macro: 0.3821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1943, Accuracy: 0.8914, F1 Micro: 0.6815, F1 Macro: 0.4695\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8914, F1 Micro: 0.6815, F1 Macro: 0.4695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.86      0.81      1134\n",
      "      Abusive       0.84      0.83      0.84       992\n",
      "HS_Individual       0.62      0.70      0.66       732\n",
      "     HS_Group       0.58      0.50      0.54       402\n",
      "  HS_Religion       0.59      0.24      0.34       157\n",
      "      HS_Race       0.84      0.34      0.49       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.75      0.70       762\n",
      "      HS_Weak       0.60      0.66      0.63       689\n",
      "  HS_Moderate       0.46      0.36      0.40       331\n",
      "    HS_Strong       0.94      0.14      0.24       114\n",
      "\n",
      "    micro avg       0.69      0.67      0.68      5556\n",
      "    macro avg       0.57      0.45      0.47      5556\n",
      " weighted avg       0.68      0.67      0.66      5556\n",
      "  samples avg       0.39      0.38      0.36      5556\n",
      "\n",
      "Training completed in 71.57828378677368 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5355, Accuracy: 0.8287, F1 Micro: 0.0174, F1 Macro: 0.0077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4041, Accuracy: 0.8353, F1 Micro: 0.1009, F1 Macro: 0.0385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3699, Accuracy: 0.8506, F1 Micro: 0.3105, F1 Macro: 0.1\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3527, Accuracy: 0.8724, F1 Micro: 0.5192, F1 Macro: 0.2321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3125, Accuracy: 0.8803, F1 Micro: 0.5679, F1 Macro: 0.2634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2892, Accuracy: 0.8804, F1 Micro: 0.6486, F1 Macro: 0.3702\n",
      "Epoch 7/10, Train Loss: 0.2619, Accuracy: 0.8823, F1 Micro: 0.5516, F1 Macro: 0.2925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2335, Accuracy: 0.8938, F1 Micro: 0.665, F1 Macro: 0.424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2022, Accuracy: 0.8956, F1 Micro: 0.6722, F1 Macro: 0.4568\n",
      "Epoch 10/10, Train Loss: 0.1905, Accuracy: 0.8963, F1 Micro: 0.6714, F1 Macro: 0.4566\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8956, F1 Micro: 0.6722, F1 Macro: 0.4568\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.81      1134\n",
      "      Abusive       0.87      0.79      0.83       992\n",
      "HS_Individual       0.67      0.60      0.64       732\n",
      "     HS_Group       0.59      0.52      0.55       402\n",
      "  HS_Religion       0.68      0.13      0.22       157\n",
      "      HS_Race       0.85      0.42      0.57       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.68      0.69       762\n",
      "      HS_Weak       0.64      0.55      0.59       689\n",
      "  HS_Moderate       0.49      0.39      0.44       331\n",
      "    HS_Strong       1.00      0.08      0.15       114\n",
      "\n",
      "    micro avg       0.73      0.62      0.67      5556\n",
      "    macro avg       0.61      0.41      0.46      5556\n",
      " weighted avg       0.72      0.62      0.65      5556\n",
      "  samples avg       0.38      0.34      0.34      5556\n",
      "\n",
      "Training completed in 71.74174642562866 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8852, F1 Micro: 0.6202, F1 Macro: 0.3635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 128.75644278526306 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.8373, F1 Micro: 0.1472, F1 Macro: 0.0556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3795, Accuracy: 0.8581, F1 Micro: 0.4004, F1 Macro: 0.1391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.334, Accuracy: 0.8801, F1 Micro: 0.5671, F1 Macro: 0.2639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2977, Accuracy: 0.8875, F1 Micro: 0.6259, F1 Macro: 0.3269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.25, Accuracy: 0.892, F1 Micro: 0.6629, F1 Macro: 0.4091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.232, Accuracy: 0.8954, F1 Micro: 0.6851, F1 Macro: 0.4759\n",
      "Epoch 7/10, Train Loss: 0.2046, Accuracy: 0.8996, F1 Micro: 0.6821, F1 Macro: 0.486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1822, Accuracy: 0.9025, F1 Micro: 0.6988, F1 Macro: 0.4831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1576, Accuracy: 0.9037, F1 Micro: 0.7127, F1 Macro: 0.5322\n",
      "Epoch 10/10, Train Loss: 0.1376, Accuracy: 0.904, F1 Micro: 0.7009, F1 Macro: 0.5407\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9037, F1 Micro: 0.7127, F1 Macro: 0.5322\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.87      0.83      0.85       992\n",
      "HS_Individual       0.68      0.70      0.69       732\n",
      "     HS_Group       0.67      0.59      0.63       402\n",
      "  HS_Religion       0.71      0.32      0.45       157\n",
      "      HS_Race       0.78      0.52      0.62       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.76      0.72       762\n",
      "      HS_Weak       0.64      0.67      0.65       689\n",
      "  HS_Moderate       0.54      0.46      0.50       331\n",
      "    HS_Strong       0.97      0.31      0.47       114\n",
      "\n",
      "    micro avg       0.73      0.69      0.71      5556\n",
      "    macro avg       0.61      0.50      0.53      5556\n",
      " weighted avg       0.72      0.69      0.70      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 87.69844388961792 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4869, Accuracy: 0.8361, F1 Micro: 0.1184, F1 Macro: 0.0449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3794, Accuracy: 0.8591, F1 Micro: 0.3959, F1 Macro: 0.1283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3284, Accuracy: 0.8784, F1 Micro: 0.5538, F1 Macro: 0.2564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2909, Accuracy: 0.8852, F1 Micro: 0.642, F1 Macro: 0.3342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2499, Accuracy: 0.8925, F1 Micro: 0.6554, F1 Macro: 0.3777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2333, Accuracy: 0.8949, F1 Micro: 0.6863, F1 Macro: 0.4759\n",
      "Epoch 7/10, Train Loss: 0.205, Accuracy: 0.8972, F1 Micro: 0.6804, F1 Macro: 0.4841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1832, Accuracy: 0.9027, F1 Micro: 0.6906, F1 Macro: 0.4921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.163, Accuracy: 0.9027, F1 Micro: 0.705, F1 Macro: 0.5194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1408, Accuracy: 0.9047, F1 Micro: 0.7073, F1 Macro: 0.5455\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9047, F1 Micro: 0.7073, F1 Macro: 0.5455\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.79      0.80      1134\n",
      "      Abusive       0.89      0.82      0.85       992\n",
      "HS_Individual       0.68      0.65      0.66       732\n",
      "     HS_Group       0.67      0.59      0.63       402\n",
      "  HS_Religion       0.67      0.42      0.52       157\n",
      "      HS_Race       0.73      0.57      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.70      0.72       762\n",
      "      HS_Weak       0.65      0.62      0.64       689\n",
      "  HS_Moderate       0.57      0.47      0.52       331\n",
      "    HS_Strong       0.86      0.42      0.56       114\n",
      "\n",
      "    micro avg       0.75      0.67      0.71      5556\n",
      "    macro avg       0.61      0.50      0.55      5556\n",
      " weighted avg       0.73      0.67      0.70      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 88.82851052284241 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4929, Accuracy: 0.8306, F1 Micro: 0.0367, F1 Macro: 0.0156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3861, Accuracy: 0.8535, F1 Micro: 0.3506, F1 Macro: 0.1104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3468, Accuracy: 0.8753, F1 Micro: 0.5078, F1 Macro: 0.2278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3044, Accuracy: 0.8846, F1 Micro: 0.6218, F1 Macro: 0.3262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.254, Accuracy: 0.8944, F1 Micro: 0.6608, F1 Macro: 0.4223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2341, Accuracy: 0.8961, F1 Micro: 0.6984, F1 Macro: 0.5076\n",
      "Epoch 7/10, Train Loss: 0.2058, Accuracy: 0.8991, F1 Micro: 0.6888, F1 Macro: 0.4993\n",
      "Epoch 8/10, Train Loss: 0.1814, Accuracy: 0.9032, F1 Micro: 0.6889, F1 Macro: 0.4623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1594, Accuracy: 0.9034, F1 Micro: 0.71, F1 Macro: 0.5193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1356, Accuracy: 0.9053, F1 Micro: 0.7102, F1 Macro: 0.5433\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9053, F1 Micro: 0.7102, F1 Macro: 0.5433\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.82      1134\n",
      "      Abusive       0.90      0.81      0.85       992\n",
      "HS_Individual       0.68      0.65      0.67       732\n",
      "     HS_Group       0.69      0.57      0.63       402\n",
      "  HS_Religion       0.65      0.37      0.47       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.73      0.72       762\n",
      "      HS_Weak       0.64      0.63      0.64       689\n",
      "  HS_Moderate       0.57      0.46      0.51       331\n",
      "    HS_Strong       0.91      0.38      0.53       114\n",
      "\n",
      "    micro avg       0.75      0.67      0.71      5556\n",
      "    macro avg       0.61      0.50      0.54      5556\n",
      " weighted avg       0.73      0.67      0.70      5556\n",
      "  samples avg       0.40      0.37      0.36      5556\n",
      "\n",
      "Training completed in 86.0799629688263 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8917, F1 Micro: 0.6502, F1 Macro: 0.4224\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 115.80685067176819 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4593, Accuracy: 0.8371, F1 Micro: 0.1389, F1 Macro: 0.0495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3541, Accuracy: 0.88, F1 Micro: 0.5824, F1 Macro: 0.2714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2987, Accuracy: 0.8894, F1 Micro: 0.6495, F1 Macro: 0.343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2559, Accuracy: 0.8983, F1 Micro: 0.6698, F1 Macro: 0.4291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2217, Accuracy: 0.9027, F1 Micro: 0.6902, F1 Macro: 0.4558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.192, Accuracy: 0.9049, F1 Micro: 0.7038, F1 Macro: 0.5377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1654, Accuracy: 0.9083, F1 Micro: 0.712, F1 Macro: 0.5438\n",
      "Epoch 8/10, Train Loss: 0.1416, Accuracy: 0.907, F1 Micro: 0.7109, F1 Macro: 0.5459\n",
      "Epoch 9/10, Train Loss: 0.116, Accuracy: 0.9081, F1 Micro: 0.7118, F1 Macro: 0.5534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1084, Accuracy: 0.9108, F1 Micro: 0.7298, F1 Macro: 0.5806\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9108, F1 Micro: 0.7298, F1 Macro: 0.5806\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.81      0.82      1134\n",
      "      Abusive       0.88      0.84      0.86       992\n",
      "HS_Individual       0.72      0.66      0.69       732\n",
      "     HS_Group       0.66      0.64      0.65       402\n",
      "  HS_Religion       0.77      0.46      0.58       157\n",
      "      HS_Race       0.73      0.62      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.74      0.73       762\n",
      "      HS_Weak       0.69      0.63      0.66       689\n",
      "  HS_Moderate       0.57      0.53      0.55       331\n",
      "    HS_Strong       0.88      0.67      0.76       114\n",
      "\n",
      "    micro avg       0.76      0.70      0.73      5556\n",
      "    macro avg       0.62      0.55      0.58      5556\n",
      " weighted avg       0.75      0.70      0.72      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 99.59779906272888 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4665, Accuracy: 0.8404, F1 Micro: 0.1777, F1 Macro: 0.0615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.352, Accuracy: 0.8773, F1 Micro: 0.5792, F1 Macro: 0.2641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2965, Accuracy: 0.8871, F1 Micro: 0.6417, F1 Macro: 0.3331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2588, Accuracy: 0.896, F1 Micro: 0.6546, F1 Macro: 0.4101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2255, Accuracy: 0.9018, F1 Micro: 0.6863, F1 Macro: 0.4635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1964, Accuracy: 0.9056, F1 Micro: 0.7063, F1 Macro: 0.5215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1676, Accuracy: 0.9072, F1 Micro: 0.7132, F1 Macro: 0.5457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1462, Accuracy: 0.9038, F1 Micro: 0.7202, F1 Macro: 0.5589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1219, Accuracy: 0.9081, F1 Micro: 0.7238, F1 Macro: 0.5592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1141, Accuracy: 0.9087, F1 Micro: 0.7321, F1 Macro: 0.5787\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9087, F1 Micro: 0.7321, F1 Macro: 0.5787\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1134\n",
      "      Abusive       0.87      0.86      0.87       992\n",
      "HS_Individual       0.68      0.69      0.69       732\n",
      "     HS_Group       0.66      0.65      0.66       402\n",
      "  HS_Religion       0.74      0.45      0.56       157\n",
      "      HS_Race       0.75      0.65      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.79      0.75       762\n",
      "      HS_Weak       0.66      0.65      0.65       689\n",
      "  HS_Moderate       0.57      0.56      0.56       331\n",
      "    HS_Strong       0.86      0.59      0.70       114\n",
      "\n",
      "    micro avg       0.74      0.72      0.73      5556\n",
      "    macro avg       0.61      0.56      0.58      5556\n",
      " weighted avg       0.72      0.72      0.72      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 103.23040509223938 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4729, Accuracy: 0.8343, F1 Micro: 0.0958, F1 Macro: 0.0351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3637, Accuracy: 0.8737, F1 Micro: 0.5348, F1 Macro: 0.2374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3078, Accuracy: 0.8861, F1 Micro: 0.6469, F1 Macro: 0.3636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2616, Accuracy: 0.8976, F1 Micro: 0.6631, F1 Macro: 0.4223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2239, Accuracy: 0.9023, F1 Micro: 0.6923, F1 Macro: 0.4534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1976, Accuracy: 0.9051, F1 Micro: 0.7104, F1 Macro: 0.5246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1661, Accuracy: 0.9064, F1 Micro: 0.7262, F1 Macro: 0.5562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1449, Accuracy: 0.9069, F1 Micro: 0.7271, F1 Macro: 0.5625\n",
      "Epoch 9/10, Train Loss: 0.1167, Accuracy: 0.9074, F1 Micro: 0.7081, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1061, Accuracy: 0.9111, F1 Micro: 0.7327, F1 Macro: 0.5862\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9111, F1 Micro: 0.7327, F1 Macro: 0.5862\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.82      0.83      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.72      0.63      0.67       732\n",
      "     HS_Group       0.66      0.66      0.66       402\n",
      "  HS_Religion       0.73      0.43      0.54       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.76      0.74       762\n",
      "      HS_Weak       0.71      0.60      0.65       689\n",
      "  HS_Moderate       0.56      0.58      0.57       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.71      0.73      5556\n",
      "    macro avg       0.62      0.56      0.59      5556\n",
      " weighted avg       0.74      0.71      0.72      5556\n",
      "  samples avg       0.41      0.39      0.39      5556\n",
      "\n",
      "Training completed in 102.32095122337341 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8963, F1 Micro: 0.6705, F1 Macro: 0.4623\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 104.33182263374329 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4485, Accuracy: 0.8499, F1 Micro: 0.3059, F1 Macro: 0.102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3442, Accuracy: 0.8815, F1 Micro: 0.564, F1 Macro: 0.2721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2811, Accuracy: 0.891, F1 Micro: 0.6222, F1 Macro: 0.3414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2436, Accuracy: 0.9019, F1 Micro: 0.6775, F1 Macro: 0.4227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2073, Accuracy: 0.9065, F1 Micro: 0.7038, F1 Macro: 0.4885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.18, Accuracy: 0.9081, F1 Micro: 0.7205, F1 Macro: 0.5151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1538, Accuracy: 0.9086, F1 Micro: 0.7345, F1 Macro: 0.5684\n",
      "Epoch 8/10, Train Loss: 0.1289, Accuracy: 0.9092, F1 Micro: 0.7312, F1 Macro: 0.5739\n",
      "Epoch 9/10, Train Loss: 0.1113, Accuracy: 0.9086, F1 Micro: 0.728, F1 Macro: 0.5707\n",
      "Epoch 10/10, Train Loss: 0.0995, Accuracy: 0.9088, F1 Micro: 0.7258, F1 Macro: 0.5868\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9086, F1 Micro: 0.7345, F1 Macro: 0.5684\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.70      0.68      0.69       732\n",
      "     HS_Group       0.61      0.67      0.64       402\n",
      "  HS_Religion       0.71      0.51      0.59       157\n",
      "      HS_Race       0.70      0.70      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.78      0.74       762\n",
      "      HS_Weak       0.67      0.66      0.67       689\n",
      "  HS_Moderate       0.51      0.56      0.53       331\n",
      "    HS_Strong       0.91      0.38      0.53       114\n",
      "\n",
      "    micro avg       0.73      0.73      0.73      5556\n",
      "    macro avg       0.60      0.56      0.57      5556\n",
      " weighted avg       0.72      0.73      0.72      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 110.0998957157135 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4542, Accuracy: 0.8556, F1 Micro: 0.3509, F1 Macro: 0.1119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3416, Accuracy: 0.882, F1 Micro: 0.5819, F1 Macro: 0.284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2785, Accuracy: 0.8901, F1 Micro: 0.6167, F1 Macro: 0.338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2448, Accuracy: 0.9009, F1 Micro: 0.6699, F1 Macro: 0.4097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2098, Accuracy: 0.9043, F1 Micro: 0.712, F1 Macro: 0.51\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1865, Accuracy: 0.9061, F1 Micro: 0.7208, F1 Macro: 0.5142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1583, Accuracy: 0.9061, F1 Micro: 0.7257, F1 Macro: 0.5701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1329, Accuracy: 0.9109, F1 Micro: 0.7327, F1 Macro: 0.5812\n",
      "Epoch 9/10, Train Loss: 0.1158, Accuracy: 0.9048, F1 Micro: 0.7269, F1 Macro: 0.5722\n",
      "Epoch 10/10, Train Loss: 0.1011, Accuracy: 0.9099, F1 Micro: 0.7297, F1 Macro: 0.6015\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9109, F1 Micro: 0.7327, F1 Macro: 0.5812\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.83      1134\n",
      "      Abusive       0.88      0.87      0.87       992\n",
      "HS_Individual       0.69      0.67      0.68       732\n",
      "     HS_Group       0.70      0.58      0.63       402\n",
      "  HS_Religion       0.70      0.52      0.60       157\n",
      "      HS_Race       0.75      0.66      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.75      0.75      0.75       762\n",
      "      HS_Weak       0.66      0.65      0.66       689\n",
      "  HS_Moderate       0.58      0.48      0.52       331\n",
      "    HS_Strong       0.88      0.57      0.69       114\n",
      "\n",
      "    micro avg       0.76      0.71      0.73      5556\n",
      "    macro avg       0.70      0.55      0.58      5556\n",
      " weighted avg       0.75      0.71      0.72      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 111.57367706298828 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4588, Accuracy: 0.8491, F1 Micro: 0.2964, F1 Macro: 0.098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.354, Accuracy: 0.8808, F1 Micro: 0.5777, F1 Macro: 0.2743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2858, Accuracy: 0.8941, F1 Micro: 0.6398, F1 Macro: 0.3822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2446, Accuracy: 0.9026, F1 Micro: 0.6822, F1 Macro: 0.4108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2068, Accuracy: 0.9074, F1 Micro: 0.6967, F1 Macro: 0.4928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1797, Accuracy: 0.9092, F1 Micro: 0.7306, F1 Macro: 0.5449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1549, Accuracy: 0.9034, F1 Micro: 0.7343, F1 Macro: 0.5811\n",
      "Epoch 8/10, Train Loss: 0.1303, Accuracy: 0.9108, F1 Micro: 0.7338, F1 Macro: 0.583\n",
      "Epoch 9/10, Train Loss: 0.1099, Accuracy: 0.9125, F1 Micro: 0.7275, F1 Macro: 0.5779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.097, Accuracy: 0.9132, F1 Micro: 0.742, F1 Macro: 0.6039\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9132, F1 Micro: 0.742, F1 Macro: 0.6039\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.90      0.84      0.87       992\n",
      "HS_Individual       0.69      0.68      0.69       732\n",
      "     HS_Group       0.66      0.64      0.65       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.75      0.66      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.74      0.76      0.75       762\n",
      "      HS_Weak       0.68      0.66      0.67       689\n",
      "  HS_Moderate       0.59      0.53      0.56       331\n",
      "    HS_Strong       0.83      0.79      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.74      0.59      0.60      5556\n",
      " weighted avg       0.76      0.72      0.73      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 111.3725414276123 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.8992, F1 Micro: 0.6837, F1 Macro: 0.4867\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 93.9564471244812 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4395, Accuracy: 0.8547, F1 Micro: 0.3755, F1 Macro: 0.1237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.329, Accuracy: 0.8867, F1 Micro: 0.6146, F1 Macro: 0.3175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2737, Accuracy: 0.8983, F1 Micro: 0.6819, F1 Macro: 0.4176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.235, Accuracy: 0.9036, F1 Micro: 0.7012, F1 Macro: 0.4845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2016, Accuracy: 0.9095, F1 Micro: 0.7212, F1 Macro: 0.5353\n",
      "Epoch 6/10, Train Loss: 0.1725, Accuracy: 0.9106, F1 Micro: 0.7139, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1498, Accuracy: 0.9118, F1 Micro: 0.7256, F1 Macro: 0.5644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1232, Accuracy: 0.9088, F1 Micro: 0.741, F1 Macro: 0.5833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.107, Accuracy: 0.9133, F1 Micro: 0.7459, F1 Macro: 0.6056\n",
      "Epoch 10/10, Train Loss: 0.0947, Accuracy: 0.9114, F1 Micro: 0.7446, F1 Macro: 0.6052\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9133, F1 Micro: 0.7459, F1 Macro: 0.6056\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.84      1134\n",
      "      Abusive       0.89      0.86      0.87       992\n",
      "HS_Individual       0.72      0.69      0.70       732\n",
      "     HS_Group       0.64      0.70      0.67       402\n",
      "  HS_Religion       0.68      0.56      0.62       157\n",
      "      HS_Race       0.65      0.67      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.43      0.06      0.10        51\n",
      "     HS_Other       0.73      0.78      0.76       762\n",
      "      HS_Weak       0.71      0.67      0.69       689\n",
      "  HS_Moderate       0.54      0.60      0.57       331\n",
      "    HS_Strong       0.89      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5556\n",
      "    macro avg       0.64      0.60      0.61      5556\n",
      " weighted avg       0.74      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 122.21369576454163 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4424, Accuracy: 0.8577, F1 Micro: 0.4077, F1 Macro: 0.1224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3264, Accuracy: 0.8868, F1 Micro: 0.6044, F1 Macro: 0.2983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.275, Accuracy: 0.8966, F1 Micro: 0.6848, F1 Macro: 0.4179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2366, Accuracy: 0.902, F1 Micro: 0.7, F1 Macro: 0.4852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.9066, F1 Micro: 0.7088, F1 Macro: 0.5189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.176, Accuracy: 0.9094, F1 Micro: 0.7123, F1 Macro: 0.539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1506, Accuracy: 0.9135, F1 Micro: 0.7322, F1 Macro: 0.5709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1217, Accuracy: 0.9119, F1 Micro: 0.7359, F1 Macro: 0.5871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1072, Accuracy: 0.9117, F1 Micro: 0.7422, F1 Macro: 0.618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0956, Accuracy: 0.9158, F1 Micro: 0.7451, F1 Macro: 0.6234\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9158, F1 Micro: 0.7451, F1 Macro: 0.6234\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.70      0.69      0.69       732\n",
      "     HS_Group       0.73      0.59      0.65       402\n",
      "  HS_Religion       0.78      0.61      0.68       157\n",
      "      HS_Race       0.72      0.65      0.68       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.53      0.16      0.24        51\n",
      "     HS_Other       0.78      0.73      0.76       762\n",
      "      HS_Weak       0.67      0.66      0.67       689\n",
      "  HS_Moderate       0.63      0.47      0.54       331\n",
      "    HS_Strong       0.86      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.71      0.75      5556\n",
      "    macro avg       0.73      0.59      0.62      5556\n",
      " weighted avg       0.77      0.71      0.74      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 125.47664022445679 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4476, Accuracy: 0.8508, F1 Micro: 0.3168, F1 Macro: 0.1026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3364, Accuracy: 0.8844, F1 Micro: 0.5948, F1 Macro: 0.298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.8995, F1 Micro: 0.6839, F1 Macro: 0.4439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2335, Accuracy: 0.9045, F1 Micro: 0.7115, F1 Macro: 0.4946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.203, Accuracy: 0.9097, F1 Micro: 0.7253, F1 Macro: 0.5386\n",
      "Epoch 6/10, Train Loss: 0.1733, Accuracy: 0.9094, F1 Micro: 0.7023, F1 Macro: 0.5395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1485, Accuracy: 0.9134, F1 Micro: 0.7367, F1 Macro: 0.58\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 0.913, F1 Micro: 0.7465, F1 Macro: 0.6117\n",
      "Epoch 9/10, Train Loss: 0.1028, Accuracy: 0.9102, F1 Micro: 0.7464, F1 Macro: 0.6223\n",
      "Epoch 10/10, Train Loss: 0.0924, Accuracy: 0.916, F1 Micro: 0.7448, F1 Macro: 0.6166\n",
      "Model 3 - Iteration 4703: Accuracy: 0.913, F1 Micro: 0.7465, F1 Macro: 0.6117\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.70      0.71      0.70       732\n",
      "     HS_Group       0.67      0.66      0.66       402\n",
      "  HS_Religion       0.71      0.54      0.61       157\n",
      "      HS_Race       0.76      0.62      0.68       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.50      0.08      0.14        51\n",
      "     HS_Other       0.72      0.79      0.75       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.56      0.55      0.55       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5556\n",
      "    macro avg       0.74      0.60      0.61      5556\n",
      " weighted avg       0.75      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 120.87795686721802 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9017, F1 Micro: 0.6941, F1 Macro: 0.5079\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 84.739675283432 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4286, Accuracy: 0.8646, F1 Micro: 0.4642, F1 Macro: 0.1872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3116, Accuracy: 0.8898, F1 Micro: 0.6182, F1 Macro: 0.3217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2564, Accuracy: 0.906, F1 Micro: 0.6991, F1 Macro: 0.5007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2175, Accuracy: 0.9085, F1 Micro: 0.7137, F1 Macro: 0.553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1866, Accuracy: 0.9084, F1 Micro: 0.7405, F1 Macro: 0.5851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1581, Accuracy: 0.9145, F1 Micro: 0.7493, F1 Macro: 0.6013\n",
      "Epoch 7/10, Train Loss: 0.1367, Accuracy: 0.9152, F1 Micro: 0.7459, F1 Macro: 0.5999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1133, Accuracy: 0.9146, F1 Micro: 0.7531, F1 Macro: 0.6149\n",
      "Epoch 9/10, Train Loss: 0.0995, Accuracy: 0.9156, F1 Micro: 0.7521, F1 Macro: 0.6194\n",
      "Epoch 10/10, Train Loss: 0.087, Accuracy: 0.9173, F1 Micro: 0.7476, F1 Macro: 0.6294\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9146, F1 Micro: 0.7531, F1 Macro: 0.6149\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.68      0.75      0.71       732\n",
      "     HS_Group       0.70      0.64      0.66       402\n",
      "  HS_Religion       0.76      0.56      0.64       157\n",
      "      HS_Race       0.74      0.60      0.66       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.55      0.12      0.19        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.67      0.73      0.70       689\n",
      "  HS_Moderate       0.59      0.53      0.56       331\n",
      "    HS_Strong       0.91      0.62      0.74       114\n",
      "\n",
      "    micro avg       0.75      0.76      0.75      5556\n",
      "    macro avg       0.71      0.59      0.61      5556\n",
      " weighted avg       0.75      0.76      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 131.78248238563538 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4313, Accuracy: 0.8634, F1 Micro: 0.4357, F1 Macro: 0.1538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3091, Accuracy: 0.8884, F1 Micro: 0.6238, F1 Macro: 0.316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2588, Accuracy: 0.9033, F1 Micro: 0.6966, F1 Macro: 0.4838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2195, Accuracy: 0.9082, F1 Micro: 0.7117, F1 Macro: 0.5387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1909, Accuracy: 0.9029, F1 Micro: 0.731, F1 Macro: 0.5753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1648, Accuracy: 0.9098, F1 Micro: 0.7452, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1392, Accuracy: 0.9143, F1 Micro: 0.7491, F1 Macro: 0.6046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1153, Accuracy: 0.9165, F1 Micro: 0.7527, F1 Macro: 0.6182\n",
      "Epoch 9/10, Train Loss: 0.1027, Accuracy: 0.914, F1 Micro: 0.7497, F1 Macro: 0.6207\n",
      "Epoch 10/10, Train Loss: 0.0907, Accuracy: 0.9151, F1 Micro: 0.7466, F1 Macro: 0.6249\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9165, F1 Micro: 0.7527, F1 Macro: 0.6182\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.69      0.74      0.71       732\n",
      "     HS_Group       0.71      0.59      0.64       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.78      0.62      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.64      0.14      0.23        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.60      0.50      0.55       331\n",
      "    HS_Strong       0.89      0.66      0.76       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.68      0.59      0.62      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 132.99193596839905 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4383, Accuracy: 0.8541, F1 Micro: 0.3569, F1 Macro: 0.1133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3209, Accuracy: 0.888, F1 Micro: 0.6059, F1 Macro: 0.3209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2614, Accuracy: 0.9056, F1 Micro: 0.6971, F1 Macro: 0.5016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2211, Accuracy: 0.9101, F1 Micro: 0.715, F1 Macro: 0.5443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1888, Accuracy: 0.9079, F1 Micro: 0.7401, F1 Macro: 0.5833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.16, Accuracy: 0.9107, F1 Micro: 0.75, F1 Macro: 0.6056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1369, Accuracy: 0.9166, F1 Micro: 0.7592, F1 Macro: 0.6176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1137, Accuracy: 0.9176, F1 Micro: 0.7611, F1 Macro: 0.6332\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9152, F1 Micro: 0.7548, F1 Macro: 0.6249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0856, Accuracy: 0.9209, F1 Micro: 0.7635, F1 Macro: 0.6448\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9209, F1 Micro: 0.7635, F1 Macro: 0.6448\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.80      0.53      0.64       157\n",
      "      HS_Race       0.81      0.68      0.74       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.62      0.16      0.25        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.73      0.67      0.70       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.76      5556\n",
      "    macro avg       0.76      0.61      0.64      5556\n",
      " weighted avg       0.78      0.74      0.76      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 134.27158427238464 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9039, F1 Micro: 0.703, F1 Macro: 0.5247\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 75.88957834243774 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4244, Accuracy: 0.8684, F1 Micro: 0.4808, F1 Macro: 0.2119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3121, Accuracy: 0.8944, F1 Micro: 0.6525, F1 Macro: 0.3543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2514, Accuracy: 0.905, F1 Micro: 0.7156, F1 Macro: 0.5274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9096, F1 Micro: 0.7178, F1 Macro: 0.5278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1808, Accuracy: 0.9143, F1 Micro: 0.7368, F1 Macro: 0.5588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1574, Accuracy: 0.9174, F1 Micro: 0.7521, F1 Macro: 0.5953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1328, Accuracy: 0.9164, F1 Micro: 0.7541, F1 Macro: 0.6089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1113, Accuracy: 0.9178, F1 Micro: 0.7634, F1 Macro: 0.6422\n",
      "Epoch 9/10, Train Loss: 0.0952, Accuracy: 0.9191, F1 Micro: 0.7633, F1 Macro: 0.6447\n",
      "Epoch 10/10, Train Loss: 0.0832, Accuracy: 0.9174, F1 Micro: 0.7574, F1 Macro: 0.6339\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9178, F1 Micro: 0.7634, F1 Macro: 0.6422\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.74      0.59      0.65       157\n",
      "      HS_Race       0.78      0.61      0.69       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.72      0.82      0.77       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.64      0.52      0.57       331\n",
      "    HS_Strong       0.90      0.70      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.73      0.62      0.64      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 142.25273275375366 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4282, Accuracy: 0.8704, F1 Micro: 0.4887, F1 Macro: 0.2023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3115, Accuracy: 0.8917, F1 Micro: 0.6362, F1 Macro: 0.3349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2527, Accuracy: 0.9023, F1 Micro: 0.7122, F1 Macro: 0.5132\n",
      "Epoch 4/10, Train Loss: 0.2115, Accuracy: 0.9075, F1 Micro: 0.7022, F1 Macro: 0.5046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1855, Accuracy: 0.9134, F1 Micro: 0.7325, F1 Macro: 0.5587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1589, Accuracy: 0.918, F1 Micro: 0.7489, F1 Macro: 0.5921\n",
      "Epoch 7/10, Train Loss: 0.1347, Accuracy: 0.9146, F1 Micro: 0.7475, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.9161, F1 Micro: 0.7566, F1 Macro: 0.649\n",
      "Epoch 9/10, Train Loss: 0.0978, Accuracy: 0.9149, F1 Micro: 0.7546, F1 Macro: 0.6457\n",
      "Epoch 10/10, Train Loss: 0.0843, Accuracy: 0.9159, F1 Micro: 0.7563, F1 Macro: 0.6417\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9161, F1 Micro: 0.7566, F1 Macro: 0.649\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.67      0.77      0.72       732\n",
      "     HS_Group       0.74      0.56      0.64       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.75      0.63      0.68       120\n",
      "  HS_Physical       0.45      0.07      0.12        72\n",
      "    HS_Gender       0.46      0.37      0.41        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.65      0.75      0.70       689\n",
      "  HS_Moderate       0.66      0.46      0.54       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.71      0.63      0.65      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 140.24339056015015 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4351, Accuracy: 0.8575, F1 Micro: 0.4093, F1 Macro: 0.1385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3211, Accuracy: 0.8912, F1 Micro: 0.6466, F1 Macro: 0.3765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2551, Accuracy: 0.9061, F1 Micro: 0.7128, F1 Macro: 0.5183\n",
      "Epoch 4/10, Train Loss: 0.2115, Accuracy: 0.9094, F1 Micro: 0.7048, F1 Macro: 0.5027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1795, Accuracy: 0.9141, F1 Micro: 0.736, F1 Macro: 0.5624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1568, Accuracy: 0.9172, F1 Micro: 0.7543, F1 Macro: 0.6076\n",
      "Epoch 7/10, Train Loss: 0.1298, Accuracy: 0.9178, F1 Micro: 0.7491, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1107, Accuracy: 0.9143, F1 Micro: 0.7569, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0962, Accuracy: 0.9191, F1 Micro: 0.7652, F1 Macro: 0.6518\n",
      "Epoch 10/10, Train Loss: 0.0845, Accuracy: 0.9201, F1 Micro: 0.7623, F1 Macro: 0.6475\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9191, F1 Micro: 0.7652, F1 Macro: 0.6518\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.72      0.59      0.65       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.62      0.07      0.12        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.67      0.75      0.71       689\n",
      "  HS_Moderate       0.65      0.48      0.55       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.62      0.65      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 141.62219166755676 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9056, F1 Micro: 0.7103, F1 Macro: 0.5401\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 68.812091588974 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4137, Accuracy: 0.876, F1 Micro: 0.5581, F1 Macro: 0.2574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2965, Accuracy: 0.8961, F1 Micro: 0.6763, F1 Macro: 0.4282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.9074, F1 Micro: 0.7134, F1 Macro: 0.5058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2038, Accuracy: 0.9054, F1 Micro: 0.7366, F1 Macro: 0.5699\n",
      "Epoch 5/10, Train Loss: 0.1733, Accuracy: 0.9143, F1 Micro: 0.729, F1 Macro: 0.5617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1486, Accuracy: 0.9155, F1 Micro: 0.753, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1322, Accuracy: 0.916, F1 Micro: 0.7593, F1 Macro: 0.6279\n",
      "Epoch 8/10, Train Loss: 0.1134, Accuracy: 0.9188, F1 Micro: 0.7586, F1 Macro: 0.6387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0917, Accuracy: 0.9195, F1 Micro: 0.7623, F1 Macro: 0.656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9177, F1 Micro: 0.7628, F1 Macro: 0.6434\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9177, F1 Micro: 0.7628, F1 Macro: 0.6434\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.70      0.75      0.73       732\n",
      "     HS_Group       0.68      0.61      0.65       402\n",
      "  HS_Religion       0.75      0.58      0.65       157\n",
      "      HS_Race       0.73      0.63      0.68       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.69      0.22      0.33        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.73      0.71       689\n",
      "  HS_Moderate       0.60      0.53      0.57       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.74      0.63      0.64      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 152.07783889770508 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.416, Accuracy: 0.8748, F1 Micro: 0.5237, F1 Macro: 0.226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2959, Accuracy: 0.8945, F1 Micro: 0.6821, F1 Macro: 0.4515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2427, Accuracy: 0.9053, F1 Micro: 0.7057, F1 Macro: 0.5029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2064, Accuracy: 0.911, F1 Micro: 0.7386, F1 Macro: 0.5677\n",
      "Epoch 5/10, Train Loss: 0.1752, Accuracy: 0.9125, F1 Micro: 0.7366, F1 Macro: 0.5708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1522, Accuracy: 0.9171, F1 Micro: 0.7545, F1 Macro: 0.6122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1341, Accuracy: 0.9186, F1 Micro: 0.7591, F1 Macro: 0.6308\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.9195, F1 Micro: 0.7569, F1 Macro: 0.6429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0971, Accuracy: 0.9176, F1 Micro: 0.7599, F1 Macro: 0.6578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9176, F1 Micro: 0.7606, F1 Macro: 0.642\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9176, F1 Micro: 0.7606, F1 Macro: 0.642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.71      0.72      0.71       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.76      0.61      0.67       157\n",
      "      HS_Race       0.73      0.69      0.71       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.71      0.20      0.31        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.57      0.57      0.57       331\n",
      "    HS_Strong       0.82      0.80      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.72      0.63      0.64      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 152.09805464744568 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4243, Accuracy: 0.8664, F1 Micro: 0.4811, F1 Macro: 0.1921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.303, Accuracy: 0.8962, F1 Micro: 0.6837, F1 Macro: 0.4617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.244, Accuracy: 0.9076, F1 Micro: 0.7145, F1 Macro: 0.5033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2053, Accuracy: 0.9087, F1 Micro: 0.7425, F1 Macro: 0.5847\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9163, F1 Micro: 0.7388, F1 Macro: 0.5788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1472, Accuracy: 0.9185, F1 Micro: 0.751, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1311, Accuracy: 0.9156, F1 Micro: 0.7634, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1118, Accuracy: 0.9192, F1 Micro: 0.7663, F1 Macro: 0.6617\n",
      "Epoch 9/10, Train Loss: 0.091, Accuracy: 0.9211, F1 Micro: 0.7631, F1 Macro: 0.6576\n",
      "Epoch 10/10, Train Loss: 0.0786, Accuracy: 0.92, F1 Micro: 0.7631, F1 Macro: 0.6602\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9192, F1 Micro: 0.7663, F1 Macro: 0.6617\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.76      0.59      0.67       157\n",
      "      HS_Race       0.83      0.67      0.74       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.62      0.29      0.40        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.59      0.58      0.58       331\n",
      "    HS_Strong       0.87      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.74      0.64      0.66      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 151.0384545326233 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.907, F1 Micro: 0.7162, F1 Macro: 0.5522\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 63.826919078826904 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.414, Accuracy: 0.8717, F1 Micro: 0.6028, F1 Macro: 0.2786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2963, Accuracy: 0.8988, F1 Micro: 0.6625, F1 Macro: 0.4064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.9088, F1 Micro: 0.7097, F1 Macro: 0.5182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2078, Accuracy: 0.9132, F1 Micro: 0.7465, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1721, Accuracy: 0.915, F1 Micro: 0.7473, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1458, Accuracy: 0.9144, F1 Micro: 0.7608, F1 Macro: 0.6226\n",
      "Epoch 7/10, Train Loss: 0.1198, Accuracy: 0.9161, F1 Micro: 0.7525, F1 Macro: 0.6333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1074, Accuracy: 0.9153, F1 Micro: 0.7618, F1 Macro: 0.646\n",
      "Epoch 9/10, Train Loss: 0.088, Accuracy: 0.9177, F1 Micro: 0.7616, F1 Macro: 0.6369\n",
      "Epoch 10/10, Train Loss: 0.0782, Accuracy: 0.9202, F1 Micro: 0.7579, F1 Macro: 0.6524\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9153, F1 Micro: 0.7618, F1 Macro: 0.646\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.90      0.84      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.68      0.78      0.73       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.72      0.55      0.63       157\n",
      "      HS_Race       0.74      0.65      0.69       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.72      0.83      0.77       762\n",
      "      HS_Weak       0.66      0.75      0.70       689\n",
      "  HS_Moderate       0.60      0.56      0.58       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.74      0.79      0.76      5556\n",
      "    macro avg       0.72      0.64      0.65      5556\n",
      " weighted avg       0.74      0.79      0.75      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 154.4193844795227 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4154, Accuracy: 0.8705, F1 Micro: 0.6034, F1 Macro: 0.2753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2941, Accuracy: 0.8977, F1 Micro: 0.6599, F1 Macro: 0.4062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2415, Accuracy: 0.908, F1 Micro: 0.6984, F1 Macro: 0.5048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2113, Accuracy: 0.9135, F1 Micro: 0.7361, F1 Macro: 0.5726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1748, Accuracy: 0.9112, F1 Micro: 0.7456, F1 Macro: 0.5911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1476, Accuracy: 0.9161, F1 Micro: 0.7546, F1 Macro: 0.6223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.123, Accuracy: 0.9163, F1 Micro: 0.7553, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1095, Accuracy: 0.9138, F1 Micro: 0.7564, F1 Macro: 0.6303\n",
      "Epoch 9/10, Train Loss: 0.0933, Accuracy: 0.9134, F1 Micro: 0.7564, F1 Macro: 0.6361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9184, F1 Micro: 0.7582, F1 Macro: 0.6545\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9184, F1 Micro: 0.7582, F1 Macro: 0.6545\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.72      0.70      0.71       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.74      0.59      0.65       157\n",
      "      HS_Race       0.75      0.59      0.66       120\n",
      "  HS_Physical       0.75      0.08      0.15        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.77      0.78      0.77       762\n",
      "      HS_Weak       0.69      0.67      0.68       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5556\n",
      "    macro avg       0.74      0.62      0.65      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 158.40624117851257 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4252, Accuracy: 0.8659, F1 Micro: 0.5318, F1 Macro: 0.2283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3014, Accuracy: 0.8983, F1 Micro: 0.6663, F1 Macro: 0.4249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2415, Accuracy: 0.9102, F1 Micro: 0.7193, F1 Macro: 0.5307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2083, Accuracy: 0.9151, F1 Micro: 0.7503, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1721, Accuracy: 0.914, F1 Micro: 0.7552, F1 Macro: 0.5975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1445, Accuracy: 0.9209, F1 Micro: 0.7646, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9216, F1 Micro: 0.7703, F1 Macro: 0.6586\n",
      "Epoch 8/10, Train Loss: 0.105, Accuracy: 0.9178, F1 Micro: 0.7654, F1 Macro: 0.652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0866, Accuracy: 0.9192, F1 Micro: 0.7704, F1 Macro: 0.6589\n",
      "Epoch 10/10, Train Loss: 0.0761, Accuracy: 0.9226, F1 Micro: 0.7702, F1 Macro: 0.673\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9192, F1 Micro: 0.7704, F1 Macro: 0.6589\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.75      0.55      0.63       157\n",
      "      HS_Race       0.77      0.72      0.74       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.69      0.22      0.33        51\n",
      "     HS_Other       0.73      0.84      0.78       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.61      0.57      0.59       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.75      0.65      0.66      5556\n",
      " weighted avg       0.75      0.79      0.76      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 157.49090266227722 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9081, F1 Micro: 0.7209, F1 Macro: 0.5623\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 59.3438766002655 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4097, Accuracy: 0.8789, F1 Micro: 0.5821, F1 Macro: 0.2721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2883, Accuracy: 0.8992, F1 Micro: 0.6887, F1 Macro: 0.4834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2342, Accuracy: 0.9078, F1 Micro: 0.712, F1 Macro: 0.4957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.9111, F1 Micro: 0.7195, F1 Macro: 0.5285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1715, Accuracy: 0.9153, F1 Micro: 0.7504, F1 Macro: 0.597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1454, Accuracy: 0.9171, F1 Micro: 0.7532, F1 Macro: 0.6065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1269, Accuracy: 0.919, F1 Micro: 0.7565, F1 Macro: 0.6371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1051, Accuracy: 0.9191, F1 Micro: 0.762, F1 Macro: 0.6489\n",
      "Epoch 9/10, Train Loss: 0.0873, Accuracy: 0.9171, F1 Micro: 0.7602, F1 Macro: 0.6414\n",
      "Epoch 10/10, Train Loss: 0.0774, Accuracy: 0.9193, F1 Micro: 0.7547, F1 Macro: 0.6551\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9191, F1 Micro: 0.762, F1 Macro: 0.6489\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.78      0.56      0.65       157\n",
      "      HS_Race       0.86      0.60      0.71       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.69      0.18      0.28        51\n",
      "     HS_Other       0.72      0.82      0.77       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.77      0.62      0.65      5556\n",
      " weighted avg       0.77      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 163.5455183982849 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4108, Accuracy: 0.8774, F1 Micro: 0.5776, F1 Macro: 0.267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2874, Accuracy: 0.8993, F1 Micro: 0.6903, F1 Macro: 0.479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.9085, F1 Micro: 0.7175, F1 Macro: 0.5116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2013, Accuracy: 0.9119, F1 Micro: 0.7269, F1 Macro: 0.5467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1734, Accuracy: 0.9142, F1 Micro: 0.7482, F1 Macro: 0.5981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1477, Accuracy: 0.9158, F1 Micro: 0.7555, F1 Macro: 0.6218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1285, Accuracy: 0.9213, F1 Micro: 0.7631, F1 Macro: 0.6468\n",
      "Epoch 8/10, Train Loss: 0.1068, Accuracy: 0.9204, F1 Micro: 0.7621, F1 Macro: 0.65\n",
      "Epoch 9/10, Train Loss: 0.0908, Accuracy: 0.9182, F1 Micro: 0.7517, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.92, F1 Micro: 0.7679, F1 Macro: 0.6761\n",
      "Model 2 - Iteration 6980: Accuracy: 0.92, F1 Micro: 0.7679, F1 Macro: 0.6761\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.69      0.74      0.72       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.57      0.11      0.19        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.67      0.72      0.70       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.65      0.68      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.02344346046448 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4191, Accuracy: 0.8745, F1 Micro: 0.5482, F1 Macro: 0.246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2908, Accuracy: 0.8998, F1 Micro: 0.6947, F1 Macro: 0.4971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2375, Accuracy: 0.9098, F1 Micro: 0.7225, F1 Macro: 0.518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2001, Accuracy: 0.9138, F1 Micro: 0.7271, F1 Macro: 0.5486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1718, Accuracy: 0.9123, F1 Micro: 0.7536, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1441, Accuracy: 0.9172, F1 Micro: 0.7629, F1 Macro: 0.6282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.9188, F1 Micro: 0.7634, F1 Macro: 0.6443\n",
      "Epoch 8/10, Train Loss: 0.1044, Accuracy: 0.9222, F1 Micro: 0.7617, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0872, Accuracy: 0.9217, F1 Micro: 0.7683, F1 Macro: 0.6586\n",
      "Epoch 10/10, Train Loss: 0.0767, Accuracy: 0.9217, F1 Micro: 0.7628, F1 Macro: 0.6655\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9217, F1 Micro: 0.7683, F1 Macro: 0.6586\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.91      0.85      0.88       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.71      0.68      0.69       402\n",
      "  HS_Religion       0.81      0.58      0.68       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.64      0.61      0.62       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.78      0.63      0.66      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.42      0.42      0.41      5556\n",
      "\n",
      "Training completed in 163.0577609539032 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9092, F1 Micro: 0.725, F1 Macro: 0.5713\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 52.428489446640015 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4064, Accuracy: 0.8807, F1 Micro: 0.5985, F1 Macro: 0.2789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2877, Accuracy: 0.9012, F1 Micro: 0.6842, F1 Macro: 0.4852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2396, Accuracy: 0.9067, F1 Micro: 0.7283, F1 Macro: 0.5478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9146, F1 Micro: 0.7337, F1 Macro: 0.5727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.169, Accuracy: 0.9188, F1 Micro: 0.7573, F1 Macro: 0.5977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1421, Accuracy: 0.9168, F1 Micro: 0.763, F1 Macro: 0.6309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1224, Accuracy: 0.9181, F1 Micro: 0.7655, F1 Macro: 0.6546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1027, Accuracy: 0.9198, F1 Micro: 0.7703, F1 Macro: 0.6692\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.92, F1 Micro: 0.7685, F1 Macro: 0.6689\n",
      "Epoch 10/10, Train Loss: 0.0761, Accuracy: 0.9214, F1 Micro: 0.7576, F1 Macro: 0.6684\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9198, F1 Micro: 0.7703, F1 Macro: 0.6692\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.68      0.79      0.73       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.66      0.77      0.71       689\n",
      "  HS_Moderate       0.67      0.49      0.56       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 172.6266052722931 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4092, Accuracy: 0.8783, F1 Micro: 0.5947, F1 Macro: 0.2752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2853, Accuracy: 0.8983, F1 Micro: 0.6707, F1 Macro: 0.4796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2369, Accuracy: 0.9102, F1 Micro: 0.7256, F1 Macro: 0.5583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1995, Accuracy: 0.9139, F1 Micro: 0.7267, F1 Macro: 0.5565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1718, Accuracy: 0.9191, F1 Micro: 0.7556, F1 Macro: 0.6075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1431, Accuracy: 0.9174, F1 Micro: 0.76, F1 Macro: 0.6262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1244, Accuracy: 0.9154, F1 Micro: 0.7619, F1 Macro: 0.6689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1046, Accuracy: 0.9218, F1 Micro: 0.7674, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0885, Accuracy: 0.9207, F1 Micro: 0.7688, F1 Macro: 0.6669\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9197, F1 Micro: 0.7611, F1 Macro: 0.6723\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9207, F1 Micro: 0.7688, F1 Macro: 0.6669\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.69      0.68      0.68       402\n",
      "  HS_Religion       0.74      0.62      0.68       157\n",
      "      HS_Race       0.78      0.62      0.69       120\n",
      "  HS_Physical       0.70      0.10      0.17        72\n",
      "    HS_Gender       0.59      0.31      0.41        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.60      0.59      0.59       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 173.2738435268402 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4153, Accuracy: 0.8754, F1 Micro: 0.5861, F1 Macro: 0.2685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2912, Accuracy: 0.9, F1 Micro: 0.6899, F1 Macro: 0.5002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2389, Accuracy: 0.9108, F1 Micro: 0.7263, F1 Macro: 0.5503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1971, Accuracy: 0.915, F1 Micro: 0.7332, F1 Macro: 0.5736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1704, Accuracy: 0.9194, F1 Micro: 0.7616, F1 Macro: 0.6078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.138, Accuracy: 0.9187, F1 Micro: 0.7648, F1 Macro: 0.6387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1221, Accuracy: 0.9214, F1 Micro: 0.7716, F1 Macro: 0.6613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.9231, F1 Micro: 0.7749, F1 Macro: 0.6761\n",
      "Epoch 9/10, Train Loss: 0.0886, Accuracy: 0.9206, F1 Micro: 0.7693, F1 Macro: 0.6757\n",
      "Epoch 10/10, Train Loss: 0.0736, Accuracy: 0.9227, F1 Micro: 0.7636, F1 Macro: 0.6818\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9231, F1 Micro: 0.7749, F1 Macro: 0.6761\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.59      0.31      0.41        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.66      0.50      0.57       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.64      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 171.0486524105072 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9102, F1 Micro: 0.7289, F1 Macro: 0.5796\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 49.01062846183777 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4045, Accuracy: 0.8801, F1 Micro: 0.6, F1 Macro: 0.2811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.8994, F1 Micro: 0.6991, F1 Macro: 0.4752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2308, Accuracy: 0.9096, F1 Micro: 0.7318, F1 Macro: 0.5598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1952, Accuracy: 0.9165, F1 Micro: 0.7413, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9111, F1 Micro: 0.755, F1 Macro: 0.6109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1432, Accuracy: 0.9226, F1 Micro: 0.7674, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9171, F1 Micro: 0.7678, F1 Macro: 0.6606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0979, Accuracy: 0.9217, F1 Micro: 0.7702, F1 Macro: 0.6599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9243, F1 Micro: 0.776, F1 Macro: 0.6738\n",
      "Epoch 10/10, Train Loss: 0.0731, Accuracy: 0.9195, F1 Micro: 0.7697, F1 Macro: 0.681\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9243, F1 Micro: 0.776, F1 Macro: 0.6738\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.73      0.66      0.69       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.75      0.72      0.73       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.61       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.65      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 175.01614713668823 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4044, Accuracy: 0.8786, F1 Micro: 0.5906, F1 Macro: 0.2739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2792, Accuracy: 0.8982, F1 Micro: 0.6978, F1 Macro: 0.4789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2333, Accuracy: 0.9081, F1 Micro: 0.7302, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1977, Accuracy: 0.9163, F1 Micro: 0.7449, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9126, F1 Micro: 0.7523, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1434, Accuracy: 0.9206, F1 Micro: 0.7651, F1 Macro: 0.647\n",
      "Epoch 7/10, Train Loss: 0.118, Accuracy: 0.9169, F1 Micro: 0.7612, F1 Macro: 0.6636\n",
      "Epoch 8/10, Train Loss: 0.0984, Accuracy: 0.921, F1 Micro: 0.7637, F1 Macro: 0.6639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.922, F1 Micro: 0.7708, F1 Macro: 0.6783\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9201, F1 Micro: 0.764, F1 Macro: 0.6863\n",
      "Model 2 - Iteration 7656: Accuracy: 0.922, F1 Micro: 0.7708, F1 Macro: 0.6783\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.73      0.70      0.72       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.72      0.65      0.68       157\n",
      "      HS_Race       0.72      0.74      0.73       120\n",
      "  HS_Physical       0.56      0.12      0.20        72\n",
      "    HS_Gender       0.55      0.43      0.48        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.68      0.69       689\n",
      "  HS_Moderate       0.61      0.58      0.59       331\n",
      "    HS_Strong       0.86      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.72      0.66      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 172.8161222934723 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4126, Accuracy: 0.8768, F1 Micro: 0.5717, F1 Macro: 0.2625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2833, Accuracy: 0.9007, F1 Micro: 0.7077, F1 Macro: 0.4992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2313, Accuracy: 0.9099, F1 Micro: 0.7379, F1 Macro: 0.5733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1953, Accuracy: 0.9185, F1 Micro: 0.7496, F1 Macro: 0.5974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1611, Accuracy: 0.915, F1 Micro: 0.7589, F1 Macro: 0.6162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1419, Accuracy: 0.9206, F1 Micro: 0.7628, F1 Macro: 0.6395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9219, F1 Micro: 0.7686, F1 Macro: 0.6642\n",
      "Epoch 8/10, Train Loss: 0.0962, Accuracy: 0.9227, F1 Micro: 0.7643, F1 Macro: 0.6666\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9213, F1 Micro: 0.7649, F1 Macro: 0.6738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9203, F1 Micro: 0.7708, F1 Macro: 0.6849\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9203, F1 Micro: 0.7708, F1 Macro: 0.6849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.70      0.60      0.65       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.67      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 174.33826327323914 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9111, F1 Micro: 0.7322, F1 Macro: 0.5872\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 43.36696100234985 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.398, Accuracy: 0.883, F1 Micro: 0.6046, F1 Macro: 0.2911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2729, Accuracy: 0.9011, F1 Micro: 0.7026, F1 Macro: 0.4699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2254, Accuracy: 0.9124, F1 Micro: 0.7305, F1 Macro: 0.5563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1909, Accuracy: 0.9179, F1 Micro: 0.7484, F1 Macro: 0.5961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1624, Accuracy: 0.9158, F1 Micro: 0.7581, F1 Macro: 0.61\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.134, Accuracy: 0.9219, F1 Micro: 0.7628, F1 Macro: 0.6363\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.9219, F1 Micro: 0.7612, F1 Macro: 0.6415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9227, F1 Micro: 0.773, F1 Macro: 0.6679\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9222, F1 Micro: 0.7659, F1 Macro: 0.6652\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9201, F1 Micro: 0.759, F1 Macro: 0.6593\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9227, F1 Micro: 0.773, F1 Macro: 0.6679\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.58      0.27      0.37        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 177.48969745635986 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3993, Accuracy: 0.8818, F1 Micro: 0.5993, F1 Macro: 0.2801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2762, Accuracy: 0.8996, F1 Micro: 0.7027, F1 Macro: 0.4793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.9102, F1 Micro: 0.7335, F1 Macro: 0.5534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1922, Accuracy: 0.9194, F1 Micro: 0.7481, F1 Macro: 0.5927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1636, Accuracy: 0.9143, F1 Micro: 0.7583, F1 Macro: 0.6244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9214, F1 Micro: 0.7669, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1157, Accuracy: 0.9215, F1 Micro: 0.7706, F1 Macro: 0.6559\n",
      "Epoch 8/10, Train Loss: 0.0987, Accuracy: 0.9232, F1 Micro: 0.7676, F1 Macro: 0.6766\n",
      "Epoch 9/10, Train Loss: 0.0841, Accuracy: 0.922, F1 Micro: 0.7669, F1 Macro: 0.6783\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9229, F1 Micro: 0.7653, F1 Macro: 0.6689\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9215, F1 Micro: 0.7706, F1 Macro: 0.6559\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.83      0.63      0.72       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.61      0.54      0.57       331\n",
      "    HS_Strong       0.87      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 178.2818660736084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.409, Accuracy: 0.8788, F1 Micro: 0.5618, F1 Macro: 0.2577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.279, Accuracy: 0.901, F1 Micro: 0.7043, F1 Macro: 0.4586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2283, Accuracy: 0.913, F1 Micro: 0.7378, F1 Macro: 0.5743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.192, Accuracy: 0.9177, F1 Micro: 0.7462, F1 Macro: 0.6005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9158, F1 Micro: 0.7557, F1 Macro: 0.6125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1321, Accuracy: 0.9216, F1 Micro: 0.767, F1 Macro: 0.6482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1161, Accuracy: 0.9226, F1 Micro: 0.7706, F1 Macro: 0.6594\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.9204, F1 Micro: 0.764, F1 Macro: 0.6717\n",
      "Epoch 9/10, Train Loss: 0.084, Accuracy: 0.9219, F1 Micro: 0.7649, F1 Macro: 0.6588\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9222, F1 Micro: 0.7694, F1 Macro: 0.6704\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9226, F1 Micro: 0.7706, F1 Macro: 0.6594\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.83      0.59      0.69       157\n",
      "      HS_Race       0.82      0.68      0.74       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.73      0.67      0.70       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.63      0.66      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 176.99664425849915 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9119, F1 Micro: 0.735, F1 Macro: 0.5925\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 39.108346700668335 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3932, Accuracy: 0.883, F1 Micro: 0.5986, F1 Macro: 0.2841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2766, Accuracy: 0.8961, F1 Micro: 0.6207, F1 Macro: 0.4025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.227, Accuracy: 0.912, F1 Micro: 0.7355, F1 Macro: 0.5676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.9149, F1 Micro: 0.748, F1 Macro: 0.5975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1605, Accuracy: 0.92, F1 Micro: 0.7662, F1 Macro: 0.6164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9204, F1 Micro: 0.7681, F1 Macro: 0.6519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9218, F1 Micro: 0.7707, F1 Macro: 0.6722\n",
      "Epoch 8/10, Train Loss: 0.0962, Accuracy: 0.9191, F1 Micro: 0.7701, F1 Macro: 0.6738\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.921, F1 Micro: 0.7702, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0717, Accuracy: 0.9232, F1 Micro: 0.7712, F1 Macro: 0.6806\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9232, F1 Micro: 0.7712, F1 Macro: 0.6806\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.76      0.61      0.68       157\n",
      "      HS_Race       0.69      0.72      0.71       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.81      0.75      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.56      0.61       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.75      0.65      0.68      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 183.31110644340515 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.8822, F1 Micro: 0.5836, F1 Macro: 0.2785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.8983, F1 Micro: 0.6343, F1 Macro: 0.4193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2266, Accuracy: 0.9118, F1 Micro: 0.725, F1 Macro: 0.5507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1984, Accuracy: 0.9135, F1 Micro: 0.7451, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.9217, F1 Micro: 0.7623, F1 Macro: 0.6276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1386, Accuracy: 0.9181, F1 Micro: 0.7679, F1 Macro: 0.6574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1174, Accuracy: 0.9216, F1 Micro: 0.7692, F1 Macro: 0.6706\n",
      "Epoch 8/10, Train Loss: 0.0998, Accuracy: 0.9223, F1 Micro: 0.7614, F1 Macro: 0.6617\n",
      "Epoch 9/10, Train Loss: 0.0855, Accuracy: 0.9203, F1 Micro: 0.7617, F1 Macro: 0.6667\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9221, F1 Micro: 0.7659, F1 Macro: 0.6761\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9216, F1 Micro: 0.7692, F1 Macro: 0.6706\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.84      0.56      0.67       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.67      0.11      0.19        72\n",
      "    HS_Gender       0.53      0.33      0.41        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.61      0.59      0.60       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 181.5867636203766 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4037, Accuracy: 0.881, F1 Micro: 0.5911, F1 Macro: 0.2789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2794, Accuracy: 0.9006, F1 Micro: 0.6513, F1 Macro: 0.4378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9129, F1 Micro: 0.7199, F1 Macro: 0.5496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.196, Accuracy: 0.9157, F1 Micro: 0.7519, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1571, Accuracy: 0.92, F1 Micro: 0.7676, F1 Macro: 0.6258\n",
      "Epoch 6/10, Train Loss: 0.1334, Accuracy: 0.9204, F1 Micro: 0.7653, F1 Macro: 0.6503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1137, Accuracy: 0.9197, F1 Micro: 0.7696, F1 Macro: 0.6764\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.9206, F1 Micro: 0.7644, F1 Macro: 0.6686\n",
      "Epoch 9/10, Train Loss: 0.0829, Accuracy: 0.9192, F1 Micro: 0.7659, F1 Macro: 0.6707\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9228, F1 Micro: 0.7663, F1 Macro: 0.6806\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9197, F1 Micro: 0.7696, F1 Macro: 0.6764\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.56      0.12      0.20        72\n",
      "    HS_Gender       0.56      0.37      0.45        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.59      0.62      0.60       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.71      0.67      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 180.85025787353516 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9126, F1 Micro: 0.7374, F1 Macro: 0.5981\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 36.84804081916809 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3956, Accuracy: 0.8846, F1 Micro: 0.6029, F1 Macro: 0.2863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2701, Accuracy: 0.9025, F1 Micro: 0.7158, F1 Macro: 0.5297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.9118, F1 Micro: 0.7378, F1 Macro: 0.5656\n",
      "Epoch 4/10, Train Loss: 0.1861, Accuracy: 0.915, F1 Micro: 0.7375, F1 Macro: 0.5837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.9184, F1 Micro: 0.7579, F1 Macro: 0.6043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.9219, F1 Micro: 0.7659, F1 Macro: 0.6252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.9227, F1 Micro: 0.7719, F1 Macro: 0.6628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0956, Accuracy: 0.9194, F1 Micro: 0.7725, F1 Macro: 0.6665\n",
      "Epoch 9/10, Train Loss: 0.0835, Accuracy: 0.9213, F1 Micro: 0.7702, F1 Macro: 0.676\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9234, F1 Micro: 0.7705, F1 Macro: 0.6821\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9194, F1 Micro: 0.7725, F1 Macro: 0.6665\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.74      0.59      0.66       157\n",
      "      HS_Race       0.70      0.74      0.72       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.70      0.27      0.39        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.61      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.75      0.66      0.67      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 186.6712839603424 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3953, Accuracy: 0.881, F1 Micro: 0.624, F1 Macro: 0.2975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2708, Accuracy: 0.9018, F1 Micro: 0.7107, F1 Macro: 0.5276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2273, Accuracy: 0.9133, F1 Micro: 0.7372, F1 Macro: 0.564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.915, F1 Micro: 0.7407, F1 Macro: 0.5915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.9176, F1 Micro: 0.7566, F1 Macro: 0.6217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1323, Accuracy: 0.9165, F1 Micro: 0.7597, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9207, F1 Micro: 0.7668, F1 Macro: 0.6586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9222, F1 Micro: 0.7709, F1 Macro: 0.6712\n",
      "Epoch 9/10, Train Loss: 0.0821, Accuracy: 0.9209, F1 Micro: 0.7642, F1 Macro: 0.6738\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9208, F1 Micro: 0.7674, F1 Macro: 0.6838\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9222, F1 Micro: 0.7709, F1 Macro: 0.6712\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.73      0.71      0.72       120\n",
      "  HS_Physical       0.67      0.08      0.15        72\n",
      "    HS_Gender       0.57      0.41      0.48        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 187.34949612617493 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4039, Accuracy: 0.8807, F1 Micro: 0.5933, F1 Macro: 0.2779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2728, Accuracy: 0.9037, F1 Micro: 0.7159, F1 Macro: 0.5426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2262, Accuracy: 0.9134, F1 Micro: 0.7351, F1 Macro: 0.5579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.919, F1 Micro: 0.7503, F1 Macro: 0.5998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.9187, F1 Micro: 0.7627, F1 Macro: 0.6274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1305, Accuracy: 0.9195, F1 Micro: 0.7672, F1 Macro: 0.6468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1162, Accuracy: 0.9219, F1 Micro: 0.7737, F1 Macro: 0.6656\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9207, F1 Micro: 0.7721, F1 Macro: 0.6745\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9231, F1 Micro: 0.7681, F1 Macro: 0.6832\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9231, F1 Micro: 0.7704, F1 Macro: 0.6886\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9219, F1 Micro: 0.7737, F1 Macro: 0.6656\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.76      0.69      0.72       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.71      0.24      0.35        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 186.56412816047668 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9131, F1 Micro: 0.7396, F1 Macro: 0.6024\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 31.839757919311523 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3891, Accuracy: 0.8854, F1 Micro: 0.5934, F1 Macro: 0.2909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2714, Accuracy: 0.9059, F1 Micro: 0.6948, F1 Macro: 0.4969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.9115, F1 Micro: 0.7429, F1 Macro: 0.5884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1858, Accuracy: 0.9187, F1 Micro: 0.758, F1 Macro: 0.6088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.917, F1 Micro: 0.7628, F1 Macro: 0.6171\n",
      "Epoch 6/10, Train Loss: 0.135, Accuracy: 0.9223, F1 Micro: 0.7555, F1 Macro: 0.6295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.9242, F1 Micro: 0.7708, F1 Macro: 0.6548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.6844\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.9241, F1 Micro: 0.7698, F1 Macro: 0.67\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9235, F1 Micro: 0.771, F1 Macro: 0.6599\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.6844\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.72      0.67      0.70       402\n",
      "  HS_Religion       0.76      0.63      0.69       157\n",
      "      HS_Race       0.75      0.73      0.74       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 191.06567478179932 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3925, Accuracy: 0.8836, F1 Micro: 0.5851, F1 Macro: 0.2824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.273, Accuracy: 0.905, F1 Micro: 0.695, F1 Macro: 0.4982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.912, F1 Micro: 0.7399, F1 Macro: 0.5835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9177, F1 Micro: 0.7542, F1 Macro: 0.6035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9202, F1 Micro: 0.7678, F1 Macro: 0.6536\n",
      "Epoch 6/10, Train Loss: 0.132, Accuracy: 0.9219, F1 Micro: 0.7633, F1 Macro: 0.64\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9214, F1 Micro: 0.7551, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0961, Accuracy: 0.9222, F1 Micro: 0.7684, F1 Macro: 0.6717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9227, F1 Micro: 0.772, F1 Macro: 0.6788\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9212, F1 Micro: 0.7625, F1 Macro: 0.6451\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9227, F1 Micro: 0.772, F1 Macro: 0.6788\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.80      0.61      0.69       157\n",
      "      HS_Race       0.75      0.63      0.69       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.86      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 190.35061597824097 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4007, Accuracy: 0.8812, F1 Micro: 0.5926, F1 Macro: 0.2935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2745, Accuracy: 0.9055, F1 Micro: 0.6901, F1 Macro: 0.4832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2227, Accuracy: 0.9124, F1 Micro: 0.7449, F1 Macro: 0.5937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1861, Accuracy: 0.9176, F1 Micro: 0.7577, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.919, F1 Micro: 0.7639, F1 Macro: 0.632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1323, Accuracy: 0.9219, F1 Micro: 0.7691, F1 Macro: 0.6482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9246, F1 Micro: 0.7718, F1 Macro: 0.6613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9244, F1 Micro: 0.7802, F1 Macro: 0.6855\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.924, F1 Micro: 0.7747, F1 Macro: 0.6749\n",
      "Epoch 10/10, Train Loss: 0.069, Accuracy: 0.9239, F1 Micro: 0.7738, F1 Macro: 0.673\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9244, F1 Micro: 0.7802, F1 Macro: 0.6855\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.70      0.31      0.43        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 192.83778285980225 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9137, F1 Micro: 0.7418, F1 Macro: 0.6071\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 28.99623680114746 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3874, Accuracy: 0.8843, F1 Micro: 0.6309, F1 Macro: 0.3152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2669, Accuracy: 0.9051, F1 Micro: 0.7154, F1 Macro: 0.5132\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.9113, F1 Micro: 0.7069, F1 Macro: 0.5249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9194, F1 Micro: 0.7489, F1 Macro: 0.585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9204, F1 Micro: 0.7613, F1 Macro: 0.6114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1315, Accuracy: 0.9222, F1 Micro: 0.7722, F1 Macro: 0.6488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9235, F1 Micro: 0.7751, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9241, F1 Micro: 0.779, F1 Macro: 0.6828\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9244, F1 Micro: 0.7736, F1 Macro: 0.6809\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9224, F1 Micro: 0.7661, F1 Macro: 0.6651\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9241, F1 Micro: 0.779, F1 Macro: 0.6828\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.72      0.69      0.70       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.29424810409546 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3869, Accuracy: 0.8835, F1 Micro: 0.6049, F1 Macro: 0.2967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.9053, F1 Micro: 0.7084, F1 Macro: 0.5006\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.9114, F1 Micro: 0.7053, F1 Macro: 0.5381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9197, F1 Micro: 0.7476, F1 Macro: 0.5724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1552, Accuracy: 0.9191, F1 Micro: 0.7605, F1 Macro: 0.6114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1331, Accuracy: 0.9221, F1 Micro: 0.7717, F1 Macro: 0.6516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9221, F1 Micro: 0.7748, F1 Macro: 0.6787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9245, F1 Micro: 0.778, F1 Macro: 0.6881\n",
      "Epoch 9/10, Train Loss: 0.0812, Accuracy: 0.9231, F1 Micro: 0.769, F1 Macro: 0.6767\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9234, F1 Micro: 0.7763, F1 Macro: 0.6929\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9245, F1 Micro: 0.778, F1 Macro: 0.6881\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.72      0.65      0.69       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       0.61      0.19      0.29        72\n",
      "    HS_Gender       0.53      0.35      0.42        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.64      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.74      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.500807762146 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3969, Accuracy: 0.8816, F1 Micro: 0.5942, F1 Macro: 0.2887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9055, F1 Micro: 0.7046, F1 Macro: 0.4887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2217, Accuracy: 0.9149, F1 Micro: 0.7294, F1 Macro: 0.5599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1889, Accuracy: 0.921, F1 Micro: 0.7538, F1 Macro: 0.5917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9209, F1 Micro: 0.7669, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9218, F1 Micro: 0.7719, F1 Macro: 0.646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9215, F1 Micro: 0.7756, F1 Macro: 0.6891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.924, F1 Micro: 0.7776, F1 Macro: 0.6983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9251, F1 Micro: 0.7788, F1 Macro: 0.6969\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9213, F1 Micro: 0.7757, F1 Macro: 0.707\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9251, F1 Micro: 0.7788, F1 Macro: 0.6969\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.76      0.62      0.68       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.75      0.35      0.48        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.62      0.63      0.63       331\n",
      "    HS_Strong       0.84      0.86      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 196.7220892906189 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9143, F1 Micro: 0.7438, F1 Macro: 0.6117\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.958691358566284 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3857, Accuracy: 0.8804, F1 Micro: 0.5342, F1 Macro: 0.2639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2714, Accuracy: 0.9069, F1 Micro: 0.7074, F1 Macro: 0.5129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9127, F1 Micro: 0.7357, F1 Macro: 0.5634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9176, F1 Micro: 0.75, F1 Macro: 0.5821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1551, Accuracy: 0.9215, F1 Micro: 0.7664, F1 Macro: 0.6273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1325, Accuracy: 0.9216, F1 Micro: 0.7682, F1 Macro: 0.6499\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9218, F1 Micro: 0.7673, F1 Macro: 0.659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9211, F1 Micro: 0.7714, F1 Macro: 0.6709\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9194, F1 Micro: 0.7692, F1 Macro: 0.6817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.924, F1 Micro: 0.7776, F1 Macro: 0.6838\n",
      "Model 1 - Iteration 9016: Accuracy: 0.924, F1 Micro: 0.7776, F1 Macro: 0.6838\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.91      0.88      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.76      0.59      0.67       402\n",
      "  HS_Religion       0.82      0.54      0.65       157\n",
      "      HS_Race       0.85      0.64      0.73       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.77      0.39      0.52        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.68      0.51      0.58       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.79      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 199.2118480205536 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3886, Accuracy: 0.8793, F1 Micro: 0.534, F1 Macro: 0.2503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.9061, F1 Micro: 0.7124, F1 Macro: 0.5295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2217, Accuracy: 0.9124, F1 Micro: 0.7274, F1 Macro: 0.5481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1842, Accuracy: 0.9172, F1 Micro: 0.7495, F1 Macro: 0.5983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1557, Accuracy: 0.9202, F1 Micro: 0.7657, F1 Macro: 0.6413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1344, Accuracy: 0.9212, F1 Micro: 0.766, F1 Macro: 0.6517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9202, F1 Micro: 0.7666, F1 Macro: 0.6605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9204, F1 Micro: 0.7721, F1 Macro: 0.6706\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9188, F1 Micro: 0.7685, F1 Macro: 0.6654\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9219, F1 Micro: 0.7706, F1 Macro: 0.6791\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9204, F1 Micro: 0.7721, F1 Macro: 0.6706\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.65      0.69      0.67       402\n",
      "  HS_Religion       0.67      0.60      0.63       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.58      0.61      0.59       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.66      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 199.51664781570435 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3966, Accuracy: 0.8781, F1 Micro: 0.5153, F1 Macro: 0.2421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2727, Accuracy: 0.9067, F1 Micro: 0.7074, F1 Macro: 0.5128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2224, Accuracy: 0.9122, F1 Micro: 0.7276, F1 Macro: 0.5622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1851, Accuracy: 0.9185, F1 Micro: 0.758, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1562, Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6387\n",
      "Epoch 6/10, Train Loss: 0.1309, Accuracy: 0.9219, F1 Micro: 0.7696, F1 Macro: 0.6625\n",
      "Epoch 7/10, Train Loss: 0.1041, Accuracy: 0.9195, F1 Micro: 0.7715, F1 Macro: 0.6745\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.9202, F1 Micro: 0.7694, F1 Macro: 0.6711\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9193, F1 Micro: 0.7716, F1 Macro: 0.686\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9228, F1 Micro: 0.7708, F1 Macro: 0.6887\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6387\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.71      0.65      0.68       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.50      0.06      0.11        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.62      0.57      0.59       331\n",
      "    HS_Strong       0.85      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.62      0.64      5556\n",
      " weighted avg       0.78      0.77      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.16032218933105 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9148, F1 Micro: 0.7454, F1 Macro: 0.6145\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.626517295837402 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8829, F1 Micro: 0.6461, F1 Macro: 0.337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2691, Accuracy: 0.9068, F1 Micro: 0.7085, F1 Macro: 0.5047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2199, Accuracy: 0.9141, F1 Micro: 0.7358, F1 Macro: 0.5678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.185, Accuracy: 0.9171, F1 Micro: 0.7524, F1 Macro: 0.6069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1572, Accuracy: 0.9228, F1 Micro: 0.7704, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1308, Accuracy: 0.9251, F1 Micro: 0.7729, F1 Macro: 0.648\n",
      "Epoch 7/10, Train Loss: 0.1148, Accuracy: 0.9258, F1 Micro: 0.7707, F1 Macro: 0.6658\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9247, F1 Micro: 0.7697, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0827, Accuracy: 0.922, F1 Micro: 0.7769, F1 Macro: 0.6841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0737, Accuracy: 0.9229, F1 Micro: 0.779, F1 Macro: 0.6975\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9229, F1 Micro: 0.779, F1 Macro: 0.6975\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.64      0.76      0.69       402\n",
      "  HS_Religion       0.66      0.66      0.66       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.18      0.31        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.57      0.70      0.63       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 201.19401121139526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3893, Accuracy: 0.8809, F1 Micro: 0.6483, F1 Macro: 0.3324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2692, Accuracy: 0.9063, F1 Micro: 0.7037, F1 Macro: 0.4913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2207, Accuracy: 0.9114, F1 Micro: 0.7405, F1 Macro: 0.5836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1865, Accuracy: 0.9151, F1 Micro: 0.7491, F1 Macro: 0.6003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1568, Accuracy: 0.9204, F1 Micro: 0.7638, F1 Macro: 0.6263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1329, Accuracy: 0.9233, F1 Micro: 0.7658, F1 Macro: 0.644\n",
      "Epoch 7/10, Train Loss: 0.1144, Accuracy: 0.9228, F1 Micro: 0.7617, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9195, F1 Micro: 0.7709, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0832, Accuracy: 0.9213, F1 Micro: 0.7719, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9217, F1 Micro: 0.7727, F1 Macro: 0.692\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9217, F1 Micro: 0.7727, F1 Macro: 0.692\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.75      0.63      0.69       120\n",
      "  HS_Physical       0.77      0.24      0.36        72\n",
      "    HS_Gender       0.60      0.41      0.49        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.60      0.61      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 202.9268388748169 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3991, Accuracy: 0.8787, F1 Micro: 0.6317, F1 Macro: 0.3174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2718, Accuracy: 0.9064, F1 Micro: 0.7086, F1 Macro: 0.513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.9145, F1 Micro: 0.74, F1 Macro: 0.5851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1853, Accuracy: 0.9159, F1 Micro: 0.7513, F1 Macro: 0.6047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1564, Accuracy: 0.9182, F1 Micro: 0.7688, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1293, Accuracy: 0.9221, F1 Micro: 0.7715, F1 Macro: 0.6513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.925, F1 Micro: 0.7726, F1 Macro: 0.6816\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.9228, F1 Micro: 0.7723, F1 Macro: 0.6812\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9253, F1 Micro: 0.7718, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9242, F1 Micro: 0.7774, F1 Macro: 0.706\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9242, F1 Micro: 0.7774, F1 Macro: 0.706\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.67      0.70      0.69       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.78      0.73      0.76       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.74      0.67      0.71       689\n",
      "  HS_Moderate       0.61      0.63      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 201.4622642993927 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9152, F1 Micro: 0.747, F1 Macro: 0.6187\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.58993887901306 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3837, Accuracy: 0.8867, F1 Micro: 0.6188, F1 Macro: 0.3163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2638, Accuracy: 0.9063, F1 Micro: 0.704, F1 Macro: 0.4863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2154, Accuracy: 0.9147, F1 Micro: 0.7415, F1 Macro: 0.5609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.9193, F1 Micro: 0.7534, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.9221, F1 Micro: 0.7688, F1 Macro: 0.622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1294, Accuracy: 0.9235, F1 Micro: 0.7697, F1 Macro: 0.6469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9215, F1 Micro: 0.7722, F1 Macro: 0.6387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9239, F1 Micro: 0.7761, F1 Macro: 0.6747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0755, Accuracy: 0.9255, F1 Micro: 0.7776, F1 Macro: 0.6902\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9233, F1 Micro: 0.7703, F1 Macro: 0.6819\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9255, F1 Micro: 0.7776, F1 Macro: 0.6902\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.76      0.73      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.52      0.43      0.47        51\n",
      "     HS_Other       0.81      0.76      0.78       762\n",
      "      HS_Weak       0.74      0.69      0.72       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 205.29822373390198 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.384, Accuracy: 0.8828, F1 Micro: 0.622, F1 Macro: 0.3048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2657, Accuracy: 0.9037, F1 Micro: 0.6834, F1 Macro: 0.4513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.217, Accuracy: 0.9133, F1 Micro: 0.7415, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9198, F1 Micro: 0.755, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1508, Accuracy: 0.9226, F1 Micro: 0.7625, F1 Macro: 0.6096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.133, Accuracy: 0.9249, F1 Micro: 0.7693, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9242, F1 Micro: 0.7726, F1 Macro: 0.6355\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9227, F1 Micro: 0.7696, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.924, F1 Micro: 0.7739, F1 Macro: 0.6909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9247, F1 Micro: 0.7761, F1 Macro: 0.6944\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9247, F1 Micro: 0.7761, F1 Macro: 0.6944\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.90      0.25      0.39        72\n",
      "    HS_Gender       0.61      0.37      0.46        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 205.12251615524292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3937, Accuracy: 0.8812, F1 Micro: 0.5847, F1 Macro: 0.2853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.9052, F1 Micro: 0.6924, F1 Macro: 0.4754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9165, F1 Micro: 0.7468, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.9207, F1 Micro: 0.7581, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.9244, F1 Micro: 0.7708, F1 Macro: 0.6348\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.924, F1 Micro: 0.7707, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9231, F1 Micro: 0.7768, F1 Macro: 0.6683\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9222, F1 Micro: 0.7765, F1 Macro: 0.698\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9246, F1 Micro: 0.7736, F1 Macro: 0.7023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9257, F1 Micro: 0.7789, F1 Macro: 0.6982\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9257, F1 Micro: 0.7789, F1 Macro: 0.6982\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.81      0.59      0.68       157\n",
      "      HS_Race       0.84      0.73      0.78       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.69      0.51      0.58       331\n",
      "    HS_Strong       0.84      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.80      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 201.3832836151123 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9156, F1 Micro: 0.7484, F1 Macro: 0.6223\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.545787572860718 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3831, Accuracy: 0.8883, F1 Micro: 0.6351, F1 Macro: 0.3245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2659, Accuracy: 0.9063, F1 Micro: 0.7093, F1 Macro: 0.492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9131, F1 Micro: 0.7392, F1 Macro: 0.564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.9189, F1 Micro: 0.7464, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9213, F1 Micro: 0.7699, F1 Macro: 0.6362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1309, Accuracy: 0.9217, F1 Micro: 0.771, F1 Macro: 0.6442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.9215, F1 Micro: 0.773, F1 Macro: 0.665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9215, F1 Micro: 0.7733, F1 Macro: 0.6799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.9231, F1 Micro: 0.7744, F1 Macro: 0.6819\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9254, F1 Micro: 0.7679, F1 Macro: 0.6868\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9231, F1 Micro: 0.7744, F1 Macro: 0.6819\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.77      0.59      0.67       402\n",
      "  HS_Religion       0.78      0.58      0.67       157\n",
      "      HS_Race       0.80      0.69      0.74       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.57      0.33      0.42        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.71      0.51      0.59       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 207.01351308822632 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3851, Accuracy: 0.8836, F1 Micro: 0.5815, F1 Macro: 0.2822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2674, Accuracy: 0.9056, F1 Micro: 0.7081, F1 Macro: 0.4939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2177, Accuracy: 0.9128, F1 Micro: 0.7429, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.184, Accuracy: 0.92, F1 Micro: 0.7584, F1 Macro: 0.6108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9209, F1 Micro: 0.7712, F1 Macro: 0.6364\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9211, F1 Micro: 0.7705, F1 Macro: 0.6641\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9205, F1 Micro: 0.7702, F1 Macro: 0.6772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.919, F1 Micro: 0.7713, F1 Macro: 0.6793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.9195, F1 Micro: 0.7725, F1 Macro: 0.69\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.924, F1 Micro: 0.7686, F1 Macro: 0.6843\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9195, F1 Micro: 0.7725, F1 Macro: 0.69\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.86      0.93      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.64      0.66      0.65       402\n",
      "  HS_Religion       0.67      0.62      0.65       157\n",
      "      HS_Race       0.72      0.72      0.73       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.59      0.60      0.59       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.73      0.69      0.69      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 203.67135763168335 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3939, Accuracy: 0.8832, F1 Micro: 0.6039, F1 Macro: 0.3035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9061, F1 Micro: 0.7033, F1 Macro: 0.4815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2179, Accuracy: 0.9155, F1 Micro: 0.7314, F1 Macro: 0.562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.92, F1 Micro: 0.7566, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9216, F1 Micro: 0.7712, F1 Macro: 0.6478\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9183, F1 Micro: 0.77, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9225, F1 Micro: 0.7769, F1 Macro: 0.6838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9222, F1 Micro: 0.7777, F1 Macro: 0.6963\n",
      "Epoch 9/10, Train Loss: 0.0769, Accuracy: 0.9213, F1 Micro: 0.7774, F1 Macro: 0.695\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9235, F1 Micro: 0.7738, F1 Macro: 0.7057\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9222, F1 Micro: 0.7777, F1 Macro: 0.6963\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.68      0.80      0.74       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.71      0.68      0.69       157\n",
      "      HS_Race       0.79      0.73      0.76       120\n",
      "  HS_Physical       0.64      0.22      0.33        72\n",
      "    HS_Gender       0.60      0.41      0.49        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.66      0.80      0.72       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.68      0.70      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 204.26769375801086 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9159, F1 Micro: 0.7496, F1 Macro: 0.6253\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.7330539226532 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3811, Accuracy: 0.8891, F1 Micro: 0.6402, F1 Macro: 0.3328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.9082, F1 Micro: 0.6955, F1 Macro: 0.4994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2171, Accuracy: 0.9127, F1 Micro: 0.7469, F1 Macro: 0.5707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9197, F1 Micro: 0.7537, F1 Macro: 0.5938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9228, F1 Micro: 0.7593, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1241, Accuracy: 0.92, F1 Micro: 0.7707, F1 Macro: 0.6555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.9204, F1 Micro: 0.7763, F1 Macro: 0.6756\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9236, F1 Micro: 0.7733, F1 Macro: 0.6864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9253, F1 Micro: 0.7824, F1 Macro: 0.7058\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9253, F1 Micro: 0.7787, F1 Macro: 0.6977\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9253, F1 Micro: 0.7824, F1 Macro: 0.7058\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.71      0.64      0.68       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.72      0.70      0.71       120\n",
      "  HS_Physical       0.87      0.28      0.42        72\n",
      "    HS_Gender       0.53      0.45      0.49        51\n",
      "     HS_Other       0.79      0.81      0.80       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 209.92150568962097 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3814, Accuracy: 0.8881, F1 Micro: 0.6341, F1 Macro: 0.3258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2586, Accuracy: 0.9082, F1 Micro: 0.6998, F1 Macro: 0.5091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.218, Accuracy: 0.9115, F1 Micro: 0.7467, F1 Macro: 0.5779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9208, F1 Micro: 0.7571, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.923, F1 Micro: 0.7693, F1 Macro: 0.6526\n",
      "Epoch 6/10, Train Loss: 0.1269, Accuracy: 0.9217, F1 Micro: 0.7654, F1 Macro: 0.656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.921, F1 Micro: 0.7699, F1 Macro: 0.6649\n",
      "Epoch 8/10, Train Loss: 0.0911, Accuracy: 0.9215, F1 Micro: 0.7677, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9202, F1 Micro: 0.7735, F1 Macro: 0.6903\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.9223, F1 Micro: 0.7586, F1 Macro: 0.6806\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9202, F1 Micro: 0.7735, F1 Macro: 0.6903\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.68      0.79      0.73       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.69      0.62      0.65       157\n",
      "      HS_Race       0.66      0.76      0.71       120\n",
      "  HS_Physical       0.75      0.25      0.38        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.66      0.77      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.73      0.68      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 208.25496554374695 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3915, Accuracy: 0.8842, F1 Micro: 0.6151, F1 Macro: 0.3217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2616, Accuracy: 0.9087, F1 Micro: 0.7083, F1 Macro: 0.5235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2176, Accuracy: 0.9179, F1 Micro: 0.7499, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9201, F1 Micro: 0.7588, F1 Macro: 0.6086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9248, F1 Micro: 0.7749, F1 Macro: 0.6468\n",
      "Epoch 6/10, Train Loss: 0.1213, Accuracy: 0.92, F1 Micro: 0.7721, F1 Macro: 0.6585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9199, F1 Micro: 0.7754, F1 Macro: 0.6858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9243, F1 Micro: 0.7777, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9246, F1 Micro: 0.7859, F1 Macro: 0.7111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.9283, F1 Micro: 0.7872, F1 Macro: 0.7243\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9283, F1 Micro: 0.7872, F1 Macro: 0.7243\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.90      0.92      0.91       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.82      0.59      0.69       157\n",
      "      HS_Race       0.85      0.70      0.77       120\n",
      "  HS_Physical       0.96      0.36      0.53        72\n",
      "    HS_Gender       0.61      0.45      0.52        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.74      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.58      0.62       331\n",
      "    HS_Strong       0.89      0.86      0.88       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5556\n",
      "    macro avg       0.80      0.68      0.72      5556\n",
      " weighted avg       0.80      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 211.45604634284973 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9163, F1 Micro: 0.751, F1 Macro: 0.6289\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.809594631195068 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3824, Accuracy: 0.8878, F1 Micro: 0.6154, F1 Macro: 0.313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2601, Accuracy: 0.9052, F1 Micro: 0.719, F1 Macro: 0.5134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.9136, F1 Micro: 0.7417, F1 Macro: 0.5531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9206, F1 Micro: 0.7606, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1475, Accuracy: 0.9213, F1 Micro: 0.7687, F1 Macro: 0.6531\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.9237, F1 Micro: 0.768, F1 Macro: 0.6479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9258, F1 Micro: 0.7799, F1 Macro: 0.6646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9253, F1 Micro: 0.7805, F1 Macro: 0.6802\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9236, F1 Micro: 0.7719, F1 Macro: 0.6842\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9246, F1 Micro: 0.7771, F1 Macro: 0.7031\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9253, F1 Micro: 0.7805, F1 Macro: 0.6802\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.70      0.76      0.73       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.65      0.59      0.61       331\n",
      "    HS_Strong       0.88      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 209.91707849502563 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3829, Accuracy: 0.8868, F1 Micro: 0.6266, F1 Macro: 0.3151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.261, Accuracy: 0.9064, F1 Micro: 0.7232, F1 Macro: 0.5289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2183, Accuracy: 0.9138, F1 Micro: 0.7491, F1 Macro: 0.5849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1792, Accuracy: 0.9208, F1 Micro: 0.7568, F1 Macro: 0.6109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9212, F1 Micro: 0.7642, F1 Macro: 0.6444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.9235, F1 Micro: 0.7666, F1 Macro: 0.6535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9212, F1 Micro: 0.7719, F1 Macro: 0.6586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0915, Accuracy: 0.9222, F1 Micro: 0.7769, F1 Macro: 0.6908\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9216, F1 Micro: 0.7736, F1 Macro: 0.6867\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9233, F1 Micro: 0.7681, F1 Macro: 0.6951\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9222, F1 Micro: 0.7769, F1 Macro: 0.6908\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.71      0.79      0.75       120\n",
      "  HS_Physical       0.78      0.19      0.31        72\n",
      "    HS_Gender       0.49      0.41      0.45        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.68      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 212.84238123893738 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3912, Accuracy: 0.8852, F1 Micro: 0.6134, F1 Macro: 0.3144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2607, Accuracy: 0.9092, F1 Micro: 0.7221, F1 Macro: 0.5376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2174, Accuracy: 0.9166, F1 Micro: 0.7514, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9229, F1 Micro: 0.763, F1 Macro: 0.6102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1475, Accuracy: 0.9239, F1 Micro: 0.7706, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1248, Accuracy: 0.9259, F1 Micro: 0.7731, F1 Macro: 0.6595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1044, Accuracy: 0.921, F1 Micro: 0.7733, F1 Macro: 0.6691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0883, Accuracy: 0.9213, F1 Micro: 0.7765, F1 Macro: 0.6916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9239, F1 Micro: 0.7768, F1 Macro: 0.6901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9253, F1 Micro: 0.7834, F1 Macro: 0.7084\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9253, F1 Micro: 0.7834, F1 Macro: 0.7084\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.74      0.65      0.69       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.91      0.29      0.44        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 215.83423519134521 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9166, F1 Micro: 0.7522, F1 Macro: 0.6315\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.993061780929565 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3781, Accuracy: 0.8888, F1 Micro: 0.6535, F1 Macro: 0.3311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.9089, F1 Micro: 0.7204, F1 Macro: 0.5304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2097, Accuracy: 0.9146, F1 Micro: 0.7518, F1 Macro: 0.5932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9199, F1 Micro: 0.7662, F1 Macro: 0.6148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1486, Accuracy: 0.9239, F1 Micro: 0.7733, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.922, F1 Micro: 0.774, F1 Macro: 0.6632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9239, F1 Micro: 0.7761, F1 Macro: 0.6624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.927, F1 Micro: 0.7832, F1 Macro: 0.6887\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9208, F1 Micro: 0.7761, F1 Macro: 0.6802\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9207, F1 Micro: 0.7769, F1 Macro: 0.6974\n",
      "Model 1 - Iteration 10018: Accuracy: 0.927, F1 Micro: 0.7832, F1 Macro: 0.6887\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.73      0.67      0.70       402\n",
      "  HS_Religion       0.76      0.61      0.68       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.52      0.33      0.40        51\n",
      "     HS_Other       0.80      0.79      0.80       762\n",
      "      HS_Weak       0.75      0.71      0.73       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.80      0.77      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 215.95059657096863 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.379, Accuracy: 0.8859, F1 Micro: 0.6078, F1 Macro: 0.2972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.9083, F1 Micro: 0.7186, F1 Macro: 0.5357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9118, F1 Micro: 0.7464, F1 Macro: 0.584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9205, F1 Micro: 0.7632, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.9226, F1 Micro: 0.7678, F1 Macro: 0.6335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9224, F1 Micro: 0.7746, F1 Macro: 0.6635\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9163, F1 Micro: 0.7685, F1 Macro: 0.6702\n",
      "Epoch 8/10, Train Loss: 0.0915, Accuracy: 0.9238, F1 Micro: 0.7715, F1 Macro: 0.679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9245, F1 Micro: 0.7792, F1 Macro: 0.6936\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9223, F1 Micro: 0.772, F1 Macro: 0.6876\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9245, F1 Micro: 0.7792, F1 Macro: 0.6936\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.80      0.59      0.68       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.64      0.60      0.62       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 215.335143327713 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.886, F1 Micro: 0.6133, F1 Macro: 0.3073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.91, F1 Micro: 0.7234, F1 Macro: 0.5327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2098, Accuracy: 0.9116, F1 Micro: 0.7523, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1729, Accuracy: 0.9221, F1 Micro: 0.764, F1 Macro: 0.6229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1463, Accuracy: 0.9233, F1 Micro: 0.7729, F1 Macro: 0.658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1186, Accuracy: 0.9242, F1 Micro: 0.7769, F1 Macro: 0.6717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.9254, F1 Micro: 0.7808, F1 Macro: 0.6892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9265, F1 Micro: 0.7841, F1 Macro: 0.6981\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9243, F1 Micro: 0.778, F1 Macro: 0.6952\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9218, F1 Micro: 0.7727, F1 Macro: 0.6955\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9265, F1 Micro: 0.7841, F1 Macro: 0.6981\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.82      0.60      0.69       157\n",
      "      HS_Race       0.79      0.72      0.75       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.78      0.82      0.80       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.69      0.53      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.79      0.66      0.70      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 216.0133514404297 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.917, F1 Micro: 0.7534, F1 Macro: 0.634\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.322773218154907 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3725, Accuracy: 0.8896, F1 Micro: 0.633, F1 Macro: 0.3466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2532, Accuracy: 0.9085, F1 Micro: 0.7251, F1 Macro: 0.548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9157, F1 Micro: 0.753, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.9189, F1 Micro: 0.7642, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.924, F1 Micro: 0.7732, F1 Macro: 0.6441\n",
      "Epoch 6/10, Train Loss: 0.1271, Accuracy: 0.9231, F1 Micro: 0.7626, F1 Macro: 0.6492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9231, F1 Micro: 0.7794, F1 Macro: 0.6771\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9224, F1 Micro: 0.7762, F1 Macro: 0.682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9251, F1 Micro: 0.7817, F1 Macro: 0.696\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9248, F1 Micro: 0.7782, F1 Macro: 0.7038\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9251, F1 Micro: 0.7817, F1 Macro: 0.696\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.86      0.94      0.89       992\n",
      "HS_Individual       0.75      0.74      0.74       732\n",
      "     HS_Group       0.70      0.68      0.69       402\n",
      "  HS_Religion       0.74      0.60      0.66       157\n",
      "      HS_Race       0.86      0.64      0.73       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.56      0.45      0.50        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 217.17435431480408 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3724, Accuracy: 0.8883, F1 Micro: 0.6278, F1 Macro: 0.3148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2541, Accuracy: 0.9071, F1 Micro: 0.7235, F1 Macro: 0.5441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2092, Accuracy: 0.9145, F1 Micro: 0.7498, F1 Macro: 0.5679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1764, Accuracy: 0.9191, F1 Micro: 0.7671, F1 Macro: 0.6248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.15, Accuracy: 0.9211, F1 Micro: 0.773, F1 Macro: 0.6685\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.922, F1 Micro: 0.763, F1 Macro: 0.6494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9218, F1 Micro: 0.776, F1 Macro: 0.6876\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9233, F1 Micro: 0.7676, F1 Macro: 0.6775\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.922, F1 Micro: 0.7693, F1 Macro: 0.6958\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9225, F1 Micro: 0.7691, F1 Macro: 0.6985\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9218, F1 Micro: 0.776, F1 Macro: 0.6876\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.75      0.72      0.73       120\n",
      "  HS_Physical       0.62      0.18      0.28        72\n",
      "    HS_Gender       0.50      0.43      0.46        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.61      0.58      0.59       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.71      0.68      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 214.9512641429901 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3803, Accuracy: 0.885, F1 Micro: 0.6333, F1 Macro: 0.3424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2542, Accuracy: 0.9063, F1 Micro: 0.7296, F1 Macro: 0.5503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.9173, F1 Micro: 0.7557, F1 Macro: 0.5746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1754, Accuracy: 0.9195, F1 Micro: 0.7668, F1 Macro: 0.6198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1467, Accuracy: 0.9226, F1 Micro: 0.7748, F1 Macro: 0.6722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1249, Accuracy: 0.9256, F1 Micro: 0.7783, F1 Macro: 0.6873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9254, F1 Micro: 0.7809, F1 Macro: 0.6869\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9236, F1 Micro: 0.7696, F1 Macro: 0.6871\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9255, F1 Micro: 0.78, F1 Macro: 0.7038\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9235, F1 Micro: 0.7746, F1 Macro: 0.7071\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9254, F1 Micro: 0.7809, F1 Macro: 0.6869\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.76      0.61      0.68       402\n",
      "  HS_Religion       0.75      0.64      0.69       157\n",
      "      HS_Race       0.81      0.72      0.77       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.62      0.35      0.45        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.71      0.50      0.59       331\n",
      "    HS_Strong       0.84      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 216.804438829422 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9173, F1 Micro: 0.7544, F1 Macro: 0.6362\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.7938597202301025 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3699, Accuracy: 0.8886, F1 Micro: 0.6036, F1 Macro: 0.3232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2446, Accuracy: 0.9096, F1 Micro: 0.7235, F1 Macro: 0.5264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2029, Accuracy: 0.919, F1 Micro: 0.7493, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1699, Accuracy: 0.9227, F1 Micro: 0.7641, F1 Macro: 0.6195\n",
      "Epoch 5/10, Train Loss: 0.1433, Accuracy: 0.9216, F1 Micro: 0.7514, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9252, F1 Micro: 0.7727, F1 Macro: 0.6504\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9247, F1 Micro: 0.7718, F1 Macro: 0.6609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.9244, F1 Micro: 0.7751, F1 Macro: 0.6645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9227, F1 Micro: 0.7767, F1 Macro: 0.6913\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9225, F1 Micro: 0.7743, F1 Macro: 0.6931\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9227, F1 Micro: 0.7767, F1 Macro: 0.6913\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.71      0.61      0.65       157\n",
      "      HS_Race       0.80      0.62      0.70       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.55      0.41      0.47        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.68      0.54      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 224.3613736629486 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3687, Accuracy: 0.8885, F1 Micro: 0.6063, F1 Macro: 0.3132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9081, F1 Micro: 0.708, F1 Macro: 0.5037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2031, Accuracy: 0.9178, F1 Micro: 0.7461, F1 Macro: 0.5863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1689, Accuracy: 0.9221, F1 Micro: 0.7626, F1 Macro: 0.6201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1452, Accuracy: 0.9227, F1 Micro: 0.7627, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.9245, F1 Micro: 0.7677, F1 Macro: 0.6398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9233, F1 Micro: 0.7691, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9225, F1 Micro: 0.7747, F1 Macro: 0.6654\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9211, F1 Micro: 0.7731, F1 Macro: 0.6929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9237, F1 Micro: 0.7794, F1 Macro: 0.7013\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9237, F1 Micro: 0.7794, F1 Macro: 0.7013\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.93      0.91       992\n",
      "HS_Individual       0.71      0.77      0.73       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.81      0.31      0.44        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.77      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 225.24367237091064 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3777, Accuracy: 0.8851, F1 Micro: 0.5826, F1 Macro: 0.3174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9116, F1 Micro: 0.7275, F1 Macro: 0.5335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9191, F1 Micro: 0.7561, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9228, F1 Micro: 0.7626, F1 Macro: 0.6232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1406, Accuracy: 0.9253, F1 Micro: 0.7686, F1 Macro: 0.6468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9243, F1 Micro: 0.7767, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0985, Accuracy: 0.9262, F1 Micro: 0.7814, F1 Macro: 0.6807\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9249, F1 Micro: 0.7701, F1 Macro: 0.6808\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.927, F1 Micro: 0.7784, F1 Macro: 0.7054\n",
      "Epoch 10/10, Train Loss: 0.0619, Accuracy: 0.924, F1 Micro: 0.7706, F1 Macro: 0.6958\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9262, F1 Micro: 0.7814, F1 Macro: 0.6807\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.74      0.75      0.74       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.80      0.59      0.68       157\n",
      "      HS_Race       0.84      0.69      0.76       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.84      0.85      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5556\n",
      "    macro avg       0.79      0.64      0.68      5556\n",
      " weighted avg       0.80      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 222.70059370994568 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9175, F1 Micro: 0.7553, F1 Macro: 0.6382\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.5656802654266357 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3702, Accuracy: 0.8889, F1 Micro: 0.6053, F1 Macro: 0.3049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2465, Accuracy: 0.9078, F1 Micro: 0.7255, F1 Macro: 0.5559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9169, F1 Micro: 0.7441, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9165, F1 Micro: 0.76, F1 Macro: 0.6149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9223, F1 Micro: 0.7696, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.117, Accuracy: 0.9258, F1 Micro: 0.7765, F1 Macro: 0.6653\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.925, F1 Micro: 0.7728, F1 Macro: 0.668\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9218, F1 Micro: 0.776, F1 Macro: 0.6788\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9205, F1 Micro: 0.774, F1 Macro: 0.6852\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.923, F1 Micro: 0.7757, F1 Macro: 0.7006\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9258, F1 Micro: 0.7765, F1 Macro: 0.6653\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.75      0.64      0.69       157\n",
      "      HS_Race       0.80      0.66      0.72       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.81      0.76      0.78       762\n",
      "      HS_Weak       0.75      0.69      0.72       689\n",
      "  HS_Moderate       0.65      0.55      0.60       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.79      0.63      0.67      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 222.2948911190033 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3709, Accuracy: 0.8873, F1 Micro: 0.5906, F1 Macro: 0.2867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.9098, F1 Micro: 0.7206, F1 Macro: 0.5496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.9175, F1 Micro: 0.7506, F1 Macro: 0.5884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9166, F1 Micro: 0.7586, F1 Macro: 0.6254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1416, Accuracy: 0.9201, F1 Micro: 0.7695, F1 Macro: 0.6503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1175, Accuracy: 0.9251, F1 Micro: 0.7745, F1 Macro: 0.6711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0985, Accuracy: 0.9214, F1 Micro: 0.7747, F1 Macro: 0.6842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.082, Accuracy: 0.9245, F1 Micro: 0.7784, F1 Macro: 0.6821\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.924, F1 Micro: 0.7761, F1 Macro: 0.6893\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.9162, F1 Micro: 0.7681, F1 Macro: 0.6921\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9245, F1 Micro: 0.7784, F1 Macro: 0.6821\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.75      0.70      0.72       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.59      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 225.3576695919037 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3807, Accuracy: 0.8862, F1 Micro: 0.5856, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2482, Accuracy: 0.909, F1 Micro: 0.7257, F1 Macro: 0.5635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.9175, F1 Micro: 0.7564, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1652, Accuracy: 0.9187, F1 Micro: 0.7635, F1 Macro: 0.6267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9217, F1 Micro: 0.7736, F1 Macro: 0.6516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9227, F1 Micro: 0.7809, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.9253, F1 Micro: 0.7811, F1 Macro: 0.6834\n",
      "Epoch 8/10, Train Loss: 0.0797, Accuracy: 0.9254, F1 Micro: 0.7778, F1 Macro: 0.6875\n",
      "Epoch 9/10, Train Loss: 0.068, Accuracy: 0.9251, F1 Micro: 0.7769, F1 Macro: 0.7012\n",
      "Epoch 10/10, Train Loss: 0.0569, Accuracy: 0.9206, F1 Micro: 0.7758, F1 Macro: 0.7109\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9253, F1 Micro: 0.7811, F1 Macro: 0.6834\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.74      0.65      0.69       402\n",
      "  HS_Religion       0.80      0.60      0.68       157\n",
      "      HS_Race       0.81      0.71      0.76       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.70      0.31      0.43        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.73      0.72       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.79      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 223.16946029663086 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9178, F1 Micro: 0.7562, F1 Macro: 0.6396\n",
      "Total sampling time: 1369.98 seconds\n",
      "Total runtime: 15969.28159737587 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cd931",
   "metadata": {
    "papermill": {
     "duration": 0.012084,
     "end_time": "2025-04-13T07:01:23.517025",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.504941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369776bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.8279, F1 Micro: 0.0872, F1 Macro: 0.0282\n",
      "Epoch 2/10, Train Loss: 0.4622, Accuracy: 0.8279, F1 Micro: 0.0229, F1 Macro: 0.0086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3927, Accuracy: 0.8335, F1 Micro: 0.1068, F1 Macro: 0.0416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3964, Accuracy: 0.8362, F1 Micro: 0.1315, F1 Macro: 0.046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3802, Accuracy: 0.8472, F1 Micro: 0.2582, F1 Macro: 0.0889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3611, Accuracy: 0.8539, F1 Micro: 0.347, F1 Macro: 0.1151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3431, Accuracy: 0.8712, F1 Micro: 0.5052, F1 Macro: 0.2252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3176, Accuracy: 0.8724, F1 Micro: 0.5088, F1 Macro: 0.2394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2825, Accuracy: 0.8742, F1 Micro: 0.5594, F1 Macro: 0.2676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2533, Accuracy: 0.8761, F1 Micro: 0.5629, F1 Macro: 0.2795\n",
      "Model 1 - Iteration 658: Accuracy: 0.8761, F1 Micro: 0.5629, F1 Macro: 0.2795\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.73      0.76      1134\n",
      "      Abusive       0.84      0.67      0.74       992\n",
      "HS_Individual       0.64      0.46      0.54       732\n",
      "     HS_Group       0.57      0.10      0.17       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.53      0.58       762\n",
      "      HS_Weak       0.59      0.41      0.49       689\n",
      "  HS_Moderate       0.45      0.05      0.08       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.46      0.56      5556\n",
      "    macro avg       0.38      0.25      0.28      5556\n",
      " weighted avg       0.62      0.46      0.52      5556\n",
      "  samples avg       0.35      0.26      0.27      5556\n",
      "\n",
      "Training completed in 54.633593797683716 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6617, Accuracy: 0.8259, F1 Micro: 0.1583, F1 Macro: 0.0585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4885, Accuracy: 0.8383, F1 Micro: 0.2329, F1 Macro: 0.0784\n",
      "Epoch 3/10, Train Loss: 0.4062, Accuracy: 0.8355, F1 Micro: 0.1232, F1 Macro: 0.047\n",
      "Epoch 4/10, Train Loss: 0.4042, Accuracy: 0.8358, F1 Micro: 0.1132, F1 Macro: 0.0443\n",
      "Epoch 5/10, Train Loss: 0.3882, Accuracy: 0.8426, F1 Micro: 0.1993, F1 Macro: 0.0725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3714, Accuracy: 0.8533, F1 Micro: 0.3255, F1 Macro: 0.106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3558, Accuracy: 0.8608, F1 Micro: 0.4029, F1 Macro: 0.1366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.335, Accuracy: 0.8717, F1 Micro: 0.4978, F1 Macro: 0.2183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3044, Accuracy: 0.8757, F1 Micro: 0.5599, F1 Macro: 0.2579\n",
      "Epoch 10/10, Train Loss: 0.2723, Accuracy: 0.876, F1 Micro: 0.5502, F1 Macro: 0.2581\n",
      "Model 2 - Iteration 658: Accuracy: 0.8757, F1 Micro: 0.5599, F1 Macro: 0.2579\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.76      0.76      1134\n",
      "      Abusive       0.77      0.71      0.74       992\n",
      "HS_Individual       0.64      0.45      0.53       732\n",
      "     HS_Group       0.50      0.00      0.01       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.51      0.58       762\n",
      "      HS_Weak       0.63      0.38      0.47       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.46      0.56      5556\n",
      "    macro avg       0.33      0.23      0.26      5556\n",
      " weighted avg       0.58      0.46      0.50      5556\n",
      "  samples avg       0.37      0.26      0.28      5556\n",
      "\n",
      "Training completed in 49.792359828948975 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6172, Accuracy: 0.8198, F1 Micro: 0.3684, F1 Macro: 0.1118\n",
      "Epoch 2/10, Train Loss: 0.4799, Accuracy: 0.8282, F1 Micro: 0.0142, F1 Macro: 0.0063\n",
      "Epoch 3/10, Train Loss: 0.4012, Accuracy: 0.833, F1 Micro: 0.0831, F1 Macro: 0.031\n",
      "Epoch 4/10, Train Loss: 0.4015, Accuracy: 0.8381, F1 Micro: 0.1679, F1 Macro: 0.0518\n",
      "Epoch 5/10, Train Loss: 0.3869, Accuracy: 0.8431, F1 Micro: 0.215, F1 Macro: 0.0724\n",
      "Epoch 6/10, Train Loss: 0.3725, Accuracy: 0.8499, F1 Micro: 0.2925, F1 Macro: 0.0972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3585, Accuracy: 0.8575, F1 Micro: 0.3854, F1 Macro: 0.1216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3352, Accuracy: 0.8693, F1 Micro: 0.4738, F1 Macro: 0.2005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2977, Accuracy: 0.8755, F1 Micro: 0.5714, F1 Macro: 0.2756\n",
      "Epoch 10/10, Train Loss: 0.2676, Accuracy: 0.8765, F1 Micro: 0.561, F1 Macro: 0.2835\n",
      "Model 3 - Iteration 658: Accuracy: 0.8755, F1 Micro: 0.5714, F1 Macro: 0.2756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.73      0.75      1134\n",
      "      Abusive       0.80      0.74      0.77       992\n",
      "HS_Individual       0.63      0.45      0.53       732\n",
      "     HS_Group       0.45      0.04      0.08       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.58      0.60       762\n",
      "      HS_Weak       0.60      0.45      0.52       689\n",
      "  HS_Moderate       0.40      0.04      0.07       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.48      0.57      5556\n",
      "    macro avg       0.36      0.25      0.28      5556\n",
      " weighted avg       0.60      0.48      0.52      5556\n",
      "  samples avg       0.38      0.29      0.30      5556\n",
      "\n",
      "Training completed in 47.164233922958374 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8757, F1 Micro: 0.5647, F1 Macro: 0.271\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 147.9106686115265 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.518, Accuracy: 0.8313, F1 Micro: 0.09, F1 Macro: 0.0294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4065, Accuracy: 0.84, F1 Micro: 0.1728, F1 Macro: 0.0632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3667, Accuracy: 0.8618, F1 Micro: 0.4153, F1 Macro: 0.16\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3241, Accuracy: 0.8758, F1 Micro: 0.5832, F1 Macro: 0.282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2909, Accuracy: 0.8791, F1 Micro: 0.6157, F1 Macro: 0.3186\n",
      "Epoch 6/10, Train Loss: 0.2619, Accuracy: 0.8855, F1 Micro: 0.6072, F1 Macro: 0.3384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2376, Accuracy: 0.8894, F1 Micro: 0.6328, F1 Macro: 0.3867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2131, Accuracy: 0.8901, F1 Micro: 0.6693, F1 Macro: 0.4291\n",
      "Epoch 9/10, Train Loss: 0.1946, Accuracy: 0.8933, F1 Micro: 0.6631, F1 Macro: 0.4354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1817, Accuracy: 0.8938, F1 Micro: 0.6756, F1 Macro: 0.4525\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8938, F1 Micro: 0.6756, F1 Macro: 0.4525\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.83      0.81      1134\n",
      "      Abusive       0.87      0.80      0.83       992\n",
      "HS_Individual       0.64      0.67      0.66       732\n",
      "     HS_Group       0.61      0.46      0.52       402\n",
      "  HS_Religion       0.61      0.28      0.38       157\n",
      "      HS_Race       0.83      0.28      0.42       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.71      0.69       762\n",
      "      HS_Weak       0.61      0.64      0.62       689\n",
      "  HS_Moderate       0.45      0.29      0.35       331\n",
      "    HS_Strong       1.00      0.07      0.13       114\n",
      "\n",
      "    micro avg       0.71      0.64      0.68      5556\n",
      "    macro avg       0.59      0.42      0.45      5556\n",
      " weighted avg       0.70      0.64      0.65      5556\n",
      "  samples avg       0.38      0.36      0.35      5556\n",
      "\n",
      "Training completed in 71.40200543403625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5645, Accuracy: 0.8425, F1 Micro: 0.323, F1 Macro: 0.0965\n",
      "Epoch 2/10, Train Loss: 0.4141, Accuracy: 0.8489, F1 Micro: 0.2899, F1 Macro: 0.0933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3732, Accuracy: 0.8589, F1 Micro: 0.38, F1 Macro: 0.1293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3374, Accuracy: 0.8755, F1 Micro: 0.5819, F1 Macro: 0.2701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3047, Accuracy: 0.8797, F1 Micro: 0.6069, F1 Macro: 0.2948\n",
      "Epoch 6/10, Train Loss: 0.274, Accuracy: 0.884, F1 Micro: 0.6069, F1 Macro: 0.3126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2488, Accuracy: 0.8852, F1 Micro: 0.6115, F1 Macro: 0.3381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2231, Accuracy: 0.889, F1 Micro: 0.6373, F1 Macro: 0.3685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2005, Accuracy: 0.8905, F1 Micro: 0.6431, F1 Macro: 0.3843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1898, Accuracy: 0.8916, F1 Micro: 0.662, F1 Macro: 0.4205\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8916, F1 Micro: 0.662, F1 Macro: 0.4205\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.81      0.81      1134\n",
      "      Abusive       0.86      0.80      0.83       992\n",
      "HS_Individual       0.65      0.64      0.65       732\n",
      "     HS_Group       0.64      0.41      0.50       402\n",
      "  HS_Religion       0.57      0.25      0.35       157\n",
      "      HS_Race       1.00      0.14      0.25       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.68      0.68       762\n",
      "      HS_Weak       0.60      0.59      0.60       689\n",
      "  HS_Moderate       0.43      0.25      0.32       331\n",
      "    HS_Strong       1.00      0.04      0.07       114\n",
      "\n",
      "    micro avg       0.71      0.62      0.66      5556\n",
      "    macro avg       0.60      0.39      0.42      5556\n",
      " weighted avg       0.70      0.62      0.64      5556\n",
      "  samples avg       0.38      0.35      0.34      5556\n",
      "\n",
      "Training completed in 71.48895621299744 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.8286, F1 Micro: 0.0195, F1 Macro: 0.0085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4115, Accuracy: 0.8457, F1 Micro: 0.2524, F1 Macro: 0.0861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3738, Accuracy: 0.8531, F1 Micro: 0.3209, F1 Macro: 0.1051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3393, Accuracy: 0.876, F1 Micro: 0.5961, F1 Macro: 0.2905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3014, Accuracy: 0.8819, F1 Micro: 0.6009, F1 Macro: 0.3176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.271, Accuracy: 0.8873, F1 Micro: 0.6234, F1 Macro: 0.3503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2468, Accuracy: 0.8887, F1 Micro: 0.6277, F1 Macro: 0.3605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2173, Accuracy: 0.8885, F1 Micro: 0.6635, F1 Macro: 0.4039\n",
      "Epoch 9/10, Train Loss: 0.1972, Accuracy: 0.8918, F1 Micro: 0.649, F1 Macro: 0.399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1882, Accuracy: 0.8899, F1 Micro: 0.6709, F1 Macro: 0.4429\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8899, F1 Micro: 0.6709, F1 Macro: 0.4429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.82      0.81      1134\n",
      "      Abusive       0.86      0.82      0.84       992\n",
      "HS_Individual       0.61      0.69      0.65       732\n",
      "     HS_Group       0.62      0.43      0.50       402\n",
      "  HS_Religion       0.64      0.31      0.42       157\n",
      "      HS_Race       0.93      0.21      0.34       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.73      0.69       762\n",
      "      HS_Weak       0.57      0.67      0.62       689\n",
      "  HS_Moderate       0.43      0.30      0.35       331\n",
      "    HS_Strong       1.00      0.05      0.10       114\n",
      "\n",
      "    micro avg       0.69      0.65      0.67      5556\n",
      "    macro avg       0.59      0.42      0.44      5556\n",
      " weighted avg       0.68      0.65      0.65      5556\n",
      "  samples avg       0.38      0.36      0.35      5556\n",
      "\n",
      "Training completed in 73.69576001167297 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8837, F1 Micro: 0.6171, F1 Macro: 0.3548\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 135.21123337745667 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4846, Accuracy: 0.8453, F1 Micro: 0.3098, F1 Macro: 0.0881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3793, Accuracy: 0.8675, F1 Micro: 0.5066, F1 Macro: 0.212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3154, Accuracy: 0.878, F1 Micro: 0.6046, F1 Macro: 0.316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2736, Accuracy: 0.8884, F1 Micro: 0.629, F1 Macro: 0.3597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2383, Accuracy: 0.891, F1 Micro: 0.6651, F1 Macro: 0.4687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2115, Accuracy: 0.8934, F1 Micro: 0.6826, F1 Macro: 0.469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2029, Accuracy: 0.8999, F1 Micro: 0.6897, F1 Macro: 0.5092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1709, Accuracy: 0.8968, F1 Micro: 0.7004, F1 Macro: 0.4928\n",
      "Epoch 9/10, Train Loss: 0.1495, Accuracy: 0.9009, F1 Micro: 0.6906, F1 Macro: 0.4956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1343, Accuracy: 0.896, F1 Micro: 0.7031, F1 Macro: 0.5366\n",
      "Model 1 - Iteration 2535: Accuracy: 0.896, F1 Micro: 0.7031, F1 Macro: 0.5366\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.86      0.82      1134\n",
      "      Abusive       0.84      0.88      0.86       992\n",
      "HS_Individual       0.68      0.64      0.66       732\n",
      "     HS_Group       0.52      0.66      0.58       402\n",
      "  HS_Religion       0.63      0.64      0.64       157\n",
      "      HS_Race       0.66      0.56      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.76      0.72       762\n",
      "      HS_Weak       0.64      0.62      0.63       689\n",
      "  HS_Moderate       0.42      0.56      0.48       331\n",
      "    HS_Strong       0.94      0.30      0.45       114\n",
      "\n",
      "    micro avg       0.69      0.72      0.70      5556\n",
      "    macro avg       0.57      0.54      0.54      5556\n",
      " weighted avg       0.68      0.72      0.69      5556\n",
      "  samples avg       0.40      0.40      0.38      5556\n",
      "\n",
      "Training completed in 88.29880809783936 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.518, Accuracy: 0.843, F1 Micro: 0.3172, F1 Macro: 0.0852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.387, Accuracy: 0.8594, F1 Micro: 0.4547, F1 Macro: 0.1586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3306, Accuracy: 0.879, F1 Micro: 0.5917, F1 Macro: 0.2904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2873, Accuracy: 0.8851, F1 Micro: 0.6189, F1 Macro: 0.3204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2484, Accuracy: 0.891, F1 Micro: 0.6539, F1 Macro: 0.3941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2221, Accuracy: 0.8917, F1 Micro: 0.6673, F1 Macro: 0.4166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2113, Accuracy: 0.8946, F1 Micro: 0.6789, F1 Macro: 0.4773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1829, Accuracy: 0.8969, F1 Micro: 0.6876, F1 Macro: 0.4675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1614, Accuracy: 0.897, F1 Micro: 0.6946, F1 Macro: 0.4782\n",
      "Epoch 10/10, Train Loss: 0.1427, Accuracy: 0.8968, F1 Micro: 0.6874, F1 Macro: 0.512\n",
      "Model 2 - Iteration 2535: Accuracy: 0.897, F1 Micro: 0.6946, F1 Macro: 0.4782\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.84      0.82      1134\n",
      "      Abusive       0.82      0.89      0.86       992\n",
      "HS_Individual       0.65      0.69      0.67       732\n",
      "     HS_Group       0.66      0.48      0.55       402\n",
      "  HS_Religion       0.76      0.28      0.41       157\n",
      "      HS_Race       0.97      0.33      0.49       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.75      0.70       762\n",
      "      HS_Weak       0.60      0.68      0.64       689\n",
      "  HS_Moderate       0.47      0.31      0.37       331\n",
      "    HS_Strong       1.00      0.13      0.23       114\n",
      "\n",
      "    micro avg       0.71      0.68      0.69      5556\n",
      "    macro avg       0.62      0.45      0.48      5556\n",
      " weighted avg       0.70      0.68      0.67      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 87.51320672035217 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4982, Accuracy: 0.8389, F1 Micro: 0.1668, F1 Macro: 0.0614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3867, Accuracy: 0.8559, F1 Micro: 0.4222, F1 Macro: 0.1429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3298, Accuracy: 0.8768, F1 Micro: 0.6105, F1 Macro: 0.3233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2835, Accuracy: 0.8887, F1 Micro: 0.6289, F1 Macro: 0.3561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2436, Accuracy: 0.8913, F1 Micro: 0.6502, F1 Macro: 0.4188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2182, Accuracy: 0.8924, F1 Micro: 0.6819, F1 Macro: 0.4527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2082, Accuracy: 0.8983, F1 Micro: 0.6853, F1 Macro: 0.4935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1805, Accuracy: 0.8964, F1 Micro: 0.6895, F1 Macro: 0.4841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1561, Accuracy: 0.8983, F1 Micro: 0.6929, F1 Macro: 0.4806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.137, Accuracy: 0.9005, F1 Micro: 0.696, F1 Macro: 0.5106\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9005, F1 Micro: 0.696, F1 Macro: 0.5106\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.80      0.81      1134\n",
      "      Abusive       0.87      0.84      0.86       992\n",
      "HS_Individual       0.65      0.66      0.66       732\n",
      "     HS_Group       0.66      0.50      0.57       402\n",
      "  HS_Religion       0.70      0.42      0.53       157\n",
      "      HS_Race       0.76      0.44      0.56       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.70      0.69       762\n",
      "      HS_Weak       0.62      0.64      0.63       689\n",
      "  HS_Moderate       0.54      0.39      0.45       331\n",
      "    HS_Strong       0.96      0.23      0.37       114\n",
      "\n",
      "    micro avg       0.73      0.66      0.70      5556\n",
      "    macro avg       0.61      0.47      0.51      5556\n",
      " weighted avg       0.72      0.66      0.68      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 89.20862913131714 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8884, F1 Micro: 0.644, F1 Macro: 0.406\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 121.47702813148499 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4614, Accuracy: 0.8366, F1 Micro: 0.121, F1 Macro: 0.0456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3565, Accuracy: 0.8774, F1 Micro: 0.5767, F1 Macro: 0.2768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2966, Accuracy: 0.8873, F1 Micro: 0.6158, F1 Macro: 0.3343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2562, Accuracy: 0.8913, F1 Micro: 0.6732, F1 Macro: 0.4849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2269, Accuracy: 0.8996, F1 Micro: 0.6888, F1 Macro: 0.4578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.193, Accuracy: 0.8964, F1 Micro: 0.7021, F1 Macro: 0.509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1649, Accuracy: 0.9036, F1 Micro: 0.7113, F1 Macro: 0.551\n",
      "Epoch 8/10, Train Loss: 0.1441, Accuracy: 0.8953, F1 Micro: 0.7101, F1 Macro: 0.5506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1247, Accuracy: 0.902, F1 Micro: 0.7139, F1 Macro: 0.562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1096, Accuracy: 0.905, F1 Micro: 0.7202, F1 Macro: 0.5769\n",
      "Model 1 - Iteration 3335: Accuracy: 0.905, F1 Micro: 0.7202, F1 Macro: 0.5769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.87      0.86      0.87       992\n",
      "HS_Individual       0.66      0.67      0.67       732\n",
      "     HS_Group       0.61      0.58      0.59       402\n",
      "  HS_Religion       0.70      0.61      0.65       157\n",
      "      HS_Race       0.68      0.70      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.73      0.73       762\n",
      "      HS_Weak       0.64      0.66      0.65       689\n",
      "  HS_Moderate       0.52      0.48      0.50       331\n",
      "    HS_Strong       0.86      0.67      0.75       114\n",
      "\n",
      "    micro avg       0.73      0.71      0.72      5556\n",
      "    macro avg       0.59      0.57      0.58      5556\n",
      " weighted avg       0.71      0.71      0.71      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 101.33095812797546 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4827, Accuracy: 0.8386, F1 Micro: 0.1349, F1 Macro: 0.0513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3647, Accuracy: 0.8749, F1 Micro: 0.5626, F1 Macro: 0.2583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3042, Accuracy: 0.8854, F1 Micro: 0.6331, F1 Macro: 0.3294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2646, Accuracy: 0.8887, F1 Micro: 0.6706, F1 Macro: 0.4362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2322, Accuracy: 0.8959, F1 Micro: 0.6804, F1 Macro: 0.4307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1996, Accuracy: 0.8913, F1 Micro: 0.6953, F1 Macro: 0.4837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1718, Accuracy: 0.8983, F1 Micro: 0.7083, F1 Macro: 0.5415\n",
      "Epoch 8/10, Train Loss: 0.1525, Accuracy: 0.8935, F1 Micro: 0.7062, F1 Macro: 0.5519\n",
      "Epoch 9/10, Train Loss: 0.1332, Accuracy: 0.8999, F1 Micro: 0.7076, F1 Macro: 0.5398\n",
      "Epoch 10/10, Train Loss: 0.1152, Accuracy: 0.9055, F1 Micro: 0.7072, F1 Macro: 0.5538\n",
      "Model 2 - Iteration 3335: Accuracy: 0.8983, F1 Micro: 0.7083, F1 Macro: 0.5415\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.87      0.82      1134\n",
      "      Abusive       0.85      0.84      0.85       992\n",
      "HS_Individual       0.65      0.70      0.67       732\n",
      "     HS_Group       0.58      0.58      0.58       402\n",
      "  HS_Religion       0.64      0.48      0.55       157\n",
      "      HS_Race       0.77      0.60      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.77      0.72       762\n",
      "      HS_Weak       0.62      0.69      0.65       689\n",
      "  HS_Moderate       0.48      0.46      0.47       331\n",
      "    HS_Strong       0.93      0.35      0.51       114\n",
      "\n",
      "    micro avg       0.70      0.72      0.71      5556\n",
      "    macro avg       0.58      0.53      0.54      5556\n",
      " weighted avg       0.69      0.72      0.70      5556\n",
      "  samples avg       0.40      0.39      0.38      5556\n",
      "\n",
      "Training completed in 98.40283942222595 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.471, Accuracy: 0.8379, F1 Micro: 0.154, F1 Macro: 0.0492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3663, Accuracy: 0.8766, F1 Micro: 0.5664, F1 Macro: 0.2686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.304, Accuracy: 0.8877, F1 Micro: 0.6391, F1 Macro: 0.3605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2599, Accuracy: 0.8888, F1 Micro: 0.6778, F1 Macro: 0.4461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2306, Accuracy: 0.8952, F1 Micro: 0.6922, F1 Macro: 0.4565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1987, Accuracy: 0.896, F1 Micro: 0.7054, F1 Macro: 0.5142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1709, Accuracy: 0.8982, F1 Micro: 0.7092, F1 Macro: 0.5503\n",
      "Epoch 8/10, Train Loss: 0.1537, Accuracy: 0.8976, F1 Micro: 0.7071, F1 Macro: 0.549\n",
      "Epoch 9/10, Train Loss: 0.1271, Accuracy: 0.8986, F1 Micro: 0.7091, F1 Macro: 0.5577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1143, Accuracy: 0.9057, F1 Micro: 0.7132, F1 Macro: 0.5646\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9057, F1 Micro: 0.7132, F1 Macro: 0.5646\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.80      0.82      1134\n",
      "      Abusive       0.88      0.85      0.86       992\n",
      "HS_Individual       0.66      0.67      0.67       732\n",
      "     HS_Group       0.66      0.50      0.57       402\n",
      "  HS_Religion       0.71      0.52      0.60       157\n",
      "      HS_Race       0.69      0.68      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.04      0.08        51\n",
      "     HS_Other       0.75      0.69      0.72       762\n",
      "      HS_Weak       0.63      0.66      0.64       689\n",
      "  HS_Moderate       0.56      0.43      0.48       331\n",
      "    HS_Strong       0.89      0.51      0.65       114\n",
      "\n",
      "    micro avg       0.75      0.68      0.71      5556\n",
      "    macro avg       0.69      0.53      0.56      5556\n",
      " weighted avg       0.74      0.68      0.70      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 99.12243628501892 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8921, F1 Micro: 0.6615, F1 Macro: 0.4448\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 109.17238807678223 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4458, Accuracy: 0.8533, F1 Micro: 0.3456, F1 Macro: 0.1082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3396, Accuracy: 0.8797, F1 Micro: 0.6102, F1 Macro: 0.3018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2824, Accuracy: 0.8934, F1 Micro: 0.6554, F1 Macro: 0.4209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2401, Accuracy: 0.8977, F1 Micro: 0.677, F1 Macro: 0.4385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2102, Accuracy: 0.903, F1 Micro: 0.7059, F1 Macro: 0.511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1815, Accuracy: 0.9059, F1 Micro: 0.7178, F1 Macro: 0.542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1518, Accuracy: 0.9033, F1 Micro: 0.7213, F1 Macro: 0.5535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.13, Accuracy: 0.9072, F1 Micro: 0.7257, F1 Macro: 0.572\n",
      "Epoch 9/10, Train Loss: 0.114, Accuracy: 0.9098, F1 Micro: 0.7241, F1 Macro: 0.5784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0976, Accuracy: 0.9084, F1 Micro: 0.7338, F1 Macro: 0.5876\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9084, F1 Micro: 0.7338, F1 Macro: 0.5876\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.85      0.83      1134\n",
      "      Abusive       0.86      0.88      0.87       992\n",
      "HS_Individual       0.70      0.69      0.70       732\n",
      "     HS_Group       0.60      0.64      0.62       402\n",
      "  HS_Religion       0.72      0.57      0.63       157\n",
      "      HS_Race       0.69      0.68      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.72      0.77      0.74       762\n",
      "      HS_Weak       0.68      0.67      0.67       689\n",
      "  HS_Moderate       0.51      0.55      0.53       331\n",
      "    HS_Strong       0.85      0.66      0.74       114\n",
      "\n",
      "    micro avg       0.73      0.73      0.73      5556\n",
      "    macro avg       0.68      0.58      0.59      5556\n",
      " weighted avg       0.73      0.73      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 111.43191385269165 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4661, Accuracy: 0.8539, F1 Micro: 0.3636, F1 Macro: 0.1112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3511, Accuracy: 0.8796, F1 Micro: 0.6003, F1 Macro: 0.283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2918, Accuracy: 0.8901, F1 Micro: 0.6421, F1 Macro: 0.3575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2489, Accuracy: 0.8952, F1 Micro: 0.6732, F1 Macro: 0.4033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2184, Accuracy: 0.9004, F1 Micro: 0.6915, F1 Macro: 0.478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1909, Accuracy: 0.9038, F1 Micro: 0.705, F1 Macro: 0.5127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1611, Accuracy: 0.902, F1 Micro: 0.7189, F1 Macro: 0.5355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1345, Accuracy: 0.9051, F1 Micro: 0.7222, F1 Macro: 0.5721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1215, Accuracy: 0.9056, F1 Micro: 0.7224, F1 Macro: 0.5709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1037, Accuracy: 0.9068, F1 Micro: 0.7269, F1 Macro: 0.574\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9068, F1 Micro: 0.7269, F1 Macro: 0.574\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.83      1134\n",
      "      Abusive       0.84      0.89      0.86       992\n",
      "HS_Individual       0.69      0.67      0.68       732\n",
      "     HS_Group       0.62      0.61      0.62       402\n",
      "  HS_Religion       0.75      0.52      0.62       157\n",
      "      HS_Race       0.79      0.61      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.78      0.74       762\n",
      "      HS_Weak       0.66      0.65      0.65       689\n",
      "  HS_Moderate       0.51      0.51      0.51       331\n",
      "    HS_Strong       0.91      0.55      0.69       114\n",
      "\n",
      "    micro avg       0.73      0.72      0.73      5556\n",
      "    macro avg       0.61      0.55      0.57      5556\n",
      " weighted avg       0.72      0.72      0.72      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 114.2154951095581 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4562, Accuracy: 0.8497, F1 Micro: 0.2947, F1 Macro: 0.0979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.349, Accuracy: 0.8803, F1 Micro: 0.6038, F1 Macro: 0.3004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2868, Accuracy: 0.8919, F1 Micro: 0.6652, F1 Macro: 0.4076\n",
      "Epoch 4/10, Train Loss: 0.2459, Accuracy: 0.8981, F1 Micro: 0.665, F1 Macro: 0.4357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2192, Accuracy: 0.9013, F1 Micro: 0.7113, F1 Macro: 0.5332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1892, Accuracy: 0.9068, F1 Micro: 0.7185, F1 Macro: 0.5393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1581, Accuracy: 0.9058, F1 Micro: 0.7253, F1 Macro: 0.5535\n",
      "Epoch 8/10, Train Loss: 0.1333, Accuracy: 0.9059, F1 Micro: 0.7189, F1 Macro: 0.5695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1175, Accuracy: 0.9079, F1 Micro: 0.73, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1034, Accuracy: 0.9105, F1 Micro: 0.7333, F1 Macro: 0.5786\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9105, F1 Micro: 0.7333, F1 Macro: 0.5786\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.88      0.87      0.87       992\n",
      "HS_Individual       0.70      0.69      0.69       732\n",
      "     HS_Group       0.66      0.60      0.63       402\n",
      "  HS_Religion       0.78      0.48      0.59       157\n",
      "      HS_Race       0.75      0.57      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.72      0.76      0.74       762\n",
      "      HS_Weak       0.68      0.67      0.67       689\n",
      "  HS_Moderate       0.53      0.51      0.52       331\n",
      "    HS_Strong       0.92      0.57      0.70       114\n",
      "\n",
      "    micro avg       0.75      0.71      0.73      5556\n",
      "    macro avg       0.70      0.55      0.58      5556\n",
      " weighted avg       0.75      0.71      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 110.5540030002594 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.8954, F1 Micro: 0.6755, F1 Macro: 0.4718\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 98.86022567749023 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4379, Accuracy: 0.8566, F1 Micro: 0.3928, F1 Macro: 0.1191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3258, Accuracy: 0.8858, F1 Micro: 0.6088, F1 Macro: 0.317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.262, Accuracy: 0.8961, F1 Micro: 0.6763, F1 Macro: 0.465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2304, Accuracy: 0.9032, F1 Micro: 0.7076, F1 Macro: 0.5193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1976, Accuracy: 0.9071, F1 Micro: 0.7109, F1 Macro: 0.5371\n",
      "Epoch 6/10, Train Loss: 0.1696, Accuracy: 0.9095, F1 Micro: 0.7098, F1 Macro: 0.5537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1414, Accuracy: 0.9121, F1 Micro: 0.7286, F1 Macro: 0.5728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1218, Accuracy: 0.9133, F1 Micro: 0.7353, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1045, Accuracy: 0.9115, F1 Micro: 0.7479, F1 Macro: 0.6002\n",
      "Epoch 10/10, Train Loss: 0.0896, Accuracy: 0.9144, F1 Micro: 0.7397, F1 Macro: 0.6115\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9115, F1 Micro: 0.7479, F1 Macro: 0.6002\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1134\n",
      "      Abusive       0.85      0.90      0.87       992\n",
      "HS_Individual       0.68      0.77      0.72       732\n",
      "     HS_Group       0.67      0.62      0.64       402\n",
      "  HS_Religion       0.73      0.51      0.60       157\n",
      "      HS_Race       0.71      0.52      0.60       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       1.00      0.04      0.08        51\n",
      "     HS_Other       0.69      0.83      0.75       762\n",
      "      HS_Weak       0.65      0.74      0.70       689\n",
      "  HS_Moderate       0.58      0.50      0.53       331\n",
      "    HS_Strong       0.86      0.73      0.79       114\n",
      "\n",
      "    micro avg       0.73      0.76      0.75      5556\n",
      "    macro avg       0.77      0.59      0.60      5556\n",
      " weighted avg       0.74      0.76      0.74      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 120.84821629524231 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4567, Accuracy: 0.8569, F1 Micro: 0.3788, F1 Macro: 0.1169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3343, Accuracy: 0.8842, F1 Micro: 0.6065, F1 Macro: 0.309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2703, Accuracy: 0.8933, F1 Micro: 0.6566, F1 Macro: 0.3907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2356, Accuracy: 0.901, F1 Micro: 0.6739, F1 Macro: 0.451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2063, Accuracy: 0.9036, F1 Micro: 0.6891, F1 Macro: 0.5086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1759, Accuracy: 0.9069, F1 Micro: 0.7093, F1 Macro: 0.5414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1497, Accuracy: 0.9112, F1 Micro: 0.7217, F1 Macro: 0.5731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1268, Accuracy: 0.9081, F1 Micro: 0.7279, F1 Macro: 0.5759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1055, Accuracy: 0.9117, F1 Micro: 0.737, F1 Macro: 0.5923\n",
      "Epoch 10/10, Train Loss: 0.0946, Accuracy: 0.9122, F1 Micro: 0.7349, F1 Macro: 0.6084\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9117, F1 Micro: 0.737, F1 Macro: 0.5923\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.69      0.70      0.70       732\n",
      "     HS_Group       0.67      0.59      0.63       402\n",
      "  HS_Religion       0.69      0.53      0.60       157\n",
      "      HS_Race       0.83      0.58      0.69       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.73      0.76      0.74       762\n",
      "      HS_Weak       0.67      0.68      0.67       689\n",
      "  HS_Moderate       0.58      0.50      0.53       331\n",
      "    HS_Strong       0.90      0.69      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.79      0.56      0.59      5556\n",
      " weighted avg       0.76      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 122.35384678840637 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4464, Accuracy: 0.8537, F1 Micro: 0.3458, F1 Macro: 0.1099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3358, Accuracy: 0.8856, F1 Micro: 0.6183, F1 Macro: 0.3243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.268, Accuracy: 0.8962, F1 Micro: 0.6779, F1 Macro: 0.428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2334, Accuracy: 0.904, F1 Micro: 0.7024, F1 Macro: 0.5013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.202, Accuracy: 0.9078, F1 Micro: 0.7144, F1 Macro: 0.5302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1732, Accuracy: 0.9076, F1 Micro: 0.7188, F1 Macro: 0.5587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1463, Accuracy: 0.9123, F1 Micro: 0.7275, F1 Macro: 0.5745\n",
      "Epoch 8/10, Train Loss: 0.1244, Accuracy: 0.9098, F1 Micro: 0.7236, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1061, Accuracy: 0.9136, F1 Micro: 0.7353, F1 Macro: 0.5849\n",
      "Epoch 10/10, Train Loss: 0.0915, Accuracy: 0.9134, F1 Micro: 0.7353, F1 Macro: 0.5968\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9136, F1 Micro: 0.7353, F1 Macro: 0.5849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.80      0.82      1134\n",
      "      Abusive       0.90      0.85      0.87       992\n",
      "HS_Individual       0.70      0.69      0.69       732\n",
      "     HS_Group       0.74      0.54      0.63       402\n",
      "  HS_Religion       0.79      0.48      0.60       157\n",
      "      HS_Race       0.85      0.46      0.59       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.73      0.75      0.74       762\n",
      "      HS_Weak       0.68      0.66      0.67       689\n",
      "  HS_Moderate       0.64      0.45      0.53       331\n",
      "    HS_Strong       0.89      0.72      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.70      0.74      5556\n",
      "    macro avg       0.81      0.54      0.58      5556\n",
      " weighted avg       0.78      0.70      0.72      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 122.16617131233215 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.8982, F1 Micro: 0.6862, F1 Macro: 0.4919\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 88.85109615325928 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4309, Accuracy: 0.8699, F1 Micro: 0.5008, F1 Macro: 0.2175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.308, Accuracy: 0.8887, F1 Micro: 0.6493, F1 Macro: 0.3878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2536, Accuracy: 0.9002, F1 Micro: 0.6865, F1 Macro: 0.443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2205, Accuracy: 0.9065, F1 Micro: 0.6979, F1 Macro: 0.492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9044, F1 Micro: 0.7255, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1581, Accuracy: 0.9107, F1 Micro: 0.7367, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1383, Accuracy: 0.9138, F1 Micro: 0.7443, F1 Macro: 0.6039\n",
      "Epoch 8/10, Train Loss: 0.1141, Accuracy: 0.9138, F1 Micro: 0.7404, F1 Macro: 0.5873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.102, Accuracy: 0.9149, F1 Micro: 0.7466, F1 Macro: 0.6237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0867, Accuracy: 0.9161, F1 Micro: 0.7571, F1 Macro: 0.63\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9161, F1 Micro: 0.7571, F1 Macro: 0.63\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.68      0.78      0.73       732\n",
      "     HS_Group       0.72      0.56      0.63       402\n",
      "  HS_Religion       0.79      0.55      0.65       157\n",
      "      HS_Race       0.73      0.61      0.66       120\n",
      "  HS_Physical       0.62      0.07      0.12        72\n",
      "    HS_Gender       0.62      0.16      0.25        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.66      0.76      0.71       689\n",
      "  HS_Moderate       0.65      0.45      0.53       331\n",
      "    HS_Strong       0.89      0.70      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.73      0.60      0.63      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 132.6270809173584 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4498, Accuracy: 0.8616, F1 Micro: 0.4179, F1 Macro: 0.1473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3188, Accuracy: 0.8867, F1 Micro: 0.6404, F1 Macro: 0.332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2628, Accuracy: 0.8951, F1 Micro: 0.6583, F1 Macro: 0.3758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2271, Accuracy: 0.901, F1 Micro: 0.6692, F1 Macro: 0.437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1979, Accuracy: 0.9025, F1 Micro: 0.7124, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1689, Accuracy: 0.9089, F1 Micro: 0.7228, F1 Macro: 0.5614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1445, Accuracy: 0.9107, F1 Micro: 0.7311, F1 Macro: 0.5988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1196, Accuracy: 0.9132, F1 Micro: 0.7373, F1 Macro: 0.5883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1058, Accuracy: 0.9148, F1 Micro: 0.7485, F1 Macro: 0.6101\n",
      "Epoch 10/10, Train Loss: 0.0941, Accuracy: 0.9089, F1 Micro: 0.7474, F1 Macro: 0.6251\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9148, F1 Micro: 0.7485, F1 Macro: 0.6101\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.87      0.88      0.88       992\n",
      "HS_Individual       0.69      0.75      0.71       732\n",
      "     HS_Group       0.71      0.56      0.63       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.70      0.69      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.77      0.76      0.76       762\n",
      "      HS_Weak       0.66      0.72      0.69       689\n",
      "  HS_Moderate       0.61      0.47      0.53       331\n",
      "    HS_Strong       0.91      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.67      0.59      0.61      5556\n",
      " weighted avg       0.75      0.74      0.74      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 132.01318192481995 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4408, Accuracy: 0.8585, F1 Micro: 0.3937, F1 Macro: 0.1302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3156, Accuracy: 0.8881, F1 Micro: 0.65, F1 Macro: 0.378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2598, Accuracy: 0.8991, F1 Micro: 0.6778, F1 Macro: 0.4144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2253, Accuracy: 0.9054, F1 Micro: 0.6972, F1 Macro: 0.4916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1955, Accuracy: 0.9066, F1 Micro: 0.72, F1 Macro: 0.5592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1651, Accuracy: 0.9113, F1 Micro: 0.7316, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1404, Accuracy: 0.9112, F1 Micro: 0.7338, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1174, Accuracy: 0.9135, F1 Micro: 0.743, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1053, Accuracy: 0.9154, F1 Micro: 0.7493, F1 Macro: 0.6165\n",
      "Epoch 10/10, Train Loss: 0.0884, Accuracy: 0.9114, F1 Micro: 0.7487, F1 Macro: 0.6338\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9154, F1 Micro: 0.7493, F1 Macro: 0.6165\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.69      0.72      0.70       732\n",
      "     HS_Group       0.71      0.59      0.65       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.69      0.72      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.08      0.15        51\n",
      "     HS_Other       0.75      0.76      0.75       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.60      0.51      0.55       331\n",
      "    HS_Strong       0.91      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5556\n",
      "    macro avg       0.79      0.59      0.62      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 132.70607447624207 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9006, F1 Micro: 0.6956, F1 Macro: 0.5101\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 80.05682754516602 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4233, Accuracy: 0.8678, F1 Micro: 0.5903, F1 Macro: 0.2705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3041, Accuracy: 0.8906, F1 Micro: 0.6719, F1 Macro: 0.4184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2547, Accuracy: 0.9032, F1 Micro: 0.6952, F1 Macro: 0.4584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2244, Accuracy: 0.9072, F1 Micro: 0.709, F1 Macro: 0.5166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1879, Accuracy: 0.9106, F1 Micro: 0.7319, F1 Macro: 0.5727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1569, Accuracy: 0.9167, F1 Micro: 0.7502, F1 Macro: 0.5952\n",
      "Epoch 7/10, Train Loss: 0.1306, Accuracy: 0.9132, F1 Micro: 0.7457, F1 Macro: 0.6056\n",
      "Epoch 8/10, Train Loss: 0.1115, Accuracy: 0.9149, F1 Micro: 0.7445, F1 Macro: 0.6085\n",
      "Epoch 9/10, Train Loss: 0.0946, Accuracy: 0.9156, F1 Micro: 0.738, F1 Macro: 0.6324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0824, Accuracy: 0.9183, F1 Micro: 0.7533, F1 Macro: 0.6351\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9183, F1 Micro: 0.7533, F1 Macro: 0.6351\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.67      0.70       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.74      0.57      0.65       157\n",
      "      HS_Race       0.84      0.62      0.72       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.77      0.73      0.75       762\n",
      "      HS_Weak       0.71      0.64      0.68       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.91      0.74      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5556\n",
      "    macro avg       0.77      0.59      0.64      5556\n",
      " weighted avg       0.78      0.72      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 137.55107998847961 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4413, Accuracy: 0.8657, F1 Micro: 0.4988, F1 Macro: 0.1985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3144, Accuracy: 0.8885, F1 Micro: 0.6556, F1 Macro: 0.3536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2616, Accuracy: 0.8997, F1 Micro: 0.6751, F1 Macro: 0.426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2304, Accuracy: 0.9045, F1 Micro: 0.7024, F1 Macro: 0.508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.195, Accuracy: 0.9102, F1 Micro: 0.7281, F1 Macro: 0.5591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1677, Accuracy: 0.9114, F1 Micro: 0.7406, F1 Macro: 0.5728\n",
      "Epoch 7/10, Train Loss: 0.1381, Accuracy: 0.9113, F1 Micro: 0.7387, F1 Macro: 0.5747\n",
      "Epoch 8/10, Train Loss: 0.1209, Accuracy: 0.9119, F1 Micro: 0.7405, F1 Macro: 0.5937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1022, Accuracy: 0.9143, F1 Micro: 0.7429, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0894, Accuracy: 0.9156, F1 Micro: 0.7524, F1 Macro: 0.6111\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9156, F1 Micro: 0.7524, F1 Macro: 0.6111\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.69      0.75      0.72       732\n",
      "     HS_Group       0.71      0.54      0.61       402\n",
      "  HS_Religion       0.75      0.56      0.64       157\n",
      "      HS_Race       0.87      0.56      0.68       120\n",
      "  HS_Physical       0.33      0.01      0.03        72\n",
      "    HS_Gender       0.62      0.10      0.17        51\n",
      "     HS_Other       0.74      0.79      0.77       762\n",
      "      HS_Weak       0.66      0.73      0.69       689\n",
      "  HS_Moderate       0.61      0.44      0.51       331\n",
      "    HS_Strong       0.91      0.69      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.71      0.58      0.61      5556\n",
      " weighted avg       0.75      0.74      0.74      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 139.8858985900879 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4338, Accuracy: 0.8614, F1 Micro: 0.4693, F1 Macro: 0.1743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3133, Accuracy: 0.8894, F1 Micro: 0.6583, F1 Macro: 0.3752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2595, Accuracy: 0.9024, F1 Micro: 0.6909, F1 Macro: 0.4558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2307, Accuracy: 0.9056, F1 Micro: 0.6979, F1 Macro: 0.49\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1932, Accuracy: 0.9086, F1 Micro: 0.7227, F1 Macro: 0.5403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1672, Accuracy: 0.9138, F1 Micro: 0.747, F1 Macro: 0.5879\n",
      "Epoch 7/10, Train Loss: 0.1349, Accuracy: 0.9147, F1 Micro: 0.7437, F1 Macro: 0.5915\n",
      "Epoch 8/10, Train Loss: 0.12, Accuracy: 0.9058, F1 Micro: 0.7416, F1 Macro: 0.5892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.102, Accuracy: 0.9169, F1 Micro: 0.7506, F1 Macro: 0.6465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0877, Accuracy: 0.9167, F1 Micro: 0.751, F1 Macro: 0.6136\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9167, F1 Micro: 0.751, F1 Macro: 0.6136\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.74      0.53      0.62       402\n",
      "  HS_Religion       0.77      0.54      0.63       157\n",
      "      HS_Race       0.85      0.57      0.69       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.75      0.78      0.77       762\n",
      "      HS_Weak       0.66      0.71      0.68       689\n",
      "  HS_Moderate       0.64      0.44      0.52       331\n",
      "    HS_Strong       0.91      0.71      0.80       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.76      0.57      0.61      5556\n",
      " weighted avg       0.77      0.73      0.74      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 139.3175094127655 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9027, F1 Micro: 0.7027, F1 Macro: 0.5238\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 72.78702044487 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4138, Accuracy: 0.8702, F1 Micro: 0.4654, F1 Macro: 0.2098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2955, Accuracy: 0.895, F1 Micro: 0.6818, F1 Macro: 0.4313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2446, Accuracy: 0.9056, F1 Micro: 0.6965, F1 Macro: 0.4933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2037, Accuracy: 0.9071, F1 Micro: 0.7319, F1 Macro: 0.5582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9099, F1 Micro: 0.7384, F1 Macro: 0.5792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1449, Accuracy: 0.915, F1 Micro: 0.7493, F1 Macro: 0.5988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1286, Accuracy: 0.9156, F1 Micro: 0.7514, F1 Macro: 0.6215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1101, Accuracy: 0.9187, F1 Micro: 0.7591, F1 Macro: 0.6294\n",
      "Epoch 9/10, Train Loss: 0.0923, Accuracy: 0.9185, F1 Micro: 0.7489, F1 Macro: 0.64\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9186, F1 Micro: 0.7572, F1 Macro: 0.6527\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9187, F1 Micro: 0.7591, F1 Macro: 0.6294\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.81      0.50      0.62       157\n",
      "      HS_Race       0.81      0.66      0.72       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.74      0.77      0.75       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.75      0.60      0.63      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 148.34750986099243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4335, Accuracy: 0.8679, F1 Micro: 0.4418, F1 Macro: 0.1804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3068, Accuracy: 0.8906, F1 Micro: 0.6628, F1 Macro: 0.3652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2507, Accuracy: 0.902, F1 Micro: 0.6707, F1 Macro: 0.4524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2126, Accuracy: 0.906, F1 Micro: 0.727, F1 Macro: 0.5471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1835, Accuracy: 0.9108, F1 Micro: 0.7289, F1 Macro: 0.551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1522, Accuracy: 0.9134, F1 Micro: 0.7396, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1351, Accuracy: 0.9138, F1 Micro: 0.7448, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1156, Accuracy: 0.9155, F1 Micro: 0.7484, F1 Macro: 0.604\n",
      "Epoch 9/10, Train Loss: 0.0974, Accuracy: 0.9175, F1 Micro: 0.7471, F1 Macro: 0.6423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0846, Accuracy: 0.9188, F1 Micro: 0.7545, F1 Macro: 0.6349\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9188, F1 Micro: 0.7545, F1 Macro: 0.6349\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.81      0.84      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.72      0.69      0.71       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.80      0.64      0.71       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.69      0.18      0.28        51\n",
      "     HS_Other       0.76      0.76      0.76       762\n",
      "      HS_Weak       0.71      0.67      0.69       689\n",
      "  HS_Moderate       0.62      0.52      0.57       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5556\n",
      "    macro avg       0.74      0.59      0.63      5556\n",
      " weighted avg       0.78      0.72      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 149.9811234474182 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4253, Accuracy: 0.8658, F1 Micro: 0.4249, F1 Macro: 0.1686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3024, Accuracy: 0.8917, F1 Micro: 0.6779, F1 Macro: 0.392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.9037, F1 Micro: 0.6904, F1 Macro: 0.4662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2081, Accuracy: 0.9038, F1 Micro: 0.7297, F1 Macro: 0.5468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.183, Accuracy: 0.9124, F1 Micro: 0.7408, F1 Macro: 0.5742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1489, Accuracy: 0.9148, F1 Micro: 0.7482, F1 Macro: 0.5906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1309, Accuracy: 0.917, F1 Micro: 0.7522, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.9162, F1 Micro: 0.754, F1 Macro: 0.6179\n",
      "Epoch 9/10, Train Loss: 0.0932, Accuracy: 0.9183, F1 Micro: 0.75, F1 Macro: 0.6431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0842, Accuracy: 0.9167, F1 Micro: 0.7543, F1 Macro: 0.6417\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9167, F1 Micro: 0.7543, F1 Macro: 0.6417\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.71      0.59      0.64       402\n",
      "  HS_Religion       0.80      0.62      0.70       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.43      0.04      0.08        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.74      0.77      0.76       762\n",
      "      HS_Weak       0.67      0.71      0.69       689\n",
      "  HS_Moderate       0.60      0.51      0.55       331\n",
      "    HS_Strong       0.89      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.72      0.61      0.64      5556\n",
      " weighted avg       0.76      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 149.66186046600342 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9044, F1 Micro: 0.7086, F1 Macro: 0.5362\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 65.73571562767029 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4112, Accuracy: 0.8761, F1 Micro: 0.5704, F1 Macro: 0.2687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2947, Accuracy: 0.896, F1 Micro: 0.6522, F1 Macro: 0.4165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2419, Accuracy: 0.9058, F1 Micro: 0.6931, F1 Macro: 0.4767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2042, Accuracy: 0.9097, F1 Micro: 0.7381, F1 Macro: 0.5654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9129, F1 Micro: 0.7505, F1 Macro: 0.5937\n",
      "Epoch 6/10, Train Loss: 0.149, Accuracy: 0.9146, F1 Micro: 0.7457, F1 Macro: 0.6002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1253, Accuracy: 0.9187, F1 Micro: 0.754, F1 Macro: 0.6281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1101, Accuracy: 0.9167, F1 Micro: 0.7611, F1 Macro: 0.6513\n",
      "Epoch 9/10, Train Loss: 0.0907, Accuracy: 0.9175, F1 Micro: 0.7601, F1 Macro: 0.6372\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.9183, F1 Micro: 0.7552, F1 Macro: 0.6454\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9167, F1 Micro: 0.7611, F1 Macro: 0.6513\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.65      0.70      0.68       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.67      0.08      0.15        72\n",
      "    HS_Gender       0.59      0.20      0.29        51\n",
      "     HS_Other       0.74      0.79      0.77       762\n",
      "      HS_Weak       0.69      0.70      0.70       689\n",
      "  HS_Moderate       0.57      0.61      0.59       331\n",
      "    HS_Strong       0.89      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.72      0.64      0.65      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 151.24266624450684 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4282, Accuracy: 0.8739, F1 Micro: 0.5577, F1 Macro: 0.2474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3039, Accuracy: 0.8934, F1 Micro: 0.6437, F1 Macro: 0.3575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2488, Accuracy: 0.9024, F1 Micro: 0.6938, F1 Macro: 0.4717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2102, Accuracy: 0.9091, F1 Micro: 0.7266, F1 Macro: 0.5362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1759, Accuracy: 0.913, F1 Micro: 0.7406, F1 Macro: 0.574\n",
      "Epoch 6/10, Train Loss: 0.1539, Accuracy: 0.9097, F1 Micro: 0.7349, F1 Macro: 0.5796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1302, Accuracy: 0.9169, F1 Micro: 0.741, F1 Macro: 0.6082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1124, Accuracy: 0.9163, F1 Micro: 0.7522, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0952, Accuracy: 0.9171, F1 Micro: 0.7587, F1 Macro: 0.6258\n",
      "Epoch 10/10, Train Loss: 0.0829, Accuracy: 0.9175, F1 Micro: 0.7499, F1 Macro: 0.6284\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9171, F1 Micro: 0.7587, F1 Macro: 0.6258\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.71      0.61      0.66       402\n",
      "  HS_Religion       0.72      0.57      0.63       157\n",
      "      HS_Race       0.82      0.60      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.64      0.14      0.23        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.67      0.72      0.69       689\n",
      "  HS_Moderate       0.62      0.53      0.57       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.68      0.60      0.63      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 152.3388271331787 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4216, Accuracy: 0.8695, F1 Micro: 0.4704, F1 Macro: 0.1963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3, Accuracy: 0.8931, F1 Micro: 0.6258, F1 Macro: 0.3709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2463, Accuracy: 0.9038, F1 Micro: 0.6787, F1 Macro: 0.4656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2072, Accuracy: 0.9101, F1 Micro: 0.7397, F1 Macro: 0.5634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1746, Accuracy: 0.9147, F1 Micro: 0.7514, F1 Macro: 0.5871\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.913, F1 Micro: 0.7395, F1 Macro: 0.5931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1265, Accuracy: 0.9166, F1 Micro: 0.7529, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1117, Accuracy: 0.9132, F1 Micro: 0.755, F1 Macro: 0.6411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0921, Accuracy: 0.9153, F1 Micro: 0.7599, F1 Macro: 0.6375\n",
      "Epoch 10/10, Train Loss: 0.0791, Accuracy: 0.919, F1 Micro: 0.7574, F1 Macro: 0.6588\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9153, F1 Micro: 0.7599, F1 Macro: 0.6375\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.72      0.57      0.63       157\n",
      "      HS_Race       0.76      0.66      0.71       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.64      0.18      0.28        51\n",
      "     HS_Other       0.70      0.83      0.76       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.57      0.60      0.58       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5556\n",
      "    macro avg       0.75      0.63      0.64      5556\n",
      " weighted avg       0.75      0.78      0.75      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 152.67799353599548 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9056, F1 Micro: 0.7137, F1 Macro: 0.5464\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 61.528443574905396 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4055, Accuracy: 0.8787, F1 Micro: 0.5934, F1 Macro: 0.2846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2841, Accuracy: 0.8956, F1 Micro: 0.6924, F1 Macro: 0.4651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2399, Accuracy: 0.9073, F1 Micro: 0.7184, F1 Macro: 0.5235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.201, Accuracy: 0.9112, F1 Micro: 0.7193, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1723, Accuracy: 0.9147, F1 Micro: 0.7476, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1379, Accuracy: 0.9174, F1 Micro: 0.7595, F1 Macro: 0.6221\n",
      "Epoch 7/10, Train Loss: 0.1206, Accuracy: 0.9146, F1 Micro: 0.7519, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1046, Accuracy: 0.9169, F1 Micro: 0.7604, F1 Macro: 0.6371\n",
      "Epoch 9/10, Train Loss: 0.0878, Accuracy: 0.9148, F1 Micro: 0.7598, F1 Macro: 0.6542\n",
      "Epoch 10/10, Train Loss: 0.0804, Accuracy: 0.9142, F1 Micro: 0.752, F1 Macro: 0.6493\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9169, F1 Micro: 0.7604, F1 Macro: 0.6371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.76      0.57      0.65       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.81      0.60      0.69       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.73      0.80      0.77       762\n",
      "      HS_Weak       0.64      0.76      0.70       689\n",
      "  HS_Moderate       0.66      0.49      0.56       331\n",
      "    HS_Strong       0.89      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.61      0.64      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 157.0617334842682 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4233, Accuracy: 0.8756, F1 Micro: 0.5697, F1 Macro: 0.2571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2954, Accuracy: 0.8937, F1 Micro: 0.6631, F1 Macro: 0.3822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2477, Accuracy: 0.9046, F1 Micro: 0.7012, F1 Macro: 0.501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2077, Accuracy: 0.9088, F1 Micro: 0.7129, F1 Macro: 0.5121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1794, Accuracy: 0.9111, F1 Micro: 0.7387, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1423, Accuracy: 0.9151, F1 Micro: 0.7464, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.9148, F1 Micro: 0.7546, F1 Macro: 0.616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1099, Accuracy: 0.9147, F1 Micro: 0.757, F1 Macro: 0.6211\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9178, F1 Micro: 0.7511, F1 Macro: 0.6386\n",
      "Epoch 10/10, Train Loss: 0.0837, Accuracy: 0.9183, F1 Micro: 0.755, F1 Macro: 0.6474\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9147, F1 Micro: 0.757, F1 Macro: 0.6211\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.66      0.80      0.72       732\n",
      "     HS_Group       0.73      0.58      0.65       402\n",
      "  HS_Religion       0.66      0.64      0.65       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.74      0.79      0.77       762\n",
      "      HS_Weak       0.63      0.78      0.70       689\n",
      "  HS_Moderate       0.63      0.47      0.54       331\n",
      "    HS_Strong       0.88      0.72      0.79       114\n",
      "\n",
      "    micro avg       0.74      0.77      0.76      5556\n",
      "    macro avg       0.67      0.61      0.62      5556\n",
      " weighted avg       0.73      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 158.42441487312317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4156, Accuracy: 0.8752, F1 Micro: 0.5969, F1 Macro: 0.283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2895, Accuracy: 0.895, F1 Micro: 0.6856, F1 Macro: 0.4479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2442, Accuracy: 0.9059, F1 Micro: 0.704, F1 Macro: 0.5103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2044, Accuracy: 0.9124, F1 Micro: 0.7263, F1 Macro: 0.5492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.9174, F1 Micro: 0.7559, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.9175, F1 Micro: 0.7587, F1 Macro: 0.6164\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.9164, F1 Micro: 0.7523, F1 Macro: 0.6074\n",
      "Epoch 8/10, Train Loss: 0.1081, Accuracy: 0.9175, F1 Micro: 0.7557, F1 Macro: 0.6256\n",
      "Epoch 9/10, Train Loss: 0.09, Accuracy: 0.9124, F1 Micro: 0.7503, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0821, Accuracy: 0.9196, F1 Micro: 0.7591, F1 Macro: 0.6519\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9196, F1 Micro: 0.7591, F1 Macro: 0.6519\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.82      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.77      0.55      0.64       402\n",
      "  HS_Religion       0.82      0.55      0.66       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.56      0.27      0.37        51\n",
      "     HS_Other       0.77      0.76      0.76       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.68      0.46      0.55       331\n",
      "    HS_Strong       0.91      0.70      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.76      0.60      0.65      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 156.80159759521484 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9066, F1 Micro: 0.7178, F1 Macro: 0.5546\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 55.77565336227417 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4003, Accuracy: 0.8792, F1 Micro: 0.5635, F1 Macro: 0.2692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2853, Accuracy: 0.9006, F1 Micro: 0.6747, F1 Macro: 0.458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2333, Accuracy: 0.905, F1 Micro: 0.7268, F1 Macro: 0.5481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1999, Accuracy: 0.914, F1 Micro: 0.741, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.9124, F1 Micro: 0.7545, F1 Macro: 0.604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9213, F1 Micro: 0.7585, F1 Macro: 0.6263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.92, F1 Micro: 0.768, F1 Macro: 0.658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9205, F1 Micro: 0.7684, F1 Macro: 0.6691\n",
      "Epoch 9/10, Train Loss: 0.0886, Accuracy: 0.9143, F1 Micro: 0.7624, F1 Macro: 0.6517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9215, F1 Micro: 0.7703, F1 Macro: 0.673\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9215, F1 Micro: 0.7703, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.71      0.64      0.68       402\n",
      "  HS_Religion       0.69      0.63      0.66       157\n",
      "      HS_Race       0.81      0.67      0.73       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 166.19683027267456 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4177, Accuracy: 0.8732, F1 Micro: 0.4919, F1 Macro: 0.2141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.8977, F1 Micro: 0.6679, F1 Macro: 0.4321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2416, Accuracy: 0.9043, F1 Micro: 0.7221, F1 Macro: 0.5405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2067, Accuracy: 0.9085, F1 Micro: 0.7304, F1 Macro: 0.571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1666, Accuracy: 0.9128, F1 Micro: 0.7459, F1 Macro: 0.6\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9196, F1 Micro: 0.7559, F1 Macro: 0.6176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9209, F1 Micro: 0.7612, F1 Macro: 0.642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1035, Accuracy: 0.9154, F1 Micro: 0.7629, F1 Macro: 0.6528\n",
      "Epoch 9/10, Train Loss: 0.091, Accuracy: 0.9191, F1 Micro: 0.7497, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0808, Accuracy: 0.921, F1 Micro: 0.7654, F1 Macro: 0.6626\n",
      "Model 2 - Iteration 7336: Accuracy: 0.921, F1 Micro: 0.7654, F1 Macro: 0.6626\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.68      0.60      0.64       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       0.62      0.11      0.19        72\n",
      "    HS_Gender       0.59      0.31      0.41        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 167.0215723514557 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4103, Accuracy: 0.8745, F1 Micro: 0.4981, F1 Macro: 0.2258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2888, Accuracy: 0.9004, F1 Micro: 0.674, F1 Macro: 0.4454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2358, Accuracy: 0.9053, F1 Micro: 0.7263, F1 Macro: 0.5381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2024, Accuracy: 0.915, F1 Micro: 0.7376, F1 Macro: 0.5793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1651, Accuracy: 0.9146, F1 Micro: 0.7605, F1 Macro: 0.6146\n",
      "Epoch 6/10, Train Loss: 0.1468, Accuracy: 0.9195, F1 Micro: 0.7499, F1 Macro: 0.62\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1197, Accuracy: 0.9179, F1 Micro: 0.7661, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.9191, F1 Micro: 0.7676, F1 Macro: 0.6583\n",
      "Epoch 9/10, Train Loss: 0.0874, Accuracy: 0.922, F1 Micro: 0.7665, F1 Macro: 0.6559\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9194, F1 Micro: 0.7636, F1 Macro: 0.6645\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9191, F1 Micro: 0.7676, F1 Macro: 0.6583\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       0.54      0.10      0.16        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.60      0.57      0.59       331\n",
      "    HS_Strong       0.89      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.72      0.64      0.66      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.47119188308716 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9078, F1 Micro: 0.722, F1 Macro: 0.5638\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 50.67244291305542 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3985, Accuracy: 0.8811, F1 Micro: 0.5983, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2763, Accuracy: 0.9008, F1 Micro: 0.6978, F1 Macro: 0.4587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.9118, F1 Micro: 0.7199, F1 Macro: 0.5302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.9156, F1 Micro: 0.7508, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1659, Accuracy: 0.9141, F1 Micro: 0.7606, F1 Macro: 0.6174\n",
      "Epoch 6/10, Train Loss: 0.1403, Accuracy: 0.9197, F1 Micro: 0.76, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9213, F1 Micro: 0.7614, F1 Macro: 0.6541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0998, Accuracy: 0.9223, F1 Micro: 0.7672, F1 Macro: 0.6608\n",
      "Epoch 9/10, Train Loss: 0.0858, Accuracy: 0.92, F1 Micro: 0.7629, F1 Macro: 0.6584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9215, F1 Micro: 0.7731, F1 Macro: 0.6788\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9215, F1 Micro: 0.7731, F1 Macro: 0.6788\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.71      0.59      0.65       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 169.93457198143005 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4178, Accuracy: 0.8776, F1 Micro: 0.5578, F1 Macro: 0.2541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2883, Accuracy: 0.8964, F1 Micro: 0.6697, F1 Macro: 0.3902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2455, Accuracy: 0.9088, F1 Micro: 0.7022, F1 Macro: 0.5105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2058, Accuracy: 0.9137, F1 Micro: 0.7381, F1 Macro: 0.5635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1719, Accuracy: 0.9146, F1 Micro: 0.7566, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1486, Accuracy: 0.9194, F1 Micro: 0.758, F1 Macro: 0.623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1185, Accuracy: 0.9215, F1 Micro: 0.769, F1 Macro: 0.6597\n",
      "Epoch 8/10, Train Loss: 0.1058, Accuracy: 0.9211, F1 Micro: 0.764, F1 Macro: 0.662\n",
      "Epoch 9/10, Train Loss: 0.0892, Accuracy: 0.9165, F1 Micro: 0.7544, F1 Macro: 0.6358\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9225, F1 Micro: 0.7633, F1 Macro: 0.6603\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9215, F1 Micro: 0.769, F1 Macro: 0.6597\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.76      0.59      0.67       402\n",
      "  HS_Religion       0.76      0.56      0.64       157\n",
      "      HS_Race       0.88      0.60      0.71       120\n",
      "  HS_Physical       0.45      0.07      0.12        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.66      0.48      0.55       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.74      0.62      0.66      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 170.5206708908081 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.408, Accuracy: 0.8795, F1 Micro: 0.6112, F1 Macro: 0.2908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2825, Accuracy: 0.8989, F1 Micro: 0.695, F1 Macro: 0.4582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2396, Accuracy: 0.9099, F1 Micro: 0.7143, F1 Macro: 0.511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2046, Accuracy: 0.9164, F1 Micro: 0.747, F1 Macro: 0.5752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1712, Accuracy: 0.9132, F1 Micro: 0.758, F1 Macro: 0.6132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1469, Accuracy: 0.9217, F1 Micro: 0.7653, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1173, Accuracy: 0.9206, F1 Micro: 0.7688, F1 Macro: 0.647\n",
      "Epoch 8/10, Train Loss: 0.1036, Accuracy: 0.9202, F1 Micro: 0.7672, F1 Macro: 0.6623\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.9203, F1 Micro: 0.7543, F1 Macro: 0.6426\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9214, F1 Micro: 0.7618, F1 Macro: 0.6652\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9206, F1 Micro: 0.7688, F1 Macro: 0.647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.68      0.80      0.74       732\n",
      "     HS_Group       0.79      0.53      0.64       402\n",
      "  HS_Religion       0.75      0.58      0.65       157\n",
      "      HS_Race       0.81      0.59      0.68       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.62      0.25      0.36        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.67      0.79      0.72       689\n",
      "  HS_Moderate       0.71      0.44      0.54       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.78      0.62      0.65      5556\n",
      " weighted avg       0.78      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 169.9705650806427 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9088, F1 Micro: 0.7257, F1 Macro: 0.5713\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 45.40058183670044 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.8804, F1 Micro: 0.5852, F1 Macro: 0.2849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2761, Accuracy: 0.8975, F1 Micro: 0.6933, F1 Macro: 0.471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2338, Accuracy: 0.9082, F1 Micro: 0.727, F1 Macro: 0.5745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1995, Accuracy: 0.9189, F1 Micro: 0.7489, F1 Macro: 0.5893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9173, F1 Micro: 0.7591, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1353, Accuracy: 0.9222, F1 Micro: 0.761, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.9154, F1 Micro: 0.7654, F1 Macro: 0.661\n",
      "Epoch 8/10, Train Loss: 0.096, Accuracy: 0.9123, F1 Micro: 0.7549, F1 Macro: 0.6557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0886, Accuracy: 0.9215, F1 Micro: 0.7699, F1 Macro: 0.6763\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.9187, F1 Micro: 0.7667, F1 Macro: 0.6803\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9215, F1 Micro: 0.7699, F1 Macro: 0.6763\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.80      0.51      0.62       157\n",
      "      HS_Race       0.80      0.63      0.71       120\n",
      "  HS_Physical       0.63      0.17      0.26        72\n",
      "    HS_Gender       0.55      0.41      0.47        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.74      0.64      0.68      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 174.8463840484619 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4161, Accuracy: 0.8784, F1 Micro: 0.5441, F1 Macro: 0.2507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2846, Accuracy: 0.8971, F1 Micro: 0.6845, F1 Macro: 0.4305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2397, Accuracy: 0.9066, F1 Micro: 0.7148, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2051, Accuracy: 0.9149, F1 Micro: 0.7378, F1 Macro: 0.5688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1714, Accuracy: 0.9184, F1 Micro: 0.7567, F1 Macro: 0.6065\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.9203, F1 Micro: 0.7553, F1 Macro: 0.609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9178, F1 Micro: 0.7629, F1 Macro: 0.6392\n",
      "Epoch 8/10, Train Loss: 0.0999, Accuracy: 0.9189, F1 Micro: 0.7514, F1 Macro: 0.6391\n",
      "Epoch 9/10, Train Loss: 0.0893, Accuracy: 0.9197, F1 Micro: 0.7572, F1 Macro: 0.6508\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9192, F1 Micro: 0.7613, F1 Macro: 0.6699\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9178, F1 Micro: 0.7629, F1 Macro: 0.6392\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.69      0.78      0.74       732\n",
      "     HS_Group       0.71      0.58      0.64       402\n",
      "  HS_Religion       0.71      0.58      0.64       157\n",
      "      HS_Race       0.76      0.62      0.68       120\n",
      "  HS_Physical       0.38      0.04      0.07        72\n",
      "    HS_Gender       0.61      0.22      0.32        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.66      0.76      0.71       689\n",
      "  HS_Moderate       0.63      0.49      0.55       331\n",
      "    HS_Strong       0.91      0.74      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.71      0.62      0.64      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 171.7376890182495 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4104, Accuracy: 0.8803, F1 Micro: 0.5912, F1 Macro: 0.2915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2808, Accuracy: 0.8986, F1 Micro: 0.6928, F1 Macro: 0.479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2362, Accuracy: 0.9089, F1 Micro: 0.7213, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.9162, F1 Micro: 0.7446, F1 Macro: 0.5824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1645, Accuracy: 0.9213, F1 Micro: 0.7597, F1 Macro: 0.6132\n",
      "Epoch 6/10, Train Loss: 0.1384, Accuracy: 0.921, F1 Micro: 0.7558, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9209, F1 Micro: 0.7654, F1 Macro: 0.6525\n",
      "Epoch 8/10, Train Loss: 0.096, Accuracy: 0.9164, F1 Micro: 0.7603, F1 Macro: 0.6516\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9204, F1 Micro: 0.7647, F1 Macro: 0.6668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0726, Accuracy: 0.9213, F1 Micro: 0.7673, F1 Macro: 0.6832\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9213, F1 Micro: 0.7673, F1 Macro: 0.6832\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.62      0.21      0.31        72\n",
      "    HS_Gender       0.57      0.51      0.54        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.91      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.74      0.65      0.68      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 172.67458987236023 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9096, F1 Micro: 0.7286, F1 Macro: 0.5781\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 41.49314832687378 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3929, Accuracy: 0.8819, F1 Micro: 0.5952, F1 Macro: 0.3038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2768, Accuracy: 0.8965, F1 Micro: 0.7021, F1 Macro: 0.4993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.232, Accuracy: 0.9115, F1 Micro: 0.7244, F1 Macro: 0.545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9159, F1 Micro: 0.7506, F1 Macro: 0.5866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1613, Accuracy: 0.9201, F1 Micro: 0.7539, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1343, Accuracy: 0.9194, F1 Micro: 0.767, F1 Macro: 0.6524\n",
      "Epoch 7/10, Train Loss: 0.1134, Accuracy: 0.9168, F1 Micro: 0.7573, F1 Macro: 0.6418\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9197, F1 Micro: 0.7549, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0852, Accuracy: 0.9242, F1 Micro: 0.7711, F1 Macro: 0.6695\n",
      "Epoch 10/10, Train Loss: 0.0732, Accuracy: 0.9212, F1 Micro: 0.7665, F1 Macro: 0.6664\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9242, F1 Micro: 0.7711, F1 Macro: 0.6695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.74      0.58      0.65       157\n",
      "      HS_Race       0.81      0.61      0.70       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.71      0.29      0.42        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.79      0.62      0.67      5556\n",
      " weighted avg       0.80      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 176.567400932312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4092, Accuracy: 0.8794, F1 Micro: 0.5662, F1 Macro: 0.2653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2831, Accuracy: 0.8971, F1 Micro: 0.6938, F1 Macro: 0.4673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2381, Accuracy: 0.9091, F1 Micro: 0.7224, F1 Macro: 0.532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.9135, F1 Micro: 0.749, F1 Macro: 0.5791\n",
      "Epoch 5/10, Train Loss: 0.1651, Accuracy: 0.919, F1 Micro: 0.7451, F1 Macro: 0.5893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1381, Accuracy: 0.9189, F1 Micro: 0.7632, F1 Macro: 0.6529\n",
      "Epoch 7/10, Train Loss: 0.1195, Accuracy: 0.9181, F1 Micro: 0.7533, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.9191, F1 Micro: 0.7658, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9176, F1 Micro: 0.7659, F1 Macro: 0.6629\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9176, F1 Micro: 0.7607, F1 Macro: 0.6638\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9176, F1 Micro: 0.7659, F1 Macro: 0.6629\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.65      0.64      0.64       157\n",
      "      HS_Race       0.76      0.68      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.74      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.59      0.55      0.57       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5556\n",
      "    macro avg       0.73      0.66      0.66      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 177.23666954040527 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4031, Accuracy: 0.8814, F1 Micro: 0.5979, F1 Macro: 0.3004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2815, Accuracy: 0.8957, F1 Micro: 0.7038, F1 Macro: 0.4966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2355, Accuracy: 0.9124, F1 Micro: 0.7299, F1 Macro: 0.5419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1938, Accuracy: 0.9154, F1 Micro: 0.7442, F1 Macro: 0.5724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.167, Accuracy: 0.9191, F1 Micro: 0.7543, F1 Macro: 0.6002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.136, Accuracy: 0.9204, F1 Micro: 0.7697, F1 Macro: 0.6652\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9201, F1 Micro: 0.7645, F1 Macro: 0.6395\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9191, F1 Micro: 0.7592, F1 Macro: 0.6378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9196, F1 Micro: 0.7707, F1 Macro: 0.6788\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.9221, F1 Micro: 0.7673, F1 Macro: 0.669\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9196, F1 Micro: 0.7707, F1 Macro: 0.6788\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.74      0.67      0.70       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 175.86404514312744 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9104, F1 Micro: 0.7313, F1 Macro: 0.5843\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 37.330907583236694 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3951, Accuracy: 0.8819, F1 Micro: 0.6065, F1 Macro: 0.2999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2718, Accuracy: 0.9034, F1 Micro: 0.7096, F1 Macro: 0.5226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2273, Accuracy: 0.9143, F1 Micro: 0.7325, F1 Macro: 0.5402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1958, Accuracy: 0.9201, F1 Micro: 0.7562, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.9184, F1 Micro: 0.7655, F1 Macro: 0.6205\n",
      "Epoch 6/10, Train Loss: 0.1389, Accuracy: 0.921, F1 Micro: 0.7614, F1 Macro: 0.6417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1141, Accuracy: 0.9251, F1 Micro: 0.7721, F1 Macro: 0.672\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9223, F1 Micro: 0.7711, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.9226, F1 Micro: 0.7753, F1 Macro: 0.6862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9242, F1 Micro: 0.7768, F1 Macro: 0.6871\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9242, F1 Micro: 0.7768, F1 Macro: 0.6871\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.77      0.52      0.62       157\n",
      "      HS_Race       0.79      0.72      0.76       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 182.4723982810974 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4093, Accuracy: 0.8802, F1 Micro: 0.6196, F1 Macro: 0.2956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2806, Accuracy: 0.9009, F1 Micro: 0.6872, F1 Macro: 0.4747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9096, F1 Micro: 0.7302, F1 Macro: 0.5359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.202, Accuracy: 0.9173, F1 Micro: 0.7508, F1 Macro: 0.5731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1646, Accuracy: 0.9152, F1 Micro: 0.7574, F1 Macro: 0.603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1409, Accuracy: 0.9202, F1 Micro: 0.7587, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1192, Accuracy: 0.9223, F1 Micro: 0.762, F1 Macro: 0.6429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1007, Accuracy: 0.9192, F1 Micro: 0.7643, F1 Macro: 0.667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.9226, F1 Micro: 0.7649, F1 Macro: 0.6716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9195, F1 Micro: 0.7702, F1 Macro: 0.6732\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9195, F1 Micro: 0.7702, F1 Macro: 0.6732\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.66      0.63      0.64       402\n",
      "  HS_Religion       0.76      0.63      0.69       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.67      0.27      0.39        51\n",
      "     HS_Other       0.75      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.57      0.55      0.56       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 185.32067108154297 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4029, Accuracy: 0.8808, F1 Micro: 0.6332, F1 Macro: 0.3259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.276, Accuracy: 0.9027, F1 Micro: 0.7077, F1 Macro: 0.514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9115, F1 Micro: 0.7138, F1 Macro: 0.5314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1979, Accuracy: 0.9168, F1 Micro: 0.7453, F1 Macro: 0.5871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1618, Accuracy: 0.9179, F1 Micro: 0.7641, F1 Macro: 0.6152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9218, F1 Micro: 0.7658, F1 Macro: 0.6457\n",
      "Epoch 7/10, Train Loss: 0.1158, Accuracy: 0.9235, F1 Micro: 0.76, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0991, Accuracy: 0.9219, F1 Micro: 0.7737, F1 Macro: 0.6801\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9225, F1 Micro: 0.7701, F1 Macro: 0.6977\n",
      "Epoch 10/10, Train Loss: 0.0705, Accuracy: 0.9186, F1 Micro: 0.7697, F1 Macro: 0.6871\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9219, F1 Micro: 0.7737, F1 Macro: 0.6801\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.79      0.60      0.68       157\n",
      "      HS_Race       0.74      0.68      0.70       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.58      0.60      0.59       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 180.3209035396576 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9111, F1 Micro: 0.734, F1 Macro: 0.5903\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 34.17066931724548 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.392, Accuracy: 0.8809, F1 Micro: 0.5489, F1 Macro: 0.2674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2651, Accuracy: 0.9065, F1 Micro: 0.6982, F1 Macro: 0.4723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2203, Accuracy: 0.9156, F1 Micro: 0.7458, F1 Macro: 0.5708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1859, Accuracy: 0.9187, F1 Micro: 0.761, F1 Macro: 0.6012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9223, F1 Micro: 0.7654, F1 Macro: 0.6349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1335, Accuracy: 0.9214, F1 Micro: 0.7726, F1 Macro: 0.6612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9232, F1 Micro: 0.7781, F1 Macro: 0.6729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0958, Accuracy: 0.9238, F1 Micro: 0.78, F1 Macro: 0.6912\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9202, F1 Micro: 0.7719, F1 Macro: 0.6772\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9246, F1 Micro: 0.7769, F1 Macro: 0.6853\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9238, F1 Micro: 0.78, F1 Macro: 0.6912\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.75      0.63      0.69       402\n",
      "  HS_Religion       0.75      0.64      0.69       157\n",
      "      HS_Race       0.79      0.71      0.75       120\n",
      "  HS_Physical       0.67      0.17      0.27        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.89      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 186.4212772846222 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4062, Accuracy: 0.8833, F1 Micro: 0.6002, F1 Macro: 0.2922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2713, Accuracy: 0.8991, F1 Micro: 0.6634, F1 Macro: 0.4087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2274, Accuracy: 0.9113, F1 Micro: 0.7105, F1 Macro: 0.5255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1937, Accuracy: 0.915, F1 Micro: 0.745, F1 Macro: 0.5846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1622, Accuracy: 0.9174, F1 Micro: 0.7583, F1 Macro: 0.6304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.9217, F1 Micro: 0.7651, F1 Macro: 0.6388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1174, Accuracy: 0.9227, F1 Micro: 0.7697, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.097, Accuracy: 0.9222, F1 Micro: 0.7719, F1 Macro: 0.6584\n",
      "Epoch 9/10, Train Loss: 0.0831, Accuracy: 0.9238, F1 Micro: 0.7702, F1 Macro: 0.6676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9223, F1 Micro: 0.7735, F1 Macro: 0.6683\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9223, F1 Micro: 0.7735, F1 Macro: 0.6683\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.73      0.57      0.64       402\n",
      "  HS_Religion       0.80      0.48      0.60       157\n",
      "      HS_Race       0.88      0.57      0.69       120\n",
      "  HS_Physical       0.69      0.15      0.25        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.67      0.51      0.58       331\n",
      "    HS_Strong       0.88      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.63      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.68194317817688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4006, Accuracy: 0.883, F1 Micro: 0.5819, F1 Macro: 0.2879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2689, Accuracy: 0.9045, F1 Micro: 0.687, F1 Macro: 0.4634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.9149, F1 Micro: 0.7376, F1 Macro: 0.555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1878, Accuracy: 0.9158, F1 Micro: 0.7559, F1 Macro: 0.5942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.158, Accuracy: 0.9211, F1 Micro: 0.7619, F1 Macro: 0.6285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9224, F1 Micro: 0.7729, F1 Macro: 0.6416\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9235, F1 Micro: 0.7674, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0975, Accuracy: 0.9239, F1 Micro: 0.7783, F1 Macro: 0.668\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.9221, F1 Micro: 0.7674, F1 Macro: 0.6743\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.924, F1 Micro: 0.7694, F1 Macro: 0.6806\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9239, F1 Micro: 0.7783, F1 Macro: 0.668\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.71      0.68      0.69       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.77      0.72      0.75       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.63      0.60      0.62       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.65      0.67      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 184.33277487754822 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9118, F1 Micro: 0.7365, F1 Macro: 0.5953\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 30.074555158615112 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3869, Accuracy: 0.8828, F1 Micro: 0.6246, F1 Macro: 0.3206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2714, Accuracy: 0.9054, F1 Micro: 0.6928, F1 Macro: 0.5024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2192, Accuracy: 0.9142, F1 Micro: 0.7428, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1828, Accuracy: 0.9178, F1 Micro: 0.7546, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1544, Accuracy: 0.9175, F1 Micro: 0.7588, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9214, F1 Micro: 0.7732, F1 Macro: 0.6587\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9238, F1 Micro: 0.7686, F1 Macro: 0.6788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9218, F1 Micro: 0.7741, F1 Macro: 0.6869\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9218, F1 Micro: 0.7707, F1 Macro: 0.6825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9255, F1 Micro: 0.7778, F1 Macro: 0.6999\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9255, F1 Micro: 0.7778, F1 Macro: 0.6999\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.77      0.71      0.74       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.75      0.58      0.65       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       1.00      0.24      0.38        72\n",
      "    HS_Gender       0.61      0.45      0.52        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.75      0.68      0.71       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.79798531532288 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4033, Accuracy: 0.8822, F1 Micro: 0.6096, F1 Macro: 0.2964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.9021, F1 Micro: 0.6865, F1 Macro: 0.4602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2267, Accuracy: 0.9119, F1 Micro: 0.7379, F1 Macro: 0.5662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1872, Accuracy: 0.9173, F1 Micro: 0.7507, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9174, F1 Micro: 0.7589, F1 Macro: 0.6119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6429\n",
      "Epoch 7/10, Train Loss: 0.1167, Accuracy: 0.9215, F1 Micro: 0.766, F1 Macro: 0.6704\n",
      "Epoch 8/10, Train Loss: 0.0956, Accuracy: 0.9227, F1 Micro: 0.7731, F1 Macro: 0.6699\n",
      "Epoch 9/10, Train Loss: 0.0849, Accuracy: 0.9225, F1 Micro: 0.7704, F1 Macro: 0.6777\n",
      "Epoch 10/10, Train Loss: 0.0732, Accuracy: 0.9235, F1 Micro: 0.77, F1 Macro: 0.6798\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.69      0.61      0.65       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.57      0.16      0.25        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.63      0.64      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 184.88443112373352 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3973, Accuracy: 0.8837, F1 Micro: 0.598, F1 Macro: 0.2952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2755, Accuracy: 0.905, F1 Micro: 0.6892, F1 Macro: 0.4926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2221, Accuracy: 0.9124, F1 Micro: 0.736, F1 Macro: 0.567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1859, Accuracy: 0.9159, F1 Micro: 0.7528, F1 Macro: 0.6052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1558, Accuracy: 0.916, F1 Micro: 0.7574, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.133, Accuracy: 0.9237, F1 Micro: 0.7756, F1 Macro: 0.6496\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9217, F1 Micro: 0.7681, F1 Macro: 0.6611\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9232, F1 Micro: 0.7748, F1 Macro: 0.6682\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9185, F1 Micro: 0.7694, F1 Macro: 0.6785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0765, Accuracy: 0.9203, F1 Micro: 0.7758, F1 Macro: 0.6981\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9203, F1 Micro: 0.7758, F1 Macro: 0.6981\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.69      0.68      0.68       402\n",
      "  HS_Religion       0.63      0.65      0.64       157\n",
      "      HS_Race       0.72      0.68      0.70       120\n",
      "  HS_Physical       0.85      0.24      0.37        72\n",
      "    HS_Gender       0.60      0.51      0.55        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.78      5556\n",
      "    macro avg       0.73      0.70      0.70      5556\n",
      " weighted avg       0.75      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.44      5556\n",
      "\n",
      "Training completed in 186.73000717163086 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9124, F1 Micro: 0.7387, F1 Macro: 0.6\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 26.767601490020752 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3851, Accuracy: 0.8859, F1 Micro: 0.6206, F1 Macro: 0.3263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.27, Accuracy: 0.9065, F1 Micro: 0.6977, F1 Macro: 0.4819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2223, Accuracy: 0.9129, F1 Micro: 0.7245, F1 Macro: 0.557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1851, Accuracy: 0.9192, F1 Micro: 0.7604, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1541, Accuracy: 0.9231, F1 Micro: 0.7703, F1 Macro: 0.6263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.923, F1 Micro: 0.7755, F1 Macro: 0.6596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9252, F1 Micro: 0.7768, F1 Macro: 0.6716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9262, F1 Micro: 0.7816, F1 Macro: 0.6877\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9243, F1 Micro: 0.7808, F1 Macro: 0.6803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9258, F1 Micro: 0.7817, F1 Macro: 0.7012\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9258, F1 Micro: 0.7817, F1 Macro: 0.7012\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.92      0.91       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.68      0.71      0.70       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.74      0.77      0.75       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.55      0.41      0.47        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.74      0.67      0.71       689\n",
      "  HS_Moderate       0.61      0.66      0.64       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 195.2830204963684 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4004, Accuracy: 0.8828, F1 Micro: 0.5822, F1 Macro: 0.2861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.9034, F1 Micro: 0.6847, F1 Macro: 0.4494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.9082, F1 Micro: 0.6976, F1 Macro: 0.5144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9169, F1 Micro: 0.7554, F1 Macro: 0.5948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.159, Accuracy: 0.9219, F1 Micro: 0.7669, F1 Macro: 0.63\n",
      "Epoch 6/10, Train Loss: 0.1386, Accuracy: 0.9204, F1 Micro: 0.7635, F1 Macro: 0.643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9208, F1 Micro: 0.7678, F1 Macro: 0.6387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0956, Accuracy: 0.9226, F1 Micro: 0.7753, F1 Macro: 0.6767\n",
      "Epoch 9/10, Train Loss: 0.0834, Accuracy: 0.921, F1 Micro: 0.7698, F1 Macro: 0.6715\n",
      "Epoch 10/10, Train Loss: 0.0737, Accuracy: 0.9226, F1 Micro: 0.7718, F1 Macro: 0.6815\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9226, F1 Micro: 0.7753, F1 Macro: 0.6767\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.71      0.60      0.65       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.71      0.14      0.23        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.74      0.65      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 191.22993087768555 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.887, F1 Micro: 0.6088, F1 Macro: 0.3065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2733, Accuracy: 0.9068, F1 Micro: 0.7079, F1 Macro: 0.5043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.9125, F1 Micro: 0.7207, F1 Macro: 0.5283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1882, Accuracy: 0.9195, F1 Micro: 0.7603, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1568, Accuracy: 0.9225, F1 Micro: 0.769, F1 Macro: 0.6368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1351, Accuracy: 0.9213, F1 Micro: 0.7729, F1 Macro: 0.6643\n",
      "Epoch 7/10, Train Loss: 0.1127, Accuracy: 0.9219, F1 Micro: 0.7707, F1 Macro: 0.6636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9215, F1 Micro: 0.7768, F1 Macro: 0.69\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9229, F1 Micro: 0.7746, F1 Macro: 0.6909\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9201, F1 Micro: 0.7741, F1 Macro: 0.7042\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9215, F1 Micro: 0.7768, F1 Macro: 0.69\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.69      0.62      0.65       157\n",
      "      HS_Race       0.74      0.73      0.74       120\n",
      "  HS_Physical       0.75      0.17      0.27        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.75      0.83      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.73      0.68      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 191.13028573989868 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.913, F1 Micro: 0.7408, F1 Macro: 0.6047\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.143741846084595 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3844, Accuracy: 0.8833, F1 Micro: 0.6039, F1 Macro: 0.316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2654, Accuracy: 0.9054, F1 Micro: 0.7088, F1 Macro: 0.5008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2159, Accuracy: 0.9162, F1 Micro: 0.7486, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9212, F1 Micro: 0.7575, F1 Macro: 0.6006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.9231, F1 Micro: 0.7656, F1 Macro: 0.6315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9237, F1 Micro: 0.7749, F1 Macro: 0.6365\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9196, F1 Micro: 0.7697, F1 Macro: 0.6516\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.924, F1 Micro: 0.7741, F1 Macro: 0.6814\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9235, F1 Micro: 0.7733, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.925, F1 Micro: 0.7807, F1 Macro: 0.7016\n",
      "Model 1 - Iteration 9216: Accuracy: 0.925, F1 Micro: 0.7807, F1 Macro: 0.7016\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.70      0.68      0.69       402\n",
      "  HS_Religion       0.77      0.57      0.65       157\n",
      "      HS_Race       0.77      0.59      0.67       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.53      0.45      0.49        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.90      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.21421003341675 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4001, Accuracy: 0.8829, F1 Micro: 0.6253, F1 Macro: 0.3128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.271, Accuracy: 0.9031, F1 Micro: 0.687, F1 Macro: 0.4445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2234, Accuracy: 0.9129, F1 Micro: 0.7356, F1 Macro: 0.5508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1896, Accuracy: 0.9189, F1 Micro: 0.7529, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1587, Accuracy: 0.9211, F1 Micro: 0.7591, F1 Macro: 0.6254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9229, F1 Micro: 0.7679, F1 Macro: 0.6437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1137, Accuracy: 0.9194, F1 Micro: 0.7683, F1 Macro: 0.6406\n",
      "Epoch 8/10, Train Loss: 0.0968, Accuracy: 0.9222, F1 Micro: 0.7654, F1 Macro: 0.6567\n",
      "Epoch 9/10, Train Loss: 0.0828, Accuracy: 0.9224, F1 Micro: 0.7676, F1 Macro: 0.6659\n",
      "Epoch 10/10, Train Loss: 0.0697, Accuracy: 0.922, F1 Micro: 0.7659, F1 Macro: 0.6834\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9194, F1 Micro: 0.7683, F1 Macro: 0.6406\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.73      0.49      0.59       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.60      0.04      0.08        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.73      0.84      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.92      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.62      0.64      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.75723385810852 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3935, Accuracy: 0.883, F1 Micro: 0.6015, F1 Macro: 0.304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2665, Accuracy: 0.906, F1 Micro: 0.6992, F1 Macro: 0.4887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2184, Accuracy: 0.9142, F1 Micro: 0.7414, F1 Macro: 0.5619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9205, F1 Micro: 0.7536, F1 Macro: 0.5958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.922, F1 Micro: 0.7569, F1 Macro: 0.6341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1374, Accuracy: 0.9232, F1 Micro: 0.7677, F1 Macro: 0.6385\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.9149, F1 Micro: 0.76, F1 Macro: 0.6378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9207, F1 Micro: 0.7728, F1 Macro: 0.6778\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.9235, F1 Micro: 0.7676, F1 Macro: 0.6721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9245, F1 Micro: 0.7746, F1 Macro: 0.7033\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9245, F1 Micro: 0.7746, F1 Macro: 0.7033\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.80      0.65      0.72       732\n",
      "     HS_Group       0.66      0.73      0.69       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.78      0.66      0.71       120\n",
      "  HS_Physical       0.90      0.25      0.39        72\n",
      "    HS_Gender       0.58      0.55      0.57        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.78      0.62      0.69       689\n",
      "  HS_Moderate       0.59      0.68      0.63       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 195.51684975624084 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9135, F1 Micro: 0.7424, F1 Macro: 0.6086\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 20.321859121322632 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3816, Accuracy: 0.8854, F1 Micro: 0.6195, F1 Macro: 0.3308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.9058, F1 Micro: 0.7189, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2161, Accuracy: 0.9169, F1 Micro: 0.7457, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1796, Accuracy: 0.9208, F1 Micro: 0.7618, F1 Macro: 0.6081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1511, Accuracy: 0.9234, F1 Micro: 0.7734, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1246, Accuracy: 0.9219, F1 Micro: 0.7758, F1 Macro: 0.6792\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9237, F1 Micro: 0.7756, F1 Macro: 0.6787\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9207, F1 Micro: 0.7755, F1 Macro: 0.6938\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9191, F1 Micro: 0.7718, F1 Macro: 0.6848\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9204, F1 Micro: 0.7718, F1 Macro: 0.6993\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9219, F1 Micro: 0.7758, F1 Macro: 0.6792\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       0.64      0.12      0.21        72\n",
      "    HS_Gender       0.53      0.31      0.40        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.84      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.72      0.67      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.29032611846924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3993, Accuracy: 0.884, F1 Micro: 0.6118, F1 Macro: 0.3029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2698, Accuracy: 0.9033, F1 Micro: 0.7109, F1 Macro: 0.5035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2241, Accuracy: 0.9129, F1 Micro: 0.7285, F1 Macro: 0.5516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1843, Accuracy: 0.9188, F1 Micro: 0.7538, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1552, Accuracy: 0.92, F1 Micro: 0.7669, F1 Macro: 0.6242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9212, F1 Micro: 0.7705, F1 Macro: 0.644\n",
      "Epoch 7/10, Train Loss: 0.1103, Accuracy: 0.9225, F1 Micro: 0.7703, F1 Macro: 0.6584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9213, F1 Micro: 0.7706, F1 Macro: 0.6675\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9185, F1 Micro: 0.7692, F1 Macro: 0.6689\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9219, F1 Micro: 0.7597, F1 Macro: 0.668\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9213, F1 Micro: 0.7706, F1 Macro: 0.6675\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.73      0.55      0.63       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.57      0.11      0.19        72\n",
      "    HS_Gender       0.57      0.33      0.42        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.62      0.57      0.60       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.64      0.67      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 196.0826268196106 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3903, Accuracy: 0.8861, F1 Micro: 0.6349, F1 Macro: 0.3377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2655, Accuracy: 0.9033, F1 Micro: 0.7178, F1 Macro: 0.5236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9151, F1 Micro: 0.7422, F1 Macro: 0.5785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.92, F1 Micro: 0.7606, F1 Macro: 0.6013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1531, Accuracy: 0.9216, F1 Micro: 0.7636, F1 Macro: 0.616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1308, Accuracy: 0.92, F1 Micro: 0.7717, F1 Macro: 0.6612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1097, Accuracy: 0.9239, F1 Micro: 0.7738, F1 Macro: 0.6673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9241, F1 Micro: 0.7783, F1 Macro: 0.6897\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9218, F1 Micro: 0.7738, F1 Macro: 0.6942\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.924, F1 Micro: 0.7764, F1 Macro: 0.6914\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9241, F1 Micro: 0.7783, F1 Macro: 0.6897\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.75      0.62      0.68       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.80      0.67      0.73       120\n",
      "  HS_Physical       0.61      0.19      0.29        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.76      0.73       689\n",
      "  HS_Moderate       0.64      0.54      0.59       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.21573877334595 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9139, F1 Micro: 0.744, F1 Macro: 0.6119\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.409111261367798 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3825, Accuracy: 0.8876, F1 Micro: 0.6217, F1 Macro: 0.3347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2627, Accuracy: 0.9087, F1 Micro: 0.7105, F1 Macro: 0.5309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2094, Accuracy: 0.9128, F1 Micro: 0.7479, F1 Macro: 0.5917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.919, F1 Micro: 0.7617, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1489, Accuracy: 0.921, F1 Micro: 0.7659, F1 Macro: 0.6189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.924, F1 Micro: 0.7739, F1 Macro: 0.6522\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.923, F1 Micro: 0.7736, F1 Macro: 0.6813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9192, F1 Micro: 0.7761, F1 Macro: 0.6897\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9226, F1 Micro: 0.7704, F1 Macro: 0.6791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9238, F1 Micro: 0.7771, F1 Macro: 0.6994\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9238, F1 Micro: 0.7771, F1 Macro: 0.6994\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.75      0.72      0.74       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.79      0.56      0.65       157\n",
      "      HS_Race       0.79      0.71      0.75       120\n",
      "  HS_Physical       0.95      0.26      0.41        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 200.2483274936676 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3999, Accuracy: 0.8833, F1 Micro: 0.5963, F1 Macro: 0.2981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2712, Accuracy: 0.9046, F1 Micro: 0.6982, F1 Macro: 0.4881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2148, Accuracy: 0.9122, F1 Micro: 0.7379, F1 Macro: 0.571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.19, Accuracy: 0.9171, F1 Micro: 0.7521, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9207, F1 Micro: 0.7539, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1305, Accuracy: 0.9213, F1 Micro: 0.7661, F1 Macro: 0.6399\n",
      "Epoch 7/10, Train Loss: 0.1107, Accuracy: 0.9222, F1 Micro: 0.7615, F1 Macro: 0.6523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9188, F1 Micro: 0.7725, F1 Macro: 0.6643\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.9225, F1 Micro: 0.7714, F1 Macro: 0.678\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9231, F1 Micro: 0.7717, F1 Macro: 0.6832\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9188, F1 Micro: 0.7725, F1 Macro: 0.6643\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.91      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.66      0.69      0.67       402\n",
      "  HS_Religion       0.66      0.61      0.63       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.73      0.85      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.77      5556\n",
      "    macro avg       0.73      0.66      0.66      5556\n",
      " weighted avg       0.75      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 197.62591123580933 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3923, Accuracy: 0.8861, F1 Micro: 0.6079, F1 Macro: 0.3084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2675, Accuracy: 0.9075, F1 Micro: 0.7161, F1 Macro: 0.5255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2122, Accuracy: 0.9114, F1 Micro: 0.7431, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.9207, F1 Micro: 0.7573, F1 Macro: 0.6093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1501, Accuracy: 0.9233, F1 Micro: 0.7725, F1 Macro: 0.6275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.9237, F1 Micro: 0.7745, F1 Macro: 0.6501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9235, F1 Micro: 0.7765, F1 Macro: 0.6647\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9192, F1 Micro: 0.7689, F1 Macro: 0.6695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0806, Accuracy: 0.9258, F1 Micro: 0.7779, F1 Macro: 0.6978\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9237, F1 Micro: 0.7718, F1 Macro: 0.6935\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9258, F1 Micro: 0.7779, F1 Macro: 0.6978\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.75      0.61      0.67       402\n",
      "  HS_Religion       0.79      0.56      0.65       157\n",
      "      HS_Race       0.81      0.71      0.76       120\n",
      "  HS_Physical       0.76      0.22      0.34        72\n",
      "    HS_Gender       0.58      0.49      0.53        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.77      0.66      0.70      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 199.7479567527771 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9143, F1 Micro: 0.7454, F1 Macro: 0.6153\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 18.48676300048828 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3801, Accuracy: 0.8879, F1 Micro: 0.6123, F1 Macro: 0.3354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.9076, F1 Micro: 0.693, F1 Macro: 0.4812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9165, F1 Micro: 0.7339, F1 Macro: 0.5722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9206, F1 Micro: 0.7663, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1513, Accuracy: 0.9187, F1 Micro: 0.7687, F1 Macro: 0.6502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1257, Accuracy: 0.9243, F1 Micro: 0.7711, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9247, F1 Micro: 0.7756, F1 Macro: 0.6693\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9212, F1 Micro: 0.7735, F1 Macro: 0.6813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.9248, F1 Micro: 0.7774, F1 Macro: 0.7014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9223, F1 Micro: 0.7787, F1 Macro: 0.7093\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9223, F1 Micro: 0.7787, F1 Macro: 0.7093\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.69      0.66      0.67       157\n",
      "      HS_Race       0.70      0.77      0.73       120\n",
      "  HS_Physical       0.80      0.33      0.47        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.76      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 207.78242802619934 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3935, Accuracy: 0.8875, F1 Micro: 0.6219, F1 Macro: 0.3187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2651, Accuracy: 0.904, F1 Micro: 0.6841, F1 Macro: 0.4467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2184, Accuracy: 0.9134, F1 Micro: 0.7206, F1 Macro: 0.5432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9199, F1 Micro: 0.7539, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9208, F1 Micro: 0.7689, F1 Macro: 0.6457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9225, F1 Micro: 0.7712, F1 Macro: 0.6583\n",
      "Epoch 7/10, Train Loss: 0.1097, Accuracy: 0.9226, F1 Micro: 0.7707, F1 Macro: 0.6657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0905, Accuracy: 0.9225, F1 Micro: 0.7759, F1 Macro: 0.6814\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.9195, F1 Micro: 0.7716, F1 Macro: 0.6818\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9221, F1 Micro: 0.7732, F1 Macro: 0.693\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9225, F1 Micro: 0.7759, F1 Macro: 0.6814\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.66      0.72      0.69       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.59      0.62      0.60       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 202.38735270500183 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.8886, F1 Micro: 0.6431, F1 Macro: 0.3401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.26, Accuracy: 0.9073, F1 Micro: 0.6994, F1 Macro: 0.4856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9155, F1 Micro: 0.7333, F1 Macro: 0.5654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9207, F1 Micro: 0.7637, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9167, F1 Micro: 0.7654, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9211, F1 Micro: 0.7734, F1 Macro: 0.6657\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9236, F1 Micro: 0.7723, F1 Macro: 0.6632\n",
      "Epoch 8/10, Train Loss: 0.093, Accuracy: 0.9213, F1 Micro: 0.771, F1 Macro: 0.6724\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9186, F1 Micro: 0.7722, F1 Macro: 0.6974\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9237, F1 Micro: 0.7727, F1 Macro: 0.704\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9211, F1 Micro: 0.7734, F1 Macro: 0.6657\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.79      0.55      0.65       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.70      0.10      0.17        72\n",
      "    HS_Gender       0.62      0.31      0.42        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.68      0.78      0.72       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 200.3148069381714 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9146, F1 Micro: 0.7468, F1 Macro: 0.6184\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.939445734024048 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3794, Accuracy: 0.8844, F1 Micro: 0.6631, F1 Macro: 0.4086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.9047, F1 Micro: 0.7215, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2129, Accuracy: 0.916, F1 Micro: 0.7468, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1785, Accuracy: 0.921, F1 Micro: 0.7507, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1556, Accuracy: 0.9232, F1 Micro: 0.7747, F1 Macro: 0.6748\n",
      "Epoch 6/10, Train Loss: 0.1249, Accuracy: 0.9198, F1 Micro: 0.7705, F1 Macro: 0.6534\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9255, F1 Micro: 0.7683, F1 Macro: 0.6581\n",
      "Epoch 8/10, Train Loss: 0.0924, Accuracy: 0.9235, F1 Micro: 0.7732, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.9254, F1 Micro: 0.7764, F1 Macro: 0.6942\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9226, F1 Micro: 0.7762, F1 Macro: 0.6853\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9254, F1 Micro: 0.7764, F1 Macro: 0.6942\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.76      0.72      0.74       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.82      0.65      0.73       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.70      0.41      0.52        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.74      0.70      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 202.47696781158447 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3941, Accuracy: 0.8853, F1 Micro: 0.6398, F1 Macro: 0.3293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.9052, F1 Micro: 0.7051, F1 Macro: 0.4942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2183, Accuracy: 0.9146, F1 Micro: 0.7298, F1 Macro: 0.5513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.9209, F1 Micro: 0.7584, F1 Macro: 0.622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.917, F1 Micro: 0.7604, F1 Macro: 0.6553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1282, Accuracy: 0.9196, F1 Micro: 0.7675, F1 Macro: 0.6482\n",
      "Epoch 7/10, Train Loss: 0.11, Accuracy: 0.923, F1 Micro: 0.7659, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9212, F1 Micro: 0.771, F1 Macro: 0.6713\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9222, F1 Micro: 0.7638, F1 Macro: 0.679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0692, Accuracy: 0.9227, F1 Micro: 0.7744, F1 Macro: 0.6889\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9227, F1 Micro: 0.7744, F1 Macro: 0.6889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.71      0.57      0.63       157\n",
      "      HS_Race       0.79      0.63      0.70       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.69      0.47      0.56        51\n",
      "     HS_Other       0.78      0.82      0.80       762\n",
      "      HS_Weak       0.68      0.73      0.71       689\n",
      "  HS_Moderate       0.64      0.54      0.58       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 205.54122161865234 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3865, Accuracy: 0.8845, F1 Micro: 0.6556, F1 Macro: 0.3644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2615, Accuracy: 0.9058, F1 Micro: 0.7169, F1 Macro: 0.511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2136, Accuracy: 0.9155, F1 Micro: 0.7387, F1 Macro: 0.5672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.9219, F1 Micro: 0.7606, F1 Macro: 0.6151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1551, Accuracy: 0.9212, F1 Micro: 0.7698, F1 Macro: 0.661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9183, F1 Micro: 0.7711, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9245, F1 Micro: 0.7728, F1 Macro: 0.6624\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.9226, F1 Micro: 0.7698, F1 Macro: 0.6758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9253, F1 Micro: 0.7735, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9264, F1 Micro: 0.7799, F1 Macro: 0.6979\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9264, F1 Micro: 0.7799, F1 Macro: 0.6979\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.80      0.66      0.72       732\n",
      "     HS_Group       0.67      0.73      0.70       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.77      0.64      0.70       120\n",
      "  HS_Physical       1.00      0.21      0.34        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.79      0.64      0.71       689\n",
      "  HS_Moderate       0.59      0.67      0.63       331\n",
      "    HS_Strong       0.91      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.66      0.70      5556\n",
      " weighted avg       0.81      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 207.21490168571472 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9151, F1 Micro: 0.748, F1 Macro: 0.6215\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.167563676834106 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.377, Accuracy: 0.8895, F1 Micro: 0.6545, F1 Macro: 0.3837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.9075, F1 Micro: 0.7137, F1 Macro: 0.5135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2075, Accuracy: 0.9188, F1 Micro: 0.7426, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1748, Accuracy: 0.9184, F1 Micro: 0.7433, F1 Macro: 0.6003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1508, Accuracy: 0.9231, F1 Micro: 0.7694, F1 Macro: 0.6491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.9232, F1 Micro: 0.7749, F1 Macro: 0.6554\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.922, F1 Micro: 0.7746, F1 Macro: 0.677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9238, F1 Micro: 0.7779, F1 Macro: 0.6864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.6932\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9221, F1 Micro: 0.7721, F1 Macro: 0.7031\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.6932\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.77      0.73      0.75       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 208.78100538253784 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3929, Accuracy: 0.8867, F1 Micro: 0.6425, F1 Macro: 0.3361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2605, Accuracy: 0.9065, F1 Micro: 0.7094, F1 Macro: 0.4978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.9148, F1 Micro: 0.7278, F1 Macro: 0.5608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.9176, F1 Micro: 0.7418, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.154, Accuracy: 0.9218, F1 Micro: 0.768, F1 Macro: 0.639\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.9209, F1 Micro: 0.7669, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9214, F1 Micro: 0.7708, F1 Macro: 0.663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9235, F1 Micro: 0.772, F1 Macro: 0.6647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9237, F1 Micro: 0.7722, F1 Macro: 0.6746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0681, Accuracy: 0.9253, F1 Micro: 0.7742, F1 Macro: 0.6889\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9253, F1 Micro: 0.7742, F1 Macro: 0.6889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.85      1134\n",
      "      Abusive       0.92      0.88      0.90       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.81      0.65      0.72       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.67      0.55      0.60       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.78      0.64      0.69      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 209.73493075370789 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3853, Accuracy: 0.8889, F1 Micro: 0.6502, F1 Macro: 0.3462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.9071, F1 Micro: 0.7112, F1 Macro: 0.5146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.211, Accuracy: 0.9169, F1 Micro: 0.7334, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9175, F1 Micro: 0.7426, F1 Macro: 0.5975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9155, F1 Micro: 0.7656, F1 Macro: 0.6511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1236, Accuracy: 0.921, F1 Micro: 0.7711, F1 Macro: 0.6464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1074, Accuracy: 0.9209, F1 Micro: 0.7711, F1 Macro: 0.678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.9237, F1 Micro: 0.776, F1 Macro: 0.6759\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9196, F1 Micro: 0.7714, F1 Macro: 0.6771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9239, F1 Micro: 0.7807, F1 Macro: 0.7114\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9239, F1 Micro: 0.7807, F1 Macro: 0.7114\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.70      0.70      0.70       402\n",
      "  HS_Religion       0.76      0.62      0.68       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.91      0.29      0.44        72\n",
      "    HS_Gender       0.64      0.45      0.53        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.61      0.64      0.62       331\n",
      "    HS_Strong       0.92      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.77      0.69      0.71      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 209.86584377288818 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9154, F1 Micro: 0.7492, F1 Macro: 0.6246\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.88512396812439 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3739, Accuracy: 0.8847, F1 Micro: 0.5723, F1 Macro: 0.2921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2512, Accuracy: 0.904, F1 Micro: 0.6627, F1 Macro: 0.4787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9163, F1 Micro: 0.7484, F1 Macro: 0.5937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1745, Accuracy: 0.9204, F1 Micro: 0.7515, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.9241, F1 Micro: 0.7668, F1 Macro: 0.6262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1238, Accuracy: 0.9253, F1 Micro: 0.7732, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9264, F1 Micro: 0.7799, F1 Macro: 0.6707\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9244, F1 Micro: 0.7777, F1 Macro: 0.6836\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.924, F1 Micro: 0.778, F1 Macro: 0.6891\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9256, F1 Micro: 0.7761, F1 Macro: 0.6994\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9264, F1 Micro: 0.7799, F1 Macro: 0.6707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.76      0.73      0.74       732\n",
      "     HS_Group       0.74      0.66      0.70       402\n",
      "  HS_Religion       0.85      0.54      0.66       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.50      0.18      0.26        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.74      0.72      0.73       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.91      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 208.3965926170349 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.8846, F1 Micro: 0.5907, F1 Macro: 0.2917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.257, Accuracy: 0.9027, F1 Micro: 0.6652, F1 Macro: 0.4755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2143, Accuracy: 0.9147, F1 Micro: 0.7358, F1 Macro: 0.569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9203, F1 Micro: 0.7577, F1 Macro: 0.6092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.9207, F1 Micro: 0.7623, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1292, Accuracy: 0.9218, F1 Micro: 0.7715, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9245, F1 Micro: 0.7724, F1 Macro: 0.6534\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9222, F1 Micro: 0.7679, F1 Macro: 0.6693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.9224, F1 Micro: 0.7772, F1 Macro: 0.6923\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9238, F1 Micro: 0.7682, F1 Macro: 0.6867\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9224, F1 Micro: 0.7772, F1 Macro: 0.6923\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.76      0.69      0.72       120\n",
      "  HS_Physical       0.74      0.19      0.31        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.61      0.62      0.62       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 209.90381717681885 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3807, Accuracy: 0.8851, F1 Micro: 0.5751, F1 Macro: 0.2901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.903, F1 Micro: 0.6574, F1 Macro: 0.4802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9151, F1 Micro: 0.7449, F1 Macro: 0.5776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9229, F1 Micro: 0.7665, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.923, F1 Micro: 0.7689, F1 Macro: 0.6195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9231, F1 Micro: 0.7714, F1 Macro: 0.6479\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.9263, F1 Micro: 0.7686, F1 Macro: 0.6546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9236, F1 Micro: 0.7778, F1 Macro: 0.6907\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9208, F1 Micro: 0.7702, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9263, F1 Micro: 0.7804, F1 Macro: 0.7062\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9263, F1 Micro: 0.7804, F1 Macro: 0.7062\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.74      0.66      0.70       402\n",
      "  HS_Religion       0.71      0.63      0.67       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.86      0.25      0.39        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.66      0.58      0.61       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 210.8548126220703 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9158, F1 Micro: 0.7504, F1 Macro: 0.6271\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.2032225131988525 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3643, Accuracy: 0.8912, F1 Micro: 0.6325, F1 Macro: 0.3629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2472, Accuracy: 0.9079, F1 Micro: 0.6894, F1 Macro: 0.4892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.9157, F1 Micro: 0.7276, F1 Macro: 0.5546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.9235, F1 Micro: 0.7661, F1 Macro: 0.6217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9227, F1 Micro: 0.7708, F1 Macro: 0.6544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9257, F1 Micro: 0.7719, F1 Macro: 0.6648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1021, Accuracy: 0.9248, F1 Micro: 0.7732, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9238, F1 Micro: 0.7759, F1 Macro: 0.6767\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9198, F1 Micro: 0.7747, F1 Macro: 0.6845\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9227, F1 Micro: 0.7745, F1 Macro: 0.6963\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9238, F1 Micro: 0.7759, F1 Macro: 0.6767\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.77      0.63      0.69       402\n",
      "  HS_Religion       0.81      0.55      0.65       157\n",
      "      HS_Race       0.82      0.70      0.75       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.69      0.55      0.61       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.64      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 214.96667981147766 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3802, Accuracy: 0.8863, F1 Micro: 0.6197, F1 Macro: 0.323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.9024, F1 Micro: 0.666, F1 Macro: 0.4689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2066, Accuracy: 0.9127, F1 Micro: 0.7121, F1 Macro: 0.5255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1711, Accuracy: 0.921, F1 Micro: 0.7662, F1 Macro: 0.6168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9204, F1 Micro: 0.7678, F1 Macro: 0.6481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9245, F1 Micro: 0.7688, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9244, F1 Micro: 0.7752, F1 Macro: 0.6624\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.9247, F1 Micro: 0.7751, F1 Macro: 0.6532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9245, F1 Micro: 0.7777, F1 Macro: 0.6814\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9226, F1 Micro: 0.7699, F1 Macro: 0.6832\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9245, F1 Micro: 0.7777, F1 Macro: 0.6814\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.70      0.68      0.69       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.83      0.66      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.78      0.82      0.80       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.62      0.63      0.62       331\n",
      "    HS_Strong       0.91      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 213.4210352897644 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.8907, F1 Micro: 0.6344, F1 Macro: 0.336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.905, F1 Micro: 0.6727, F1 Macro: 0.4728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.9158, F1 Micro: 0.7354, F1 Macro: 0.5544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9218, F1 Micro: 0.7705, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9225, F1 Micro: 0.772, F1 Macro: 0.6559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1209, Accuracy: 0.9268, F1 Micro: 0.7756, F1 Macro: 0.6713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1024, Accuracy: 0.9247, F1 Micro: 0.777, F1 Macro: 0.6691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.9226, F1 Micro: 0.7802, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9259, F1 Micro: 0.783, F1 Macro: 0.6949\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9247, F1 Micro: 0.776, F1 Macro: 0.7029\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9259, F1 Micro: 0.783, F1 Macro: 0.6949\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.80      0.67      0.73       732\n",
      "     HS_Group       0.64      0.76      0.70       402\n",
      "  HS_Religion       0.81      0.61      0.70       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.78      0.65      0.71       689\n",
      "  HS_Moderate       0.57      0.73      0.64       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.78      0.67      0.69      5556\n",
      " weighted avg       0.80      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 216.2511122226715 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9161, F1 Micro: 0.7514, F1 Macro: 0.6292\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 3.0820724964141846 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3707, Accuracy: 0.8885, F1 Micro: 0.6458, F1 Macro: 0.3509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2462, Accuracy: 0.9049, F1 Micro: 0.7225, F1 Macro: 0.5133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2028, Accuracy: 0.914, F1 Micro: 0.7519, F1 Macro: 0.588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1714, Accuracy: 0.9219, F1 Micro: 0.7649, F1 Macro: 0.6148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1432, Accuracy: 0.9249, F1 Micro: 0.7739, F1 Macro: 0.6486\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.9222, F1 Micro: 0.7706, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9225, F1 Micro: 0.7742, F1 Macro: 0.6608\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9222, F1 Micro: 0.7736, F1 Macro: 0.6801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0685, Accuracy: 0.9223, F1 Micro: 0.7779, F1 Macro: 0.6949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9238, F1 Micro: 0.7817, F1 Macro: 0.7049\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9238, F1 Micro: 0.7817, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.86      0.94      0.90       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.94      0.24      0.38        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.66      0.58      0.62       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.76      0.69      0.70      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 217.57833886146545 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3843, Accuracy: 0.8866, F1 Micro: 0.6301, F1 Macro: 0.3199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2538, Accuracy: 0.9046, F1 Micro: 0.7149, F1 Macro: 0.4775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2081, Accuracy: 0.9153, F1 Micro: 0.7473, F1 Macro: 0.5801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1769, Accuracy: 0.9218, F1 Micro: 0.7601, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.9233, F1 Micro: 0.7695, F1 Macro: 0.6294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9247, F1 Micro: 0.7708, F1 Macro: 0.6305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.101, Accuracy: 0.9233, F1 Micro: 0.7725, F1 Macro: 0.6349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9233, F1 Micro: 0.7736, F1 Macro: 0.6809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9235, F1 Micro: 0.774, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.9223, F1 Micro: 0.7761, F1 Macro: 0.6897\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9223, F1 Micro: 0.7761, F1 Macro: 0.6897\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.69      0.68      0.68       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.74      0.19      0.31        72\n",
      "    HS_Gender       0.60      0.41      0.49        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 223.61079835891724 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3775, Accuracy: 0.8894, F1 Micro: 0.6491, F1 Macro: 0.3447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.9065, F1 Micro: 0.7187, F1 Macro: 0.5044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9158, F1 Micro: 0.7514, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9221, F1 Micro: 0.765, F1 Macro: 0.6176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1438, Accuracy: 0.9222, F1 Micro: 0.7667, F1 Macro: 0.6395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9208, F1 Micro: 0.7726, F1 Macro: 0.6399\n",
      "Epoch 7/10, Train Loss: 0.1002, Accuracy: 0.92, F1 Micro: 0.7726, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9258, F1 Micro: 0.7776, F1 Macro: 0.6949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9252, F1 Micro: 0.7812, F1 Macro: 0.6997\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.9182, F1 Micro: 0.7743, F1 Macro: 0.6996\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9252, F1 Micro: 0.7812, F1 Macro: 0.6997\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.54      0.51      0.53        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.73      0.73       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 219.06681847572327 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9164, F1 Micro: 0.7524, F1 Macro: 0.6317\n",
      "Total sampling time: 1429.92 seconds\n",
      "Total runtime: 15677.643550157547 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819a71",
   "metadata": {
    "papermill": {
     "duration": 0.012209,
     "end_time": "2025-04-13T07:01:23.571865",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.559656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50297a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6119, Accuracy: 0.83, F1 Micro: 0.1445, F1 Macro: 0.045\n",
      "Epoch 2/10, Train Loss: 0.4724, Accuracy: 0.8287, F1 Micro: 0.0209, F1 Macro: 0.0091\n",
      "Epoch 3/10, Train Loss: 0.3967, Accuracy: 0.8346, F1 Micro: 0.1256, F1 Macro: 0.0415\n",
      "Epoch 4/10, Train Loss: 0.4024, Accuracy: 0.8353, F1 Micro: 0.122, F1 Macro: 0.0414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3867, Accuracy: 0.8389, F1 Micro: 0.1605, F1 Macro: 0.0558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3732, Accuracy: 0.8487, F1 Micro: 0.2747, F1 Macro: 0.0919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3557, Accuracy: 0.8622, F1 Micro: 0.4166, F1 Macro: 0.1524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3336, Accuracy: 0.8704, F1 Micro: 0.4692, F1 Macro: 0.2104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3008, Accuracy: 0.8767, F1 Micro: 0.5572, F1 Macro: 0.2597\n",
      "Epoch 10/10, Train Loss: 0.2662, Accuracy: 0.8768, F1 Micro: 0.5415, F1 Macro: 0.2556\n",
      "Model 1 - Iteration 658: Accuracy: 0.8767, F1 Micro: 0.5572, F1 Macro: 0.2597\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.69      0.74      1134\n",
      "      Abusive       0.80      0.73      0.77       992\n",
      "HS_Individual       0.66      0.44      0.53       732\n",
      "     HS_Group       0.25      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.53      0.59       762\n",
      "      HS_Weak       0.62      0.38      0.47       689\n",
      "  HS_Moderate       0.11      0.01      0.01       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.73      0.45      0.56      5556\n",
      "    macro avg       0.33      0.23      0.26      5556\n",
      " weighted avg       0.59      0.45      0.50      5556\n",
      "  samples avg       0.38      0.27      0.28      5556\n",
      "\n",
      "Training completed in 50.15723991394043 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5875, Accuracy: 0.8215, F1 Micro: 0.2912, F1 Macro: 0.1065\n",
      "Epoch 2/10, Train Loss: 0.4609, Accuracy: 0.829, F1 Micro: 0.0373, F1 Macro: 0.0153\n",
      "Epoch 3/10, Train Loss: 0.3929, Accuracy: 0.8345, F1 Micro: 0.1121, F1 Macro: 0.041\n",
      "Epoch 4/10, Train Loss: 0.3973, Accuracy: 0.838, F1 Micro: 0.1566, F1 Macro: 0.0553\n",
      "Epoch 5/10, Train Loss: 0.381, Accuracy: 0.8457, F1 Micro: 0.2304, F1 Macro: 0.0814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3626, Accuracy: 0.8547, F1 Micro: 0.3324, F1 Macro: 0.1095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3452, Accuracy: 0.8677, F1 Micro: 0.4694, F1 Macro: 0.1905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3219, Accuracy: 0.872, F1 Micro: 0.4893, F1 Macro: 0.2195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2911, Accuracy: 0.8765, F1 Micro: 0.5515, F1 Macro: 0.2656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2588, Accuracy: 0.8769, F1 Micro: 0.5702, F1 Macro: 0.2909\n",
      "Model 2 - Iteration 658: Accuracy: 0.8769, F1 Micro: 0.5702, F1 Macro: 0.2909\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.71      0.75      1134\n",
      "      Abusive       0.84      0.69      0.76       992\n",
      "HS_Individual       0.64      0.48      0.55       732\n",
      "     HS_Group       0.55      0.15      0.23       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.55      0.59       762\n",
      "      HS_Weak       0.62      0.42      0.50       689\n",
      "  HS_Moderate       0.32      0.07      0.11       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.71      0.47      0.57      5556\n",
      "    macro avg       0.37      0.26      0.29      5556\n",
      " weighted avg       0.62      0.47      0.53      5556\n",
      "  samples avg       0.35      0.27      0.28      5556\n",
      "\n",
      "Training completed in 49.77457785606384 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5819, Accuracy: 0.8291, F1 Micro: 0.0282, F1 Macro: 0.0117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4862, Accuracy: 0.8332, F1 Micro: 0.1093, F1 Macro: 0.0373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4155, Accuracy: 0.8362, F1 Micro: 0.1859, F1 Macro: 0.0522\n",
      "Epoch 4/10, Train Loss: 0.4114, Accuracy: 0.8302, F1 Micro: 0.0304, F1 Macro: 0.0132\n",
      "Epoch 5/10, Train Loss: 0.3932, Accuracy: 0.8364, F1 Micro: 0.1256, F1 Macro: 0.0443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3778, Accuracy: 0.8429, F1 Micro: 0.1953, F1 Macro: 0.07\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3695, Accuracy: 0.8553, F1 Micro: 0.335, F1 Macro: 0.1159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.348, Accuracy: 0.8654, F1 Micro: 0.4275, F1 Macro: 0.1796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3138, Accuracy: 0.874, F1 Micro: 0.5643, F1 Macro: 0.2599\n",
      "Epoch 10/10, Train Loss: 0.2796, Accuracy: 0.8757, F1 Micro: 0.5374, F1 Macro: 0.2532\n",
      "Model 3 - Iteration 658: Accuracy: 0.874, F1 Micro: 0.5643, F1 Macro: 0.2599\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.77      0.76      1134\n",
      "      Abusive       0.76      0.70      0.73       992\n",
      "HS_Individual       0.63      0.48      0.54       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.58      0.60       762\n",
      "      HS_Weak       0.63      0.40      0.49       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.47      0.56      5556\n",
      "    macro avg       0.28      0.24      0.26      5556\n",
      " weighted avg       0.54      0.47      0.50      5556\n",
      "  samples avg       0.37      0.27      0.29      5556\n",
      "\n",
      "Training completed in 52.141016244888306 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8758, F1 Micro: 0.5639, F1 Macro: 0.2702\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 122.20028042793274 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5364, Accuracy: 0.8315, F1 Micro: 0.0679, F1 Macro: 0.0261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4102, Accuracy: 0.8399, F1 Micro: 0.1751, F1 Macro: 0.0627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3756, Accuracy: 0.8564, F1 Micro: 0.3932, F1 Macro: 0.1258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3429, Accuracy: 0.8767, F1 Micro: 0.5399, F1 Macro: 0.2497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3049, Accuracy: 0.8803, F1 Micro: 0.5918, F1 Macro: 0.2748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.274, Accuracy: 0.884, F1 Micro: 0.6128, F1 Macro: 0.3013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.252, Accuracy: 0.8888, F1 Micro: 0.6431, F1 Macro: 0.3485\n",
      "Epoch 8/10, Train Loss: 0.2211, Accuracy: 0.8883, F1 Micro: 0.6315, F1 Macro: 0.3892\n",
      "Epoch 9/10, Train Loss: 0.1976, Accuracy: 0.8898, F1 Micro: 0.6342, F1 Macro: 0.4009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1851, Accuracy: 0.8938, F1 Micro: 0.6658, F1 Macro: 0.4436\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8938, F1 Micro: 0.6658, F1 Macro: 0.4436\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.79      0.80      1134\n",
      "      Abusive       0.86      0.83      0.84       992\n",
      "HS_Individual       0.68      0.56      0.62       732\n",
      "     HS_Group       0.63      0.53      0.58       402\n",
      "  HS_Religion       0.57      0.28      0.38       157\n",
      "      HS_Race       0.78      0.21      0.33       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.64      0.66       762\n",
      "      HS_Weak       0.63      0.55      0.59       689\n",
      "  HS_Moderate       0.47      0.42      0.45       331\n",
      "    HS_Strong       1.00      0.04      0.08       114\n",
      "\n",
      "    micro avg       0.73      0.61      0.67      5556\n",
      "    macro avg       0.60      0.40      0.44      5556\n",
      " weighted avg       0.71      0.61      0.65      5556\n",
      "  samples avg       0.38      0.35      0.34      5556\n",
      "\n",
      "Training completed in 67.35543441772461 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5171, Accuracy: 0.8339, F1 Micro: 0.1052, F1 Macro: 0.0405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4034, Accuracy: 0.8445, F1 Micro: 0.2297, F1 Macro: 0.0806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3682, Accuracy: 0.8609, F1 Micro: 0.4458, F1 Macro: 0.1623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3345, Accuracy: 0.8748, F1 Micro: 0.5056, F1 Macro: 0.2329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2963, Accuracy: 0.8795, F1 Micro: 0.5986, F1 Macro: 0.2859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2692, Accuracy: 0.8843, F1 Micro: 0.6038, F1 Macro: 0.3112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2473, Accuracy: 0.8898, F1 Micro: 0.642, F1 Macro: 0.3724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2162, Accuracy: 0.8904, F1 Micro: 0.6435, F1 Macro: 0.4149\n",
      "Epoch 9/10, Train Loss: 0.1893, Accuracy: 0.8896, F1 Micro: 0.6335, F1 Macro: 0.3772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1825, Accuracy: 0.8933, F1 Micro: 0.6735, F1 Macro: 0.4634\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8933, F1 Micro: 0.6735, F1 Macro: 0.4634\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.81      0.80      1134\n",
      "      Abusive       0.84      0.81      0.83       992\n",
      "HS_Individual       0.68      0.59      0.64       732\n",
      "     HS_Group       0.60      0.57      0.59       402\n",
      "  HS_Religion       0.63      0.17      0.26       157\n",
      "      HS_Race       0.79      0.31      0.44       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.72      0.69       762\n",
      "      HS_Weak       0.64      0.54      0.59       689\n",
      "  HS_Moderate       0.47      0.47      0.47       331\n",
      "    HS_Strong       0.89      0.15      0.26       114\n",
      "\n",
      "    micro avg       0.71      0.64      0.67      5556\n",
      "    macro avg       0.58      0.43      0.46      5556\n",
      " weighted avg       0.70      0.64      0.66      5556\n",
      "  samples avg       0.39      0.36      0.35      5556\n",
      "\n",
      "Training completed in 69.42804503440857 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5326, Accuracy: 0.8352, F1 Micro: 0.1926, F1 Macro: 0.0525\n",
      "Epoch 2/10, Train Loss: 0.4218, Accuracy: 0.8321, F1 Micro: 0.0581, F1 Macro: 0.0241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3833, Accuracy: 0.8508, F1 Micro: 0.3072, F1 Macro: 0.1035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3556, Accuracy: 0.8701, F1 Micro: 0.4812, F1 Macro: 0.2161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3143, Accuracy: 0.8797, F1 Micro: 0.5662, F1 Macro: 0.2643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2806, Accuracy: 0.8836, F1 Micro: 0.6126, F1 Macro: 0.3072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2569, Accuracy: 0.8861, F1 Micro: 0.6398, F1 Macro: 0.3608\n",
      "Epoch 8/10, Train Loss: 0.2228, Accuracy: 0.8859, F1 Micro: 0.6187, F1 Macro: 0.3885\n",
      "Epoch 9/10, Train Loss: 0.199, Accuracy: 0.8886, F1 Micro: 0.6317, F1 Macro: 0.4036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1865, Accuracy: 0.8905, F1 Micro: 0.6699, F1 Macro: 0.4595\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8905, F1 Micro: 0.6699, F1 Macro: 0.4595\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.82      0.80      1134\n",
      "      Abusive       0.83      0.81      0.82       992\n",
      "HS_Individual       0.66      0.62      0.64       732\n",
      "     HS_Group       0.58      0.57      0.58       402\n",
      "  HS_Religion       0.59      0.33      0.42       157\n",
      "      HS_Race       0.82      0.26      0.39       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.72      0.69       762\n",
      "      HS_Weak       0.63      0.56      0.59       689\n",
      "  HS_Moderate       0.44      0.43      0.43       331\n",
      "    HS_Strong       1.00      0.08      0.15       114\n",
      "\n",
      "    micro avg       0.70      0.65      0.67      5556\n",
      "    macro avg       0.58      0.43      0.46      5556\n",
      " weighted avg       0.69      0.65      0.65      5556\n",
      "  samples avg       0.38      0.36      0.35      5556\n",
      "\n",
      "Training completed in 66.22734355926514 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8842, F1 Micro: 0.6168, F1 Macro: 0.3628\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 109.15748929977417 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.489, Accuracy: 0.8372, F1 Micro: 0.1791, F1 Macro: 0.0559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3811, Accuracy: 0.8519, F1 Micro: 0.3044, F1 Macro: 0.1042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3317, Accuracy: 0.8743, F1 Micro: 0.4952, F1 Macro: 0.2321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2989, Accuracy: 0.8833, F1 Micro: 0.571, F1 Macro: 0.2706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2551, Accuracy: 0.8909, F1 Micro: 0.6479, F1 Macro: 0.3513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2346, Accuracy: 0.8954, F1 Micro: 0.6593, F1 Macro: 0.4079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2074, Accuracy: 0.8979, F1 Micro: 0.6862, F1 Macro: 0.4728\n",
      "Epoch 8/10, Train Loss: 0.1793, Accuracy: 0.8992, F1 Micro: 0.6848, F1 Macro: 0.4568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1622, Accuracy: 0.8995, F1 Micro: 0.7003, F1 Macro: 0.5213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1418, Accuracy: 0.9008, F1 Micro: 0.7083, F1 Macro: 0.5217\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9008, F1 Micro: 0.7083, F1 Macro: 0.5217\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1134\n",
      "      Abusive       0.82      0.87      0.85       992\n",
      "HS_Individual       0.66      0.69      0.67       732\n",
      "     HS_Group       0.69      0.57      0.62       402\n",
      "  HS_Religion       0.72      0.36      0.48       157\n",
      "      HS_Race       0.81      0.54      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.75      0.71       762\n",
      "      HS_Weak       0.62      0.67      0.64       689\n",
      "  HS_Moderate       0.55      0.48      0.51       331\n",
      "    HS_Strong       0.95      0.18      0.31       114\n",
      "\n",
      "    micro avg       0.72      0.70      0.71      5556\n",
      "    macro avg       0.61      0.50      0.52      5556\n",
      " weighted avg       0.70      0.70      0.69      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 83.85480332374573 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4755, Accuracy: 0.8429, F1 Micro: 0.2357, F1 Macro: 0.079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3769, Accuracy: 0.8532, F1 Micro: 0.3205, F1 Macro: 0.1095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3301, Accuracy: 0.8788, F1 Micro: 0.5501, F1 Macro: 0.2568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2933, Accuracy: 0.8864, F1 Micro: 0.6208, F1 Macro: 0.3174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2523, Accuracy: 0.8943, F1 Micro: 0.651, F1 Macro: 0.3925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2301, Accuracy: 0.8978, F1 Micro: 0.6673, F1 Macro: 0.4714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2052, Accuracy: 0.9, F1 Micro: 0.6907, F1 Macro: 0.4864\n",
      "Epoch 8/10, Train Loss: 0.1768, Accuracy: 0.901, F1 Micro: 0.6868, F1 Macro: 0.4913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1597, Accuracy: 0.9034, F1 Micro: 0.6977, F1 Macro: 0.5131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1383, Accuracy: 0.9023, F1 Micro: 0.7007, F1 Macro: 0.5142\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9023, F1 Micro: 0.7007, F1 Macro: 0.5142\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.80      0.81      1134\n",
      "      Abusive       0.85      0.85      0.85       992\n",
      "HS_Individual       0.67      0.68      0.67       732\n",
      "     HS_Group       0.76      0.52      0.62       402\n",
      "  HS_Religion       0.69      0.27      0.39       157\n",
      "      HS_Race       0.85      0.38      0.52       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.70      0.70       762\n",
      "      HS_Weak       0.63      0.65      0.64       689\n",
      "  HS_Moderate       0.58      0.41      0.48       331\n",
      "    HS_Strong       0.95      0.34      0.50       114\n",
      "\n",
      "    micro avg       0.74      0.66      0.70      5556\n",
      "    macro avg       0.62      0.47      0.51      5556\n",
      " weighted avg       0.73      0.66      0.69      5556\n",
      "  samples avg       0.40      0.37      0.36      5556\n",
      "\n",
      "Training completed in 83.05799007415771 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4907, Accuracy: 0.8349, F1 Micro: 0.1309, F1 Macro: 0.043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3878, Accuracy: 0.8498, F1 Micro: 0.2916, F1 Macro: 0.1008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3424, Accuracy: 0.8779, F1 Micro: 0.555, F1 Macro: 0.256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3011, Accuracy: 0.8841, F1 Micro: 0.5968, F1 Macro: 0.2925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2586, Accuracy: 0.8905, F1 Micro: 0.6331, F1 Macro: 0.3527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2345, Accuracy: 0.8961, F1 Micro: 0.668, F1 Macro: 0.4343\n",
      "Epoch 7/10, Train Loss: 0.2056, Accuracy: 0.8972, F1 Micro: 0.6551, F1 Macro: 0.4573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.179, Accuracy: 0.8977, F1 Micro: 0.6881, F1 Macro: 0.4684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1587, Accuracy: 0.899, F1 Micro: 0.7066, F1 Macro: 0.5319\n",
      "Epoch 10/10, Train Loss: 0.1402, Accuracy: 0.9001, F1 Micro: 0.7027, F1 Macro: 0.5284\n",
      "Model 3 - Iteration 2535: Accuracy: 0.899, F1 Micro: 0.7066, F1 Macro: 0.5319\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.85      0.83      1134\n",
      "      Abusive       0.82      0.88      0.85       992\n",
      "HS_Individual       0.66      0.67      0.66       732\n",
      "     HS_Group       0.62      0.62      0.62       402\n",
      "  HS_Religion       0.65      0.45      0.53       157\n",
      "      HS_Race       0.83      0.53      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.73      0.70       762\n",
      "      HS_Weak       0.63      0.65      0.64       689\n",
      "  HS_Moderate       0.48      0.52      0.50       331\n",
      "    HS_Strong       0.91      0.26      0.41       114\n",
      "\n",
      "    micro avg       0.71      0.71      0.71      5556\n",
      "    macro avg       0.59      0.51      0.53      5556\n",
      " weighted avg       0.69      0.71      0.69      5556\n",
      "  samples avg       0.40      0.39      0.38      5556\n",
      "\n",
      "Training completed in 82.3316171169281 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8897, F1 Micro: 0.6463, F1 Macro: 0.4161\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 97.8678867816925 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.464, Accuracy: 0.8374, F1 Micro: 0.152, F1 Macro: 0.052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3556, Accuracy: 0.8798, F1 Micro: 0.5712, F1 Macro: 0.2647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3029, Accuracy: 0.888, F1 Micro: 0.6227, F1 Macro: 0.302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2607, Accuracy: 0.8936, F1 Micro: 0.6586, F1 Macro: 0.3881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2313, Accuracy: 0.8979, F1 Micro: 0.6873, F1 Macro: 0.4301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2018, Accuracy: 0.9022, F1 Micro: 0.7047, F1 Macro: 0.5066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1676, Accuracy: 0.9029, F1 Micro: 0.712, F1 Macro: 0.5194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.156, Accuracy: 0.9039, F1 Micro: 0.7164, F1 Macro: 0.5414\n",
      "Epoch 9/10, Train Loss: 0.1305, Accuracy: 0.905, F1 Micro: 0.7064, F1 Macro: 0.5346\n",
      "Epoch 10/10, Train Loss: 0.1111, Accuracy: 0.9051, F1 Micro: 0.6936, F1 Macro: 0.5259\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9039, F1 Micro: 0.7164, F1 Macro: 0.5414\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1134\n",
      "      Abusive       0.87      0.85      0.86       992\n",
      "HS_Individual       0.68      0.68      0.68       732\n",
      "     HS_Group       0.64      0.61      0.63       402\n",
      "  HS_Religion       0.70      0.50      0.58       157\n",
      "      HS_Race       0.71      0.58      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.76      0.72       762\n",
      "      HS_Weak       0.64      0.64      0.64       689\n",
      "  HS_Moderate       0.51      0.52      0.51       331\n",
      "    HS_Strong       0.86      0.26      0.40       114\n",
      "\n",
      "    micro avg       0.73      0.70      0.72      5556\n",
      "    macro avg       0.59      0.52      0.54      5556\n",
      " weighted avg       0.71      0.70      0.71      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 95.84492015838623 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4522, Accuracy: 0.8423, F1 Micro: 0.1987, F1 Macro: 0.0715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3529, Accuracy: 0.8786, F1 Micro: 0.5734, F1 Macro: 0.2636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2987, Accuracy: 0.8874, F1 Micro: 0.6186, F1 Macro: 0.3334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.256, Accuracy: 0.8955, F1 Micro: 0.6614, F1 Macro: 0.4158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2294, Accuracy: 0.8989, F1 Micro: 0.6883, F1 Macro: 0.4422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2021, Accuracy: 0.9023, F1 Micro: 0.711, F1 Macro: 0.5191\n",
      "Epoch 7/10, Train Loss: 0.1654, Accuracy: 0.9046, F1 Micro: 0.7001, F1 Macro: 0.5278\n",
      "Epoch 8/10, Train Loss: 0.159, Accuracy: 0.9052, F1 Micro: 0.704, F1 Macro: 0.5462\n",
      "Epoch 9/10, Train Loss: 0.1302, Accuracy: 0.9061, F1 Micro: 0.7028, F1 Macro: 0.5298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1104, Accuracy: 0.9073, F1 Micro: 0.7181, F1 Macro: 0.5583\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9073, F1 Micro: 0.7181, F1 Macro: 0.5583\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.81      0.82      1134\n",
      "      Abusive       0.86      0.84      0.85       992\n",
      "HS_Individual       0.70      0.67      0.69       732\n",
      "     HS_Group       0.71      0.56      0.63       402\n",
      "  HS_Religion       0.68      0.48      0.57       157\n",
      "      HS_Race       0.77      0.53      0.62       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.72      0.72       762\n",
      "      HS_Weak       0.67      0.64      0.65       689\n",
      "  HS_Moderate       0.54      0.48      0.51       331\n",
      "    HS_Strong       0.93      0.49      0.64       114\n",
      "\n",
      "    micro avg       0.75      0.69      0.72      5556\n",
      "    macro avg       0.62      0.52      0.56      5556\n",
      " weighted avg       0.74      0.69      0.71      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 94.5787742137909 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4679, Accuracy: 0.8327, F1 Micro: 0.0696, F1 Macro: 0.0274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.369, Accuracy: 0.8667, F1 Micro: 0.4299, F1 Macro: 0.1784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3108, Accuracy: 0.8837, F1 Micro: 0.5907, F1 Macro: 0.2886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2636, Accuracy: 0.8911, F1 Micro: 0.6496, F1 Macro: 0.3783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2342, Accuracy: 0.8968, F1 Micro: 0.6791, F1 Macro: 0.4306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.206, Accuracy: 0.9007, F1 Micro: 0.7083, F1 Macro: 0.5137\n",
      "Epoch 7/10, Train Loss: 0.1699, Accuracy: 0.9031, F1 Micro: 0.7058, F1 Macro: 0.5233\n",
      "Epoch 8/10, Train Loss: 0.1566, Accuracy: 0.905, F1 Micro: 0.7067, F1 Macro: 0.5443\n",
      "Epoch 9/10, Train Loss: 0.1319, Accuracy: 0.9033, F1 Micro: 0.7001, F1 Macro: 0.5414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1119, Accuracy: 0.9059, F1 Micro: 0.7127, F1 Macro: 0.5578\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9059, F1 Micro: 0.7127, F1 Macro: 0.5578\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.81      0.82      1134\n",
      "      Abusive       0.85      0.85      0.85       992\n",
      "HS_Individual       0.72      0.61      0.66       732\n",
      "     HS_Group       0.63      0.61      0.62       402\n",
      "  HS_Religion       0.73      0.49      0.59       157\n",
      "      HS_Race       0.77      0.58      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.72      0.72       762\n",
      "      HS_Weak       0.70      0.57      0.63       689\n",
      "  HS_Moderate       0.48      0.52      0.50       331\n",
      "    HS_Strong       0.92      0.48      0.63       114\n",
      "\n",
      "    micro avg       0.75      0.68      0.71      5556\n",
      "    macro avg       0.61      0.52      0.56      5556\n",
      " weighted avg       0.74      0.68      0.70      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 94.1794683933258 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8937, F1 Micro: 0.6637, F1 Macro: 0.4502\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 87.63161206245422 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4579, Accuracy: 0.84, F1 Micro: 0.1828, F1 Macro: 0.0621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3452, Accuracy: 0.8778, F1 Micro: 0.5319, F1 Macro: 0.2426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2868, Accuracy: 0.8917, F1 Micro: 0.658, F1 Macro: 0.3719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2426, Accuracy: 0.9005, F1 Micro: 0.6852, F1 Macro: 0.4517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2196, Accuracy: 0.9024, F1 Micro: 0.7119, F1 Macro: 0.5169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1877, Accuracy: 0.9054, F1 Micro: 0.7267, F1 Macro: 0.5376\n",
      "Epoch 7/10, Train Loss: 0.1574, Accuracy: 0.9012, F1 Micro: 0.7259, F1 Macro: 0.5513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1446, Accuracy: 0.9071, F1 Micro: 0.7297, F1 Macro: 0.5625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1222, Accuracy: 0.9042, F1 Micro: 0.7318, F1 Macro: 0.5677\n",
      "Epoch 10/10, Train Loss: 0.1006, Accuracy: 0.9109, F1 Micro: 0.7296, F1 Macro: 0.5731\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9042, F1 Micro: 0.7318, F1 Macro: 0.5677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.89      0.83      1134\n",
      "      Abusive       0.83      0.90      0.87       992\n",
      "HS_Individual       0.66      0.76      0.70       732\n",
      "     HS_Group       0.65      0.60      0.63       402\n",
      "  HS_Religion       0.63      0.58      0.60       157\n",
      "      HS_Race       0.71      0.58      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.82      0.74       762\n",
      "      HS_Weak       0.63      0.73      0.68       689\n",
      "  HS_Moderate       0.51      0.52      0.51       331\n",
      "    HS_Strong       0.89      0.47      0.62       114\n",
      "\n",
      "    micro avg       0.71      0.76      0.73      5556\n",
      "    macro avg       0.58      0.57      0.57      5556\n",
      " weighted avg       0.69      0.76      0.72      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 107.05462694168091 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4468, Accuracy: 0.8512, F1 Micro: 0.3153, F1 Macro: 0.1029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3405, Accuracy: 0.8773, F1 Micro: 0.5194, F1 Macro: 0.238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.285, Accuracy: 0.8937, F1 Micro: 0.6624, F1 Macro: 0.4185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2405, Accuracy: 0.9012, F1 Micro: 0.6903, F1 Macro: 0.4633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2162, Accuracy: 0.902, F1 Micro: 0.7116, F1 Macro: 0.5239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1878, Accuracy: 0.9054, F1 Micro: 0.7186, F1 Macro: 0.5136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1604, Accuracy: 0.9096, F1 Micro: 0.722, F1 Macro: 0.562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1414, Accuracy: 0.9103, F1 Micro: 0.7322, F1 Macro: 0.5761\n",
      "Epoch 9/10, Train Loss: 0.1195, Accuracy: 0.9116, F1 Micro: 0.7301, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0999, Accuracy: 0.9124, F1 Micro: 0.7358, F1 Macro: 0.5889\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9124, F1 Micro: 0.7358, F1 Macro: 0.5889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.81      0.82      1134\n",
      "      Abusive       0.86      0.87      0.86       992\n",
      "HS_Individual       0.71      0.69      0.70       732\n",
      "     HS_Group       0.71      0.60      0.65       402\n",
      "  HS_Religion       0.70      0.56      0.62       157\n",
      "      HS_Race       0.80      0.62      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.76      0.73      0.74       762\n",
      "      HS_Weak       0.69      0.65      0.67       689\n",
      "  HS_Moderate       0.57      0.52      0.54       331\n",
      "    HS_Strong       0.94      0.63      0.75       114\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5556\n",
      "    macro avg       0.63      0.56      0.59      5556\n",
      " weighted avg       0.75      0.71      0.73      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 108.29006099700928 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4617, Accuracy: 0.8409, F1 Micro: 0.2034, F1 Macro: 0.0669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3533, Accuracy: 0.8737, F1 Micro: 0.4907, F1 Macro: 0.2185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2909, Accuracy: 0.8912, F1 Micro: 0.651, F1 Macro: 0.3872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2438, Accuracy: 0.9003, F1 Micro: 0.6892, F1 Macro: 0.4705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2194, Accuracy: 0.9033, F1 Micro: 0.7116, F1 Macro: 0.5132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1883, Accuracy: 0.9039, F1 Micro: 0.7206, F1 Macro: 0.5249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1585, Accuracy: 0.9064, F1 Micro: 0.7334, F1 Macro: 0.5736\n",
      "Epoch 8/10, Train Loss: 0.141, Accuracy: 0.9079, F1 Micro: 0.7262, F1 Macro: 0.5741\n",
      "Epoch 9/10, Train Loss: 0.1217, Accuracy: 0.9054, F1 Micro: 0.7261, F1 Macro: 0.5749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1003, Accuracy: 0.9085, F1 Micro: 0.7351, F1 Macro: 0.5808\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9085, F1 Micro: 0.7351, F1 Macro: 0.5808\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1134\n",
      "      Abusive       0.82      0.91      0.86       992\n",
      "HS_Individual       0.67      0.74      0.70       732\n",
      "     HS_Group       0.71      0.59      0.65       402\n",
      "  HS_Religion       0.66      0.57      0.61       157\n",
      "      HS_Race       0.74      0.62      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.73      0.74      0.73       762\n",
      "      HS_Weak       0.65      0.73      0.69       689\n",
      "  HS_Moderate       0.55      0.51      0.53       331\n",
      "    HS_Strong       0.92      0.51      0.66       114\n",
      "\n",
      "    micro avg       0.73      0.74      0.74      5556\n",
      "    macro avg       0.69      0.57      0.58      5556\n",
      " weighted avg       0.73      0.74      0.72      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 106.05558395385742 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.8966, F1 Micro: 0.6778, F1 Macro: 0.476\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 79.17708945274353 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4466, Accuracy: 0.8511, F1 Micro: 0.3108, F1 Macro: 0.1018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3358, Accuracy: 0.8834, F1 Micro: 0.5912, F1 Macro: 0.28\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2744, Accuracy: 0.8952, F1 Micro: 0.6873, F1 Macro: 0.4415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.237, Accuracy: 0.904, F1 Micro: 0.7004, F1 Macro: 0.4999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2063, Accuracy: 0.9082, F1 Micro: 0.7251, F1 Macro: 0.5464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.181, Accuracy: 0.9095, F1 Micro: 0.7253, F1 Macro: 0.5289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1509, Accuracy: 0.9101, F1 Micro: 0.7326, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1302, Accuracy: 0.9152, F1 Micro: 0.7454, F1 Macro: 0.6017\n",
      "Epoch 9/10, Train Loss: 0.1091, Accuracy: 0.9157, F1 Micro: 0.7451, F1 Macro: 0.608\n",
      "Epoch 10/10, Train Loss: 0.0942, Accuracy: 0.9118, F1 Micro: 0.7436, F1 Macro: 0.6268\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9152, F1 Micro: 0.7454, F1 Macro: 0.6017\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1134\n",
      "      Abusive       0.85      0.89      0.87       992\n",
      "HS_Individual       0.74      0.68      0.71       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.72      0.62      0.66       157\n",
      "      HS_Race       0.72      0.68      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.76      0.72      0.74       762\n",
      "      HS_Weak       0.71      0.66      0.68       689\n",
      "  HS_Moderate       0.56      0.57      0.57       331\n",
      "    HS_Strong       0.88      0.61      0.72       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.75      5556\n",
      "    macro avg       0.75      0.58      0.60      5556\n",
      " weighted avg       0.77      0.72      0.74      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 117.29825687408447 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4364, Accuracy: 0.8531, F1 Micro: 0.3186, F1 Macro: 0.1059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3299, Accuracy: 0.8837, F1 Micro: 0.5803, F1 Macro: 0.2963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2716, Accuracy: 0.8938, F1 Micro: 0.6926, F1 Macro: 0.4621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2368, Accuracy: 0.9058, F1 Micro: 0.7034, F1 Macro: 0.5106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2051, Accuracy: 0.9075, F1 Micro: 0.7134, F1 Macro: 0.5469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1797, Accuracy: 0.9109, F1 Micro: 0.7287, F1 Macro: 0.5559\n",
      "Epoch 7/10, Train Loss: 0.1505, Accuracy: 0.9115, F1 Micro: 0.7252, F1 Macro: 0.565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1333, Accuracy: 0.9131, F1 Micro: 0.7401, F1 Macro: 0.5831\n",
      "Epoch 9/10, Train Loss: 0.1098, Accuracy: 0.9119, F1 Micro: 0.7276, F1 Macro: 0.5866\n",
      "Epoch 10/10, Train Loss: 0.0958, Accuracy: 0.9121, F1 Micro: 0.7387, F1 Macro: 0.6049\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9131, F1 Micro: 0.7401, F1 Macro: 0.5831\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.84      0.90      0.87       992\n",
      "HS_Individual       0.69      0.72      0.71       732\n",
      "     HS_Group       0.76      0.56      0.64       402\n",
      "  HS_Religion       0.73      0.51      0.60       157\n",
      "      HS_Race       0.73      0.59      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.75      0.73      0.74       762\n",
      "      HS_Weak       0.67      0.70      0.69       689\n",
      "  HS_Moderate       0.61      0.48      0.54       331\n",
      "    HS_Strong       0.92      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.63      0.55      0.58      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 116.04589748382568 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4506, Accuracy: 0.8501, F1 Micro: 0.2897, F1 Macro: 0.0975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3429, Accuracy: 0.8831, F1 Micro: 0.5976, F1 Macro: 0.2915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2779, Accuracy: 0.892, F1 Micro: 0.6864, F1 Macro: 0.4539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2404, Accuracy: 0.9039, F1 Micro: 0.6926, F1 Macro: 0.5004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2049, Accuracy: 0.9075, F1 Micro: 0.7225, F1 Macro: 0.5525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1793, Accuracy: 0.9107, F1 Micro: 0.7269, F1 Macro: 0.5457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1498, Accuracy: 0.9114, F1 Micro: 0.7271, F1 Macro: 0.5707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1315, Accuracy: 0.9139, F1 Micro: 0.7364, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1102, Accuracy: 0.913, F1 Micro: 0.7391, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.097, Accuracy: 0.9134, F1 Micro: 0.7452, F1 Macro: 0.619\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9134, F1 Micro: 0.7452, F1 Macro: 0.619\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.83      1134\n",
      "      Abusive       0.86      0.89      0.87       992\n",
      "HS_Individual       0.72      0.69      0.70       732\n",
      "     HS_Group       0.65      0.65      0.65       402\n",
      "  HS_Religion       0.66      0.65      0.65       157\n",
      "      HS_Race       0.69      0.68      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.70      0.14      0.23        51\n",
      "     HS_Other       0.76      0.74      0.75       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.54      0.56      0.55       331\n",
      "    HS_Strong       0.86      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.66      0.61      0.62      5556\n",
      " weighted avg       0.74      0.74      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 119.87081217765808 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.8995, F1 Micro: 0.6887, F1 Macro: 0.4969\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 71.80175185203552 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4396, Accuracy: 0.8602, F1 Micro: 0.4253, F1 Macro: 0.1511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3255, Accuracy: 0.8867, F1 Micro: 0.6536, F1 Macro: 0.3457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2704, Accuracy: 0.9006, F1 Micro: 0.6874, F1 Macro: 0.4587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2278, Accuracy: 0.905, F1 Micro: 0.6913, F1 Macro: 0.4802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2, Accuracy: 0.909, F1 Micro: 0.7315, F1 Macro: 0.5557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1711, Accuracy: 0.9129, F1 Micro: 0.7401, F1 Macro: 0.5678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1467, Accuracy: 0.9139, F1 Micro: 0.7491, F1 Macro: 0.5895\n",
      "Epoch 8/10, Train Loss: 0.1194, Accuracy: 0.9147, F1 Micro: 0.7489, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.104, Accuracy: 0.9121, F1 Micro: 0.7501, F1 Macro: 0.6219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0947, Accuracy: 0.9165, F1 Micro: 0.7561, F1 Macro: 0.641\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9165, F1 Micro: 0.7561, F1 Macro: 0.641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.80      0.56      0.66       402\n",
      "  HS_Religion       0.74      0.49      0.59       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.62      0.20      0.30        51\n",
      "     HS_Other       0.73      0.77      0.75       762\n",
      "      HS_Weak       0.66      0.75      0.70       689\n",
      "  HS_Moderate       0.70      0.47      0.56       331\n",
      "    HS_Strong       0.86      0.77      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.76      0.61      0.64      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 128.9870617389679 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.43, Accuracy: 0.8624, F1 Micro: 0.4593, F1 Macro: 0.175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.32, Accuracy: 0.8895, F1 Micro: 0.6589, F1 Macro: 0.3805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.266, Accuracy: 0.9023, F1 Micro: 0.6892, F1 Macro: 0.4884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2264, Accuracy: 0.905, F1 Micro: 0.6916, F1 Macro: 0.4792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1979, Accuracy: 0.9114, F1 Micro: 0.7282, F1 Macro: 0.5535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1668, Accuracy: 0.9151, F1 Micro: 0.7389, F1 Macro: 0.5808\n",
      "Epoch 7/10, Train Loss: 0.1428, Accuracy: 0.9132, F1 Micro: 0.732, F1 Macro: 0.5776\n",
      "Epoch 8/10, Train Loss: 0.1201, Accuracy: 0.9161, F1 Micro: 0.7382, F1 Macro: 0.5999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0996, Accuracy: 0.9119, F1 Micro: 0.7449, F1 Macro: 0.6041\n",
      "Epoch 10/10, Train Loss: 0.0891, Accuracy: 0.9131, F1 Micro: 0.7389, F1 Macro: 0.6213\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9119, F1 Micro: 0.7449, F1 Macro: 0.6041\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.65      0.76      0.70       732\n",
      "     HS_Group       0.76      0.56      0.65       402\n",
      "  HS_Religion       0.71      0.48      0.57       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.75      0.06      0.11        51\n",
      "     HS_Other       0.71      0.78      0.74       762\n",
      "      HS_Weak       0.63      0.72      0.68       689\n",
      "  HS_Moderate       0.66      0.47      0.54       331\n",
      "    HS_Strong       0.84      0.80      0.82       114\n",
      "\n",
      "    micro avg       0.74      0.75      0.74      5556\n",
      "    macro avg       0.76      0.59      0.60      5556\n",
      " weighted avg       0.75      0.75      0.73      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 125.97240614891052 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4453, Accuracy: 0.8537, F1 Micro: 0.4033, F1 Macro: 0.1262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3328, Accuracy: 0.8876, F1 Micro: 0.6397, F1 Macro: 0.3238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2711, Accuracy: 0.9, F1 Micro: 0.6707, F1 Macro: 0.4388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2301, Accuracy: 0.9054, F1 Micro: 0.6951, F1 Macro: 0.4858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2006, Accuracy: 0.9103, F1 Micro: 0.726, F1 Macro: 0.5573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1683, Accuracy: 0.9109, F1 Micro: 0.7416, F1 Macro: 0.5787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1438, Accuracy: 0.9134, F1 Micro: 0.7456, F1 Macro: 0.599\n",
      "Epoch 8/10, Train Loss: 0.1166, Accuracy: 0.915, F1 Micro: 0.7343, F1 Macro: 0.5892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1019, Accuracy: 0.9128, F1 Micro: 0.7496, F1 Macro: 0.62\n",
      "Epoch 10/10, Train Loss: 0.0886, Accuracy: 0.9139, F1 Micro: 0.7487, F1 Macro: 0.6214\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9128, F1 Micro: 0.7496, F1 Macro: 0.62\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1134\n",
      "      Abusive       0.82      0.93      0.87       992\n",
      "HS_Individual       0.66      0.77      0.71       732\n",
      "     HS_Group       0.73      0.59      0.65       402\n",
      "  HS_Religion       0.72      0.50      0.59       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.50      0.14      0.22        51\n",
      "     HS_Other       0.72      0.78      0.75       762\n",
      "      HS_Weak       0.66      0.73      0.69       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.83      0.80      0.81       114\n",
      "\n",
      "    micro avg       0.74      0.76      0.75      5556\n",
      "    macro avg       0.74      0.60      0.62      5556\n",
      " weighted avg       0.74      0.76      0.74      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 127.12353801727295 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9015, F1 Micro: 0.6975, F1 Macro: 0.5147\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 64.65028357505798 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4316, Accuracy: 0.8687, F1 Micro: 0.5081, F1 Macro: 0.2035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3127, Accuracy: 0.8908, F1 Micro: 0.6469, F1 Macro: 0.3446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2596, Accuracy: 0.9006, F1 Micro: 0.7046, F1 Macro: 0.4822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2186, Accuracy: 0.911, F1 Micro: 0.7166, F1 Macro: 0.5149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1905, Accuracy: 0.9125, F1 Micro: 0.7187, F1 Macro: 0.5191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1611, Accuracy: 0.9144, F1 Micro: 0.7513, F1 Macro: 0.5914\n",
      "Epoch 7/10, Train Loss: 0.1354, Accuracy: 0.9156, F1 Micro: 0.749, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1162, Accuracy: 0.9159, F1 Micro: 0.7539, F1 Macro: 0.6482\n",
      "Epoch 9/10, Train Loss: 0.0952, Accuracy: 0.9149, F1 Micro: 0.749, F1 Macro: 0.6503\n",
      "Epoch 10/10, Train Loss: 0.0878, Accuracy: 0.9157, F1 Micro: 0.7507, F1 Macro: 0.6544\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9159, F1 Micro: 0.7539, F1 Macro: 0.6482\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.72      0.70      0.71       732\n",
      "     HS_Group       0.69      0.64      0.67       402\n",
      "  HS_Religion       0.71      0.59      0.65       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.44      0.22      0.29        51\n",
      "     HS_Other       0.75      0.75      0.75       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.56      0.56      0.56       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.75      5556\n",
      "    macro avg       0.72      0.62      0.65      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 135.13167023658752 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4226, Accuracy: 0.8665, F1 Micro: 0.5019, F1 Macro: 0.1951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3087, Accuracy: 0.8908, F1 Micro: 0.6639, F1 Macro: 0.3797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2545, Accuracy: 0.9022, F1 Micro: 0.7099, F1 Macro: 0.4977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2163, Accuracy: 0.9102, F1 Micro: 0.7162, F1 Macro: 0.529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9147, F1 Micro: 0.7387, F1 Macro: 0.5609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1594, Accuracy: 0.913, F1 Micro: 0.7488, F1 Macro: 0.583\n",
      "Epoch 7/10, Train Loss: 0.1331, Accuracy: 0.9158, F1 Micro: 0.7458, F1 Macro: 0.6113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1154, Accuracy: 0.9166, F1 Micro: 0.7535, F1 Macro: 0.6325\n",
      "Epoch 9/10, Train Loss: 0.0959, Accuracy: 0.9149, F1 Micro: 0.7413, F1 Macro: 0.6375\n",
      "Epoch 10/10, Train Loss: 0.0839, Accuracy: 0.9167, F1 Micro: 0.7363, F1 Macro: 0.6322\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9166, F1 Micro: 0.7535, F1 Macro: 0.6325\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.69      0.72      0.71       732\n",
      "     HS_Group       0.71      0.60      0.65       402\n",
      "  HS_Religion       0.71      0.57      0.63       157\n",
      "      HS_Race       0.78      0.64      0.70       120\n",
      "  HS_Physical       0.75      0.08      0.15        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.76      0.74      0.75       762\n",
      "      HS_Weak       0.69      0.70      0.70       689\n",
      "  HS_Moderate       0.61      0.52      0.56       331\n",
      "    HS_Strong       0.89      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.74      0.60      0.63      5556\n",
      " weighted avg       0.76      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 134.19105648994446 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4361, Accuracy: 0.8621, F1 Micro: 0.4922, F1 Macro: 0.1958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3179, Accuracy: 0.8894, F1 Micro: 0.653, F1 Macro: 0.3776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2592, Accuracy: 0.8996, F1 Micro: 0.7034, F1 Macro: 0.4991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2196, Accuracy: 0.9097, F1 Micro: 0.7123, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1907, Accuracy: 0.912, F1 Micro: 0.7202, F1 Macro: 0.5401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1614, Accuracy: 0.9144, F1 Micro: 0.7452, F1 Macro: 0.5881\n",
      "Epoch 7/10, Train Loss: 0.1359, Accuracy: 0.9136, F1 Micro: 0.7314, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1157, Accuracy: 0.9152, F1 Micro: 0.7491, F1 Macro: 0.6286\n",
      "Epoch 9/10, Train Loss: 0.0963, Accuracy: 0.9126, F1 Micro: 0.7478, F1 Macro: 0.6323\n",
      "Epoch 10/10, Train Loss: 0.0854, Accuracy: 0.9165, F1 Micro: 0.7468, F1 Macro: 0.6275\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9152, F1 Micro: 0.7491, F1 Macro: 0.6286\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.68      0.62      0.65       402\n",
      "  HS_Religion       0.72      0.55      0.62       157\n",
      "      HS_Race       0.80      0.63      0.71       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.43      0.18      0.25        51\n",
      "     HS_Other       0.76      0.73      0.75       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.55      0.53      0.54       331\n",
      "    HS_Strong       0.91      0.76      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5556\n",
      "    macro avg       0.73      0.60      0.63      5556\n",
      " weighted avg       0.76      0.73      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 134.6626751422882 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9033, F1 Micro: 0.7043, F1 Macro: 0.5299\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 58.38488006591797 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4212, Accuracy: 0.8742, F1 Micro: 0.5582, F1 Macro: 0.2462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2987, Accuracy: 0.8956, F1 Micro: 0.6513, F1 Macro: 0.3996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2517, Accuracy: 0.9015, F1 Micro: 0.7133, F1 Macro: 0.491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2106, Accuracy: 0.9104, F1 Micro: 0.7306, F1 Macro: 0.5534\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9129, F1 Micro: 0.7294, F1 Macro: 0.5405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1529, Accuracy: 0.9147, F1 Micro: 0.7354, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1327, Accuracy: 0.9177, F1 Micro: 0.7582, F1 Macro: 0.6405\n",
      "Epoch 8/10, Train Loss: 0.1109, Accuracy: 0.9166, F1 Micro: 0.7543, F1 Macro: 0.6284\n",
      "Epoch 9/10, Train Loss: 0.0944, Accuracy: 0.9194, F1 Micro: 0.7578, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0792, Accuracy: 0.917, F1 Micro: 0.7604, F1 Macro: 0.6726\n",
      "Model 1 - Iteration 6285: Accuracy: 0.917, F1 Micro: 0.7604, F1 Macro: 0.6726\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.68      0.67      0.67       402\n",
      "  HS_Religion       0.68      0.65      0.66       157\n",
      "      HS_Race       0.75      0.66      0.70       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.67      0.69      0.68       689\n",
      "  HS_Moderate       0.58      0.58      0.58       331\n",
      "    HS_Strong       0.85      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.66      0.67      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 142.51300811767578 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4117, Accuracy: 0.8749, F1 Micro: 0.546, F1 Macro: 0.2331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2934, Accuracy: 0.8969, F1 Micro: 0.6619, F1 Macro: 0.4246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2475, Accuracy: 0.9023, F1 Micro: 0.7172, F1 Macro: 0.5205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2074, Accuracy: 0.9118, F1 Micro: 0.7298, F1 Macro: 0.5678\n",
      "Epoch 5/10, Train Loss: 0.1766, Accuracy: 0.9138, F1 Micro: 0.7277, F1 Macro: 0.5477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1542, Accuracy: 0.916, F1 Micro: 0.7382, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1274, Accuracy: 0.9122, F1 Micro: 0.752, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1095, Accuracy: 0.9169, F1 Micro: 0.7535, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0943, Accuracy: 0.9168, F1 Micro: 0.7536, F1 Macro: 0.6257\n",
      "Epoch 10/10, Train Loss: 0.0776, Accuracy: 0.9158, F1 Micro: 0.7507, F1 Macro: 0.6552\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9168, F1 Micro: 0.7536, F1 Macro: 0.6257\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.68      0.74      0.71       732\n",
      "     HS_Group       0.76      0.56      0.64       402\n",
      "  HS_Religion       0.74      0.50      0.60       157\n",
      "      HS_Race       0.83      0.59      0.69       120\n",
      "  HS_Physical       1.00      0.07      0.13        72\n",
      "    HS_Gender       0.50      0.12      0.19        51\n",
      "     HS_Other       0.74      0.79      0.76       762\n",
      "      HS_Weak       0.66      0.72      0.69       689\n",
      "  HS_Moderate       0.66      0.48      0.55       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.76      0.59      0.63      5556\n",
      " weighted avg       0.77      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 144.29059600830078 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4268, Accuracy: 0.8667, F1 Micro: 0.5238, F1 Macro: 0.2236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.304, Accuracy: 0.8926, F1 Micro: 0.6337, F1 Macro: 0.3844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2538, Accuracy: 0.901, F1 Micro: 0.7144, F1 Macro: 0.518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2128, Accuracy: 0.91, F1 Micro: 0.7235, F1 Macro: 0.5423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.9156, F1 Micro: 0.7409, F1 Macro: 0.5697\n",
      "Epoch 6/10, Train Loss: 0.1522, Accuracy: 0.9136, F1 Micro: 0.729, F1 Macro: 0.5836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1311, Accuracy: 0.9115, F1 Micro: 0.7503, F1 Macro: 0.6216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1125, Accuracy: 0.9183, F1 Micro: 0.7533, F1 Macro: 0.6334\n",
      "Epoch 9/10, Train Loss: 0.093, Accuracy: 0.9168, F1 Micro: 0.7522, F1 Macro: 0.6414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0811, Accuracy: 0.9167, F1 Micro: 0.7554, F1 Macro: 0.645\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9167, F1 Micro: 0.7554, F1 Macro: 0.645\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.72      0.69      0.71       732\n",
      "     HS_Group       0.66      0.67      0.66       402\n",
      "  HS_Religion       0.69      0.61      0.65       157\n",
      "      HS_Race       0.71      0.64      0.68       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.54      0.27      0.36        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.71      0.66      0.68       689\n",
      "  HS_Moderate       0.57      0.60      0.58       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.74      0.62      0.64      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 144.00395107269287 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9048, F1 Micro: 0.7101, F1 Macro: 0.543\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 53.156776905059814 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4224, Accuracy: 0.8772, F1 Micro: 0.5371, F1 Macro: 0.2414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.294, Accuracy: 0.8955, F1 Micro: 0.6451, F1 Macro: 0.3651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2442, Accuracy: 0.9065, F1 Micro: 0.7096, F1 Macro: 0.517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2072, Accuracy: 0.9137, F1 Micro: 0.7309, F1 Macro: 0.5372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1803, Accuracy: 0.916, F1 Micro: 0.7524, F1 Macro: 0.5986\n",
      "Epoch 6/10, Train Loss: 0.1452, Accuracy: 0.9181, F1 Micro: 0.7472, F1 Macro: 0.6094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1181, Accuracy: 0.9188, F1 Micro: 0.7588, F1 Macro: 0.6464\n",
      "Epoch 8/10, Train Loss: 0.1053, Accuracy: 0.918, F1 Micro: 0.7561, F1 Macro: 0.6373\n",
      "Epoch 9/10, Train Loss: 0.0917, Accuracy: 0.9196, F1 Micro: 0.756, F1 Macro: 0.6614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.921, F1 Micro: 0.766, F1 Macro: 0.6799\n",
      "Model 1 - Iteration 6584: Accuracy: 0.921, F1 Micro: 0.766, F1 Macro: 0.6799\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.70      0.62      0.66       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.56      0.39      0.46        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.62      0.53      0.57       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.75      0.65      0.68      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 146.61953997612 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4137, Accuracy: 0.8728, F1 Micro: 0.4913, F1 Macro: 0.2126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2938, Accuracy: 0.8946, F1 Micro: 0.634, F1 Macro: 0.3621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2437, Accuracy: 0.9068, F1 Micro: 0.7021, F1 Macro: 0.5067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2067, Accuracy: 0.9137, F1 Micro: 0.737, F1 Macro: 0.5739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1808, Accuracy: 0.9167, F1 Micro: 0.7512, F1 Macro: 0.5954\n",
      "Epoch 6/10, Train Loss: 0.1476, Accuracy: 0.9164, F1 Micro: 0.7328, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.9187, F1 Micro: 0.7543, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1064, Accuracy: 0.92, F1 Micro: 0.761, F1 Macro: 0.6499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.9173, F1 Micro: 0.7636, F1 Macro: 0.6676\n",
      "Epoch 10/10, Train Loss: 0.0737, Accuracy: 0.92, F1 Micro: 0.7634, F1 Macro: 0.6736\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9173, F1 Micro: 0.7636, F1 Macro: 0.6676\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.65      0.65      0.65       402\n",
      "  HS_Religion       0.66      0.68      0.67       157\n",
      "      HS_Race       0.69      0.72      0.71       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.58      0.29      0.39        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.58      0.56      0.57       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.76      5556\n",
      "    macro avg       0.72      0.66      0.67      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 148.22992205619812 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.428, Accuracy: 0.8624, F1 Micro: 0.3876, F1 Macro: 0.157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3017, Accuracy: 0.8914, F1 Micro: 0.6148, F1 Macro: 0.3524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.9067, F1 Micro: 0.7034, F1 Macro: 0.5033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2099, Accuracy: 0.9114, F1 Micro: 0.7139, F1 Macro: 0.5283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1834, Accuracy: 0.9182, F1 Micro: 0.7491, F1 Macro: 0.5949\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9177, F1 Micro: 0.7407, F1 Macro: 0.606\n",
      "Epoch 7/10, Train Loss: 0.1207, Accuracy: 0.9175, F1 Micro: 0.7476, F1 Macro: 0.6184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1062, Accuracy: 0.9186, F1 Micro: 0.7636, F1 Macro: 0.6442\n",
      "Epoch 9/10, Train Loss: 0.093, Accuracy: 0.9126, F1 Micro: 0.7549, F1 Macro: 0.6499\n",
      "Epoch 10/10, Train Loss: 0.0776, Accuracy: 0.9183, F1 Micro: 0.763, F1 Macro: 0.6559\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9186, F1 Micro: 0.7636, F1 Macro: 0.6442\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.73      0.63      0.64      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 144.9113531112671 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9063, F1 Micro: 0.7156, F1 Macro: 0.5551\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 49.41858386993408 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4172, Accuracy: 0.877, F1 Micro: 0.5289, F1 Macro: 0.2412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2957, Accuracy: 0.8955, F1 Micro: 0.64, F1 Macro: 0.368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2483, Accuracy: 0.9044, F1 Micro: 0.7147, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2067, Accuracy: 0.9146, F1 Micro: 0.7324, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1759, Accuracy: 0.9172, F1 Micro: 0.7606, F1 Macro: 0.6094\n",
      "Epoch 6/10, Train Loss: 0.1461, Accuracy: 0.92, F1 Micro: 0.7575, F1 Macro: 0.6137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1208, Accuracy: 0.9162, F1 Micro: 0.7646, F1 Macro: 0.6477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1027, Accuracy: 0.9208, F1 Micro: 0.7657, F1 Macro: 0.6497\n",
      "Epoch 9/10, Train Loss: 0.0892, Accuracy: 0.9189, F1 Micro: 0.7586, F1 Macro: 0.652\n",
      "Epoch 10/10, Train Loss: 0.0787, Accuracy: 0.9182, F1 Micro: 0.7601, F1 Macro: 0.6743\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9208, F1 Micro: 0.7657, F1 Macro: 0.6497\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.78      0.48      0.60       157\n",
      "      HS_Race       0.84      0.68      0.75       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.56      0.27      0.37        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.78      0.61      0.65      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 153.20031571388245 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4088, Accuracy: 0.8739, F1 Micro: 0.5057, F1 Macro: 0.2247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2913, Accuracy: 0.8961, F1 Micro: 0.648, F1 Macro: 0.4299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2442, Accuracy: 0.9062, F1 Micro: 0.7219, F1 Macro: 0.5588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.205, Accuracy: 0.9144, F1 Micro: 0.7384, F1 Macro: 0.5747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1745, Accuracy: 0.9169, F1 Micro: 0.7498, F1 Macro: 0.6075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1468, Accuracy: 0.9174, F1 Micro: 0.7548, F1 Macro: 0.6105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1199, Accuracy: 0.9174, F1 Micro: 0.7603, F1 Macro: 0.6391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1036, Accuracy: 0.9192, F1 Micro: 0.7642, F1 Macro: 0.6555\n",
      "Epoch 9/10, Train Loss: 0.0911, Accuracy: 0.9195, F1 Micro: 0.7586, F1 Macro: 0.6572\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9198, F1 Micro: 0.7602, F1 Macro: 0.6582\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9192, F1 Micro: 0.7642, F1 Macro: 0.6555\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.81      0.50      0.62       157\n",
      "      HS_Race       0.71      0.74      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.61      0.22      0.32        51\n",
      "     HS_Other       0.73      0.79      0.76       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.76      0.63      0.66      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 155.4185311794281 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4223, Accuracy: 0.8734, F1 Micro: 0.5181, F1 Macro: 0.2289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2971, Accuracy: 0.8945, F1 Micro: 0.6394, F1 Macro: 0.3871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2468, Accuracy: 0.9053, F1 Micro: 0.7177, F1 Macro: 0.5434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.9156, F1 Micro: 0.7339, F1 Macro: 0.5745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1773, Accuracy: 0.9161, F1 Micro: 0.753, F1 Macro: 0.6139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1448, Accuracy: 0.9196, F1 Micro: 0.7565, F1 Macro: 0.6237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1217, Accuracy: 0.918, F1 Micro: 0.7624, F1 Macro: 0.6455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1061, Accuracy: 0.9199, F1 Micro: 0.7693, F1 Macro: 0.6591\n",
      "Epoch 9/10, Train Loss: 0.0916, Accuracy: 0.9191, F1 Micro: 0.7604, F1 Macro: 0.6445\n",
      "Epoch 10/10, Train Loss: 0.0788, Accuracy: 0.9205, F1 Micro: 0.7591, F1 Macro: 0.6543\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9199, F1 Micro: 0.7693, F1 Macro: 0.6591\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.85      0.92      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.76      0.62      0.68       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.62      0.59      0.60       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.73      0.65      0.66      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 154.3816487789154 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9075, F1 Micro: 0.7202, F1 Macro: 0.5642\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 44.55060529708862 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4095, Accuracy: 0.8795, F1 Micro: 0.5752, F1 Macro: 0.2671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2887, Accuracy: 0.8991, F1 Micro: 0.6842, F1 Macro: 0.447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2385, Accuracy: 0.9087, F1 Micro: 0.7166, F1 Macro: 0.5187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2041, Accuracy: 0.9148, F1 Micro: 0.7416, F1 Macro: 0.5703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1672, Accuracy: 0.9188, F1 Micro: 0.7437, F1 Macro: 0.5614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1445, Accuracy: 0.9188, F1 Micro: 0.7626, F1 Macro: 0.6362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9185, F1 Micro: 0.7656, F1 Macro: 0.6595\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.9201, F1 Micro: 0.7519, F1 Macro: 0.6637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0884, Accuracy: 0.9222, F1 Micro: 0.7696, F1 Macro: 0.6709\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.9211, F1 Micro: 0.7622, F1 Macro: 0.6756\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9222, F1 Micro: 0.7696, F1 Macro: 0.6709\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.77      0.58      0.66       402\n",
      "  HS_Religion       0.70      0.67      0.68       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.61      0.37      0.46        51\n",
      "     HS_Other       0.79      0.75      0.77       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.68      0.49      0.57       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 162.66367840766907 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4033, Accuracy: 0.8784, F1 Micro: 0.5397, F1 Macro: 0.2522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2854, Accuracy: 0.9005, F1 Micro: 0.6903, F1 Macro: 0.476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.91, F1 Micro: 0.728, F1 Macro: 0.5515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2032, Accuracy: 0.9152, F1 Micro: 0.7379, F1 Macro: 0.5833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1667, Accuracy: 0.9185, F1 Micro: 0.7485, F1 Macro: 0.5856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1466, Accuracy: 0.9187, F1 Micro: 0.7641, F1 Macro: 0.6241\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9187, F1 Micro: 0.7619, F1 Macro: 0.6389\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9193, F1 Micro: 0.7568, F1 Macro: 0.6635\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9218, F1 Micro: 0.7606, F1 Macro: 0.6521\n",
      "Epoch 10/10, Train Loss: 0.074, Accuracy: 0.9194, F1 Micro: 0.7591, F1 Macro: 0.67\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9187, F1 Micro: 0.7641, F1 Macro: 0.6241\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.69      0.74      0.72       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.80      0.61      0.62      5556\n",
      " weighted avg       0.77      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 159.17903399467468 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4171, Accuracy: 0.8763, F1 Micro: 0.5322, F1 Macro: 0.2417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2911, Accuracy: 0.8983, F1 Micro: 0.691, F1 Macro: 0.4765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2379, Accuracy: 0.9091, F1 Micro: 0.7199, F1 Macro: 0.5354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2011, Accuracy: 0.9151, F1 Micro: 0.7465, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1655, Accuracy: 0.9177, F1 Micro: 0.7483, F1 Macro: 0.5799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1457, Accuracy: 0.9188, F1 Micro: 0.7549, F1 Macro: 0.6171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1192, Accuracy: 0.9192, F1 Micro: 0.7591, F1 Macro: 0.6394\n",
      "Epoch 8/10, Train Loss: 0.1, Accuracy: 0.9171, F1 Micro: 0.7521, F1 Macro: 0.6566\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9185, F1 Micro: 0.7581, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9201, F1 Micro: 0.7629, F1 Macro: 0.6682\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9201, F1 Micro: 0.7629, F1 Macro: 0.6682\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.70      0.64      0.67       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.78      0.62      0.69       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 161.68555974960327 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9086, F1 Micro: 0.724, F1 Macro: 0.5717\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 40.142462968826294 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4077, Accuracy: 0.8799, F1 Micro: 0.5651, F1 Macro: 0.2616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2856, Accuracy: 0.8996, F1 Micro: 0.6907, F1 Macro: 0.4591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2353, Accuracy: 0.907, F1 Micro: 0.702, F1 Macro: 0.5376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2035, Accuracy: 0.9103, F1 Micro: 0.7377, F1 Macro: 0.5694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1709, Accuracy: 0.9173, F1 Micro: 0.7543, F1 Macro: 0.5962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1468, Accuracy: 0.9202, F1 Micro: 0.7697, F1 Macro: 0.6633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1222, Accuracy: 0.9212, F1 Micro: 0.7734, F1 Macro: 0.6769\n",
      "Epoch 8/10, Train Loss: 0.102, Accuracy: 0.9242, F1 Micro: 0.7688, F1 Macro: 0.6594\n",
      "Epoch 9/10, Train Loss: 0.0842, Accuracy: 0.9205, F1 Micro: 0.7698, F1 Macro: 0.68\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9226, F1 Micro: 0.7684, F1 Macro: 0.6898\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9212, F1 Micro: 0.7734, F1 Macro: 0.6769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.69      0.65      0.67       157\n",
      "      HS_Race       0.73      0.71      0.72       120\n",
      "  HS_Physical       0.75      0.12      0.21        72\n",
      "    HS_Gender       0.59      0.33      0.42        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.1789436340332 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3995, Accuracy: 0.8791, F1 Micro: 0.5618, F1 Macro: 0.2638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2816, Accuracy: 0.901, F1 Micro: 0.6895, F1 Macro: 0.461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2346, Accuracy: 0.9084, F1 Micro: 0.7128, F1 Macro: 0.5618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2002, Accuracy: 0.9145, F1 Micro: 0.7452, F1 Macro: 0.5886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1698, Accuracy: 0.9179, F1 Micro: 0.7521, F1 Macro: 0.5935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1447, Accuracy: 0.9163, F1 Micro: 0.763, F1 Macro: 0.627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1213, Accuracy: 0.9168, F1 Micro: 0.7677, F1 Macro: 0.6647\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.921, F1 Micro: 0.7594, F1 Macro: 0.6508\n",
      "Epoch 9/10, Train Loss: 0.0825, Accuracy: 0.92, F1 Micro: 0.7669, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0749, Accuracy: 0.9227, F1 Micro: 0.7685, F1 Macro: 0.6818\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9227, F1 Micro: 0.7685, F1 Macro: 0.6818\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.76      0.70      0.72       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.74      0.65      0.69       157\n",
      "      HS_Race       0.78      0.68      0.72       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.79      0.76      0.77       762\n",
      "      HS_Weak       0.75      0.67      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5556\n",
      "    macro avg       0.77      0.64      0.68      5556\n",
      " weighted avg       0.79      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 165.7377483844757 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4142, Accuracy: 0.8786, F1 Micro: 0.5692, F1 Macro: 0.2632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2895, Accuracy: 0.8995, F1 Micro: 0.674, F1 Macro: 0.4534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2397, Accuracy: 0.909, F1 Micro: 0.7114, F1 Macro: 0.5434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.912, F1 Micro: 0.7406, F1 Macro: 0.5781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.173, Accuracy: 0.9173, F1 Micro: 0.7512, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1451, Accuracy: 0.9203, F1 Micro: 0.7667, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1227, Accuracy: 0.916, F1 Micro: 0.7668, F1 Macro: 0.6626\n",
      "Epoch 8/10, Train Loss: 0.1045, Accuracy: 0.9178, F1 Micro: 0.7628, F1 Macro: 0.6559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0844, Accuracy: 0.9212, F1 Micro: 0.769, F1 Macro: 0.6582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9218, F1 Micro: 0.7707, F1 Macro: 0.675\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9218, F1 Micro: 0.7707, F1 Macro: 0.675\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.70      0.62      0.66       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.77      0.78      0.77       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 165.94132447242737 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9096, F1 Micro: 0.7276, F1 Macro: 0.5798\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 36.090322494506836 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4077, Accuracy: 0.8787, F1 Micro: 0.6183, F1 Macro: 0.2874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2815, Accuracy: 0.8992, F1 Micro: 0.6847, F1 Macro: 0.4238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.911, F1 Micro: 0.7168, F1 Macro: 0.5335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1945, Accuracy: 0.9148, F1 Micro: 0.7487, F1 Macro: 0.5859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.166, Accuracy: 0.9191, F1 Micro: 0.7571, F1 Macro: 0.6092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1357, Accuracy: 0.9217, F1 Micro: 0.7624, F1 Macro: 0.6495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.9201, F1 Micro: 0.7663, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.924, F1 Micro: 0.7698, F1 Macro: 0.659\n",
      "Epoch 9/10, Train Loss: 0.0828, Accuracy: 0.9229, F1 Micro: 0.7665, F1 Macro: 0.682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0714, Accuracy: 0.918, F1 Micro: 0.7699, F1 Macro: 0.6903\n",
      "Model 1 - Iteration 7901: Accuracy: 0.918, F1 Micro: 0.7699, F1 Macro: 0.6903\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.85      0.94      0.89       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.68      0.66      0.67       402\n",
      "  HS_Religion       0.69      0.68      0.69       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.56      0.43      0.49        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.66      0.74      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.61       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.74      0.80      0.77      5556\n",
      "    macro avg       0.73      0.69      0.69      5556\n",
      " weighted avg       0.74      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 171.43392729759216 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4004, Accuracy: 0.8801, F1 Micro: 0.6123, F1 Macro: 0.2888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2781, Accuracy: 0.9018, F1 Micro: 0.7007, F1 Macro: 0.4894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2335, Accuracy: 0.9115, F1 Micro: 0.7181, F1 Macro: 0.5443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9169, F1 Micro: 0.7533, F1 Macro: 0.6079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9192, F1 Micro: 0.7615, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.139, Accuracy: 0.9199, F1 Micro: 0.7628, F1 Macro: 0.6413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1205, Accuracy: 0.9227, F1 Micro: 0.7638, F1 Macro: 0.6711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9182, F1 Micro: 0.7646, F1 Macro: 0.6364\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.92, F1 Micro: 0.7616, F1 Macro: 0.6663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.9198, F1 Micro: 0.7687, F1 Macro: 0.6884\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9198, F1 Micro: 0.7687, F1 Macro: 0.6884\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.76      0.22      0.34        72\n",
      "    HS_Gender       0.67      0.43      0.52        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.74      0.66      0.69      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 172.06455397605896 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4132, Accuracy: 0.8797, F1 Micro: 0.5562, F1 Macro: 0.2581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2834, Accuracy: 0.8995, F1 Micro: 0.6822, F1 Macro: 0.4405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2369, Accuracy: 0.9106, F1 Micro: 0.7165, F1 Macro: 0.5291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1957, Accuracy: 0.9144, F1 Micro: 0.7452, F1 Macro: 0.5975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1673, Accuracy: 0.9173, F1 Micro: 0.7598, F1 Macro: 0.6162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1399, Accuracy: 0.9211, F1 Micro: 0.7656, F1 Macro: 0.6442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1225, Accuracy: 0.9232, F1 Micro: 0.7691, F1 Macro: 0.6617\n",
      "Epoch 8/10, Train Loss: 0.096, Accuracy: 0.9199, F1 Micro: 0.7684, F1 Macro: 0.6328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9215, F1 Micro: 0.7698, F1 Macro: 0.6763\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9156, F1 Micro: 0.7664, F1 Macro: 0.6765\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9215, F1 Micro: 0.7698, F1 Macro: 0.6763\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.71      0.66      0.68       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.79      0.76      0.77       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.68      0.53      0.60       331\n",
      "    HS_Strong       0.82      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.65      0.68      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 170.4591727256775 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9103, F1 Micro: 0.7306, F1 Macro: 0.5874\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 33.54712176322937 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4068, Accuracy: 0.8796, F1 Micro: 0.6062, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2819, Accuracy: 0.9002, F1 Micro: 0.7038, F1 Macro: 0.4703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2315, Accuracy: 0.9118, F1 Micro: 0.7155, F1 Macro: 0.5456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9183, F1 Micro: 0.7449, F1 Macro: 0.5617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.9194, F1 Micro: 0.7532, F1 Macro: 0.6092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.921, F1 Micro: 0.7725, F1 Macro: 0.6668\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9219, F1 Micro: 0.766, F1 Macro: 0.672\n",
      "Epoch 8/10, Train Loss: 0.0988, Accuracy: 0.9186, F1 Micro: 0.7678, F1 Macro: 0.6685\n",
      "Epoch 9/10, Train Loss: 0.0828, Accuracy: 0.9231, F1 Micro: 0.7616, F1 Macro: 0.6781\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9235, F1 Micro: 0.7634, F1 Macro: 0.6817\n",
      "Model 1 - Iteration 8165: Accuracy: 0.921, F1 Micro: 0.7725, F1 Macro: 0.6668\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.80      0.64      0.71       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 170.8062071800232 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8809, F1 Micro: 0.6065, F1 Macro: 0.2929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2794, Accuracy: 0.9009, F1 Micro: 0.7092, F1 Macro: 0.5179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2292, Accuracy: 0.911, F1 Micro: 0.7333, F1 Macro: 0.5664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.197, Accuracy: 0.9171, F1 Micro: 0.7345, F1 Macro: 0.5701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9198, F1 Micro: 0.7492, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1362, Accuracy: 0.9218, F1 Micro: 0.7659, F1 Macro: 0.644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.92, F1 Micro: 0.7699, F1 Macro: 0.6702\n",
      "Epoch 8/10, Train Loss: 0.101, Accuracy: 0.9203, F1 Micro: 0.7696, F1 Macro: 0.6676\n",
      "Epoch 9/10, Train Loss: 0.0845, Accuracy: 0.9217, F1 Micro: 0.7667, F1 Macro: 0.6776\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9235, F1 Micro: 0.7678, F1 Macro: 0.679\n",
      "Model 2 - Iteration 8165: Accuracy: 0.92, F1 Micro: 0.7699, F1 Macro: 0.6702\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.68      0.69      0.68       402\n",
      "  HS_Religion       0.65      0.68      0.66       157\n",
      "      HS_Race       0.70      0.78      0.74       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.63      0.24      0.34        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.69      0.70      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.67      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 172.50822138786316 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4117, Accuracy: 0.88, F1 Micro: 0.594, F1 Macro: 0.2781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2844, Accuracy: 0.9011, F1 Micro: 0.6919, F1 Macro: 0.4746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2305, Accuracy: 0.9107, F1 Micro: 0.7291, F1 Macro: 0.5652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1944, Accuracy: 0.9167, F1 Micro: 0.7455, F1 Macro: 0.5783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1633, Accuracy: 0.9212, F1 Micro: 0.7589, F1 Macro: 0.6129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1373, Accuracy: 0.9173, F1 Micro: 0.7649, F1 Macro: 0.6414\n",
      "Epoch 7/10, Train Loss: 0.112, Accuracy: 0.9218, F1 Micro: 0.7587, F1 Macro: 0.6548\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.9188, F1 Micro: 0.7635, F1 Macro: 0.6419\n",
      "Epoch 9/10, Train Loss: 0.0855, Accuracy: 0.9197, F1 Micro: 0.7646, F1 Macro: 0.6751\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9198, F1 Micro: 0.7445, F1 Macro: 0.6558\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9173, F1 Micro: 0.7649, F1 Macro: 0.6414\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.84      0.93      0.88       992\n",
      "HS_Individual       0.70      0.76      0.72       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.75      0.18      0.29        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.67      0.73      0.70       689\n",
      "  HS_Moderate       0.58      0.59      0.59       331\n",
      "    HS_Strong       0.89      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.76      5556\n",
      "    macro avg       0.73      0.63      0.64      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 170.64691543579102 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9109, F1 Micro: 0.7331, F1 Macro: 0.5922\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 29.96940588951111 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3987, Accuracy: 0.8806, F1 Micro: 0.62, F1 Macro: 0.2919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2767, Accuracy: 0.9035, F1 Micro: 0.6897, F1 Macro: 0.4784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2289, Accuracy: 0.912, F1 Micro: 0.7387, F1 Macro: 0.5658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9175, F1 Micro: 0.758, F1 Macro: 0.5953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1641, Accuracy: 0.9217, F1 Micro: 0.7585, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1374, Accuracy: 0.9241, F1 Micro: 0.7721, F1 Macro: 0.6637\n",
      "Epoch 7/10, Train Loss: 0.1092, Accuracy: 0.9219, F1 Micro: 0.7702, F1 Macro: 0.6755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9227, F1 Micro: 0.7747, F1 Macro: 0.681\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9219, F1 Micro: 0.764, F1 Macro: 0.6834\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.922, F1 Micro: 0.7664, F1 Macro: 0.6943\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9227, F1 Micro: 0.7747, F1 Macro: 0.681\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.77      0.59      0.67       402\n",
      "  HS_Religion       0.78      0.55      0.64       157\n",
      "      HS_Race       0.74      0.67      0.70       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.68      0.52      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 176.43009781837463 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3918, Accuracy: 0.8815, F1 Micro: 0.6127, F1 Macro: 0.3001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2739, Accuracy: 0.9028, F1 Micro: 0.687, F1 Macro: 0.4873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9092, F1 Micro: 0.7387, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1901, Accuracy: 0.9179, F1 Micro: 0.7602, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9219, F1 Micro: 0.7608, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1354, Accuracy: 0.922, F1 Micro: 0.7703, F1 Macro: 0.647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1094, Accuracy: 0.9212, F1 Micro: 0.7727, F1 Macro: 0.6646\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9205, F1 Micro: 0.77, F1 Macro: 0.6769\n",
      "Epoch 9/10, Train Loss: 0.08, Accuracy: 0.921, F1 Micro: 0.7722, F1 Macro: 0.6873\n",
      "Epoch 10/10, Train Loss: 0.0705, Accuracy: 0.9226, F1 Micro: 0.7725, F1 Macro: 0.6903\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9212, F1 Micro: 0.7727, F1 Macro: 0.6646\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.66      0.67      0.67       157\n",
      "      HS_Race       0.70      0.77      0.73       120\n",
      "  HS_Physical       0.70      0.10      0.17        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.73      0.66      0.66      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 177.07228016853333 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4045, Accuracy: 0.8809, F1 Micro: 0.5991, F1 Macro: 0.2828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.902, F1 Micro: 0.6725, F1 Macro: 0.4726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9107, F1 Micro: 0.7352, F1 Macro: 0.5655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9152, F1 Micro: 0.7564, F1 Macro: 0.5998\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9211, F1 Micro: 0.7538, F1 Macro: 0.6151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.9204, F1 Micro: 0.765, F1 Macro: 0.6401\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9178, F1 Micro: 0.7644, F1 Macro: 0.6469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9179, F1 Micro: 0.7655, F1 Macro: 0.6613\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9198, F1 Micro: 0.7622, F1 Macro: 0.655\n",
      "Epoch 10/10, Train Loss: 0.0698, Accuracy: 0.9197, F1 Micro: 0.7627, F1 Macro: 0.6676\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9179, F1 Micro: 0.7655, F1 Macro: 0.6613\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.68      0.77      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.69      0.70      0.69       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.66      0.75      0.70       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5556\n",
      "    macro avg       0.74      0.65      0.66      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 175.65624403953552 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9115, F1 Micro: 0.7355, F1 Macro: 0.597\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 27.118870496749878 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3925, Accuracy: 0.8831, F1 Micro: 0.5894, F1 Macro: 0.2772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.9028, F1 Micro: 0.7006, F1 Macro: 0.4995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2228, Accuracy: 0.9089, F1 Micro: 0.7357, F1 Macro: 0.5607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.9167, F1 Micro: 0.7549, F1 Macro: 0.5725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1621, Accuracy: 0.9215, F1 Micro: 0.7648, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.9224, F1 Micro: 0.7689, F1 Macro: 0.6641\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9173, F1 Micro: 0.7673, F1 Macro: 0.6744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9184, F1 Micro: 0.771, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.92, F1 Micro: 0.773, F1 Macro: 0.6976\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9228, F1 Micro: 0.7705, F1 Macro: 0.6899\n",
      "Model 1 - Iteration 8616: Accuracy: 0.92, F1 Micro: 0.773, F1 Macro: 0.6976\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.68      0.69      0.69       157\n",
      "      HS_Race       0.70      0.75      0.72       120\n",
      "  HS_Physical       0.85      0.24      0.37        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.73      0.69      0.70      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 182.09392738342285 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3855, Accuracy: 0.8842, F1 Micro: 0.607, F1 Macro: 0.2984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2739, Accuracy: 0.903, F1 Micro: 0.71, F1 Macro: 0.5258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2213, Accuracy: 0.9076, F1 Micro: 0.7376, F1 Macro: 0.5862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9176, F1 Micro: 0.7476, F1 Macro: 0.5869\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.9201, F1 Micro: 0.7444, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1327, Accuracy: 0.9217, F1 Micro: 0.7617, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9227, F1 Micro: 0.7695, F1 Macro: 0.67\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9218, F1 Micro: 0.7679, F1 Macro: 0.6853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9204, F1 Micro: 0.7717, F1 Macro: 0.693\n",
      "Epoch 10/10, Train Loss: 0.07, Accuracy: 0.9237, F1 Micro: 0.7708, F1 Macro: 0.7008\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9204, F1 Micro: 0.7717, F1 Macro: 0.693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.66      0.68      0.67       157\n",
      "      HS_Race       0.72      0.68      0.70       120\n",
      "  HS_Physical       0.90      0.25      0.39        72\n",
      "    HS_Gender       0.60      0.35      0.44        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.62      0.61      0.61       331\n",
      "    HS_Strong       0.86      0.90      0.88       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 180.82937097549438 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4, Accuracy: 0.8802, F1 Micro: 0.5721, F1 Macro: 0.2692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2792, Accuracy: 0.9025, F1 Micro: 0.695, F1 Macro: 0.5031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2246, Accuracy: 0.9088, F1 Micro: 0.738, F1 Macro: 0.5676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1945, Accuracy: 0.9177, F1 Micro: 0.7524, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1628, Accuracy: 0.9223, F1 Micro: 0.7653, F1 Macro: 0.6206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9188, F1 Micro: 0.7701, F1 Macro: 0.6527\n",
      "Epoch 7/10, Train Loss: 0.1129, Accuracy: 0.9175, F1 Micro: 0.7665, F1 Macro: 0.6609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9232, F1 Micro: 0.7726, F1 Macro: 0.6834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9224, F1 Micro: 0.7752, F1 Macro: 0.6885\n",
      "Epoch 10/10, Train Loss: 0.0705, Accuracy: 0.9214, F1 Micro: 0.766, F1 Macro: 0.684\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9224, F1 Micro: 0.7752, F1 Macro: 0.6885\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.74      0.65      0.69       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.65      0.47      0.55        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.66      0.54      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 182.34607481956482 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9121, F1 Micro: 0.7377, F1 Macro: 0.6026\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.429538249969482 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.395, Accuracy: 0.8834, F1 Micro: 0.6157, F1 Macro: 0.2895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2757, Accuracy: 0.9048, F1 Micro: 0.6955, F1 Macro: 0.4522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2253, Accuracy: 0.907, F1 Micro: 0.7411, F1 Macro: 0.5681\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9178, F1 Micro: 0.7357, F1 Macro: 0.6001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.9235, F1 Micro: 0.7691, F1 Macro: 0.621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9235, F1 Micro: 0.7737, F1 Macro: 0.6586\n",
      "Epoch 7/10, Train Loss: 0.1111, Accuracy: 0.9242, F1 Micro: 0.7685, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.9217, F1 Micro: 0.7743, F1 Macro: 0.7027\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9229, F1 Micro: 0.7739, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9244, F1 Micro: 0.7769, F1 Macro: 0.7\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9244, F1 Micro: 0.7769, F1 Macro: 0.7\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.72      0.66      0.68       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.60      0.49      0.54        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.63      0.59      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 183.02193593978882 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3877, Accuracy: 0.8835, F1 Micro: 0.6128, F1 Macro: 0.3111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2729, Accuracy: 0.9044, F1 Micro: 0.7014, F1 Macro: 0.4854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.9078, F1 Micro: 0.7398, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1946, Accuracy: 0.9182, F1 Micro: 0.7493, F1 Macro: 0.5936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.9213, F1 Micro: 0.7577, F1 Macro: 0.6145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.131, Accuracy: 0.9224, F1 Micro: 0.7628, F1 Macro: 0.6454\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9225, F1 Micro: 0.7592, F1 Macro: 0.6585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9222, F1 Micro: 0.7697, F1 Macro: 0.6848\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9233, F1 Micro: 0.7697, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.9193, F1 Micro: 0.7708, F1 Macro: 0.6895\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9193, F1 Micro: 0.7708, F1 Macro: 0.6895\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.70      0.75      0.72       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.65      0.69      0.67       157\n",
      "      HS_Race       0.71      0.72      0.72       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.73      0.71       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 184.9055347442627 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.881, F1 Micro: 0.5983, F1 Macro: 0.2884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9039, F1 Micro: 0.7003, F1 Macro: 0.483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2272, Accuracy: 0.9063, F1 Micro: 0.7388, F1 Macro: 0.5734\n",
      "Epoch 4/10, Train Loss: 0.1955, Accuracy: 0.9179, F1 Micro: 0.7383, F1 Macro: 0.5943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.922, F1 Micro: 0.761, F1 Macro: 0.6217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9204, F1 Micro: 0.7654, F1 Macro: 0.6419\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9214, F1 Micro: 0.7603, F1 Macro: 0.6581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.9229, F1 Micro: 0.772, F1 Macro: 0.6766\n",
      "Epoch 9/10, Train Loss: 0.0828, Accuracy: 0.9217, F1 Micro: 0.7675, F1 Macro: 0.6761\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9217, F1 Micro: 0.7709, F1 Macro: 0.6848\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9229, F1 Micro: 0.772, F1 Macro: 0.6766\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.70      0.68      0.69       402\n",
      "  HS_Religion       0.76      0.62      0.68       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.56      0.39      0.46        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.67      0.70       689\n",
      "  HS_Moderate       0.62      0.61      0.62       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 181.60697150230408 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9126, F1 Micro: 0.7397, F1 Macro: 0.6074\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.48459267616272 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.8836, F1 Micro: 0.5996, F1 Macro: 0.2919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2746, Accuracy: 0.9055, F1 Micro: 0.6896, F1 Macro: 0.4693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.9116, F1 Micro: 0.7387, F1 Macro: 0.5313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.9129, F1 Micro: 0.7523, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9244, F1 Micro: 0.7691, F1 Macro: 0.6189\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9189, F1 Micro: 0.7682, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9257, F1 Micro: 0.7764, F1 Macro: 0.684\n",
      "Epoch 8/10, Train Loss: 0.0915, Accuracy: 0.9224, F1 Micro: 0.7753, F1 Macro: 0.7016\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.9248, F1 Micro: 0.7685, F1 Macro: 0.6978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9227, F1 Micro: 0.7768, F1 Macro: 0.7109\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9227, F1 Micro: 0.7768, F1 Macro: 0.7109\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.72      0.68      0.70       157\n",
      "      HS_Race       0.81      0.68      0.74       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.61      0.53      0.57        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.69      0.71      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 188.36274433135986 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3857, Accuracy: 0.8845, F1 Micro: 0.5974, F1 Macro: 0.303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2722, Accuracy: 0.9045, F1 Micro: 0.6825, F1 Macro: 0.4919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.222, Accuracy: 0.9114, F1 Micro: 0.7268, F1 Macro: 0.5098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1873, Accuracy: 0.9125, F1 Micro: 0.7552, F1 Macro: 0.6079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.92, F1 Micro: 0.7652, F1 Macro: 0.6191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1336, Accuracy: 0.9196, F1 Micro: 0.7672, F1 Macro: 0.6494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9219, F1 Micro: 0.7687, F1 Macro: 0.6628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9203, F1 Micro: 0.7693, F1 Macro: 0.6924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9232, F1 Micro: 0.7704, F1 Macro: 0.6971\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9231, F1 Micro: 0.7667, F1 Macro: 0.6967\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9232, F1 Micro: 0.7704, F1 Macro: 0.6971\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.77      0.65      0.70       732\n",
      "     HS_Group       0.68      0.74      0.71       402\n",
      "  HS_Religion       0.72      0.66      0.69       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.80      0.75      0.78       762\n",
      "      HS_Weak       0.75      0.62      0.68       689\n",
      "  HS_Moderate       0.61      0.67      0.64       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 190.84806370735168 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4008, Accuracy: 0.8797, F1 Micro: 0.5512, F1 Macro: 0.2584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.9046, F1 Micro: 0.6951, F1 Macro: 0.4925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2226, Accuracy: 0.9102, F1 Micro: 0.731, F1 Macro: 0.5156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1882, Accuracy: 0.9158, F1 Micro: 0.7615, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1598, Accuracy: 0.9207, F1 Micro: 0.7642, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.922, F1 Micro: 0.7737, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.114, Accuracy: 0.9221, F1 Micro: 0.7747, F1 Macro: 0.6719\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9222, F1 Micro: 0.7693, F1 Macro: 0.6824\n",
      "Epoch 9/10, Train Loss: 0.0851, Accuracy: 0.9222, F1 Micro: 0.7625, F1 Macro: 0.6729\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9217, F1 Micro: 0.7732, F1 Macro: 0.6878\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9221, F1 Micro: 0.7747, F1 Macro: 0.6719\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.81      0.57      0.67       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.60      0.35      0.44        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.60      0.63      0.61       331\n",
      "    HS_Strong       0.84      0.86      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.67      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.0480031967163 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9132, F1 Micro: 0.7415, F1 Macro: 0.6119\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.51467490196228 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8829, F1 Micro: 0.5942, F1 Macro: 0.2974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.27, Accuracy: 0.9047, F1 Micro: 0.6914, F1 Macro: 0.4715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.219, Accuracy: 0.9111, F1 Micro: 0.7485, F1 Macro: 0.5799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1846, Accuracy: 0.9196, F1 Micro: 0.7627, F1 Macro: 0.6117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9227, F1 Micro: 0.7766, F1 Macro: 0.658\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9249, F1 Micro: 0.7703, F1 Macro: 0.6711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1088, Accuracy: 0.9239, F1 Micro: 0.7781, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9241, F1 Micro: 0.7801, F1 Macro: 0.6977\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9247, F1 Micro: 0.7757, F1 Macro: 0.6964\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.924, F1 Micro: 0.7791, F1 Macro: 0.692\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9241, F1 Micro: 0.7801, F1 Macro: 0.6977\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.70      0.69      0.70       402\n",
      "  HS_Religion       0.72      0.66      0.69       157\n",
      "      HS_Race       0.70      0.77      0.73       120\n",
      "  HS_Physical       0.83      0.21      0.33        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 190.0992729663849 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3835, Accuracy: 0.8833, F1 Micro: 0.5845, F1 Macro: 0.3223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2674, Accuracy: 0.9046, F1 Micro: 0.6851, F1 Macro: 0.4898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2192, Accuracy: 0.9081, F1 Micro: 0.745, F1 Macro: 0.5843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.9179, F1 Micro: 0.7614, F1 Macro: 0.6158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.153, Accuracy: 0.9231, F1 Micro: 0.7722, F1 Macro: 0.645\n",
      "Epoch 6/10, Train Loss: 0.1372, Accuracy: 0.9225, F1 Micro: 0.7705, F1 Macro: 0.6518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.9235, F1 Micro: 0.7746, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9239, F1 Micro: 0.7749, F1 Macro: 0.6845\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9248, F1 Micro: 0.7741, F1 Macro: 0.6927\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9248, F1 Micro: 0.7732, F1 Macro: 0.6892\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9239, F1 Micro: 0.7749, F1 Macro: 0.6845\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.71      0.68      0.70       402\n",
      "  HS_Religion       0.69      0.65      0.67       157\n",
      "      HS_Race       0.72      0.79      0.75       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.73      0.67      0.70       689\n",
      "  HS_Moderate       0.66      0.60      0.63       331\n",
      "    HS_Strong       0.86      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.98062324523926 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3959, Accuracy: 0.8815, F1 Micro: 0.5865, F1 Macro: 0.2913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.272, Accuracy: 0.903, F1 Micro: 0.68, F1 Macro: 0.475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2218, Accuracy: 0.9049, F1 Micro: 0.7419, F1 Macro: 0.585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1859, Accuracy: 0.9191, F1 Micro: 0.7519, F1 Macro: 0.5988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1556, Accuracy: 0.9203, F1 Micro: 0.7666, F1 Macro: 0.6331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1356, Accuracy: 0.9242, F1 Micro: 0.771, F1 Macro: 0.6629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9245, F1 Micro: 0.7732, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9213, F1 Micro: 0.7738, F1 Macro: 0.6829\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.924, F1 Micro: 0.7636, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9241, F1 Micro: 0.7762, F1 Macro: 0.6818\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9241, F1 Micro: 0.7762, F1 Macro: 0.6818\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 193.33385252952576 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9137, F1 Micro: 0.7433, F1 Macro: 0.6157\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 16.878062963485718 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3898, Accuracy: 0.8849, F1 Micro: 0.6091, F1 Macro: 0.2882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2674, Accuracy: 0.9039, F1 Micro: 0.6766, F1 Macro: 0.4514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.9145, F1 Micro: 0.7451, F1 Macro: 0.5775\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9189, F1 Micro: 0.7426, F1 Macro: 0.5911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9216, F1 Micro: 0.7663, F1 Macro: 0.653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1307, Accuracy: 0.9226, F1 Micro: 0.7727, F1 Macro: 0.6721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9249, F1 Micro: 0.7806, F1 Macro: 0.6727\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9215, F1 Micro: 0.7758, F1 Macro: 0.6965\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9235, F1 Micro: 0.7761, F1 Macro: 0.6936\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.9209, F1 Micro: 0.7746, F1 Macro: 0.7\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9249, F1 Micro: 0.7806, F1 Macro: 0.6727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.76      0.63      0.69       402\n",
      "  HS_Religion       0.81      0.58      0.68       157\n",
      "      HS_Race       0.86      0.64      0.73       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.67      0.27      0.39        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 190.5657196044922 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3823, Accuracy: 0.885, F1 Micro: 0.6054, F1 Macro: 0.2991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9055, F1 Micro: 0.6835, F1 Macro: 0.4899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2195, Accuracy: 0.9134, F1 Micro: 0.7497, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9191, F1 Micro: 0.7512, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1577, Accuracy: 0.9228, F1 Micro: 0.7641, F1 Macro: 0.6325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9237, F1 Micro: 0.7683, F1 Macro: 0.6533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9246, F1 Micro: 0.7807, F1 Macro: 0.6769\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9209, F1 Micro: 0.7719, F1 Macro: 0.6817\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9247, F1 Micro: 0.7721, F1 Macro: 0.677\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9211, F1 Micro: 0.7733, F1 Macro: 0.6947\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9246, F1 Micro: 0.7807, F1 Macro: 0.6769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.80      0.57      0.67       157\n",
      "      HS_Race       0.81      0.66      0.73       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.78      0.81      0.80       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.69      0.55      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 192.2210671901703 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3966, Accuracy: 0.8828, F1 Micro: 0.6065, F1 Macro: 0.2872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2734, Accuracy: 0.9045, F1 Micro: 0.6844, F1 Macro: 0.4641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.224, Accuracy: 0.9147, F1 Micro: 0.7441, F1 Macro: 0.5831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9186, F1 Micro: 0.7597, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.9215, F1 Micro: 0.7636, F1 Macro: 0.6348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9207, F1 Micro: 0.768, F1 Macro: 0.64\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9248, F1 Micro: 0.7734, F1 Macro: 0.6626\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9235, F1 Micro: 0.7703, F1 Macro: 0.6737\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9198, F1 Micro: 0.7694, F1 Macro: 0.6681\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9192, F1 Micro: 0.7722, F1 Macro: 0.6882\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9248, F1 Micro: 0.7734, F1 Macro: 0.6626\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.76      0.60      0.67       402\n",
      "  HS_Religion       0.81      0.55      0.66       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.69      0.51      0.59       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.62      0.66      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 191.85946106910706 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9142, F1 Micro: 0.7449, F1 Macro: 0.6183\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.945485591888428 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.39, Accuracy: 0.8854, F1 Micro: 0.6228, F1 Macro: 0.3071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2722, Accuracy: 0.9039, F1 Micro: 0.6998, F1 Macro: 0.5041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2207, Accuracy: 0.9134, F1 Micro: 0.7415, F1 Macro: 0.5658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9156, F1 Micro: 0.758, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9183, F1 Micro: 0.77, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1292, Accuracy: 0.9206, F1 Micro: 0.7711, F1 Macro: 0.6636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1063, Accuracy: 0.9217, F1 Micro: 0.7735, F1 Macro: 0.6768\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9242, F1 Micro: 0.7712, F1 Macro: 0.6979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.924, F1 Micro: 0.775, F1 Macro: 0.6892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9249, F1 Micro: 0.7803, F1 Macro: 0.7161\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9249, F1 Micro: 0.7803, F1 Macro: 0.7161\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.74      0.58      0.65       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.83      0.67      0.74       120\n",
      "  HS_Physical       0.83      0.35      0.49        72\n",
      "    HS_Gender       0.67      0.51      0.58        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.75      0.73       689\n",
      "  HS_Moderate       0.67      0.52      0.58       331\n",
      "    HS_Strong       0.90      0.84      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.68      0.72      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.3870770931244 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8871, F1 Micro: 0.6248, F1 Macro: 0.3355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2696, Accuracy: 0.9067, F1 Micro: 0.7059, F1 Macro: 0.5221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2194, Accuracy: 0.9157, F1 Micro: 0.7477, F1 Macro: 0.5833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1816, Accuracy: 0.917, F1 Micro: 0.7573, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1521, Accuracy: 0.9145, F1 Micro: 0.7637, F1 Macro: 0.6248\n",
      "Epoch 6/10, Train Loss: 0.13, Accuracy: 0.9188, F1 Micro: 0.7598, F1 Macro: 0.6193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9204, F1 Micro: 0.7685, F1 Macro: 0.66\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.924, F1 Micro: 0.7654, F1 Macro: 0.6812\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9207, F1 Micro: 0.7586, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9198, F1 Micro: 0.7728, F1 Macro: 0.698\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9198, F1 Micro: 0.7728, F1 Macro: 0.698\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.67      0.79      0.73       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.72      0.57      0.63       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.74      0.32      0.45        72\n",
      "    HS_Gender       0.71      0.39      0.51        51\n",
      "     HS_Other       0.73      0.84      0.79       762\n",
      "      HS_Weak       0.65      0.77      0.70       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 193.71663331985474 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3966, Accuracy: 0.8835, F1 Micro: 0.6269, F1 Macro: 0.3159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2739, Accuracy: 0.9042, F1 Micro: 0.7033, F1 Macro: 0.5276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2203, Accuracy: 0.9144, F1 Micro: 0.7456, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1821, Accuracy: 0.9171, F1 Micro: 0.7605, F1 Macro: 0.6053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9173, F1 Micro: 0.7635, F1 Macro: 0.6225\n",
      "Epoch 6/10, Train Loss: 0.1279, Accuracy: 0.9176, F1 Micro: 0.7615, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9171, F1 Micro: 0.7666, F1 Macro: 0.6696\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9219, F1 Micro: 0.7643, F1 Macro: 0.6747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.921, F1 Micro: 0.7681, F1 Macro: 0.673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9222, F1 Micro: 0.7689, F1 Macro: 0.6906\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9222, F1 Micro: 0.7689, F1 Macro: 0.6906\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.81      0.56      0.66       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.70      0.19      0.30        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.76      0.65      0.69      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 195.9443838596344 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9146, F1 Micro: 0.7463, F1 Macro: 0.6221\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.455568313598633 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3899, Accuracy: 0.8825, F1 Micro: 0.5678, F1 Macro: 0.2781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9083, F1 Micro: 0.7032, F1 Macro: 0.5205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2164, Accuracy: 0.9154, F1 Micro: 0.746, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1799, Accuracy: 0.9173, F1 Micro: 0.7641, F1 Macro: 0.6137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1483, Accuracy: 0.9246, F1 Micro: 0.7696, F1 Macro: 0.6479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.9227, F1 Micro: 0.7758, F1 Macro: 0.6704\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9224, F1 Micro: 0.773, F1 Macro: 0.6859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9245, F1 Micro: 0.7784, F1 Macro: 0.6983\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.9223, F1 Micro: 0.7771, F1 Macro: 0.7032\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9258, F1 Micro: 0.7779, F1 Macro: 0.6976\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9245, F1 Micro: 0.7784, F1 Macro: 0.6983\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.70      0.66      0.68       157\n",
      "      HS_Race       0.79      0.72      0.75       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.61      0.58      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 198.44136476516724 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3827, Accuracy: 0.8842, F1 Micro: 0.584, F1 Macro: 0.3293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2648, Accuracy: 0.9083, F1 Micro: 0.7041, F1 Macro: 0.5338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.915, F1 Micro: 0.7426, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9188, F1 Micro: 0.7611, F1 Macro: 0.6098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9213, F1 Micro: 0.7622, F1 Macro: 0.62\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.9233, F1 Micro: 0.7708, F1 Macro: 0.6422\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.9216, F1 Micro: 0.765, F1 Macro: 0.6681\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9221, F1 Micro: 0.7706, F1 Macro: 0.6779\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.924, F1 Micro: 0.7706, F1 Macro: 0.6955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9179, F1 Micro: 0.7721, F1 Macro: 0.7046\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9179, F1 Micro: 0.7721, F1 Macro: 0.7046\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.68      0.67      0.67       402\n",
      "  HS_Religion       0.67      0.64      0.65       157\n",
      "      HS_Race       0.71      0.78      0.74       120\n",
      "  HS_Physical       0.87      0.28      0.42        72\n",
      "    HS_Gender       0.62      0.49      0.55        51\n",
      "     HS_Other       0.72      0.86      0.78       762\n",
      "      HS_Weak       0.66      0.75      0.70       689\n",
      "  HS_Moderate       0.60      0.59      0.59       331\n",
      "    HS_Strong       0.85      0.89      0.87       114\n",
      "\n",
      "    micro avg       0.74      0.81      0.77      5556\n",
      "    macro avg       0.73      0.71      0.70      5556\n",
      " weighted avg       0.74      0.81      0.77      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 198.33367109298706 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.395, Accuracy: 0.8832, F1 Micro: 0.5856, F1 Macro: 0.2891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2684, Accuracy: 0.9084, F1 Micro: 0.7169, F1 Macro: 0.5421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2152, Accuracy: 0.9155, F1 Micro: 0.743, F1 Macro: 0.5833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9173, F1 Micro: 0.7603, F1 Macro: 0.6125\n",
      "Epoch 5/10, Train Loss: 0.1503, Accuracy: 0.9207, F1 Micro: 0.7419, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1284, Accuracy: 0.9218, F1 Micro: 0.7716, F1 Macro: 0.6625\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.9182, F1 Micro: 0.7677, F1 Macro: 0.6723\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.924, F1 Micro: 0.7667, F1 Macro: 0.6718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.9244, F1 Micro: 0.7749, F1 Macro: 0.6888\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9214, F1 Micro: 0.7687, F1 Macro: 0.6837\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9244, F1 Micro: 0.7749, F1 Macro: 0.6888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.77      0.57      0.66       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.66      0.54      0.59       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.77      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 196.53436732292175 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9149, F1 Micro: 0.7475, F1 Macro: 0.6254\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.211878538131714 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3874, Accuracy: 0.8828, F1 Micro: 0.5546, F1 Macro: 0.2664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9071, F1 Micro: 0.7063, F1 Macro: 0.5066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2131, Accuracy: 0.9148, F1 Micro: 0.7519, F1 Macro: 0.5946\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9218, F1 Micro: 0.7517, F1 Macro: 0.5932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1519, Accuracy: 0.9237, F1 Micro: 0.77, F1 Macro: 0.6434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1271, Accuracy: 0.9245, F1 Micro: 0.7735, F1 Macro: 0.6693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9268, F1 Micro: 0.7761, F1 Macro: 0.6916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9282, F1 Micro: 0.7837, F1 Macro: 0.7012\n",
      "Epoch 9/10, Train Loss: 0.0754, Accuracy: 0.9231, F1 Micro: 0.7723, F1 Macro: 0.7052\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9247, F1 Micro: 0.7756, F1 Macro: 0.7075\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9282, F1 Micro: 0.7837, F1 Macro: 0.7012\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1134\n",
      "      Abusive       0.92      0.88      0.90       992\n",
      "HS_Individual       0.77      0.70      0.73       732\n",
      "     HS_Group       0.73      0.71      0.72       402\n",
      "  HS_Religion       0.83      0.61      0.70       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.68      0.37      0.48        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.77      0.67      0.71       689\n",
      "  HS_Moderate       0.64      0.61      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.80      0.66      0.70      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 200.2540249824524 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3803, Accuracy: 0.8856, F1 Micro: 0.5875, F1 Macro: 0.332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.9081, F1 Micro: 0.7099, F1 Macro: 0.5279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2126, Accuracy: 0.9118, F1 Micro: 0.7451, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.921, F1 Micro: 0.7539, F1 Macro: 0.6031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1525, Accuracy: 0.9206, F1 Micro: 0.7611, F1 Macro: 0.6157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9238, F1 Micro: 0.7699, F1 Macro: 0.6468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1063, Accuracy: 0.9254, F1 Micro: 0.7766, F1 Macro: 0.6835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0891, Accuracy: 0.9258, F1 Micro: 0.7791, F1 Macro: 0.6888\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.923, F1 Micro: 0.7622, F1 Macro: 0.6819\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9226, F1 Micro: 0.775, F1 Macro: 0.699\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9258, F1 Micro: 0.7791, F1 Macro: 0.6888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.75      0.66      0.70       402\n",
      "  HS_Religion       0.78      0.60      0.68       157\n",
      "      HS_Race       0.74      0.74      0.74       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.69      0.56      0.62       331\n",
      "    HS_Strong       0.86      0.89      0.88       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 203.13896536827087 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3929, Accuracy: 0.8851, F1 Micro: 0.5998, F1 Macro: 0.3021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2657, Accuracy: 0.9071, F1 Micro: 0.7126, F1 Macro: 0.5089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9171, F1 Micro: 0.7546, F1 Macro: 0.5935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.9213, F1 Micro: 0.7579, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9215, F1 Micro: 0.7666, F1 Macro: 0.637\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9223, F1 Micro: 0.7649, F1 Macro: 0.6452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9219, F1 Micro: 0.7749, F1 Macro: 0.6804\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9229, F1 Micro: 0.7744, F1 Macro: 0.6875\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9228, F1 Micro: 0.773, F1 Macro: 0.6884\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9217, F1 Micro: 0.7731, F1 Macro: 0.6994\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9219, F1 Micro: 0.7749, F1 Macro: 0.6804\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.68      0.63      0.65       157\n",
      "      HS_Race       0.71      0.75      0.73       120\n",
      "  HS_Physical       0.73      0.11      0.19        72\n",
      "    HS_Gender       0.54      0.43      0.48        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.63      0.56      0.60       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 199.90601253509521 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9154, F1 Micro: 0.7488, F1 Macro: 0.6281\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.001883506774902 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3841, Accuracy: 0.879, F1 Micro: 0.5286, F1 Macro: 0.2523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9083, F1 Micro: 0.6973, F1 Macro: 0.5101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.9146, F1 Micro: 0.7495, F1 Macro: 0.5797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.9198, F1 Micro: 0.7498, F1 Macro: 0.6092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1471, Accuracy: 0.9238, F1 Micro: 0.7729, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.923, F1 Micro: 0.7752, F1 Macro: 0.684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9248, F1 Micro: 0.7777, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.923, F1 Micro: 0.7813, F1 Macro: 0.7037\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.9246, F1 Micro: 0.7769, F1 Macro: 0.7047\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9256, F1 Micro: 0.7777, F1 Macro: 0.7109\n",
      "Model 1 - Iteration 10018: Accuracy: 0.923, F1 Micro: 0.7813, F1 Macro: 0.7037\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.86      0.94      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.69      0.72      0.70       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.78      0.65      0.71       120\n",
      "  HS_Physical       0.74      0.28      0.40        72\n",
      "    HS_Gender       0.56      0.45      0.50        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.60      0.65      0.62       331\n",
      "    HS_Strong       0.89      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5556\n",
      "    macro avg       0.74      0.69      0.70      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 206.9955415725708 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3782, Accuracy: 0.8785, F1 Micro: 0.5284, F1 Macro: 0.2672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.908, F1 Micro: 0.6997, F1 Macro: 0.5195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9123, F1 Micro: 0.7521, F1 Macro: 0.5991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1783, Accuracy: 0.9202, F1 Micro: 0.757, F1 Macro: 0.6002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1471, Accuracy: 0.9219, F1 Micro: 0.7734, F1 Macro: 0.6368\n",
      "Epoch 6/10, Train Loss: 0.1205, Accuracy: 0.922, F1 Micro: 0.7716, F1 Macro: 0.6566\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9188, F1 Micro: 0.7725, F1 Macro: 0.6774\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9183, F1 Micro: 0.773, F1 Macro: 0.6951\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9217, F1 Micro: 0.7707, F1 Macro: 0.6929\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9255, F1 Micro: 0.7715, F1 Macro: 0.6993\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9219, F1 Micro: 0.7734, F1 Macro: 0.6368\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.75      0.83      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.80      0.63      0.64      5556\n",
      " weighted avg       0.78      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 201.29384064674377 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3893, Accuracy: 0.8774, F1 Micro: 0.5179, F1 Macro: 0.246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2635, Accuracy: 0.9084, F1 Micro: 0.7039, F1 Macro: 0.5221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.9143, F1 Micro: 0.75, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9199, F1 Micro: 0.7532, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.9202, F1 Micro: 0.7716, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9216, F1 Micro: 0.7742, F1 Macro: 0.6698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1048, Accuracy: 0.9214, F1 Micro: 0.7761, F1 Macro: 0.6792\n",
      "Epoch 8/10, Train Loss: 0.0877, Accuracy: 0.9234, F1 Micro: 0.7715, F1 Macro: 0.6876\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9193, F1 Micro: 0.7665, F1 Macro: 0.6814\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9223, F1 Micro: 0.7721, F1 Macro: 0.7063\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9214, F1 Micro: 0.7761, F1 Macro: 0.6792\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.71      0.67      0.69       120\n",
      "  HS_Physical       0.69      0.15      0.25        72\n",
      "    HS_Gender       0.57      0.33      0.42        51\n",
      "     HS_Other       0.73      0.84      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.83      0.89      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 206.20023775100708 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9156, F1 Micro: 0.75, F1 Macro: 0.6299\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.616651773452759 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3795, Accuracy: 0.885, F1 Micro: 0.5862, F1 Macro: 0.286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2567, Accuracy: 0.9081, F1 Micro: 0.7029, F1 Macro: 0.4811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.914, F1 Micro: 0.749, F1 Macro: 0.5768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1757, Accuracy: 0.9198, F1 Micro: 0.7663, F1 Macro: 0.6169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.9256, F1 Micro: 0.7787, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.12, Accuracy: 0.925, F1 Micro: 0.7803, F1 Macro: 0.673\n",
      "Epoch 7/10, Train Loss: 0.1022, Accuracy: 0.9249, F1 Micro: 0.7775, F1 Macro: 0.6855\n",
      "Epoch 8/10, Train Loss: 0.0852, Accuracy: 0.9256, F1 Micro: 0.7752, F1 Macro: 0.6948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9255, F1 Micro: 0.7818, F1 Macro: 0.7121\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9229, F1 Micro: 0.7708, F1 Macro: 0.6989\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9255, F1 Micro: 0.7818, F1 Macro: 0.7121\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.81      0.59      0.68       157\n",
      "      HS_Race       0.75      0.74      0.74       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.79      0.81      0.80       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 206.2999837398529 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.8843, F1 Micro: 0.5822, F1 Macro: 0.3018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2538, Accuracy: 0.9065, F1 Micro: 0.701, F1 Macro: 0.4945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2121, Accuracy: 0.9153, F1 Micro: 0.7461, F1 Macro: 0.5905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9194, F1 Micro: 0.7654, F1 Macro: 0.6116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1486, Accuracy: 0.9232, F1 Micro: 0.7693, F1 Macro: 0.6215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9226, F1 Micro: 0.7746, F1 Macro: 0.6664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9232, F1 Micro: 0.7759, F1 Macro: 0.6868\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9228, F1 Micro: 0.7673, F1 Macro: 0.6972\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.924, F1 Micro: 0.7707, F1 Macro: 0.6927\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9243, F1 Micro: 0.7756, F1 Macro: 0.7035\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9232, F1 Micro: 0.7759, F1 Macro: 0.6868\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.71      0.66      0.68       157\n",
      "      HS_Race       0.72      0.75      0.73       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 206.04641127586365 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3861, Accuracy: 0.883, F1 Micro: 0.58, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2594, Accuracy: 0.9068, F1 Micro: 0.7065, F1 Macro: 0.5056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.9112, F1 Micro: 0.748, F1 Macro: 0.5894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1784, Accuracy: 0.9178, F1 Micro: 0.7572, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9213, F1 Micro: 0.7707, F1 Macro: 0.6308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9187, F1 Micro: 0.7718, F1 Macro: 0.6707\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9243, F1 Micro: 0.7708, F1 Macro: 0.6812\n",
      "Epoch 8/10, Train Loss: 0.0868, Accuracy: 0.9197, F1 Micro: 0.7706, F1 Macro: 0.682\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9221, F1 Micro: 0.7685, F1 Macro: 0.6884\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9216, F1 Micro: 0.7705, F1 Macro: 0.6956\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9187, F1 Micro: 0.7718, F1 Macro: 0.6707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.66      0.70      0.68       402\n",
      "  HS_Religion       0.70      0.65      0.68       157\n",
      "      HS_Race       0.65      0.82      0.72       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.57      0.66      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.77      5556\n",
      "    macro avg       0.73      0.68      0.67      5556\n",
      " weighted avg       0.75      0.80      0.77      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 206.07477974891663 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9159, F1 Micro: 0.751, F1 Macro: 0.6322\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.72005295753479 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3723, Accuracy: 0.8892, F1 Micro: 0.6303, F1 Macro: 0.3259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9099, F1 Micro: 0.727, F1 Macro: 0.5433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2044, Accuracy: 0.9152, F1 Micro: 0.7343, F1 Macro: 0.548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1701, Accuracy: 0.9213, F1 Micro: 0.7651, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.144, Accuracy: 0.9226, F1 Micro: 0.7732, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9264, F1 Micro: 0.7804, F1 Macro: 0.688\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9237, F1 Micro: 0.7763, F1 Macro: 0.6859\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9228, F1 Micro: 0.7766, F1 Macro: 0.6924\n",
      "Epoch 9/10, Train Loss: 0.0702, Accuracy: 0.9218, F1 Micro: 0.7718, F1 Macro: 0.7016\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9247, F1 Micro: 0.7728, F1 Macro: 0.7019\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9264, F1 Micro: 0.7804, F1 Macro: 0.688\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.78      0.62      0.69       157\n",
      "      HS_Race       0.80      0.68      0.73       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.55      0.33      0.41        51\n",
      "     HS_Other       0.80      0.79      0.80       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.67      0.55      0.60       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 210.3568811416626 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3655, Accuracy: 0.8907, F1 Micro: 0.6449, F1 Macro: 0.3889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2462, Accuracy: 0.9082, F1 Micro: 0.7202, F1 Macro: 0.5577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2033, Accuracy: 0.9164, F1 Micro: 0.7426, F1 Macro: 0.561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1682, Accuracy: 0.9212, F1 Micro: 0.7692, F1 Macro: 0.6141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9221, F1 Micro: 0.7702, F1 Macro: 0.6313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9249, F1 Micro: 0.7711, F1 Macro: 0.6625\n",
      "Epoch 7/10, Train Loss: 0.1008, Accuracy: 0.9245, F1 Micro: 0.764, F1 Macro: 0.6612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9227, F1 Micro: 0.7759, F1 Macro: 0.6845\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9222, F1 Micro: 0.7717, F1 Macro: 0.7001\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9237, F1 Micro: 0.7712, F1 Macro: 0.7069\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9227, F1 Micro: 0.7759, F1 Macro: 0.6845\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.81      0.63      0.71       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.73      0.31      0.44        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.67      0.50      0.58       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 211.90436506271362 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3806, Accuracy: 0.8855, F1 Micro: 0.6248, F1 Macro: 0.3188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2509, Accuracy: 0.908, F1 Micro: 0.7185, F1 Macro: 0.5355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2048, Accuracy: 0.9149, F1 Micro: 0.7351, F1 Macro: 0.5557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1692, Accuracy: 0.9212, F1 Micro: 0.769, F1 Macro: 0.615\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.9221, F1 Micro: 0.7667, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.9234, F1 Micro: 0.7771, F1 Macro: 0.6789\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.925, F1 Micro: 0.7737, F1 Macro: 0.6633\n",
      "Epoch 8/10, Train Loss: 0.0841, Accuracy: 0.9238, F1 Micro: 0.7732, F1 Macro: 0.6705\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9229, F1 Micro: 0.7705, F1 Macro: 0.6903\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9211, F1 Micro: 0.7702, F1 Macro: 0.6997\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9234, F1 Micro: 0.7771, F1 Macro: 0.6789\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.72      0.72      0.73       120\n",
      "  HS_Physical       0.58      0.10      0.17        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.73      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 210.52200365066528 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9162, F1 Micro: 0.752, F1 Macro: 0.6341\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.619260787963867 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3733, Accuracy: 0.8879, F1 Micro: 0.6153, F1 Macro: 0.3163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9049, F1 Micro: 0.727, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2039, Accuracy: 0.9142, F1 Micro: 0.7497, F1 Macro: 0.5899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1663, Accuracy: 0.92, F1 Micro: 0.7663, F1 Macro: 0.6199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9229, F1 Micro: 0.7742, F1 Macro: 0.6744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1155, Accuracy: 0.9244, F1 Micro: 0.7751, F1 Macro: 0.6527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1008, Accuracy: 0.9265, F1 Micro: 0.7765, F1 Macro: 0.6944\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9231, F1 Micro: 0.7758, F1 Macro: 0.6954\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9266, F1 Micro: 0.776, F1 Macro: 0.7002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.925, F1 Micro: 0.777, F1 Macro: 0.7118\n",
      "Model 1 - Iteration 10535: Accuracy: 0.925, F1 Micro: 0.777, F1 Macro: 0.7118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.71      0.66      0.69       402\n",
      "  HS_Religion       0.78      0.67      0.72       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.79      0.31      0.44        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.65      0.60      0.62       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 215.68873167037964 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3671, Accuracy: 0.8893, F1 Micro: 0.6273, F1 Macro: 0.3581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2432, Accuracy: 0.9053, F1 Micro: 0.7231, F1 Macro: 0.5404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2039, Accuracy: 0.9164, F1 Micro: 0.7482, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9203, F1 Micro: 0.7668, F1 Macro: 0.6146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1414, Accuracy: 0.9222, F1 Micro: 0.7711, F1 Macro: 0.6413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.9188, F1 Micro: 0.7711, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9254, F1 Micro: 0.7738, F1 Macro: 0.678\n",
      "Epoch 8/10, Train Loss: 0.0831, Accuracy: 0.9223, F1 Micro: 0.7736, F1 Macro: 0.6797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9225, F1 Micro: 0.775, F1 Macro: 0.6954\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.9227, F1 Micro: 0.7714, F1 Macro: 0.6942\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9225, F1 Micro: 0.775, F1 Macro: 0.6954\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.69      0.64      0.67       402\n",
      "  HS_Religion       0.69      0.66      0.68       157\n",
      "      HS_Race       0.74      0.71      0.72       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 215.17323184013367 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3796, Accuracy: 0.8853, F1 Micro: 0.61, F1 Macro: 0.3075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2479, Accuracy: 0.9013, F1 Micro: 0.7201, F1 Macro: 0.5315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2049, Accuracy: 0.9156, F1 Micro: 0.7431, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9176, F1 Micro: 0.7641, F1 Macro: 0.6164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1432, Accuracy: 0.9231, F1 Micro: 0.7743, F1 Macro: 0.6471\n",
      "Epoch 6/10, Train Loss: 0.1156, Accuracy: 0.9218, F1 Micro: 0.7731, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9237, F1 Micro: 0.7778, F1 Macro: 0.6751\n",
      "Epoch 8/10, Train Loss: 0.0832, Accuracy: 0.9227, F1 Micro: 0.7735, F1 Macro: 0.6798\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9237, F1 Micro: 0.7744, F1 Macro: 0.6889\n",
      "Epoch 10/10, Train Loss: 0.061, Accuracy: 0.9205, F1 Micro: 0.772, F1 Macro: 0.69\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9237, F1 Micro: 0.7778, F1 Macro: 0.6751\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.78      0.54      0.63       157\n",
      "      HS_Race       0.82      0.69      0.75       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.68      0.37      0.48        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 211.95209741592407 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9165, F1 Micro: 0.7529, F1 Macro: 0.6363\n",
      "Total sampling time: 1157.74 seconds\n",
      "Total runtime: 15060.644993066788 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bf3e4",
   "metadata": {
    "papermill": {
     "duration": 0.012185,
     "end_time": "2025-04-13T07:01:23.625001",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.612816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "363ed2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T07:01:23.650592Z",
     "iopub.status.busy": "2025-04-13T07:01:23.650343Z",
     "iopub.status.idle": "2025-04-13T11:23:42.044305Z",
     "shell.execute_reply": "2025-04-13T11:23:42.043593Z"
    },
    "papermill": {
     "duration": 15738.40849,
     "end_time": "2025-04-13T11:23:42.045671",
     "exception": false,
     "start_time": "2025-04-13T07:01:23.637181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6215, Accuracy: 0.8191, F1 Micro: 0.1615, F1 Macro: 0.0752\n",
      "Epoch 2/10, Train Loss: 0.4665, Accuracy: 0.8316, F1 Micro: 0.073, F1 Macro: 0.0276\n",
      "Epoch 3/10, Train Loss: 0.3955, Accuracy: 0.8331, F1 Micro: 0.0914, F1 Macro: 0.0331\n",
      "Epoch 4/10, Train Loss: 0.3996, Accuracy: 0.8322, F1 Micro: 0.0663, F1 Macro: 0.0261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3852, Accuracy: 0.8423, F1 Micro: 0.1862, F1 Macro: 0.0687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3697, Accuracy: 0.8528, F1 Micro: 0.3211, F1 Macro: 0.1051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3565, Accuracy: 0.8617, F1 Micro: 0.408, F1 Macro: 0.1432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3315, Accuracy: 0.8738, F1 Micro: 0.5215, F1 Macro: 0.2324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2962, Accuracy: 0.8754, F1 Micro: 0.5639, F1 Macro: 0.2627\n",
      "Epoch 10/10, Train Loss: 0.263, Accuracy: 0.8768, F1 Micro: 0.5586, F1 Macro: 0.2751\n",
      "Model 1 - Iteration 658: Accuracy: 0.8754, F1 Micro: 0.5639, F1 Macro: 0.2627\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.72      0.75      1134\n",
      "      Abusive       0.82      0.72      0.77       992\n",
      "HS_Individual       0.62      0.48      0.54       732\n",
      "     HS_Group       0.44      0.01      0.02       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.62      0.55      0.58       762\n",
      "      HS_Weak       0.59      0.43      0.49       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.71      0.47      0.56      5556\n",
      "    macro avg       0.32      0.24      0.26      5556\n",
      " weighted avg       0.58      0.47      0.50      5556\n",
      "  samples avg       0.36      0.27      0.28      5556\n",
      "\n",
      "Training completed in 49.22918963432312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6202, Accuracy: 0.8216, F1 Micro: 0.2374, F1 Macro: 0.0537\n",
      "Epoch 2/10, Train Loss: 0.4681, Accuracy: 0.8289, F1 Micro: 0.0206, F1 Macro: 0.009\n",
      "Epoch 3/10, Train Loss: 0.3926, Accuracy: 0.8333, F1 Micro: 0.0952, F1 Macro: 0.0349\n",
      "Epoch 4/10, Train Loss: 0.4, Accuracy: 0.8342, F1 Micro: 0.1006, F1 Macro: 0.0364\n",
      "Epoch 5/10, Train Loss: 0.3855, Accuracy: 0.8385, F1 Micro: 0.1788, F1 Macro: 0.0548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.372, Accuracy: 0.8472, F1 Micro: 0.2544, F1 Macro: 0.0879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3573, Accuracy: 0.8604, F1 Micro: 0.3983, F1 Macro: 0.1601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3352, Accuracy: 0.8711, F1 Micro: 0.4901, F1 Macro: 0.2237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3016, Accuracy: 0.8749, F1 Micro: 0.5685, F1 Macro: 0.264\n",
      "Epoch 10/10, Train Loss: 0.2699, Accuracy: 0.8753, F1 Micro: 0.533, F1 Macro: 0.2503\n",
      "Model 2 - Iteration 658: Accuracy: 0.8749, F1 Micro: 0.5685, F1 Macro: 0.264\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.74      0.75      1134\n",
      "      Abusive       0.76      0.72      0.74       992\n",
      "HS_Individual       0.64      0.52      0.58       732\n",
      "     HS_Group       0.00      0.00      0.00       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.54      0.58       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.00      0.00      0.00       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.48      0.57      5556\n",
      "    macro avg       0.28      0.25      0.26      5556\n",
      " weighted avg       0.54      0.48      0.51      5556\n",
      "  samples avg       0.39      0.28      0.30      5556\n",
      "\n",
      "Training completed in 47.90289306640625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5865, Accuracy: 0.8278, F1 Micro: 0.0554, F1 Macro: 0.0267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4603, Accuracy: 0.8362, F1 Micro: 0.1378, F1 Macro: 0.0461\n",
      "Epoch 3/10, Train Loss: 0.3944, Accuracy: 0.8367, F1 Micro: 0.1344, F1 Macro: 0.0452\n",
      "Epoch 4/10, Train Loss: 0.3978, Accuracy: 0.8345, F1 Micro: 0.0947, F1 Macro: 0.0352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3828, Accuracy: 0.8479, F1 Micro: 0.2674, F1 Macro: 0.091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3636, Accuracy: 0.8522, F1 Micro: 0.3052, F1 Macro: 0.1024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3506, Accuracy: 0.8657, F1 Micro: 0.4335, F1 Macro: 0.1769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3283, Accuracy: 0.8714, F1 Micro: 0.4787, F1 Macro: 0.2164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2903, Accuracy: 0.8776, F1 Micro: 0.5532, F1 Macro: 0.2618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.264, Accuracy: 0.8781, F1 Micro: 0.5728, F1 Macro: 0.2893\n",
      "Model 3 - Iteration 658: Accuracy: 0.8781, F1 Micro: 0.5728, F1 Macro: 0.2893\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.71      0.75      1134\n",
      "      Abusive       0.82      0.71      0.76       992\n",
      "HS_Individual       0.67      0.45      0.54       732\n",
      "     HS_Group       0.46      0.12      0.19       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.54      0.60       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.43      0.06      0.11       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.72      0.47      0.57      5556\n",
      "    macro avg       0.37      0.25      0.29      5556\n",
      " weighted avg       0.62      0.47      0.53      5556\n",
      "  samples avg       0.35      0.27      0.28      5556\n",
      "\n",
      "Training completed in 54.769076108932495 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8761, F1 Micro: 0.5684, F1 Macro: 0.272\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 138.55155301094055 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5323, Accuracy: 0.8355, F1 Micro: 0.1347, F1 Macro: 0.0442\n",
      "Epoch 2/10, Train Loss: 0.4102, Accuracy: 0.8336, F1 Micro: 0.0812, F1 Macro: 0.0325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.37, Accuracy: 0.8574, F1 Micro: 0.3994, F1 Macro: 0.1218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3449, Accuracy: 0.8779, F1 Micro: 0.5473, F1 Macro: 0.2513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2993, Accuracy: 0.8818, F1 Micro: 0.6145, F1 Macro: 0.3221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2758, Accuracy: 0.8888, F1 Micro: 0.6331, F1 Macro: 0.3316\n",
      "Epoch 7/10, Train Loss: 0.2491, Accuracy: 0.8883, F1 Micro: 0.6209, F1 Macro: 0.3332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2178, Accuracy: 0.8906, F1 Micro: 0.6627, F1 Macro: 0.4728\n",
      "Epoch 9/10, Train Loss: 0.204, Accuracy: 0.8933, F1 Micro: 0.6546, F1 Macro: 0.4367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1787, Accuracy: 0.8945, F1 Micro: 0.6739, F1 Macro: 0.4608\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8945, F1 Micro: 0.6739, F1 Macro: 0.4608\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.80      0.80      1134\n",
      "      Abusive       0.85      0.81      0.83       992\n",
      "HS_Individual       0.65      0.67      0.66       732\n",
      "     HS_Group       0.67      0.43      0.52       402\n",
      "  HS_Religion       0.54      0.18      0.27       157\n",
      "      HS_Race       0.85      0.29      0.43       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.69      0.69       762\n",
      "      HS_Weak       0.62      0.63      0.62       689\n",
      "  HS_Moderate       0.48      0.28      0.35       331\n",
      "    HS_Strong       0.92      0.21      0.34       114\n",
      "\n",
      "    micro avg       0.72      0.63      0.67      5556\n",
      "    macro avg       0.59      0.42      0.46      5556\n",
      " weighted avg       0.70      0.63      0.65      5556\n",
      "  samples avg       0.38      0.35      0.35      5556\n",
      "\n",
      "Training completed in 69.54517269134521 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5327, Accuracy: 0.8295, F1 Micro: 0.029, F1 Macro: 0.0125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4094, Accuracy: 0.8356, F1 Micro: 0.121, F1 Macro: 0.0428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3716, Accuracy: 0.8546, F1 Micro: 0.3306, F1 Macro: 0.1197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3447, Accuracy: 0.8775, F1 Micro: 0.569, F1 Macro: 0.2638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3039, Accuracy: 0.8807, F1 Micro: 0.5782, F1 Macro: 0.2781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2798, Accuracy: 0.8865, F1 Micro: 0.6311, F1 Macro: 0.3247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2543, Accuracy: 0.8905, F1 Micro: 0.6392, F1 Macro: 0.3469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.224, Accuracy: 0.8936, F1 Micro: 0.665, F1 Macro: 0.424\n",
      "Epoch 9/10, Train Loss: 0.2099, Accuracy: 0.8952, F1 Micro: 0.66, F1 Macro: 0.4327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1829, Accuracy: 0.8967, F1 Micro: 0.6848, F1 Macro: 0.4911\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8967, F1 Micro: 0.6848, F1 Macro: 0.4911\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.83      0.81      1134\n",
      "      Abusive       0.85      0.80      0.83       992\n",
      "HS_Individual       0.67      0.63      0.65       732\n",
      "     HS_Group       0.61      0.54      0.57       402\n",
      "  HS_Religion       0.66      0.31      0.42       157\n",
      "      HS_Race       0.81      0.48      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.71      0.70       762\n",
      "      HS_Weak       0.64      0.59      0.62       689\n",
      "  HS_Moderate       0.47      0.40      0.43       331\n",
      "    HS_Strong       0.94      0.15      0.26       114\n",
      "\n",
      "    micro avg       0.72      0.65      0.68      5556\n",
      "    macro avg       0.60      0.45      0.49      5556\n",
      " weighted avg       0.71      0.65      0.67      5556\n",
      "  samples avg       0.39      0.36      0.35      5556\n",
      "\n",
      "Training completed in 74.11136102676392 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5175, Accuracy: 0.8387, F1 Micro: 0.1823, F1 Macro: 0.0563\n",
      "Epoch 2/10, Train Loss: 0.4091, Accuracy: 0.8374, F1 Micro: 0.1306, F1 Macro: 0.049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3674, Accuracy: 0.8606, F1 Micro: 0.436, F1 Macro: 0.1554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3418, Accuracy: 0.8776, F1 Micro: 0.5404, F1 Macro: 0.251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3022, Accuracy: 0.8824, F1 Micro: 0.5859, F1 Macro: 0.2902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2744, Accuracy: 0.8873, F1 Micro: 0.6416, F1 Macro: 0.3483\n",
      "Epoch 7/10, Train Loss: 0.2503, Accuracy: 0.8884, F1 Micro: 0.6159, F1 Macro: 0.3433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2197, Accuracy: 0.8927, F1 Micro: 0.6592, F1 Macro: 0.4247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2033, Accuracy: 0.8945, F1 Micro: 0.6605, F1 Macro: 0.4259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1797, Accuracy: 0.8965, F1 Micro: 0.6823, F1 Macro: 0.4708\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8965, F1 Micro: 0.6823, F1 Macro: 0.4708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.82      0.81      1134\n",
      "      Abusive       0.86      0.82      0.84       992\n",
      "HS_Individual       0.65      0.64      0.65       732\n",
      "     HS_Group       0.65      0.51      0.57       402\n",
      "  HS_Religion       0.60      0.34      0.43       157\n",
      "      HS_Race       0.93      0.23      0.37       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.69      0.68       762\n",
      "      HS_Weak       0.63      0.62      0.62       689\n",
      "  HS_Moderate       0.51      0.36      0.42       331\n",
      "    HS_Strong       0.94      0.14      0.24       114\n",
      "\n",
      "    micro avg       0.72      0.64      0.68      5556\n",
      "    macro avg       0.61      0.43      0.47      5556\n",
      " weighted avg       0.71      0.64      0.66      5556\n",
      "  samples avg       0.38      0.36      0.35      5556\n",
      "\n",
      "Training completed in 71.49227833747864 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.886, F1 Micro: 0.6244, F1 Macro: 0.3731\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 124.32946228981018 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.49, Accuracy: 0.832, F1 Micro: 0.071, F1 Macro: 0.0275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3814, Accuracy: 0.8562, F1 Micro: 0.3634, F1 Macro: 0.1196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3304, Accuracy: 0.8772, F1 Micro: 0.6015, F1 Macro: 0.2875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2885, Accuracy: 0.8874, F1 Micro: 0.6427, F1 Macro: 0.3534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2452, Accuracy: 0.8919, F1 Micro: 0.6445, F1 Macro: 0.404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2234, Accuracy: 0.8973, F1 Micro: 0.6926, F1 Macro: 0.4737\n",
      "Epoch 7/10, Train Loss: 0.2018, Accuracy: 0.8994, F1 Micro: 0.681, F1 Macro: 0.4704\n",
      "Epoch 8/10, Train Loss: 0.1749, Accuracy: 0.9009, F1 Micro: 0.6863, F1 Macro: 0.4919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1558, Accuracy: 0.8943, F1 Micro: 0.7059, F1 Macro: 0.5349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1352, Accuracy: 0.9028, F1 Micro: 0.7079, F1 Macro: 0.5442\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9028, F1 Micro: 0.7079, F1 Macro: 0.5442\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.82      0.82      1134\n",
      "      Abusive       0.87      0.84      0.85       992\n",
      "HS_Individual       0.70      0.62      0.66       732\n",
      "     HS_Group       0.60      0.63      0.62       402\n",
      "  HS_Religion       0.60      0.52      0.55       157\n",
      "      HS_Race       0.74      0.65      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.72      0.72       762\n",
      "      HS_Weak       0.66      0.60      0.63       689\n",
      "  HS_Moderate       0.50      0.53      0.51       331\n",
      "    HS_Strong       0.90      0.32      0.48       114\n",
      "\n",
      "    micro avg       0.73      0.68      0.71      5556\n",
      "    macro avg       0.59      0.52      0.54      5556\n",
      " weighted avg       0.72      0.68      0.70      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 85.80198264122009 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4914, Accuracy: 0.8367, F1 Micro: 0.1446, F1 Macro: 0.0537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3828, Accuracy: 0.8554, F1 Micro: 0.3599, F1 Macro: 0.1343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3339, Accuracy: 0.8782, F1 Micro: 0.6036, F1 Macro: 0.2862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2912, Accuracy: 0.8832, F1 Micro: 0.6473, F1 Macro: 0.3524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2543, Accuracy: 0.8919, F1 Micro: 0.6669, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2336, Accuracy: 0.8974, F1 Micro: 0.6719, F1 Macro: 0.4262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2051, Accuracy: 0.9006, F1 Micro: 0.6824, F1 Macro: 0.4832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1821, Accuracy: 0.9034, F1 Micro: 0.7049, F1 Macro: 0.513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1596, Accuracy: 0.9005, F1 Micro: 0.7095, F1 Macro: 0.5194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.138, Accuracy: 0.9047, F1 Micro: 0.7101, F1 Macro: 0.5244\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9047, F1 Micro: 0.7101, F1 Macro: 0.5244\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.82      1134\n",
      "      Abusive       0.86      0.85      0.86       992\n",
      "HS_Individual       0.69      0.64      0.67       732\n",
      "     HS_Group       0.65      0.60      0.62       402\n",
      "  HS_Religion       0.66      0.38      0.48       157\n",
      "      HS_Race       0.77      0.53      0.62       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.73      0.73       762\n",
      "      HS_Weak       0.66      0.61      0.63       689\n",
      "  HS_Moderate       0.54      0.45      0.49       331\n",
      "    HS_Strong       0.90      0.23      0.36       114\n",
      "\n",
      "    micro avg       0.75      0.68      0.71      5556\n",
      "    macro avg       0.61      0.49      0.52      5556\n",
      " weighted avg       0.73      0.68      0.70      5556\n",
      "  samples avg       0.41      0.38      0.37      5556\n",
      "\n",
      "Training completed in 89.88311123847961 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4804, Accuracy: 0.8344, F1 Micro: 0.0927, F1 Macro: 0.035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3793, Accuracy: 0.854, F1 Micro: 0.3291, F1 Macro: 0.1165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3302, Accuracy: 0.881, F1 Micro: 0.5608, F1 Macro: 0.2628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2867, Accuracy: 0.8882, F1 Micro: 0.6496, F1 Macro: 0.3717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2513, Accuracy: 0.8928, F1 Micro: 0.6733, F1 Macro: 0.4104\n",
      "Epoch 6/10, Train Loss: 0.2293, Accuracy: 0.8969, F1 Micro: 0.6722, F1 Macro: 0.4292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2052, Accuracy: 0.9, F1 Micro: 0.6818, F1 Macro: 0.468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1807, Accuracy: 0.9016, F1 Micro: 0.6877, F1 Macro: 0.4847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1577, Accuracy: 0.8994, F1 Micro: 0.7116, F1 Macro: 0.5245\n",
      "Epoch 10/10, Train Loss: 0.1371, Accuracy: 0.9023, F1 Micro: 0.7103, F1 Macro: 0.5356\n",
      "Model 3 - Iteration 2535: Accuracy: 0.8994, F1 Micro: 0.7116, F1 Macro: 0.5245\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.87      0.82      1134\n",
      "      Abusive       0.82      0.91      0.86       992\n",
      "HS_Individual       0.67      0.66      0.66       732\n",
      "     HS_Group       0.59      0.65      0.62       402\n",
      "  HS_Religion       0.65      0.41      0.50       157\n",
      "      HS_Race       0.83      0.48      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.79      0.73       762\n",
      "      HS_Weak       0.63      0.66      0.64       689\n",
      "  HS_Moderate       0.48      0.53      0.51       331\n",
      "    HS_Strong       0.92      0.21      0.34       114\n",
      "\n",
      "    micro avg       0.70      0.72      0.71      5556\n",
      "    macro avg       0.59      0.51      0.52      5556\n",
      " weighted avg       0.69      0.72      0.70      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 85.83182048797607 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8915, F1 Micro: 0.6529, F1 Macro: 0.4258\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 112.60283303260803 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4687, Accuracy: 0.8404, F1 Micro: 0.1726, F1 Macro: 0.064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3537, Accuracy: 0.8771, F1 Micro: 0.5541, F1 Macro: 0.2509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2997, Accuracy: 0.8851, F1 Micro: 0.6445, F1 Macro: 0.3259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2558, Accuracy: 0.8927, F1 Micro: 0.6499, F1 Macro: 0.368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2296, Accuracy: 0.8987, F1 Micro: 0.6844, F1 Macro: 0.4779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1959, Accuracy: 0.9038, F1 Micro: 0.6977, F1 Macro: 0.5243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1706, Accuracy: 0.907, F1 Micro: 0.7121, F1 Macro: 0.5196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1539, Accuracy: 0.9066, F1 Micro: 0.7181, F1 Macro: 0.5504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1312, Accuracy: 0.9038, F1 Micro: 0.7248, F1 Macro: 0.5641\n",
      "Epoch 10/10, Train Loss: 0.116, Accuracy: 0.9103, F1 Micro: 0.7185, F1 Macro: 0.5591\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9038, F1 Micro: 0.7248, F1 Macro: 0.5641\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.86      0.82      1134\n",
      "      Abusive       0.83      0.90      0.86       992\n",
      "HS_Individual       0.68      0.68      0.68       732\n",
      "     HS_Group       0.61      0.65      0.63       402\n",
      "  HS_Religion       0.69      0.50      0.58       157\n",
      "      HS_Race       0.70      0.64      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.79      0.74       762\n",
      "      HS_Weak       0.66      0.65      0.66       689\n",
      "  HS_Moderate       0.49      0.56      0.53       331\n",
      "    HS_Strong       0.90      0.46      0.60       114\n",
      "\n",
      "    micro avg       0.71      0.74      0.72      5556\n",
      "    macro avg       0.59      0.56      0.56      5556\n",
      " weighted avg       0.70      0.74      0.71      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 102.23313975334167 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4695, Accuracy: 0.8369, F1 Micro: 0.1413, F1 Macro: 0.0489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3567, Accuracy: 0.8752, F1 Micro: 0.5953, F1 Macro: 0.2736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3063, Accuracy: 0.8853, F1 Micro: 0.6263, F1 Macro: 0.3087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2634, Accuracy: 0.8937, F1 Micro: 0.6412, F1 Macro: 0.3742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2369, Accuracy: 0.8999, F1 Micro: 0.6873, F1 Macro: 0.4817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2005, Accuracy: 0.9033, F1 Micro: 0.6915, F1 Macro: 0.4963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1762, Accuracy: 0.9079, F1 Micro: 0.7178, F1 Macro: 0.5147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.154, Accuracy: 0.905, F1 Micro: 0.7184, F1 Macro: 0.5416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1318, Accuracy: 0.9081, F1 Micro: 0.7191, F1 Macro: 0.5473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1123, Accuracy: 0.9073, F1 Micro: 0.729, F1 Macro: 0.5601\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9073, F1 Micro: 0.729, F1 Macro: 0.5601\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1134\n",
      "      Abusive       0.83      0.90      0.87       992\n",
      "HS_Individual       0.68      0.69      0.69       732\n",
      "     HS_Group       0.66      0.62      0.64       402\n",
      "  HS_Religion       0.79      0.36      0.50       157\n",
      "      HS_Race       0.80      0.62      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.79      0.74       762\n",
      "      HS_Weak       0.66      0.66      0.66       689\n",
      "  HS_Moderate       0.55      0.50      0.52       331\n",
      "    HS_Strong       0.92      0.43      0.59       114\n",
      "\n",
      "    micro avg       0.73      0.72      0.73      5556\n",
      "    macro avg       0.62      0.54      0.56      5556\n",
      " weighted avg       0.72      0.72      0.72      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 103.93268966674805 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4628, Accuracy: 0.8445, F1 Micro: 0.2319, F1 Macro: 0.0816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3518, Accuracy: 0.8762, F1 Micro: 0.5217, F1 Macro: 0.2317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2997, Accuracy: 0.8885, F1 Micro: 0.6413, F1 Macro: 0.3358\n",
      "Epoch 4/10, Train Loss: 0.256, Accuracy: 0.8905, F1 Micro: 0.6247, F1 Macro: 0.3475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2298, Accuracy: 0.8994, F1 Micro: 0.6884, F1 Macro: 0.49\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.196, Accuracy: 0.9054, F1 Micro: 0.7108, F1 Macro: 0.5229\n",
      "Epoch 7/10, Train Loss: 0.1688, Accuracy: 0.9067, F1 Micro: 0.7061, F1 Macro: 0.5165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1514, Accuracy: 0.9053, F1 Micro: 0.72, F1 Macro: 0.5273\n",
      "Epoch 9/10, Train Loss: 0.1302, Accuracy: 0.9045, F1 Micro: 0.7102, F1 Macro: 0.539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.113, Accuracy: 0.9068, F1 Micro: 0.7248, F1 Macro: 0.5624\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9068, F1 Micro: 0.7248, F1 Macro: 0.5624\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.82      1134\n",
      "      Abusive       0.85      0.89      0.87       992\n",
      "HS_Individual       0.70      0.66      0.68       732\n",
      "     HS_Group       0.63      0.65      0.64       402\n",
      "  HS_Religion       0.67      0.49      0.57       157\n",
      "      HS_Race       0.71      0.59      0.65       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.77      0.75       762\n",
      "      HS_Weak       0.66      0.63      0.64       689\n",
      "  HS_Moderate       0.51      0.53      0.52       331\n",
      "    HS_Strong       0.93      0.46      0.62       114\n",
      "\n",
      "    micro avg       0.74      0.71      0.72      5556\n",
      "    macro avg       0.60      0.54      0.56      5556\n",
      " weighted avg       0.72      0.71      0.71      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 99.07550930976868 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8951, F1 Micro: 0.6712, F1 Macro: 0.4599\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 100.91688084602356 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4525, Accuracy: 0.8487, F1 Micro: 0.295, F1 Macro: 0.0975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3402, Accuracy: 0.88, F1 Micro: 0.6044, F1 Macro: 0.2915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2814, Accuracy: 0.8921, F1 Micro: 0.6338, F1 Macro: 0.3297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2427, Accuracy: 0.8985, F1 Micro: 0.7021, F1 Macro: 0.4849\n",
      "Epoch 5/10, Train Loss: 0.2053, Accuracy: 0.9047, F1 Micro: 0.6871, F1 Macro: 0.4885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1863, Accuracy: 0.9054, F1 Micro: 0.7185, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1583, Accuracy: 0.9087, F1 Micro: 0.7235, F1 Macro: 0.5314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1391, Accuracy: 0.9099, F1 Micro: 0.7292, F1 Macro: 0.5531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.118, Accuracy: 0.9109, F1 Micro: 0.7345, F1 Macro: 0.5758\n",
      "Epoch 10/10, Train Loss: 0.1069, Accuracy: 0.9039, F1 Micro: 0.7229, F1 Macro: 0.5814\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9109, F1 Micro: 0.7345, F1 Macro: 0.5758\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1134\n",
      "      Abusive       0.89      0.84      0.86       992\n",
      "HS_Individual       0.68      0.71      0.70       732\n",
      "     HS_Group       0.74      0.58      0.65       402\n",
      "  HS_Religion       0.67      0.57      0.61       157\n",
      "      HS_Race       0.73      0.66      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.77      0.74       762\n",
      "      HS_Weak       0.66      0.69      0.67       689\n",
      "  HS_Moderate       0.60      0.47      0.52       331\n",
      "    HS_Strong       0.92      0.47      0.62       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.73      5556\n",
      "    macro avg       0.62      0.55      0.58      5556\n",
      " weighted avg       0.74      0.72      0.72      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 111.61607313156128 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4514, Accuracy: 0.8466, F1 Micro: 0.2735, F1 Macro: 0.0906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3414, Accuracy: 0.88, F1 Micro: 0.6066, F1 Macro: 0.2869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2844, Accuracy: 0.8927, F1 Micro: 0.6332, F1 Macro: 0.3623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2479, Accuracy: 0.8903, F1 Micro: 0.7001, F1 Macro: 0.4895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2121, Accuracy: 0.9047, F1 Micro: 0.7072, F1 Macro: 0.5014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1884, Accuracy: 0.9062, F1 Micro: 0.7235, F1 Macro: 0.537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1601, Accuracy: 0.91, F1 Micro: 0.7324, F1 Macro: 0.5363\n",
      "Epoch 8/10, Train Loss: 0.1381, Accuracy: 0.9113, F1 Micro: 0.7247, F1 Macro: 0.5555\n",
      "Epoch 9/10, Train Loss: 0.1171, Accuracy: 0.9067, F1 Micro: 0.7316, F1 Macro: 0.5585\n",
      "Epoch 10/10, Train Loss: 0.1038, Accuracy: 0.9049, F1 Micro: 0.7262, F1 Macro: 0.5854\n",
      "Model 2 - Iteration 4055: Accuracy: 0.91, F1 Micro: 0.7324, F1 Macro: 0.5363\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.86      0.86      0.86       992\n",
      "HS_Individual       0.68      0.73      0.71       732\n",
      "     HS_Group       0.74      0.54      0.62       402\n",
      "  HS_Religion       0.76      0.50      0.60       157\n",
      "      HS_Race       0.81      0.55      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.79      0.75       762\n",
      "      HS_Weak       0.65      0.71      0.68       689\n",
      "  HS_Moderate       0.58      0.43      0.50       331\n",
      "    HS_Strong       1.00      0.12      0.22       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.73      5556\n",
      "    macro avg       0.64      0.51      0.54      5556\n",
      " weighted avg       0.74      0.72      0.72      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 110.21615099906921 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4477, Accuracy: 0.8499, F1 Micro: 0.325, F1 Macro: 0.1041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3405, Accuracy: 0.8815, F1 Micro: 0.607, F1 Macro: 0.2946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2824, Accuracy: 0.8911, F1 Micro: 0.6219, F1 Macro: 0.3463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2426, Accuracy: 0.8961, F1 Micro: 0.7056, F1 Macro: 0.4954\n",
      "Epoch 5/10, Train Loss: 0.2073, Accuracy: 0.9036, F1 Micro: 0.6831, F1 Macro: 0.4798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1869, Accuracy: 0.9086, F1 Micro: 0.7208, F1 Macro: 0.5303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1605, Accuracy: 0.9109, F1 Micro: 0.7315, F1 Macro: 0.5364\n",
      "Epoch 8/10, Train Loss: 0.1402, Accuracy: 0.9095, F1 Micro: 0.7286, F1 Macro: 0.5521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1185, Accuracy: 0.9105, F1 Micro: 0.7362, F1 Macro: 0.5556\n",
      "Epoch 10/10, Train Loss: 0.1074, Accuracy: 0.9102, F1 Micro: 0.7359, F1 Macro: 0.5832\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9105, F1 Micro: 0.7362, F1 Macro: 0.5556\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.69      0.73      0.71       732\n",
      "     HS_Group       0.70      0.59      0.64       402\n",
      "  HS_Religion       0.63      0.61      0.62       157\n",
      "      HS_Race       0.71      0.53      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.77      0.75       762\n",
      "      HS_Weak       0.66      0.71      0.68       689\n",
      "  HS_Moderate       0.58      0.48      0.52       331\n",
      "    HS_Strong       0.97      0.28      0.44       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.74      5556\n",
      "    macro avg       0.61      0.53      0.56      5556\n",
      " weighted avg       0.73      0.72      0.72      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 109.68489241600037 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.8982, F1 Micro: 0.6838, F1 Macro: 0.4791\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 91.20245575904846 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4459, Accuracy: 0.8525, F1 Micro: 0.425, F1 Macro: 0.1247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.328, Accuracy: 0.8842, F1 Micro: 0.6369, F1 Macro: 0.3501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.268, Accuracy: 0.8997, F1 Micro: 0.6854, F1 Macro: 0.44\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2273, Accuracy: 0.9029, F1 Micro: 0.6996, F1 Macro: 0.4645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.203, Accuracy: 0.909, F1 Micro: 0.7215, F1 Macro: 0.5344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1703, Accuracy: 0.9103, F1 Micro: 0.7319, F1 Macro: 0.5617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1484, Accuracy: 0.9122, F1 Micro: 0.7403, F1 Macro: 0.5774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1237, Accuracy: 0.9131, F1 Micro: 0.7487, F1 Macro: 0.5884\n",
      "Epoch 9/10, Train Loss: 0.1117, Accuracy: 0.91, F1 Micro: 0.7385, F1 Macro: 0.5896\n",
      "Epoch 10/10, Train Loss: 0.0959, Accuracy: 0.9144, F1 Micro: 0.7463, F1 Macro: 0.5983\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9131, F1 Micro: 0.7487, F1 Macro: 0.5884\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.74      0.56      0.63       402\n",
      "  HS_Religion       0.75      0.49      0.59       157\n",
      "      HS_Race       0.77      0.60      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.82      0.76       762\n",
      "      HS_Weak       0.65      0.76      0.70       689\n",
      "  HS_Moderate       0.62      0.42      0.50       331\n",
      "    HS_Strong       0.89      0.68      0.77       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.62      0.57      0.59      5556\n",
      " weighted avg       0.73      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 122.83175134658813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4449, Accuracy: 0.8554, F1 Micro: 0.3797, F1 Macro: 0.1243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3321, Accuracy: 0.886, F1 Micro: 0.6202, F1 Macro: 0.3057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2728, Accuracy: 0.8974, F1 Micro: 0.6861, F1 Macro: 0.4508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2332, Accuracy: 0.9032, F1 Micro: 0.6974, F1 Macro: 0.4717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2082, Accuracy: 0.9065, F1 Micro: 0.7155, F1 Macro: 0.538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1727, Accuracy: 0.9111, F1 Micro: 0.7272, F1 Macro: 0.5355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1496, Accuracy: 0.9108, F1 Micro: 0.7347, F1 Macro: 0.5682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1242, Accuracy: 0.9123, F1 Micro: 0.7372, F1 Macro: 0.5697\n",
      "Epoch 9/10, Train Loss: 0.1137, Accuracy: 0.9126, F1 Micro: 0.7337, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.097, Accuracy: 0.9129, F1 Micro: 0.7392, F1 Macro: 0.5817\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9129, F1 Micro: 0.7392, F1 Macro: 0.5817\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.83      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.69      0.73      0.71       732\n",
      "     HS_Group       0.73      0.56      0.63       402\n",
      "  HS_Religion       0.79      0.45      0.57       157\n",
      "      HS_Race       0.79      0.60      0.68       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.77      0.75       762\n",
      "      HS_Weak       0.67      0.70      0.68       689\n",
      "  HS_Moderate       0.64      0.45      0.53       331\n",
      "    HS_Strong       0.94      0.53      0.67       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.72      0.54      0.58      5556\n",
      " weighted avg       0.76      0.72      0.73      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 124.43004703521729 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4408, Accuracy: 0.8548, F1 Micro: 0.3862, F1 Macro: 0.1229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3284, Accuracy: 0.8854, F1 Micro: 0.6399, F1 Macro: 0.3385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2701, Accuracy: 0.8971, F1 Micro: 0.672, F1 Macro: 0.4072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2293, Accuracy: 0.9025, F1 Micro: 0.7073, F1 Macro: 0.4876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2027, Accuracy: 0.9065, F1 Micro: 0.7222, F1 Macro: 0.5395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1725, Accuracy: 0.9112, F1 Micro: 0.7242, F1 Macro: 0.5325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1529, Accuracy: 0.9114, F1 Micro: 0.7324, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1241, Accuracy: 0.9122, F1 Micro: 0.7416, F1 Macro: 0.5715\n",
      "Epoch 9/10, Train Loss: 0.1134, Accuracy: 0.9111, F1 Micro: 0.7368, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0956, Accuracy: 0.9138, F1 Micro: 0.7483, F1 Macro: 0.5977\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9138, F1 Micro: 0.7483, F1 Macro: 0.5977\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.67      0.76      0.71       732\n",
      "     HS_Group       0.72      0.58      0.64       402\n",
      "  HS_Religion       0.77      0.52      0.62       157\n",
      "      HS_Race       0.78      0.54      0.64       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       1.00      0.08      0.15        51\n",
      "     HS_Other       0.72      0.81      0.76       762\n",
      "      HS_Weak       0.66      0.74      0.70       689\n",
      "  HS_Moderate       0.61      0.46      0.53       331\n",
      "    HS_Strong       0.89      0.51      0.65       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5556\n",
      "    macro avg       0.79      0.56      0.60      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.40      5556\n",
      "\n",
      "Training completed in 124.27632999420166 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9007, F1 Micro: 0.6941, F1 Macro: 0.4974\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 82.01399397850037 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4369, Accuracy: 0.8585, F1 Micro: 0.4094, F1 Macro: 0.13\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3223, Accuracy: 0.8905, F1 Micro: 0.6421, F1 Macro: 0.3597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2575, Accuracy: 0.9009, F1 Micro: 0.7021, F1 Macro: 0.4792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2208, Accuracy: 0.9052, F1 Micro: 0.7231, F1 Macro: 0.5376\n",
      "Epoch 5/10, Train Loss: 0.1947, Accuracy: 0.9084, F1 Micro: 0.7208, F1 Macro: 0.5401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1639, Accuracy: 0.9151, F1 Micro: 0.745, F1 Macro: 0.5915\n",
      "Epoch 7/10, Train Loss: 0.1413, Accuracy: 0.9109, F1 Micro: 0.7373, F1 Macro: 0.5679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1219, Accuracy: 0.9149, F1 Micro: 0.7519, F1 Macro: 0.6115\n",
      "Epoch 9/10, Train Loss: 0.101, Accuracy: 0.9142, F1 Micro: 0.748, F1 Macro: 0.6177\n",
      "Epoch 10/10, Train Loss: 0.0915, Accuracy: 0.9126, F1 Micro: 0.7516, F1 Macro: 0.6177\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9149, F1 Micro: 0.7519, F1 Macro: 0.6115\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.69      0.75      0.72       732\n",
      "     HS_Group       0.71      0.60      0.65       402\n",
      "  HS_Religion       0.77      0.54      0.63       157\n",
      "      HS_Race       0.78      0.66      0.71       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.57      0.08      0.14        51\n",
      "     HS_Other       0.75      0.78      0.76       762\n",
      "      HS_Weak       0.66      0.73      0.70       689\n",
      "  HS_Moderate       0.59      0.51      0.55       331\n",
      "    HS_Strong       0.92      0.62      0.74       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.72      0.59      0.61      5556\n",
      " weighted avg       0.75      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 129.7161328792572 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4354, Accuracy: 0.86, F1 Micro: 0.4122, F1 Macro: 0.1527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3241, Accuracy: 0.891, F1 Micro: 0.6419, F1 Macro: 0.3463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2597, Accuracy: 0.901, F1 Micro: 0.6945, F1 Macro: 0.4491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2213, Accuracy: 0.9043, F1 Micro: 0.724, F1 Macro: 0.5265\n",
      "Epoch 5/10, Train Loss: 0.199, Accuracy: 0.9095, F1 Micro: 0.7164, F1 Macro: 0.5075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.166, Accuracy: 0.9147, F1 Micro: 0.7417, F1 Macro: 0.578\n",
      "Epoch 7/10, Train Loss: 0.1416, Accuracy: 0.9136, F1 Micro: 0.7393, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1202, Accuracy: 0.9173, F1 Micro: 0.7526, F1 Macro: 0.5975\n",
      "Epoch 9/10, Train Loss: 0.1005, Accuracy: 0.9162, F1 Micro: 0.7503, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0926, Accuracy: 0.9148, F1 Micro: 0.7527, F1 Macro: 0.6344\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9148, F1 Micro: 0.7527, F1 Macro: 0.6344\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.89      0.86      0.87       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.66      0.66      0.66       402\n",
      "  HS_Religion       0.74      0.57      0.64       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.64      0.14      0.23        51\n",
      "     HS_Other       0.72      0.80      0.76       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.56      0.57      0.57       331\n",
      "    HS_Strong       0.88      0.70      0.78       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.74      0.61      0.63      5556\n",
      " weighted avg       0.75      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 130.9387993812561 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4304, Accuracy: 0.8606, F1 Micro: 0.4115, F1 Macro: 0.1508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3206, Accuracy: 0.8919, F1 Micro: 0.6505, F1 Macro: 0.3598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2558, Accuracy: 0.9018, F1 Micro: 0.6998, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2195, Accuracy: 0.905, F1 Micro: 0.7225, F1 Macro: 0.5104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1924, Accuracy: 0.9108, F1 Micro: 0.7279, F1 Macro: 0.5549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.162, Accuracy: 0.9148, F1 Micro: 0.7391, F1 Macro: 0.5697\n",
      "Epoch 7/10, Train Loss: 0.1358, Accuracy: 0.9144, F1 Micro: 0.7346, F1 Macro: 0.5748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1175, Accuracy: 0.9156, F1 Micro: 0.7524, F1 Macro: 0.5984\n",
      "Epoch 9/10, Train Loss: 0.0993, Accuracy: 0.9153, F1 Micro: 0.7484, F1 Macro: 0.6278\n",
      "Epoch 10/10, Train Loss: 0.0896, Accuracy: 0.916, F1 Micro: 0.7473, F1 Macro: 0.607\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9156, F1 Micro: 0.7524, F1 Macro: 0.5984\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.85      0.90      0.88       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.78      0.55      0.64       157\n",
      "      HS_Race       0.76      0.61      0.68       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.66      0.73      0.70       689\n",
      "  HS_Moderate       0.57      0.53      0.55       331\n",
      "    HS_Strong       0.95      0.33      0.49       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.74      0.57      0.60      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 131.09366059303284 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9027, F1 Micro: 0.7024, F1 Macro: 0.5142\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 73.97811698913574 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4288, Accuracy: 0.8677, F1 Micro: 0.4984, F1 Macro: 0.2013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3072, Accuracy: 0.8929, F1 Micro: 0.6543, F1 Macro: 0.4167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2525, Accuracy: 0.9034, F1 Micro: 0.7122, F1 Macro: 0.5162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2168, Accuracy: 0.9093, F1 Micro: 0.7252, F1 Macro: 0.5536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1802, Accuracy: 0.9117, F1 Micro: 0.746, F1 Macro: 0.577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1552, Accuracy: 0.9136, F1 Micro: 0.7501, F1 Macro: 0.5969\n",
      "Epoch 7/10, Train Loss: 0.1342, Accuracy: 0.9182, F1 Micro: 0.7433, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1138, Accuracy: 0.9193, F1 Micro: 0.7599, F1 Macro: 0.6221\n",
      "Epoch 9/10, Train Loss: 0.099, Accuracy: 0.9191, F1 Micro: 0.7534, F1 Macro: 0.6533\n",
      "Epoch 10/10, Train Loss: 0.0834, Accuracy: 0.9194, F1 Micro: 0.7571, F1 Macro: 0.6425\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9193, F1 Micro: 0.7599, F1 Macro: 0.6221\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.72      0.71      0.71       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.79      0.48      0.60       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.57      0.08      0.14        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.63      0.53      0.58       331\n",
      "    HS_Strong       0.91      0.76      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.73      0.59      0.62      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 139.55228638648987 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4291, Accuracy: 0.8707, F1 Micro: 0.5366, F1 Macro: 0.2371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3121, Accuracy: 0.8921, F1 Micro: 0.6571, F1 Macro: 0.3889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2549, Accuracy: 0.9022, F1 Micro: 0.706, F1 Macro: 0.4871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.221, Accuracy: 0.9097, F1 Micro: 0.7135, F1 Macro: 0.5375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1838, Accuracy: 0.9125, F1 Micro: 0.7404, F1 Macro: 0.5507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1537, Accuracy: 0.9105, F1 Micro: 0.7483, F1 Macro: 0.5896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1368, Accuracy: 0.9173, F1 Micro: 0.7556, F1 Macro: 0.6137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1137, Accuracy: 0.9187, F1 Micro: 0.7574, F1 Macro: 0.6153\n",
      "Epoch 9/10, Train Loss: 0.0966, Accuracy: 0.9208, F1 Micro: 0.7572, F1 Macro: 0.655\n",
      "Epoch 10/10, Train Loss: 0.0837, Accuracy: 0.9151, F1 Micro: 0.7379, F1 Macro: 0.6128\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9187, F1 Micro: 0.7574, F1 Macro: 0.6153\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.79      0.53      0.63       157\n",
      "      HS_Race       0.85      0.60      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       1.00      0.04      0.08        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.62      0.53      0.57       331\n",
      "    HS_Strong       0.91      0.71      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.81      0.58      0.62      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 141.585129737854 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4236, Accuracy: 0.8719, F1 Micro: 0.5067, F1 Macro: 0.2124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3081, Accuracy: 0.8926, F1 Micro: 0.6605, F1 Macro: 0.4134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2519, Accuracy: 0.9039, F1 Micro: 0.7107, F1 Macro: 0.5117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.216, Accuracy: 0.9101, F1 Micro: 0.7133, F1 Macro: 0.5251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1831, Accuracy: 0.9125, F1 Micro: 0.7413, F1 Macro: 0.5468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1542, Accuracy: 0.9141, F1 Micro: 0.748, F1 Macro: 0.5768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1357, Accuracy: 0.9183, F1 Micro: 0.7538, F1 Macro: 0.6282\n",
      "Epoch 8/10, Train Loss: 0.1114, Accuracy: 0.916, F1 Micro: 0.747, F1 Macro: 0.5999\n",
      "Epoch 9/10, Train Loss: 0.0969, Accuracy: 0.9185, F1 Micro: 0.7495, F1 Macro: 0.6386\n",
      "Epoch 10/10, Train Loss: 0.0823, Accuracy: 0.9181, F1 Micro: 0.7504, F1 Macro: 0.6367\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9183, F1 Micro: 0.7538, F1 Macro: 0.6282\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.68      0.65      0.67       402\n",
      "  HS_Religion       0.79      0.52      0.63       157\n",
      "      HS_Race       0.76      0.57      0.65       120\n",
      "  HS_Physical       0.62      0.07      0.12        72\n",
      "    HS_Gender       0.55      0.22      0.31        51\n",
      "     HS_Other       0.78      0.75      0.76       762\n",
      "      HS_Weak       0.71      0.67      0.69       689\n",
      "  HS_Moderate       0.57      0.56      0.56       331\n",
      "    HS_Strong       0.93      0.55      0.69       114\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5556\n",
      "    macro avg       0.74      0.58      0.63      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 139.788565158844 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9047, F1 Micro: 0.7092, F1 Macro: 0.5277\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 67.27705693244934 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4195, Accuracy: 0.8728, F1 Micro: 0.5074, F1 Macro: 0.2198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2934, Accuracy: 0.8966, F1 Micro: 0.6795, F1 Macro: 0.4446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2425, Accuracy: 0.9045, F1 Micro: 0.6959, F1 Macro: 0.4502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2082, Accuracy: 0.9113, F1 Micro: 0.7388, F1 Macro: 0.5527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1802, Accuracy: 0.9149, F1 Micro: 0.7522, F1 Macro: 0.589\n",
      "Epoch 6/10, Train Loss: 0.1502, Accuracy: 0.9141, F1 Micro: 0.7433, F1 Macro: 0.6023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1328, Accuracy: 0.918, F1 Micro: 0.755, F1 Macro: 0.6064\n",
      "Epoch 8/10, Train Loss: 0.113, Accuracy: 0.9195, F1 Micro: 0.7519, F1 Macro: 0.5948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0923, Accuracy: 0.9179, F1 Micro: 0.7563, F1 Macro: 0.6519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0794, Accuracy: 0.9191, F1 Micro: 0.7605, F1 Macro: 0.6629\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9191, F1 Micro: 0.7605, F1 Macro: 0.6629\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.69      0.74      0.72       732\n",
      "     HS_Group       0.74      0.56      0.64       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.72      0.69      0.70       120\n",
      "  HS_Physical       0.71      0.14      0.23        72\n",
      "    HS_Gender       0.68      0.29      0.41        51\n",
      "     HS_Other       0.77      0.78      0.77       762\n",
      "      HS_Weak       0.67      0.72      0.70       689\n",
      "  HS_Moderate       0.69      0.43      0.53       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 150.5317521095276 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4199, Accuracy: 0.8723, F1 Micro: 0.5112, F1 Macro: 0.228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2985, Accuracy: 0.8928, F1 Micro: 0.6736, F1 Macro: 0.382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.246, Accuracy: 0.9026, F1 Micro: 0.686, F1 Macro: 0.4331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9107, F1 Micro: 0.7406, F1 Macro: 0.5671\n",
      "Epoch 5/10, Train Loss: 0.181, Accuracy: 0.9156, F1 Micro: 0.7368, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1501, Accuracy: 0.9179, F1 Micro: 0.7594, F1 Macro: 0.6098\n",
      "Epoch 7/10, Train Loss: 0.1334, Accuracy: 0.921, F1 Micro: 0.7547, F1 Macro: 0.6049\n",
      "Epoch 8/10, Train Loss: 0.1117, Accuracy: 0.9191, F1 Micro: 0.7473, F1 Macro: 0.6036\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9197, F1 Micro: 0.7565, F1 Macro: 0.6376\n",
      "Epoch 10/10, Train Loss: 0.0789, Accuracy: 0.9187, F1 Micro: 0.7584, F1 Macro: 0.6506\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9179, F1 Micro: 0.7594, F1 Macro: 0.6098\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.75      0.58      0.65       157\n",
      "      HS_Race       0.77      0.73      0.75       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.71      0.68      0.69       689\n",
      "  HS_Moderate       0.56      0.61      0.58       331\n",
      "    HS_Strong       0.89      0.64      0.74       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.63      0.60      0.61      5556\n",
      " weighted avg       0.75      0.75      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 145.75618648529053 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4158, Accuracy: 0.87, F1 Micro: 0.4559, F1 Macro: 0.1927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2979, Accuracy: 0.8941, F1 Micro: 0.6795, F1 Macro: 0.416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2435, Accuracy: 0.9051, F1 Micro: 0.6944, F1 Macro: 0.4702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2098, Accuracy: 0.9102, F1 Micro: 0.7388, F1 Macro: 0.554\n",
      "Epoch 5/10, Train Loss: 0.1811, Accuracy: 0.9153, F1 Micro: 0.7386, F1 Macro: 0.5666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.151, Accuracy: 0.9149, F1 Micro: 0.7462, F1 Macro: 0.59\n",
      "Epoch 7/10, Train Loss: 0.1331, Accuracy: 0.9172, F1 Micro: 0.745, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1122, Accuracy: 0.9186, F1 Micro: 0.7539, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9152, F1 Micro: 0.7574, F1 Macro: 0.6503\n",
      "Epoch 10/10, Train Loss: 0.0787, Accuracy: 0.9166, F1 Micro: 0.7574, F1 Macro: 0.6529\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9152, F1 Micro: 0.7574, F1 Macro: 0.6503\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1134\n",
      "      Abusive       0.88      0.87      0.87       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.63      0.68      0.66       402\n",
      "  HS_Religion       0.63      0.61      0.62       157\n",
      "      HS_Race       0.73      0.69      0.71       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.73      0.22      0.33        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.56      0.58      0.57       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.73      0.64      0.65      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 148.78919649124146 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9061, F1 Micro: 0.7148, F1 Macro: 0.5403\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 60.13470411300659 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.42, Accuracy: 0.8766, F1 Micro: 0.5351, F1 Macro: 0.239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.8961, F1 Micro: 0.687, F1 Macro: 0.4732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.9074, F1 Micro: 0.7046, F1 Macro: 0.4906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2037, Accuracy: 0.9131, F1 Micro: 0.724, F1 Macro: 0.5379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1709, Accuracy: 0.9093, F1 Micro: 0.7463, F1 Macro: 0.5832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1491, Accuracy: 0.9156, F1 Micro: 0.7568, F1 Macro: 0.5991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1271, Accuracy: 0.9154, F1 Micro: 0.7577, F1 Macro: 0.6094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1041, Accuracy: 0.9197, F1 Micro: 0.7612, F1 Macro: 0.647\n",
      "Epoch 9/10, Train Loss: 0.0921, Accuracy: 0.9159, F1 Micro: 0.7558, F1 Macro: 0.6554\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9193, F1 Micro: 0.7606, F1 Macro: 0.6625\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9197, F1 Micro: 0.7612, F1 Macro: 0.647\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.89      0.87      0.88       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.68      0.66      0.67       402\n",
      "  HS_Religion       0.72      0.57      0.63       157\n",
      "      HS_Race       0.72      0.72      0.73       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.77      0.78      0.77       762\n",
      "      HS_Weak       0.72      0.67      0.70       689\n",
      "  HS_Moderate       0.61      0.58      0.59       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.74      0.62      0.65      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 155.0328402519226 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4213, Accuracy: 0.8728, F1 Micro: 0.5029, F1 Macro: 0.2262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2972, Accuracy: 0.8969, F1 Micro: 0.6839, F1 Macro: 0.4405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2463, Accuracy: 0.9068, F1 Micro: 0.7012, F1 Macro: 0.4899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2046, Accuracy: 0.9116, F1 Micro: 0.7326, F1 Macro: 0.5589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.174, Accuracy: 0.9095, F1 Micro: 0.7484, F1 Macro: 0.5818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1516, Accuracy: 0.9115, F1 Micro: 0.7534, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1321, Accuracy: 0.9135, F1 Micro: 0.7571, F1 Macro: 0.615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1049, Accuracy: 0.9166, F1 Micro: 0.7612, F1 Macro: 0.6572\n",
      "Epoch 9/10, Train Loss: 0.0939, Accuracy: 0.9107, F1 Micro: 0.7522, F1 Macro: 0.63\n",
      "Epoch 10/10, Train Loss: 0.0775, Accuracy: 0.9182, F1 Micro: 0.7545, F1 Macro: 0.6589\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9166, F1 Micro: 0.7612, F1 Macro: 0.6572\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.67      0.80      0.73       732\n",
      "     HS_Group       0.73      0.59      0.65       402\n",
      "  HS_Religion       0.67      0.63      0.65       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.54      0.18      0.27        72\n",
      "    HS_Gender       0.56      0.20      0.29        51\n",
      "     HS_Other       0.76      0.77      0.77       762\n",
      "      HS_Weak       0.65      0.78      0.71       689\n",
      "  HS_Moderate       0.64      0.50      0.56       331\n",
      "    HS_Strong       0.89      0.72      0.80       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.71      0.64      0.66      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 158.24881792068481 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4152, Accuracy: 0.877, F1 Micro: 0.5318, F1 Macro: 0.2409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2923, Accuracy: 0.8955, F1 Micro: 0.6797, F1 Macro: 0.4493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2426, Accuracy: 0.9054, F1 Micro: 0.6911, F1 Macro: 0.4788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2032, Accuracy: 0.9136, F1 Micro: 0.736, F1 Macro: 0.5554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1719, Accuracy: 0.9083, F1 Micro: 0.7424, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1506, Accuracy: 0.9133, F1 Micro: 0.7546, F1 Macro: 0.5947\n",
      "Epoch 7/10, Train Loss: 0.1308, Accuracy: 0.9095, F1 Micro: 0.75, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.92, F1 Micro: 0.7602, F1 Macro: 0.6517\n",
      "Epoch 9/10, Train Loss: 0.0935, Accuracy: 0.9167, F1 Micro: 0.7577, F1 Macro: 0.6484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9187, F1 Micro: 0.7627, F1 Macro: 0.6667\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9187, F1 Micro: 0.7627, F1 Macro: 0.6667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.68      0.63      0.65       402\n",
      "  HS_Religion       0.69      0.57      0.62       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.72      0.18      0.29        72\n",
      "    HS_Gender       0.68      0.29      0.41        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.59      0.55      0.57       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.74      0.64      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 160.55347061157227 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9074, F1 Micro: 0.7195, F1 Macro: 0.5519\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 57.32991147041321 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4173, Accuracy: 0.8751, F1 Micro: 0.515, F1 Macro: 0.2313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.8988, F1 Micro: 0.6692, F1 Macro: 0.4425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2423, Accuracy: 0.8986, F1 Micro: 0.7208, F1 Macro: 0.5501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2096, Accuracy: 0.9128, F1 Micro: 0.7314, F1 Macro: 0.5363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.178, Accuracy: 0.915, F1 Micro: 0.7504, F1 Macro: 0.5955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9189, F1 Micro: 0.757, F1 Macro: 0.6107\n",
      "Epoch 7/10, Train Loss: 0.1225, Accuracy: 0.9162, F1 Micro: 0.7555, F1 Macro: 0.616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1057, Accuracy: 0.9172, F1 Micro: 0.7612, F1 Macro: 0.64\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0869, Accuracy: 0.918, F1 Micro: 0.7619, F1 Macro: 0.6614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0776, Accuracy: 0.9203, F1 Micro: 0.764, F1 Macro: 0.6826\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9203, F1 Micro: 0.764, F1 Macro: 0.6826\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.71      0.72      0.72       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.72      0.18      0.29        72\n",
      "    HS_Gender       0.73      0.37      0.49        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.65      0.55      0.59       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.76      0.64      0.68      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 169.38307309150696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4189, Accuracy: 0.8731, F1 Micro: 0.4993, F1 Macro: 0.2217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2982, Accuracy: 0.8958, F1 Micro: 0.6488, F1 Macro: 0.3822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2451, Accuracy: 0.9007, F1 Micro: 0.7201, F1 Macro: 0.5308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9114, F1 Micro: 0.7261, F1 Macro: 0.5065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.178, Accuracy: 0.9157, F1 Micro: 0.7541, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.148, Accuracy: 0.9214, F1 Micro: 0.7626, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.124, Accuracy: 0.9189, F1 Micro: 0.7638, F1 Macro: 0.6152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1044, Accuracy: 0.919, F1 Micro: 0.7665, F1 Macro: 0.6413\n",
      "Epoch 9/10, Train Loss: 0.0868, Accuracy: 0.9181, F1 Micro: 0.756, F1 Macro: 0.6419\n",
      "Epoch 10/10, Train Loss: 0.0821, Accuracy: 0.9206, F1 Micro: 0.7614, F1 Macro: 0.6583\n",
      "Model 2 - Iteration 6980: Accuracy: 0.919, F1 Micro: 0.7665, F1 Macro: 0.6413\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.91      0.71      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.75      0.62      0.64      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.42      0.42      0.41      5556\n",
      "\n",
      "Training completed in 166.9518494606018 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4131, Accuracy: 0.8761, F1 Micro: 0.5119, F1 Macro: 0.2312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2925, Accuracy: 0.8964, F1 Micro: 0.6642, F1 Macro: 0.3976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2437, Accuracy: 0.9008, F1 Micro: 0.7228, F1 Macro: 0.5454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2109, Accuracy: 0.9131, F1 Micro: 0.7308, F1 Macro: 0.5378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.9153, F1 Micro: 0.7425, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1455, Accuracy: 0.9187, F1 Micro: 0.758, F1 Macro: 0.606\n",
      "Epoch 7/10, Train Loss: 0.1236, Accuracy: 0.9169, F1 Micro: 0.7546, F1 Macro: 0.5902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9163, F1 Micro: 0.764, F1 Macro: 0.6463\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.9169, F1 Micro: 0.7503, F1 Macro: 0.6441\n",
      "Epoch 10/10, Train Loss: 0.0792, Accuracy: 0.9213, F1 Micro: 0.7621, F1 Macro: 0.6658\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9163, F1 Micro: 0.764, F1 Macro: 0.6463\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.65      0.67      0.66       402\n",
      "  HS_Religion       0.67      0.62      0.64       157\n",
      "      HS_Race       0.77      0.56      0.65       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.74      0.27      0.40        51\n",
      "     HS_Other       0.73      0.85      0.78       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.55      0.58      0.57       331\n",
      "    HS_Strong       0.93      0.67      0.78       114\n",
      "\n",
      "    micro avg       0.74      0.79      0.76      5556\n",
      "    macro avg       0.74      0.63      0.65      5556\n",
      " weighted avg       0.75      0.79      0.76      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 163.14969158172607 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9084, F1 Micro: 0.7236, F1 Macro: 0.5615\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 53.11274552345276 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4107, Accuracy: 0.878, F1 Micro: 0.5409, F1 Macro: 0.2478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2851, Accuracy: 0.8996, F1 Micro: 0.6911, F1 Macro: 0.4573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2324, Accuracy: 0.9091, F1 Micro: 0.7344, F1 Macro: 0.5558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2005, Accuracy: 0.9162, F1 Micro: 0.741, F1 Macro: 0.5831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9187, F1 Micro: 0.7621, F1 Macro: 0.6148\n",
      "Epoch 6/10, Train Loss: 0.1389, Accuracy: 0.9201, F1 Micro: 0.7613, F1 Macro: 0.6177\n",
      "Epoch 7/10, Train Loss: 0.1181, Accuracy: 0.9173, F1 Micro: 0.7565, F1 Macro: 0.6138\n",
      "Epoch 8/10, Train Loss: 0.102, Accuracy: 0.9154, F1 Micro: 0.7601, F1 Macro: 0.6605\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9195, F1 Micro: 0.7614, F1 Macro: 0.6599\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9157, F1 Micro: 0.7559, F1 Macro: 0.6691\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9187, F1 Micro: 0.7621, F1 Macro: 0.6148\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.88      0.87       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.72      0.53      0.61       157\n",
      "      HS_Race       0.69      0.78      0.73       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.67      0.04      0.07        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.60      0.53      0.56       331\n",
      "    HS_Strong       0.89      0.70      0.78       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.68      0.60      0.61      5556\n",
      " weighted avg       0.76      0.76      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 166.98663353919983 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4101, Accuracy: 0.8767, F1 Micro: 0.5255, F1 Macro: 0.2411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2868, Accuracy: 0.8974, F1 Micro: 0.6863, F1 Macro: 0.4813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2348, Accuracy: 0.9079, F1 Micro: 0.7287, F1 Macro: 0.5368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2017, Accuracy: 0.9158, F1 Micro: 0.7347, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.919, F1 Micro: 0.7552, F1 Macro: 0.5948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1402, Accuracy: 0.9213, F1 Micro: 0.7593, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1204, Accuracy: 0.9213, F1 Micro: 0.7653, F1 Macro: 0.6354\n",
      "Epoch 8/10, Train Loss: 0.1054, Accuracy: 0.9189, F1 Micro: 0.7647, F1 Macro: 0.6681\n",
      "Epoch 9/10, Train Loss: 0.0879, Accuracy: 0.9177, F1 Micro: 0.7637, F1 Macro: 0.6612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.9221, F1 Micro: 0.7661, F1 Macro: 0.6694\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9221, F1 Micro: 0.7661, F1 Macro: 0.6694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.82      0.65      0.73       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.58      0.27      0.37        51\n",
      "     HS_Other       0.78      0.76      0.77       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.92      0.77      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5556\n",
      "    macro avg       0.78      0.62      0.67      5556\n",
      " weighted avg       0.79      0.74      0.76      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 170.33578824996948 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4078, Accuracy: 0.8746, F1 Micro: 0.4978, F1 Macro: 0.2229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2874, Accuracy: 0.8999, F1 Micro: 0.6821, F1 Macro: 0.4515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2324, Accuracy: 0.906, F1 Micro: 0.7256, F1 Macro: 0.5386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2, Accuracy: 0.9159, F1 Micro: 0.7377, F1 Macro: 0.5778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1647, Accuracy: 0.9185, F1 Micro: 0.7526, F1 Macro: 0.5966\n",
      "Epoch 6/10, Train Loss: 0.1397, Accuracy: 0.92, F1 Micro: 0.7514, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9213, F1 Micro: 0.7642, F1 Macro: 0.6335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1027, Accuracy: 0.92, F1 Micro: 0.769, F1 Macro: 0.6636\n",
      "Epoch 9/10, Train Loss: 0.087, Accuracy: 0.9168, F1 Micro: 0.7565, F1 Macro: 0.6544\n",
      "Epoch 10/10, Train Loss: 0.0758, Accuracy: 0.9217, F1 Micro: 0.758, F1 Macro: 0.6505\n",
      "Model 3 - Iteration 7336: Accuracy: 0.92, F1 Micro: 0.769, F1 Macro: 0.6636\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.72      0.66      0.69       157\n",
      "      HS_Race       0.75      0.56      0.64       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.75      0.83      0.78       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.88      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.64      0.66      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 165.48081851005554 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9094, F1 Micro: 0.7271, F1 Macro: 0.5688\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 45.83756446838379 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4051, Accuracy: 0.8773, F1 Micro: 0.5324, F1 Macro: 0.2475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2833, Accuracy: 0.8992, F1 Micro: 0.692, F1 Macro: 0.449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2397, Accuracy: 0.9077, F1 Micro: 0.7206, F1 Macro: 0.4898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2014, Accuracy: 0.9157, F1 Micro: 0.7414, F1 Macro: 0.5752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1649, Accuracy: 0.9166, F1 Micro: 0.7598, F1 Macro: 0.6099\n",
      "Epoch 6/10, Train Loss: 0.1424, Accuracy: 0.9196, F1 Micro: 0.7548, F1 Macro: 0.6151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1225, Accuracy: 0.9218, F1 Micro: 0.767, F1 Macro: 0.6343\n",
      "Epoch 8/10, Train Loss: 0.1021, Accuracy: 0.9219, F1 Micro: 0.7627, F1 Macro: 0.6656\n",
      "Epoch 9/10, Train Loss: 0.0855, Accuracy: 0.9219, F1 Micro: 0.7668, F1 Macro: 0.6757\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9207, F1 Micro: 0.7603, F1 Macro: 0.6766\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9218, F1 Micro: 0.767, F1 Macro: 0.6343\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.89      0.64      0.74       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.64      0.52      0.57       331\n",
      "    HS_Strong       0.89      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.60      0.63      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 168.74895191192627 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4061, Accuracy: 0.8814, F1 Micro: 0.5664, F1 Macro: 0.2644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.899, F1 Micro: 0.6939, F1 Macro: 0.4545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2416, Accuracy: 0.9068, F1 Micro: 0.7222, F1 Macro: 0.5133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.9154, F1 Micro: 0.7429, F1 Macro: 0.5716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9173, F1 Micro: 0.7598, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1423, Accuracy: 0.9197, F1 Micro: 0.7629, F1 Macro: 0.6196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9211, F1 Micro: 0.7696, F1 Macro: 0.638\n",
      "Epoch 8/10, Train Loss: 0.1006, Accuracy: 0.9223, F1 Micro: 0.7652, F1 Macro: 0.656\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9218, F1 Micro: 0.7642, F1 Macro: 0.6625\n",
      "Epoch 10/10, Train Loss: 0.0693, Accuracy: 0.921, F1 Micro: 0.769, F1 Macro: 0.6854\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9211, F1 Micro: 0.7696, F1 Macro: 0.638\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.70      0.63      0.67       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.82      0.58      0.68       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.76      0.61      0.64      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 170.18686151504517 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4029, Accuracy: 0.877, F1 Micro: 0.5116, F1 Macro: 0.2335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2838, Accuracy: 0.9002, F1 Micro: 0.6903, F1 Macro: 0.4428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2388, Accuracy: 0.9074, F1 Micro: 0.7216, F1 Macro: 0.5035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1974, Accuracy: 0.9135, F1 Micro: 0.7432, F1 Macro: 0.5681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9125, F1 Micro: 0.7536, F1 Macro: 0.5923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.139, Accuracy: 0.9174, F1 Micro: 0.7567, F1 Macro: 0.612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1197, Accuracy: 0.9219, F1 Micro: 0.7621, F1 Macro: 0.6375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0982, Accuracy: 0.9205, F1 Micro: 0.7636, F1 Macro: 0.661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.084, Accuracy: 0.9217, F1 Micro: 0.7664, F1 Macro: 0.6775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9234, F1 Micro: 0.775, F1 Macro: 0.6869\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9234, F1 Micro: 0.775, F1 Macro: 0.6869\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.76      0.64      0.69       157\n",
      "      HS_Race       0.74      0.75      0.75       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.71      0.24      0.35        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 175.46335411071777 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9104, F1 Micro: 0.7304, F1 Macro: 0.5753\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 41.32299613952637 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4015, Accuracy: 0.8814, F1 Micro: 0.5833, F1 Macro: 0.2787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2756, Accuracy: 0.9008, F1 Micro: 0.6988, F1 Macro: 0.465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2321, Accuracy: 0.9112, F1 Micro: 0.721, F1 Macro: 0.5456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1955, Accuracy: 0.9174, F1 Micro: 0.7505, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1666, Accuracy: 0.9163, F1 Micro: 0.7506, F1 Macro: 0.6033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1447, Accuracy: 0.9204, F1 Micro: 0.761, F1 Macro: 0.6265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.116, Accuracy: 0.9225, F1 Micro: 0.7718, F1 Macro: 0.6608\n",
      "Epoch 8/10, Train Loss: 0.1, Accuracy: 0.9216, F1 Micro: 0.7688, F1 Macro: 0.6668\n",
      "Epoch 9/10, Train Loss: 0.0877, Accuracy: 0.9214, F1 Micro: 0.769, F1 Macro: 0.6854\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9226, F1 Micro: 0.7691, F1 Macro: 0.6925\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9225, F1 Micro: 0.7718, F1 Macro: 0.6608\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.75      0.59      0.66       402\n",
      "  HS_Religion       0.78      0.58      0.66       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.68      0.29      0.41        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.66      0.51      0.58       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.77      0.63      0.66      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 174.41352891921997 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.404, Accuracy: 0.8802, F1 Micro: 0.5897, F1 Macro: 0.2842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.9, F1 Micro: 0.6866, F1 Macro: 0.4511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2368, Accuracy: 0.9116, F1 Micro: 0.725, F1 Macro: 0.5502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1975, Accuracy: 0.9167, F1 Micro: 0.7467, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1665, Accuracy: 0.9179, F1 Micro: 0.7547, F1 Macro: 0.6065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1428, Accuracy: 0.92, F1 Micro: 0.7563, F1 Macro: 0.6231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1135, Accuracy: 0.9217, F1 Micro: 0.7715, F1 Macro: 0.6575\n",
      "Epoch 8/10, Train Loss: 0.0978, Accuracy: 0.922, F1 Micro: 0.7647, F1 Macro: 0.659\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9211, F1 Micro: 0.7714, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0722, Accuracy: 0.9237, F1 Micro: 0.7721, F1 Macro: 0.6887\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9237, F1 Micro: 0.7721, F1 Macro: 0.6887\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.76      0.61      0.68       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.82      0.61      0.70       120\n",
      "  HS_Physical       0.62      0.22      0.33        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.82      0.72      0.77       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.68      0.54      0.60       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.76      0.65      0.69      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 175.83880019187927 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3988, Accuracy: 0.8815, F1 Micro: 0.575, F1 Macro: 0.2802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2791, Accuracy: 0.8991, F1 Micro: 0.6975, F1 Macro: 0.4758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2347, Accuracy: 0.9111, F1 Micro: 0.7241, F1 Macro: 0.5485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1956, Accuracy: 0.9177, F1 Micro: 0.7502, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9172, F1 Micro: 0.7522, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.141, Accuracy: 0.9185, F1 Micro: 0.7529, F1 Macro: 0.6087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1115, Accuracy: 0.9213, F1 Micro: 0.7668, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.096, Accuracy: 0.922, F1 Micro: 0.7724, F1 Macro: 0.6841\n",
      "Epoch 9/10, Train Loss: 0.0833, Accuracy: 0.9193, F1 Micro: 0.7673, F1 Macro: 0.6876\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9195, F1 Micro: 0.7669, F1 Macro: 0.6892\n",
      "Model 3 - Iteration 7901: Accuracy: 0.922, F1 Micro: 0.7724, F1 Macro: 0.6841\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.69      0.59      0.63       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 175.90998721122742 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9112, F1 Micro: 0.7334, F1 Macro: 0.5826\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 37.98313236236572 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4046, Accuracy: 0.8797, F1 Micro: 0.6074, F1 Macro: 0.2945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2801, Accuracy: 0.9018, F1 Micro: 0.6699, F1 Macro: 0.4462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2334, Accuracy: 0.9094, F1 Micro: 0.736, F1 Macro: 0.549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9173, F1 Micro: 0.7513, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1678, Accuracy: 0.9196, F1 Micro: 0.7603, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9158, F1 Micro: 0.7614, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9187, F1 Micro: 0.7623, F1 Macro: 0.645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0984, Accuracy: 0.9171, F1 Micro: 0.7673, F1 Macro: 0.6646\n",
      "Epoch 9/10, Train Loss: 0.0851, Accuracy: 0.9203, F1 Micro: 0.7619, F1 Macro: 0.6599\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.9233, F1 Micro: 0.7672, F1 Macro: 0.668\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9171, F1 Micro: 0.7673, F1 Macro: 0.6646\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.84      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.68      0.79      0.73       732\n",
      "     HS_Group       0.69      0.62      0.66       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.80      0.58      0.68       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.70      0.31      0.43        51\n",
      "     HS_Other       0.71      0.85      0.77       762\n",
      "      HS_Weak       0.66      0.78      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.74      0.79      0.77      5556\n",
      "    macro avg       0.75      0.65      0.66      5556\n",
      " weighted avg       0.74      0.79      0.76      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 181.7089765071869 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.404, Accuracy: 0.8815, F1 Micro: 0.5883, F1 Macro: 0.2785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2827, Accuracy: 0.9025, F1 Micro: 0.6815, F1 Macro: 0.4646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2317, Accuracy: 0.9067, F1 Micro: 0.7359, F1 Macro: 0.5574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1953, Accuracy: 0.9173, F1 Micro: 0.7521, F1 Macro: 0.5859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1676, Accuracy: 0.9182, F1 Micro: 0.7627, F1 Macro: 0.6119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9176, F1 Micro: 0.7654, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9178, F1 Micro: 0.7654, F1 Macro: 0.6522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0967, Accuracy: 0.9218, F1 Micro: 0.7707, F1 Macro: 0.6748\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9232, F1 Micro: 0.7686, F1 Macro: 0.6665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9204, F1 Micro: 0.7712, F1 Macro: 0.6698\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9204, F1 Micro: 0.7712, F1 Macro: 0.6698\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.75      0.24      0.36        51\n",
      "     HS_Other       0.73      0.84      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.61      0.62      0.61       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.76      0.65      0.67      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 182.96503329277039 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.401, Accuracy: 0.8816, F1 Micro: 0.6011, F1 Macro: 0.2888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.9026, F1 Micro: 0.6835, F1 Macro: 0.4672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2311, Accuracy: 0.9091, F1 Micro: 0.7345, F1 Macro: 0.527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.9168, F1 Micro: 0.7481, F1 Macro: 0.5718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.165, Accuracy: 0.9199, F1 Micro: 0.7593, F1 Macro: 0.607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.137, Accuracy: 0.9191, F1 Micro: 0.7657, F1 Macro: 0.6343\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9168, F1 Micro: 0.7552, F1 Macro: 0.6356\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9175, F1 Micro: 0.7632, F1 Macro: 0.6558\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9171, F1 Micro: 0.757, F1 Macro: 0.6574\n",
      "Epoch 10/10, Train Loss: 0.0735, Accuracy: 0.9209, F1 Micro: 0.7626, F1 Macro: 0.6617\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9191, F1 Micro: 0.7657, F1 Macro: 0.6343\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.65      0.67      0.66       402\n",
      "  HS_Religion       0.72      0.56      0.63       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.56      0.59      0.57       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.73      0.62      0.63      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 177.7709662914276 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9117, F1 Micro: 0.7357, F1 Macro: 0.5875\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 33.72346496582031 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3959, Accuracy: 0.8817, F1 Micro: 0.6164, F1 Macro: 0.3038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.9027, F1 Micro: 0.7, F1 Macro: 0.4689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.91, F1 Micro: 0.7341, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1886, Accuracy: 0.912, F1 Micro: 0.7504, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.9176, F1 Micro: 0.7626, F1 Macro: 0.6098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.9173, F1 Micro: 0.7642, F1 Macro: 0.6436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.115, Accuracy: 0.9239, F1 Micro: 0.7695, F1 Macro: 0.6677\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.9148, F1 Micro: 0.7618, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0846, Accuracy: 0.9258, F1 Micro: 0.7738, F1 Macro: 0.6854\n",
      "Epoch 10/10, Train Loss: 0.0707, Accuracy: 0.9193, F1 Micro: 0.7696, F1 Macro: 0.6828\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9258, F1 Micro: 0.7738, F1 Macro: 0.6854\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.76      0.62      0.68       402\n",
      "  HS_Religion       0.82      0.54      0.65       157\n",
      "      HS_Race       0.78      0.78      0.78       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.81      0.76      0.78       762\n",
      "      HS_Weak       0.75      0.67      0.71       689\n",
      "  HS_Moderate       0.69      0.51      0.59       331\n",
      "    HS_Strong       0.84      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.77      0.64      0.69      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.44      0.41      0.41      5556\n",
      "\n",
      "Training completed in 185.045503616333 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3969, Accuracy: 0.8816, F1 Micro: 0.6092, F1 Macro: 0.2905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2788, Accuracy: 0.9022, F1 Micro: 0.7042, F1 Macro: 0.4828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2245, Accuracy: 0.913, F1 Micro: 0.7373, F1 Macro: 0.5466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.9179, F1 Micro: 0.7524, F1 Macro: 0.5881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1621, Accuracy: 0.9169, F1 Micro: 0.7631, F1 Macro: 0.6065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9163, F1 Micro: 0.768, F1 Macro: 0.6433\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9209, F1 Micro: 0.7655, F1 Macro: 0.6624\n",
      "Epoch 8/10, Train Loss: 0.0953, Accuracy: 0.9196, F1 Micro: 0.7649, F1 Macro: 0.6677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.084, Accuracy: 0.9232, F1 Micro: 0.7701, F1 Macro: 0.6764\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9189, F1 Micro: 0.7609, F1 Macro: 0.6753\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9232, F1 Micro: 0.7701, F1 Macro: 0.6764\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.77      0.58      0.66       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.62      0.31      0.42        51\n",
      "     HS_Other       0.77      0.78      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.69      0.48      0.57       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.78      0.63      0.68      5556\n",
      " weighted avg       0.79      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 183.23334646224976 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3929, Accuracy: 0.8823, F1 Micro: 0.6292, F1 Macro: 0.3165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2746, Accuracy: 0.9031, F1 Micro: 0.6931, F1 Macro: 0.4842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.9084, F1 Micro: 0.7373, F1 Macro: 0.5458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1896, Accuracy: 0.9132, F1 Micro: 0.7522, F1 Macro: 0.5766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1621, Accuracy: 0.9175, F1 Micro: 0.7589, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9187, F1 Micro: 0.7676, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1166, Accuracy: 0.9246, F1 Micro: 0.7684, F1 Macro: 0.6589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0961, Accuracy: 0.9242, F1 Micro: 0.7734, F1 Macro: 0.6773\n",
      "Epoch 9/10, Train Loss: 0.0813, Accuracy: 0.9232, F1 Micro: 0.7713, F1 Macro: 0.6815\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9179, F1 Micro: 0.7675, F1 Macro: 0.6832\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9242, F1 Micro: 0.7734, F1 Macro: 0.6773\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.78      0.55      0.64       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.71      0.33      0.45        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.73      0.68      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.64      0.68      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 184.286292552948 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9125, F1 Micro: 0.738, F1 Macro: 0.5933\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 31.019272089004517 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3972, Accuracy: 0.8827, F1 Micro: 0.5867, F1 Macro: 0.2949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.9053, F1 Micro: 0.7086, F1 Macro: 0.4991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.9121, F1 Micro: 0.7377, F1 Macro: 0.5661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9182, F1 Micro: 0.7548, F1 Macro: 0.5969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1613, Accuracy: 0.921, F1 Micro: 0.7625, F1 Macro: 0.6093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.9212, F1 Micro: 0.7703, F1 Macro: 0.6362\n",
      "Epoch 7/10, Train Loss: 0.112, Accuracy: 0.9248, F1 Micro: 0.769, F1 Macro: 0.657\n",
      "Epoch 8/10, Train Loss: 0.0963, Accuracy: 0.9218, F1 Micro: 0.7695, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0808, Accuracy: 0.9223, F1 Micro: 0.7731, F1 Macro: 0.6927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9245, F1 Micro: 0.7788, F1 Macro: 0.698\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9245, F1 Micro: 0.7788, F1 Macro: 0.698\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.77      0.58      0.66       402\n",
      "  HS_Religion       0.76      0.61      0.68       157\n",
      "      HS_Race       0.83      0.72      0.77       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.79      0.81      0.80       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.67      0.49      0.57       331\n",
      "    HS_Strong       0.91      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.15652871131897 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3998, Accuracy: 0.8807, F1 Micro: 0.5497, F1 Macro: 0.2605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2745, Accuracy: 0.9033, F1 Micro: 0.7039, F1 Macro: 0.4763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2258, Accuracy: 0.91, F1 Micro: 0.7341, F1 Macro: 0.5788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.9175, F1 Micro: 0.7491, F1 Macro: 0.5866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1613, Accuracy: 0.9191, F1 Micro: 0.7605, F1 Macro: 0.6022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9197, F1 Micro: 0.7677, F1 Macro: 0.6356\n",
      "Epoch 7/10, Train Loss: 0.1115, Accuracy: 0.9207, F1 Micro: 0.7675, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9204, F1 Micro: 0.7729, F1 Macro: 0.6592\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.9201, F1 Micro: 0.7689, F1 Macro: 0.6815\n",
      "Epoch 10/10, Train Loss: 0.0716, Accuracy: 0.9218, F1 Micro: 0.7728, F1 Macro: 0.692\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9204, F1 Micro: 0.7729, F1 Macro: 0.6592\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.67      0.16      0.25        51\n",
      "     HS_Other       0.74      0.85      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.60      0.53      0.56       331\n",
      "    HS_Strong       0.88      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.76      0.64      0.66      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 188.00245928764343 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3938, Accuracy: 0.8808, F1 Micro: 0.5546, F1 Macro: 0.2723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2712, Accuracy: 0.9037, F1 Micro: 0.6956, F1 Macro: 0.4715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.9112, F1 Micro: 0.7373, F1 Macro: 0.5575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1891, Accuracy: 0.9178, F1 Micro: 0.7506, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9191, F1 Micro: 0.7589, F1 Macro: 0.6063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1329, Accuracy: 0.9191, F1 Micro: 0.7678, F1 Macro: 0.6277\n",
      "Epoch 7/10, Train Loss: 0.115, Accuracy: 0.9213, F1 Micro: 0.757, F1 Macro: 0.6404\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9218, F1 Micro: 0.7671, F1 Macro: 0.6632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.9218, F1 Micro: 0.7698, F1 Macro: 0.6856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9236, F1 Micro: 0.7748, F1 Macro: 0.694\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9236, F1 Micro: 0.7748, F1 Macro: 0.694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.71      0.64      0.68       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.80      0.68      0.73       120\n",
      "  HS_Physical       0.89      0.24      0.37        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.91      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 188.8316786289215 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9131, F1 Micro: 0.7402, F1 Macro: 0.5986\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.8105309009552 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3959, Accuracy: 0.8832, F1 Micro: 0.6215, F1 Macro: 0.3103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2773, Accuracy: 0.9019, F1 Micro: 0.6654, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9141, F1 Micro: 0.7464, F1 Macro: 0.5752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1886, Accuracy: 0.9178, F1 Micro: 0.7485, F1 Macro: 0.6023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9163, F1 Micro: 0.757, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9209, F1 Micro: 0.7714, F1 Macro: 0.6419\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9201, F1 Micro: 0.7692, F1 Macro: 0.6822\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9224, F1 Micro: 0.7636, F1 Macro: 0.6731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9247, F1 Micro: 0.7721, F1 Macro: 0.6859\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.9234, F1 Micro: 0.7649, F1 Macro: 0.6905\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9247, F1 Micro: 0.7721, F1 Macro: 0.6859\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.91      0.88      0.89       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.76      0.64      0.69       157\n",
      "      HS_Race       0.84      0.68      0.75       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.71      0.33      0.45        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.74      0.67      0.70       689\n",
      "  HS_Moderate       0.65      0.55      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.79      0.64      0.69      5556\n",
      " weighted avg       0.80      0.74      0.77      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 190.15252494812012 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3982, Accuracy: 0.8794, F1 Micro: 0.6207, F1 Macro: 0.2948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2824, Accuracy: 0.9006, F1 Micro: 0.6625, F1 Macro: 0.4188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2232, Accuracy: 0.9128, F1 Micro: 0.735, F1 Macro: 0.5606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9171, F1 Micro: 0.743, F1 Macro: 0.5974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1601, Accuracy: 0.9169, F1 Micro: 0.7593, F1 Macro: 0.6156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9211, F1 Micro: 0.7703, F1 Macro: 0.6225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1115, Accuracy: 0.9201, F1 Micro: 0.7706, F1 Macro: 0.662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0927, Accuracy: 0.921, F1 Micro: 0.7708, F1 Macro: 0.6661\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9218, F1 Micro: 0.7669, F1 Macro: 0.6784\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9206, F1 Micro: 0.7654, F1 Macro: 0.6808\n",
      "Model 2 - Iteration 8816: Accuracy: 0.921, F1 Micro: 0.7708, F1 Macro: 0.6661\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.75      0.58      0.65       157\n",
      "      HS_Race       0.79      0.67      0.72       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.58      0.60      0.59       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.76      0.64      0.67      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 191.45234990119934 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3937, Accuracy: 0.8843, F1 Micro: 0.6275, F1 Macro: 0.3069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2789, Accuracy: 0.9005, F1 Micro: 0.6518, F1 Macro: 0.4409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2237, Accuracy: 0.9125, F1 Micro: 0.7363, F1 Macro: 0.5568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9174, F1 Micro: 0.744, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1636, Accuracy: 0.9172, F1 Micro: 0.763, F1 Macro: 0.6194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9187, F1 Micro: 0.7659, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1118, Accuracy: 0.9214, F1 Micro: 0.7707, F1 Macro: 0.6686\n",
      "Epoch 8/10, Train Loss: 0.0925, Accuracy: 0.9222, F1 Micro: 0.7682, F1 Macro: 0.6678\n",
      "Epoch 9/10, Train Loss: 0.0795, Accuracy: 0.922, F1 Micro: 0.769, F1 Macro: 0.6785\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9222, F1 Micro: 0.769, F1 Macro: 0.6868\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9214, F1 Micro: 0.7707, F1 Macro: 0.6686\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.70      0.63      0.66       402\n",
      "  HS_Religion       0.76      0.56      0.64       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.59      0.54      0.57       331\n",
      "    HS_Strong       0.89      0.74      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.82944989204407 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9137, F1 Micro: 0.742, F1 Macro: 0.6027\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.60690712928772 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3938, Accuracy: 0.8834, F1 Micro: 0.6107, F1 Macro: 0.312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2699, Accuracy: 0.9051, F1 Micro: 0.6849, F1 Macro: 0.4547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2192, Accuracy: 0.9143, F1 Micro: 0.7427, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9169, F1 Micro: 0.7582, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.9214, F1 Micro: 0.7672, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.9211, F1 Micro: 0.7731, F1 Macro: 0.6592\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9213, F1 Micro: 0.7675, F1 Macro: 0.6681\n",
      "Epoch 8/10, Train Loss: 0.0915, Accuracy: 0.9211, F1 Micro: 0.7685, F1 Macro: 0.6897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9221, F1 Micro: 0.7751, F1 Macro: 0.6929\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9184, F1 Micro: 0.7711, F1 Macro: 0.6973\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9221, F1 Micro: 0.7751, F1 Macro: 0.6929\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.76      0.61      0.67       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.60      0.61       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 193.79223132133484 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3953, Accuracy: 0.8822, F1 Micro: 0.567, F1 Macro: 0.274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2747, Accuracy: 0.904, F1 Micro: 0.6834, F1 Macro: 0.4551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.9146, F1 Micro: 0.741, F1 Macro: 0.5747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.9131, F1 Micro: 0.7565, F1 Macro: 0.6032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1588, Accuracy: 0.9226, F1 Micro: 0.765, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1323, Accuracy: 0.9239, F1 Micro: 0.7702, F1 Macro: 0.6416\n",
      "Epoch 7/10, Train Loss: 0.1099, Accuracy: 0.9233, F1 Micro: 0.7665, F1 Macro: 0.6498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9233, F1 Micro: 0.7716, F1 Macro: 0.6873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0784, Accuracy: 0.9242, F1 Micro: 0.7778, F1 Macro: 0.6827\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9223, F1 Micro: 0.7648, F1 Macro: 0.6751\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9242, F1 Micro: 0.7778, F1 Macro: 0.6827\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.92      0.76      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 196.533873796463 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3915, Accuracy: 0.8836, F1 Micro: 0.587, F1 Macro: 0.2926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2725, Accuracy: 0.9038, F1 Micro: 0.6818, F1 Macro: 0.4632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2217, Accuracy: 0.9132, F1 Micro: 0.7447, F1 Macro: 0.5694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1828, Accuracy: 0.9157, F1 Micro: 0.7582, F1 Macro: 0.5893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1559, Accuracy: 0.9204, F1 Micro: 0.7669, F1 Macro: 0.6267\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.922, F1 Micro: 0.7636, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1086, Accuracy: 0.9158, F1 Micro: 0.7679, F1 Macro: 0.678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9215, F1 Micro: 0.7682, F1 Macro: 0.6862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0775, Accuracy: 0.9203, F1 Micro: 0.7714, F1 Macro: 0.693\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9195, F1 Micro: 0.7711, F1 Macro: 0.6939\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9203, F1 Micro: 0.7714, F1 Macro: 0.693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.84      0.94      0.89       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.65      0.71      0.68       402\n",
      "  HS_Religion       0.78      0.60      0.68       157\n",
      "      HS_Race       0.71      0.72      0.72       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.57      0.64      0.60       331\n",
      "    HS_Strong       0.91      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 196.58850121498108 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9141, F1 Micro: 0.7437, F1 Macro: 0.6073\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.863428592681885 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3921, Accuracy: 0.8845, F1 Micro: 0.6047, F1 Macro: 0.3268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.905, F1 Micro: 0.7128, F1 Macro: 0.4961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.9144, F1 Micro: 0.7436, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9204, F1 Micro: 0.7632, F1 Macro: 0.6117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9191, F1 Micro: 0.7676, F1 Macro: 0.6315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9226, F1 Micro: 0.7691, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1096, Accuracy: 0.9242, F1 Micro: 0.7736, F1 Macro: 0.6796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0959, Accuracy: 0.9235, F1 Micro: 0.7739, F1 Macro: 0.6758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9202, F1 Micro: 0.7754, F1 Macro: 0.6927\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9228, F1 Micro: 0.7723, F1 Macro: 0.6882\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9202, F1 Micro: 0.7754, F1 Macro: 0.6927\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.67      0.66      0.67       402\n",
      "  HS_Religion       0.67      0.69      0.68       157\n",
      "      HS_Race       0.74      0.74      0.74       120\n",
      "  HS_Physical       1.00      0.18      0.31        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.78      5556\n",
      "    macro avg       0.75      0.69      0.69      5556\n",
      " weighted avg       0.75      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 199.73096203804016 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3928, Accuracy: 0.8862, F1 Micro: 0.619, F1 Macro: 0.3155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9059, F1 Micro: 0.7056, F1 Macro: 0.4959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2235, Accuracy: 0.9141, F1 Micro: 0.7467, F1 Macro: 0.5779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9201, F1 Micro: 0.7596, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9171, F1 Micro: 0.7673, F1 Macro: 0.6192\n",
      "Epoch 6/10, Train Loss: 0.1307, Accuracy: 0.9215, F1 Micro: 0.7656, F1 Macro: 0.6571\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9234, F1 Micro: 0.7644, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9211, F1 Micro: 0.7711, F1 Macro: 0.666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9239, F1 Micro: 0.778, F1 Macro: 0.6881\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9197, F1 Micro: 0.7662, F1 Macro: 0.6795\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9239, F1 Micro: 0.778, F1 Macro: 0.6881\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.69      0.67      0.68       157\n",
      "      HS_Race       0.77      0.74      0.75       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 196.3590292930603 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3887, Accuracy: 0.8861, F1 Micro: 0.6081, F1 Macro: 0.3249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2612, Accuracy: 0.9049, F1 Micro: 0.702, F1 Macro: 0.4735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2218, Accuracy: 0.9143, F1 Micro: 0.7428, F1 Macro: 0.5741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1823, Accuracy: 0.9176, F1 Micro: 0.7616, F1 Macro: 0.5938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1552, Accuracy: 0.9186, F1 Micro: 0.7684, F1 Macro: 0.6284\n",
      "Epoch 6/10, Train Loss: 0.1296, Accuracy: 0.921, F1 Micro: 0.7584, F1 Macro: 0.6472\n",
      "Epoch 7/10, Train Loss: 0.1101, Accuracy: 0.9225, F1 Micro: 0.7598, F1 Macro: 0.651\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9193, F1 Micro: 0.7668, F1 Macro: 0.6739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.923, F1 Micro: 0.7741, F1 Macro: 0.695\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9209, F1 Micro: 0.768, F1 Macro: 0.6861\n",
      "Model 3 - Iteration 9216: Accuracy: 0.923, F1 Micro: 0.7741, F1 Macro: 0.695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.72      0.66      0.69       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.84      0.22      0.35        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 194.31969094276428 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9145, F1 Micro: 0.7453, F1 Macro: 0.6115\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.69360065460205 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.392, Accuracy: 0.8855, F1 Micro: 0.6269, F1 Macro: 0.323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.9045, F1 Micro: 0.7091, F1 Macro: 0.4919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9132, F1 Micro: 0.7412, F1 Macro: 0.5746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1846, Accuracy: 0.9182, F1 Micro: 0.7652, F1 Macro: 0.6136\n",
      "Epoch 5/10, Train Loss: 0.1509, Accuracy: 0.9198, F1 Micro: 0.7637, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.924, F1 Micro: 0.7723, F1 Macro: 0.6638\n",
      "Epoch 7/10, Train Loss: 0.1132, Accuracy: 0.9231, F1 Micro: 0.771, F1 Macro: 0.6546\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9242, F1 Micro: 0.7663, F1 Macro: 0.6684\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9204, F1 Micro: 0.768, F1 Macro: 0.6823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9207, F1 Micro: 0.7751, F1 Macro: 0.7012\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9207, F1 Micro: 0.7751, F1 Macro: 0.7012\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.93      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.66      0.67      0.67       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.85      0.24      0.37        72\n",
      "    HS_Gender       0.59      0.51      0.55        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.59      0.61      0.60       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.70      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 196.75077390670776 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3946, Accuracy: 0.8855, F1 Micro: 0.6036, F1 Macro: 0.2999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2679, Accuracy: 0.9033, F1 Micro: 0.7139, F1 Macro: 0.5213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2236, Accuracy: 0.9143, F1 Micro: 0.7379, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.921, F1 Micro: 0.7575, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.9172, F1 Micro: 0.7641, F1 Macro: 0.6272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.9252, F1 Micro: 0.7776, F1 Macro: 0.6573\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.923, F1 Micro: 0.7738, F1 Macro: 0.6702\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9204, F1 Micro: 0.7695, F1 Macro: 0.6706\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.924, F1 Micro: 0.7718, F1 Macro: 0.6822\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9213, F1 Micro: 0.7708, F1 Macro: 0.6889\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9252, F1 Micro: 0.7776, F1 Macro: 0.6573\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.74      0.65      0.69       402\n",
      "  HS_Religion       0.74      0.66      0.70       157\n",
      "      HS_Race       0.77      0.72      0.74       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.70      0.14      0.23        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.91      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.63      0.66      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 196.72388553619385 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3889, Accuracy: 0.8856, F1 Micro: 0.6159, F1 Macro: 0.3242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2644, Accuracy: 0.9053, F1 Micro: 0.712, F1 Macro: 0.51\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2196, Accuracy: 0.9146, F1 Micro: 0.7434, F1 Macro: 0.5692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1863, Accuracy: 0.9212, F1 Micro: 0.7634, F1 Macro: 0.5985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1522, Accuracy: 0.9206, F1 Micro: 0.7663, F1 Macro: 0.65\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9223, F1 Micro: 0.7685, F1 Macro: 0.6716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1109, Accuracy: 0.9195, F1 Micro: 0.7697, F1 Macro: 0.6753\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9239, F1 Micro: 0.7682, F1 Macro: 0.6756\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9182, F1 Micro: 0.7649, F1 Macro: 0.6784\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9187, F1 Micro: 0.7695, F1 Macro: 0.6901\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9195, F1 Micro: 0.7697, F1 Macro: 0.6753\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.84      0.93      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.68      0.69      0.69       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.60      0.59      0.59       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 198.90002131462097 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9149, F1 Micro: 0.7467, F1 Macro: 0.6147\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.093160152435303 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3901, Accuracy: 0.8832, F1 Micro: 0.5746, F1 Macro: 0.2942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.9067, F1 Micro: 0.7149, F1 Macro: 0.5087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2226, Accuracy: 0.9151, F1 Micro: 0.738, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9166, F1 Micro: 0.7428, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9144, F1 Micro: 0.7653, F1 Macro: 0.6407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1276, Accuracy: 0.923, F1 Micro: 0.7724, F1 Macro: 0.6415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.9235, F1 Micro: 0.774, F1 Macro: 0.6722\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9209, F1 Micro: 0.7692, F1 Macro: 0.6807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9241, F1 Micro: 0.7748, F1 Macro: 0.6917\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9218, F1 Micro: 0.7721, F1 Macro: 0.7012\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9241, F1 Micro: 0.7748, F1 Macro: 0.6917\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.83      0.63      0.72       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.56      0.47      0.51        51\n",
      "     HS_Other       0.81      0.78      0.79       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.64      0.54      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 203.17145323753357 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3914, Accuracy: 0.8838, F1 Micro: 0.582, F1 Macro: 0.2817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2712, Accuracy: 0.9042, F1 Micro: 0.7117, F1 Macro: 0.5087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2258, Accuracy: 0.9153, F1 Micro: 0.7415, F1 Macro: 0.5751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9187, F1 Micro: 0.7575, F1 Macro: 0.5992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.913, F1 Micro: 0.7605, F1 Macro: 0.6232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1303, Accuracy: 0.9227, F1 Micro: 0.7669, F1 Macro: 0.6425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.924, F1 Micro: 0.7701, F1 Macro: 0.6492\n",
      "Epoch 8/10, Train Loss: 0.0883, Accuracy: 0.9221, F1 Micro: 0.7695, F1 Macro: 0.6813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9243, F1 Micro: 0.7772, F1 Macro: 0.6916\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.922, F1 Micro: 0.7754, F1 Macro: 0.6946\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9243, F1 Micro: 0.7772, F1 Macro: 0.6916\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.71      0.69      0.70       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.54      0.43      0.48        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 202.58355069160461 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.8842, F1 Micro: 0.5858, F1 Macro: 0.2932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2681, Accuracy: 0.9061, F1 Micro: 0.714, F1 Macro: 0.5181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.225, Accuracy: 0.9139, F1 Micro: 0.7405, F1 Macro: 0.5649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.917, F1 Micro: 0.7519, F1 Macro: 0.579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9187, F1 Micro: 0.7673, F1 Macro: 0.6381\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9236, F1 Micro: 0.7652, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9237, F1 Micro: 0.7718, F1 Macro: 0.6642\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9181, F1 Micro: 0.7674, F1 Macro: 0.6939\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9237, F1 Micro: 0.7669, F1 Macro: 0.6807\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.9223, F1 Micro: 0.7692, F1 Macro: 0.6992\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9237, F1 Micro: 0.7718, F1 Macro: 0.6642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.77      0.62      0.69       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.79      0.60      0.68       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.75      0.24      0.36        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.91      0.76      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.79      0.62      0.66      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 198.89554691314697 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9153, F1 Micro: 0.7479, F1 Macro: 0.6178\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.954224348068237 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3876, Accuracy: 0.8824, F1 Micro: 0.5766, F1 Macro: 0.2974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2631, Accuracy: 0.9076, F1 Micro: 0.7136, F1 Macro: 0.5039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2144, Accuracy: 0.9161, F1 Micro: 0.7512, F1 Macro: 0.5901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1769, Accuracy: 0.9219, F1 Micro: 0.762, F1 Macro: 0.6043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1515, Accuracy: 0.9207, F1 Micro: 0.7697, F1 Macro: 0.6481\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9233, F1 Micro: 0.7695, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9245, F1 Micro: 0.7776, F1 Macro: 0.6884\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9224, F1 Micro: 0.7738, F1 Macro: 0.6837\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9217, F1 Micro: 0.7751, F1 Macro: 0.6935\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.92, F1 Micro: 0.7704, F1 Macro: 0.6808\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9245, F1 Micro: 0.7776, F1 Macro: 0.6884\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.80      0.58      0.67       402\n",
      "  HS_Religion       0.79      0.58      0.67       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.72      0.41      0.53        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.72      0.50      0.59       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 203.44159531593323 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.389, Accuracy: 0.8832, F1 Micro: 0.5951, F1 Macro: 0.2956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.9065, F1 Micro: 0.7121, F1 Macro: 0.5218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2154, Accuracy: 0.9163, F1 Micro: 0.743, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1781, Accuracy: 0.921, F1 Micro: 0.7605, F1 Macro: 0.603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1523, Accuracy: 0.9213, F1 Micro: 0.7702, F1 Macro: 0.6506\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9232, F1 Micro: 0.7685, F1 Macro: 0.6465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.105, Accuracy: 0.924, F1 Micro: 0.7797, F1 Macro: 0.6796\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9228, F1 Micro: 0.7713, F1 Macro: 0.6905\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9216, F1 Micro: 0.7742, F1 Macro: 0.6878\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9237, F1 Micro: 0.7739, F1 Macro: 0.6964\n",
      "Model 2 - Iteration 9618: Accuracy: 0.924, F1 Micro: 0.7797, F1 Macro: 0.6796\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.73      0.77      0.75       732\n",
      "     HS_Group       0.74      0.65      0.69       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.72      0.74      0.73       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.67      0.27      0.39        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.76      0.73       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.91      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 203.4494388103485 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.8836, F1 Micro: 0.5942, F1 Macro: 0.3073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2668, Accuracy: 0.9061, F1 Micro: 0.7124, F1 Macro: 0.5203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.9149, F1 Micro: 0.7425, F1 Macro: 0.5797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.18, Accuracy: 0.9203, F1 Micro: 0.7576, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.153, Accuracy: 0.9172, F1 Micro: 0.7638, F1 Macro: 0.6438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.13, Accuracy: 0.9211, F1 Micro: 0.7651, F1 Macro: 0.6631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.9225, F1 Micro: 0.7686, F1 Macro: 0.6745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9207, F1 Micro: 0.7698, F1 Macro: 0.6924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9238, F1 Micro: 0.77, F1 Macro: 0.6908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9221, F1 Micro: 0.7731, F1 Macro: 0.6921\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9221, F1 Micro: 0.7731, F1 Macro: 0.6921\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.75      0.72      0.73       120\n",
      "  HS_Physical       1.00      0.26      0.42        72\n",
      "    HS_Gender       0.70      0.31      0.43        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 210.82177543640137 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9156, F1 Micro: 0.7492, F1 Macro: 0.6208\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.716311693191528 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.8826, F1 Micro: 0.5582, F1 Macro: 0.2834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2618, Accuracy: 0.9042, F1 Micro: 0.7125, F1 Macro: 0.548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2167, Accuracy: 0.915, F1 Micro: 0.747, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1802, Accuracy: 0.9217, F1 Micro: 0.758, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1485, Accuracy: 0.9187, F1 Micro: 0.7673, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.924, F1 Micro: 0.7781, F1 Macro: 0.6563\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9202, F1 Micro: 0.774, F1 Macro: 0.6832\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9232, F1 Micro: 0.7709, F1 Macro: 0.6724\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9224, F1 Micro: 0.7691, F1 Macro: 0.6788\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.921, F1 Micro: 0.7748, F1 Macro: 0.6997\n",
      "Model 1 - Iteration 9818: Accuracy: 0.924, F1 Micro: 0.7781, F1 Macro: 0.6563\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.69      0.66      0.68       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.64      0.18      0.28        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.74      0.64      0.66      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 205.6716504096985 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3891, Accuracy: 0.8835, F1 Micro: 0.5657, F1 Macro: 0.2794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2662, Accuracy: 0.9052, F1 Micro: 0.7104, F1 Macro: 0.5342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2189, Accuracy: 0.9152, F1 Micro: 0.7492, F1 Macro: 0.5879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9225, F1 Micro: 0.7695, F1 Macro: 0.6134\n",
      "Epoch 5/10, Train Loss: 0.1478, Accuracy: 0.9179, F1 Micro: 0.7547, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.9247, F1 Micro: 0.7748, F1 Macro: 0.6632\n",
      "Epoch 7/10, Train Loss: 0.111, Accuracy: 0.9158, F1 Micro: 0.7678, F1 Macro: 0.6757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9249, F1 Micro: 0.775, F1 Macro: 0.6668\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9169, F1 Micro: 0.7631, F1 Macro: 0.6735\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9218, F1 Micro: 0.7736, F1 Macro: 0.6996\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9249, F1 Micro: 0.775, F1 Macro: 0.6668\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.73      0.69      0.71       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.69      0.18      0.28        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 205.83251643180847 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3829, Accuracy: 0.8846, F1 Micro: 0.5777, F1 Macro: 0.3021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2615, Accuracy: 0.9047, F1 Micro: 0.7158, F1 Macro: 0.5385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2168, Accuracy: 0.9141, F1 Micro: 0.7405, F1 Macro: 0.5738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9184, F1 Micro: 0.7566, F1 Macro: 0.5968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.9211, F1 Micro: 0.7622, F1 Macro: 0.6422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.924, F1 Micro: 0.7692, F1 Macro: 0.6501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9203, F1 Micro: 0.773, F1 Macro: 0.6777\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9213, F1 Micro: 0.7677, F1 Macro: 0.6742\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9204, F1 Micro: 0.768, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9222, F1 Micro: 0.7775, F1 Macro: 0.7053\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9222, F1 Micro: 0.7775, F1 Macro: 0.7053\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.86      0.93      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.68      0.69       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.72      0.78      0.75       120\n",
      "  HS_Physical       0.83      0.26      0.40        72\n",
      "    HS_Gender       0.54      0.53      0.53        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 208.71437072753906 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.916, F1 Micro: 0.7503, F1 Macro: 0.6231\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.447553157806396 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3827, Accuracy: 0.8865, F1 Micro: 0.5988, F1 Macro: 0.3173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.9063, F1 Micro: 0.7236, F1 Macro: 0.5285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.9098, F1 Micro: 0.7462, F1 Macro: 0.5945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1793, Accuracy: 0.9207, F1 Micro: 0.7665, F1 Macro: 0.6137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.921, F1 Micro: 0.772, F1 Macro: 0.6465\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9214, F1 Micro: 0.7715, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.924, F1 Micro: 0.7781, F1 Macro: 0.69\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9232, F1 Micro: 0.7767, F1 Macro: 0.7002\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9241, F1 Micro: 0.7744, F1 Macro: 0.6939\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9203, F1 Micro: 0.7662, F1 Macro: 0.6966\n",
      "Model 1 - Iteration 10018: Accuracy: 0.924, F1 Micro: 0.7781, F1 Macro: 0.69\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.73      0.67      0.70       402\n",
      "  HS_Religion       0.67      0.68      0.68       157\n",
      "      HS_Race       0.80      0.70      0.75       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.60      0.62       331\n",
      "    HS_Strong       0.91      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 210.80423021316528 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3856, Accuracy: 0.8876, F1 Micro: 0.6325, F1 Macro: 0.3152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2607, Accuracy: 0.9075, F1 Micro: 0.7223, F1 Macro: 0.5221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2083, Accuracy: 0.9138, F1 Micro: 0.7535, F1 Macro: 0.597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1786, Accuracy: 0.9221, F1 Micro: 0.7636, F1 Macro: 0.6145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.151, Accuracy: 0.9204, F1 Micro: 0.7724, F1 Macro: 0.6381\n",
      "Epoch 6/10, Train Loss: 0.1248, Accuracy: 0.9205, F1 Micro: 0.7691, F1 Macro: 0.6545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9231, F1 Micro: 0.7802, F1 Macro: 0.6904\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9239, F1 Micro: 0.7781, F1 Macro: 0.6826\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9246, F1 Micro: 0.7781, F1 Macro: 0.6965\n",
      "Epoch 10/10, Train Loss: 0.0689, Accuracy: 0.9236, F1 Micro: 0.77, F1 Macro: 0.7061\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9231, F1 Micro: 0.7802, F1 Macro: 0.6904\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.71      0.79      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.68      0.67      0.68       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.67      0.22      0.33        72\n",
      "    HS_Gender       0.56      0.35      0.43        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.77      0.73       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.91      0.75      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.67      0.69      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 210.16134643554688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3802, Accuracy: 0.8888, F1 Micro: 0.6317, F1 Macro: 0.3415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.259, Accuracy: 0.9055, F1 Micro: 0.7196, F1 Macro: 0.5115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2058, Accuracy: 0.9122, F1 Micro: 0.7463, F1 Macro: 0.5872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9211, F1 Micro: 0.7605, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1512, Accuracy: 0.9165, F1 Micro: 0.7653, F1 Macro: 0.66\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.9199, F1 Micro: 0.7668, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9213, F1 Micro: 0.7723, F1 Macro: 0.6863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9228, F1 Micro: 0.7757, F1 Macro: 0.6916\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9236, F1 Micro: 0.7735, F1 Macro: 0.6914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0682, Accuracy: 0.9244, F1 Micro: 0.7757, F1 Macro: 0.7048\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9244, F1 Micro: 0.7757, F1 Macro: 0.7048\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.77      0.68      0.72       732\n",
      "     HS_Group       0.67      0.70      0.69       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.85      0.31      0.45        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.80      0.77      0.78       762\n",
      "      HS_Weak       0.75      0.66      0.70       689\n",
      "  HS_Moderate       0.61      0.65      0.63       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 214.50980877876282 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9163, F1 Micro: 0.7514, F1 Macro: 0.626\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.746252059936523 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3785, Accuracy: 0.8886, F1 Micro: 0.6543, F1 Macro: 0.3906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2546, Accuracy: 0.9084, F1 Micro: 0.7155, F1 Macro: 0.5104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9174, F1 Micro: 0.7465, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1732, Accuracy: 0.9174, F1 Micro: 0.7652, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1431, Accuracy: 0.9212, F1 Micro: 0.7721, F1 Macro: 0.6443\n",
      "Epoch 6/10, Train Loss: 0.1273, Accuracy: 0.9212, F1 Micro: 0.7672, F1 Macro: 0.6662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1015, Accuracy: 0.9235, F1 Micro: 0.7721, F1 Macro: 0.6855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9223, F1 Micro: 0.774, F1 Macro: 0.6944\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9215, F1 Micro: 0.7717, F1 Macro: 0.694\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.921, F1 Micro: 0.7737, F1 Macro: 0.7006\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9223, F1 Micro: 0.774, F1 Macro: 0.6944\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.75      0.70      0.72       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 213.5451123714447 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3804, Accuracy: 0.8878, F1 Micro: 0.6407, F1 Macro: 0.3492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2578, Accuracy: 0.9075, F1 Micro: 0.708, F1 Macro: 0.5057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2122, Accuracy: 0.9174, F1 Micro: 0.7509, F1 Macro: 0.5873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1735, Accuracy: 0.9202, F1 Micro: 0.7691, F1 Macro: 0.6117\n",
      "Epoch 5/10, Train Loss: 0.1461, Accuracy: 0.9171, F1 Micro: 0.767, F1 Macro: 0.6395\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9201, F1 Micro: 0.7674, F1 Macro: 0.6686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1027, Accuracy: 0.9249, F1 Micro: 0.777, F1 Macro: 0.673\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.92, F1 Micro: 0.7704, F1 Macro: 0.6835\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9145, F1 Micro: 0.7676, F1 Macro: 0.6858\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.9213, F1 Micro: 0.772, F1 Macro: 0.6972\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9249, F1 Micro: 0.777, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.80      0.63      0.71       120\n",
      "  HS_Physical       0.90      0.12      0.22        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.92      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.63      0.67      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 210.14118099212646 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3779, Accuracy: 0.8889, F1 Micro: 0.6408, F1 Macro: 0.3628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.9085, F1 Micro: 0.7145, F1 Macro: 0.5082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2097, Accuracy: 0.9169, F1 Micro: 0.7471, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1742, Accuracy: 0.9187, F1 Micro: 0.7647, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.9151, F1 Micro: 0.7647, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.9234, F1 Micro: 0.7716, F1 Macro: 0.6675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9244, F1 Micro: 0.7749, F1 Macro: 0.6834\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9226, F1 Micro: 0.7736, F1 Macro: 0.695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9242, F1 Micro: 0.778, F1 Macro: 0.6935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9246, F1 Micro: 0.7793, F1 Macro: 0.7125\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9246, F1 Micro: 0.7793, F1 Macro: 0.7125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.76      0.63      0.69       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.91      0.29      0.44        72\n",
      "    HS_Gender       0.62      0.51      0.56        51\n",
      "     HS_Other       0.79      0.81      0.80       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.69      0.71      5556\n",
      " weighted avg       0.78      0.77      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 216.2839093208313 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9166, F1 Micro: 0.7524, F1 Macro: 0.6286\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.821471691131592 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3764, Accuracy: 0.889, F1 Micro: 0.6296, F1 Macro: 0.3351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.9071, F1 Micro: 0.7127, F1 Macro: 0.5075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2019, Accuracy: 0.9169, F1 Micro: 0.7415, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9183, F1 Micro: 0.7619, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1434, Accuracy: 0.9226, F1 Micro: 0.7663, F1 Macro: 0.6335\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9231, F1 Micro: 0.7621, F1 Macro: 0.6243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0973, Accuracy: 0.9234, F1 Micro: 0.769, F1 Macro: 0.6538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.087, Accuracy: 0.9231, F1 Micro: 0.7797, F1 Macro: 0.6938\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.922, F1 Micro: 0.7752, F1 Macro: 0.6953\n",
      "Epoch 10/10, Train Loss: 0.0632, Accuracy: 0.9249, F1 Micro: 0.7764, F1 Macro: 0.6989\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9231, F1 Micro: 0.7797, F1 Macro: 0.6938\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.80      0.55      0.65       157\n",
      "      HS_Race       0.78      0.70      0.74       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.70      0.37      0.49        51\n",
      "     HS_Other       0.74      0.85      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.64      0.59      0.62       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.77      0.67      0.69      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 217.96248841285706 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3779, Accuracy: 0.8886, F1 Micro: 0.6159, F1 Macro: 0.3093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.9086, F1 Micro: 0.7133, F1 Macro: 0.4915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2015, Accuracy: 0.916, F1 Micro: 0.7415, F1 Macro: 0.5729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.919, F1 Micro: 0.7647, F1 Macro: 0.6144\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.9226, F1 Micro: 0.7646, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1202, Accuracy: 0.9228, F1 Micro: 0.7675, F1 Macro: 0.6378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1011, Accuracy: 0.9215, F1 Micro: 0.7703, F1 Macro: 0.6557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0866, Accuracy: 0.9228, F1 Micro: 0.7784, F1 Macro: 0.6834\n",
      "Epoch 9/10, Train Loss: 0.0691, Accuracy: 0.9208, F1 Micro: 0.7763, F1 Macro: 0.7003\n",
      "Epoch 10/10, Train Loss: 0.0623, Accuracy: 0.924, F1 Micro: 0.776, F1 Macro: 0.6999\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9228, F1 Micro: 0.7784, F1 Macro: 0.6834\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.77      0.57      0.65       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.91      0.14      0.24        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.64      0.57      0.61       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 218.00850915908813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3741, Accuracy: 0.8901, F1 Micro: 0.6303, F1 Macro: 0.349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.248, Accuracy: 0.9075, F1 Micro: 0.7124, F1 Macro: 0.5017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2028, Accuracy: 0.9184, F1 Micro: 0.7518, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1726, Accuracy: 0.9199, F1 Micro: 0.7571, F1 Macro: 0.604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.9221, F1 Micro: 0.7662, F1 Macro: 0.6523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.9241, F1 Micro: 0.767, F1 Macro: 0.6389\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9214, F1 Micro: 0.7659, F1 Macro: 0.6705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0859, Accuracy: 0.924, F1 Micro: 0.7765, F1 Macro: 0.6857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0688, Accuracy: 0.9236, F1 Micro: 0.7793, F1 Macro: 0.7049\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9237, F1 Micro: 0.77, F1 Macro: 0.7054\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9236, F1 Micro: 0.7793, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.70      0.68      0.69       402\n",
      "  HS_Religion       0.70      0.61      0.65       157\n",
      "      HS_Race       0.75      0.73      0.74       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.58      0.49      0.53        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.62      0.57      0.59       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.77      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 219.72974181175232 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9168, F1 Micro: 0.7534, F1 Macro: 0.631\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.49194598197937 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3711, Accuracy: 0.8858, F1 Micro: 0.5844, F1 Macro: 0.3014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.9094, F1 Micro: 0.7177, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2058, Accuracy: 0.9178, F1 Micro: 0.7442, F1 Macro: 0.5811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1686, Accuracy: 0.9164, F1 Micro: 0.7636, F1 Macro: 0.6098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1441, Accuracy: 0.923, F1 Micro: 0.7639, F1 Macro: 0.64\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1175, Accuracy: 0.9232, F1 Micro: 0.7739, F1 Macro: 0.6747\n",
      "Epoch 7/10, Train Loss: 0.1016, Accuracy: 0.9232, F1 Micro: 0.7721, F1 Macro: 0.6884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9244, F1 Micro: 0.7764, F1 Macro: 0.7015\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9218, F1 Micro: 0.7744, F1 Macro: 0.6984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9256, F1 Micro: 0.7769, F1 Macro: 0.7096\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9256, F1 Micro: 0.7769, F1 Macro: 0.7096\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.81      0.56      0.66       157\n",
      "      HS_Race       0.79      0.73      0.76       120\n",
      "  HS_Physical       0.83      0.26      0.40        72\n",
      "    HS_Gender       0.69      0.53      0.60        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.68      0.55      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 221.85688066482544 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3736, Accuracy: 0.8832, F1 Micro: 0.5621, F1 Macro: 0.2755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2549, Accuracy: 0.9092, F1 Micro: 0.7143, F1 Macro: 0.515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2067, Accuracy: 0.9176, F1 Micro: 0.7428, F1 Macro: 0.5796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.9178, F1 Micro: 0.7624, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9215, F1 Micro: 0.7631, F1 Macro: 0.6323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9241, F1 Micro: 0.7812, F1 Macro: 0.6724\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9219, F1 Micro: 0.7658, F1 Macro: 0.6757\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.9223, F1 Micro: 0.7733, F1 Macro: 0.6933\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9145, F1 Micro: 0.7646, F1 Macro: 0.6771\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9246, F1 Micro: 0.7685, F1 Macro: 0.6934\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9241, F1 Micro: 0.7812, F1 Macro: 0.6724\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.74      0.66      0.70       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.81      0.64      0.72       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.81      0.25      0.39        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.67      0.56      0.61       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.78      0.65      0.67      5556\n",
      " weighted avg       0.78      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 218.82287883758545 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3689, Accuracy: 0.8849, F1 Micro: 0.5847, F1 Macro: 0.3062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.9088, F1 Micro: 0.7157, F1 Macro: 0.5144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9174, F1 Micro: 0.7424, F1 Macro: 0.572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9176, F1 Micro: 0.7641, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1425, Accuracy: 0.9231, F1 Micro: 0.7707, F1 Macro: 0.6509\n",
      "Epoch 6/10, Train Loss: 0.1139, Accuracy: 0.9212, F1 Micro: 0.7693, F1 Macro: 0.6664\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9239, F1 Micro: 0.7685, F1 Macro: 0.6774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9214, F1 Micro: 0.7718, F1 Macro: 0.6942\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.9159, F1 Micro: 0.7701, F1 Macro: 0.6933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9248, F1 Micro: 0.777, F1 Macro: 0.7041\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9248, F1 Micro: 0.777, F1 Macro: 0.7041\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.78      0.68      0.72       120\n",
      "  HS_Physical       0.95      0.28      0.43        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 220.94002628326416 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9171, F1 Micro: 0.7543, F1 Macro: 0.6333\n",
      "Total sampling time: 1323.58 seconds\n",
      "Total runtime: 15737.069854974747 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6647971,
     "sourceId": 10724052,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15771.079103,
   "end_time": "2025-04-13T11:23:45.429090",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T07:00:54.349987",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c6ae658e24b4ab78f8ee083a5e4f9d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ed54d26e826475db6a9cf9bdd8bfcb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "12a8f21e02544cd9bbb53996ee71c70e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_45fa17e2a07e48d193e1918f95196af8",
       "placeholder": "",
       "style": "IPY_MODEL_66b8590361174ce9bc69c50fd7414bfa",
       "tabbable": null,
       "tooltip": null,
       "value": "2.00/2.00[00:00&lt;00:00,129B/s]"
      }
     },
     "226e72e9e5b54498a7956e545cfbf90c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "28764fa6813a4bbdb94208eba409bec4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32d3844d927d43d8bcb5673e0de6c485": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a24116d4abda46bfa0c1c2473c60d8a6",
       "placeholder": "",
       "style": "IPY_MODEL_a58bd98ce23d453f97a1474572dfee56",
       "tabbable": null,
       "tooltip": null,
       "value": "112/112[00:00&lt;00:00,12.4kB/s]"
      }
     },
     "34b4d5e7ab4442feb68c89916d6ad084": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f0045d2496094213aaae7a5ad43764b5",
        "IPY_MODEL_f272217d66e14ee68f25285b17f4a76e",
        "IPY_MODEL_583885ba53dc4d27adbff94fcea43ecb"
       ],
       "layout": "IPY_MODEL_a4af3adde43a45b9a34c206f360a8b6d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3ddf808c36cd465b93f55a546e642b6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28764fa6813a4bbdb94208eba409bec4",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8c094b13e83e45038f19fe7e9fd861d7",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "45fa17e2a07e48d193e1918f95196af8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "551a3198437c499291d8f9a41858a18e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab0dd3a61f8249e49bd49d6a202bb63a",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_76446436e1544dd090dfcb10d70a0f78",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "57888fcef0c94f27a908eee32fa18a67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "583885ba53dc4d27adbff94fcea43ecb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5c6ed637ae7540649ab69abc6572a16a",
       "placeholder": "",
       "style": "IPY_MODEL_0ed54d26e826475db6a9cf9bdd8bfcb3",
       "tabbable": null,
       "tooltip": null,
       "value": "229k/229k[00:00&lt;00:00,6.18MB/s]"
      }
     },
     "5a330c761b9f4d838d2b677fd4b28432": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5c6ed637ae7540649ab69abc6572a16a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61d21b4e70674aa4b47b081c82853795": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66b8590361174ce9bc69c50fd7414bfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "738eb5541fce464cabd5e866bd01b5eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad317c176e444bde9d9238faf2a183cf",
        "IPY_MODEL_551a3198437c499291d8f9a41858a18e",
        "IPY_MODEL_32d3844d927d43d8bcb5673e0de6c485"
       ],
       "layout": "IPY_MODEL_96f2fc4bdcf64cc7a9a6c20748e61c4f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "76446436e1544dd090dfcb10d70a0f78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8294734829d54e539ff7811646283716": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82b739bdf3344e98bb09b0de7794da72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ba6ed405a0ec43dfaac85583de1e86b5",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f9a3e90d81a84f658d5242f681764e67",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "82fc8fd52c284f7e8337696d84110cfc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b8ca471bda714e82a033e37ced4337dd",
        "IPY_MODEL_82b739bdf3344e98bb09b0de7794da72",
        "IPY_MODEL_ce517b8a8ea04f66afb828a801a41351"
       ],
       "layout": "IPY_MODEL_e644d7a3c78441bfb291b5ef1bb3b768",
       "tabbable": null,
       "tooltip": null
      }
     },
     "87aea29aeb25488c90ca79d1234c8007": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_61d21b4e70674aa4b47b081c82853795",
       "placeholder": "",
       "style": "IPY_MODEL_226e72e9e5b54498a7956e545cfbf90c",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:100%"
      }
     },
     "8c094b13e83e45038f19fe7e9fd861d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "945ab7322ece4e2bb8fb294313854646": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_87aea29aeb25488c90ca79d1234c8007",
        "IPY_MODEL_3ddf808c36cd465b93f55a546e642b6f",
        "IPY_MODEL_12a8f21e02544cd9bbb53996ee71c70e"
       ],
       "layout": "IPY_MODEL_ddb3370483644c2fb927571d1efb7c61",
       "tabbable": null,
       "tooltip": null
      }
     },
     "960f7e48a7de493ab15f94e10dae8951": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "96f2fc4bdcf64cc7a9a6c20748e61c4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a24116d4abda46bfa0c1c2473c60d8a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a4af3adde43a45b9a34c206f360a8b6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a58bd98ce23d453f97a1474572dfee56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a92fd492ef5a4810aed367522c03e289": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ab0dd3a61f8249e49bd49d6a202bb63a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad317c176e444bde9d9238faf2a183cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0c6ae658e24b4ab78f8ee083a5e4f9d8",
       "placeholder": "",
       "style": "IPY_MODEL_f3a5d00b7d3f4e14a0e879eeac0e1016",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:100%"
      }
     },
     "afde596d0f814080a0c10a4ad2fbc288": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8ca471bda714e82a033e37ced4337dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_57888fcef0c94f27a908eee32fa18a67",
       "placeholder": "",
       "style": "IPY_MODEL_f80977b77b71475299d018baf7cf385a",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "ba6ed405a0ec43dfaac85583de1e86b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce517b8a8ea04f66afb828a801a41351": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8294734829d54e539ff7811646283716",
       "placeholder": "",
       "style": "IPY_MODEL_960f7e48a7de493ab15f94e10dae8951",
       "tabbable": null,
       "tooltip": null,
       "value": "1.53k/1.53k[00:00&lt;00:00,169kB/s]"
      }
     },
     "ddb3370483644c2fb927571d1efb7c61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e644d7a3c78441bfb291b5ef1bb3b768": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e811294d329c4fab86510fadcdb987b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0045d2496094213aaae7a5ad43764b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_afde596d0f814080a0c10a4ad2fbc288",
       "placeholder": "",
       "style": "IPY_MODEL_a92fd492ef5a4810aed367522c03e289",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:100%"
      }
     },
     "f272217d66e14ee68f25285b17f4a76e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e811294d329c4fab86510fadcdb987b9",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5a330c761b9f4d838d2b677fd4b28432",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "f3a5d00b7d3f4e14a0e879eeac0e1016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f80977b77b71475299d018baf7cf385a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9a3e90d81a84f658d5242f681764e67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
