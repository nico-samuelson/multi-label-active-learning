{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc39754b",
   "metadata": {
    "papermill": {
     "duration": 0.012426,
     "end_time": "2025-01-30T05:58:29.779671",
     "exception": false,
     "start_time": "2025-01-30T05:58:29.767245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eeffa74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:58:29.803758Z",
     "iopub.status.busy": "2025-01-30T05:58:29.803495Z",
     "iopub.status.idle": "2025-01-30T05:59:03.659071Z",
     "shell.execute_reply": "2025-01-30T05:59:03.658347Z"
    },
    "papermill": {
     "duration": 33.868952,
     "end_time": "2025-01-30T05:59:03.660475",
     "exception": false,
     "start_time": "2025-01-30T05:58:29.791523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d089c14",
   "metadata": {
    "papermill": {
     "duration": 0.011214,
     "end_time": "2025-01-30T05:59:03.683571",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.672357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51463e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:03.707469Z",
     "iopub.status.busy": "2025-01-30T05:59:03.706997Z",
     "iopub.status.idle": "2025-01-30T05:59:03.710392Z",
     "shell.execute_reply": "2025-01-30T05:59:03.709777Z"
    },
    "papermill": {
     "duration": 0.016388,
     "end_time": "2025-01-30T05:59:03.711539",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.695151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34adac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:03.735278Z",
     "iopub.status.busy": "2025-01-30T05:59:03.735067Z",
     "iopub.status.idle": "2025-01-30T05:59:03.738665Z",
     "shell.execute_reply": "2025-01-30T05:59:03.738048Z"
    },
    "papermill": {
     "duration": 0.016976,
     "end_time": "2025-01-30T05:59:03.739864",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.722888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a5b057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:03.762813Z",
     "iopub.status.busy": "2025-01-30T05:59:03.762610Z",
     "iopub.status.idle": "2025-01-30T05:59:03.775471Z",
     "shell.execute_reply": "2025-01-30T05:59:03.774878Z"
    },
    "papermill": {
     "duration": 0.025387,
     "end_time": "2025-01-30T05:59:03.776617",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.751230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760ceae",
   "metadata": {
    "papermill": {
     "duration": 0.010848,
     "end_time": "2025-01-30T05:59:03.798675",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.787827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8c9a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:03.821422Z",
     "iopub.status.busy": "2025-01-30T05:59:03.821227Z",
     "iopub.status.idle": "2025-01-30T05:59:03.877170Z",
     "shell.execute_reply": "2025-01-30T05:59:03.875895Z"
    },
    "papermill": {
     "duration": 0.068828,
     "end_time": "2025-01-30T05:59:03.878608",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.809780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-random'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a7a5c",
   "metadata": {
    "papermill": {
     "duration": 0.012696,
     "end_time": "2025-01-30T05:59:03.902755",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.890059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6aa0d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:03.930791Z",
     "iopub.status.busy": "2025-01-30T05:59:03.930360Z",
     "iopub.status.idle": "2025-01-30T05:59:04.089903Z",
     "shell.execute_reply": "2025-01-30T05:59:04.089076Z"
    },
    "papermill": {
     "duration": 0.175625,
     "end_time": "2025-01-30T05:59:04.091386",
     "exception": false,
     "start_time": "2025-01-30T05:59:03.915761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (13169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/re_dataset.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbbb310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.118757Z",
     "iopub.status.busy": "2025-01-30T05:59:04.118497Z",
     "iopub.status.idle": "2025-01-30T05:59:04.128328Z",
     "shell.execute_reply": "2025-01-30T05:59:04.127619Z"
    },
    "papermill": {
     "duration": 0.024336,
     "end_time": "2025-01-30T05:59:04.129792",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.105456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d6f436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.156041Z",
     "iopub.status.busy": "2025-01-30T05:59:04.155794Z",
     "iopub.status.idle": "2025-01-30T05:59:04.172890Z",
     "shell.execute_reply": "2025-01-30T05:59:04.171997Z"
    },
    "papermill": {
     "duration": 0.031213,
     "end_time": "2025-01-30T05:59:04.174342",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.143129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    7608\n",
       "1    5561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17cf15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.200233Z",
     "iopub.status.busy": "2025-01-30T05:59:04.199994Z",
     "iopub.status.idle": "2025-01-30T05:59:04.205463Z",
     "shell.execute_reply": "2025-01-30T05:59:04.204785Z"
    },
    "papermill": {
     "duration": 0.0197,
     "end_time": "2025-01-30T05:59:04.206743",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.187043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8126\n",
       "1    5043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49650722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.233689Z",
     "iopub.status.busy": "2025-01-30T05:59:04.233456Z",
     "iopub.status.idle": "2025-01-30T05:59:04.247468Z",
     "shell.execute_reply": "2025-01-30T05:59:04.246640Z"
    },
    "papermill": {
     "duration": 0.029006,
     "end_time": "2025-01-30T05:59:04.248790",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.219784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic shape:  (7309, 13)\n",
      "Non-toxic shape:  (5860, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b5c2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.273051Z",
     "iopub.status.busy": "2025-01-30T05:59:04.272848Z",
     "iopub.status.idle": "2025-01-30T05:59:04.280497Z",
     "shell.execute_reply": "2025-01-30T05:59:04.279738Z"
    },
    "papermill": {
     "duration": 0.021166,
     "end_time": "2025-01-30T05:59:04.281775",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.260609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b36502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.307119Z",
     "iopub.status.busy": "2025-01-30T05:59:04.306868Z",
     "iopub.status.idle": "2025-01-30T05:59:04.320709Z",
     "shell.execute_reply": "2025-01-30T05:59:04.319937Z"
    },
    "papermill": {
     "duration": 0.027727,
     "end_time": "2025-01-30T05:59:04.321888",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.294161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "397706a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.346689Z",
     "iopub.status.busy": "2025-01-30T05:59:04.346478Z",
     "iopub.status.idle": "2025-01-30T05:59:04.349642Z",
     "shell.execute_reply": "2025-01-30T05:59:04.349034Z"
    },
    "papermill": {
     "duration": 0.016802,
     "end_time": "2025-01-30T05:59:04.350752",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.333950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11eef4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.375130Z",
     "iopub.status.busy": "2025-01-30T05:59:04.374924Z",
     "iopub.status.idle": "2025-01-30T05:59:04.734945Z",
     "shell.execute_reply": "2025-01-30T05:59:04.734255Z"
    },
    "papermill": {
     "duration": 0.373622,
     "end_time": "2025-01-30T05:59:04.736269",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.362647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535,) (10535, 12)\n",
      "(2634,) (2634, 12)\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "\n",
    "# Define the labels columns for multi-label classification\n",
    "label_columns = data.columns[1:]  # Assuming label columns start from the third column\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['Tweet'].values\n",
    "y_train = train_data[label_columns].values\n",
    "X_val = val_data['Tweet'].values\n",
    "y_val = val_data[label_columns].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96912a",
   "metadata": {
    "papermill": {
     "duration": 0.011845,
     "end_time": "2025-01-30T05:59:04.760725",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.748880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9ac3fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:04.785435Z",
     "iopub.status.busy": "2025-01-30T05:59:04.785222Z",
     "iopub.status.idle": "2025-01-30T05:59:06.037053Z",
     "shell.execute_reply": "2025-01-30T05:59:06.036352Z"
    },
    "papermill": {
     "duration": 1.265759,
     "end_time": "2025-01-30T05:59:06.038457",
     "exception": false,
     "start_time": "2025-01-30T05:59:04.772698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b0966e66904aed8e1c8f46d7b5c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7ec9af2c0a46f49769d1531ccdabc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8e7ce8dadb4d22a6b61dd91112bba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4063326370dc4782ab517eea5961ab2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32521364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.065288Z",
     "iopub.status.busy": "2025-01-30T05:59:06.065051Z",
     "iopub.status.idle": "2025-01-30T05:59:06.069125Z",
     "shell.execute_reply": "2025-01-30T05:59:06.068527Z"
    },
    "papermill": {
     "duration": 0.018779,
     "end_time": "2025-01-30T05:59:06.070600",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.051821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=64, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25674a",
   "metadata": {
    "papermill": {
     "duration": 0.012881,
     "end_time": "2025-01-30T05:59:06.096260",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.083379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ceb5f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.122173Z",
     "iopub.status.busy": "2025-01-30T05:59:06.121942Z",
     "iopub.status.idle": "2025-01-30T05:59:06.125481Z",
     "shell.execute_reply": "2025-01-30T05:59:06.124808Z"
    },
    "papermill": {
     "duration": 0.01802,
     "end_time": "2025-01-30T05:59:06.126768",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.108748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a31b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.153031Z",
     "iopub.status.busy": "2025-01-30T05:59:06.152830Z",
     "iopub.status.idle": "2025-01-30T05:59:06.157396Z",
     "shell.execute_reply": "2025-01-30T05:59:06.156779Z"
    },
    "papermill": {
     "duration": 0.018808,
     "end_time": "2025-01-30T05:59:06.158653",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.139845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7837fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.184087Z",
     "iopub.status.busy": "2025-01-30T05:59:06.183884Z",
     "iopub.status.idle": "2025-01-30T05:59:06.195111Z",
     "shell.execute_reply": "2025-01-30T05:59:06.194539Z"
    },
    "papermill": {
     "duration": 0.025243,
     "end_time": "2025-01-30T05:59:06.196266",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.171023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ba4ae",
   "metadata": {
    "papermill": {
     "duration": 0.012027,
     "end_time": "2025-01-30T05:59:06.220887",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.208860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2ef2a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.247010Z",
     "iopub.status.busy": "2025-01-30T05:59:06.246660Z",
     "iopub.status.idle": "2025-01-30T05:59:06.252944Z",
     "shell.execute_reply": "2025-01-30T05:59:06.252181Z"
    },
    "papermill": {
     "duration": 0.021096,
     "end_time": "2025-01-30T05:59:06.254120",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.233024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b4abf",
   "metadata": {
    "papermill": {
     "duration": 0.01219,
     "end_time": "2025-01-30T05:59:06.278805",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.266615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a080ae4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.304353Z",
     "iopub.status.busy": "2025-01-30T05:59:06.304154Z",
     "iopub.status.idle": "2025-01-30T05:59:06.323808Z",
     "shell.execute_reply": "2025-01-30T05:59:06.323247Z"
    },
    "papermill": {
     "duration": 0.033918,
     "end_time": "2025-01-30T05:59:06.325042",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.291124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (∆Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = HateSpeechDataset(X_pool, np.zeros((len(X_pool), 12)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = HateSpeechDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    collected_indices = set()  # Initialize set to store selected indices\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    if accelerator.is_local_main_process:\n",
    "        num_of_candidates = len(score_changes[:math.ceil(0.1 * len(score_changes))])\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of clusters\n",
    "        if num_of_candidates <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            n_clusters = n_clusters\n",
    "        elif num_of_candidates > n_clusters and num_of_candidates < nearest_cp - current_train_size:\n",
    "            n_clusters = num_of_candidates\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            n_clusters = nearest_cp - current_train_size\n",
    "\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "            \n",
    "        kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "        kmeans.fit(score_changes)\n",
    "\n",
    "        if current_train_size > checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'HS': [y_train[i][0] for i in temp],\n",
    "                'Abusive': [y_train[i][1] for i in temp],\n",
    "                'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                'HS_Group': [y_train[i][3] for i in temp],\n",
    "                'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                'HS_Race': [y_train[i][5] for i in temp],\n",
    "                'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                'HS_Other': [y_train[i][8] for i in temp],\n",
    "                'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                'HS_Strong': [y_train[i][11] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Samples above threshold:\", num_of_candidates)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]  # Indices of samples in the current cluster\n",
    "                \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances to the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "                closest_sample_index = cluster_indices[np.argmin(cluster_distances)]  # Closest sample index\n",
    "                collected_indices.add(closest_sample_index)\n",
    "\n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "            \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'HS': [y_train[i][0] for i in temp],\n",
    "                    'Abusive': [y_train[i][1] for i in temp],\n",
    "                    'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                    'HS_Group': [y_train[i][3] for i in temp],\n",
    "                    'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                    'HS_Race': [y_train[i][5] for i in temp],\n",
    "                    'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                    'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                    'HS_Other': [y_train[i][8] for i in temp],\n",
    "                    'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                    'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                    'HS_Strong': [y_train[i][11] for i in temp],\n",
    "                })\n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Samples above threshold:\", num_of_candidates)\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d2b2d",
   "metadata": {
    "papermill": {
     "duration": 0.01235,
     "end_time": "2025-01-30T05:59:06.349955",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.337605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a928941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.375330Z",
     "iopub.status.busy": "2025-01-30T05:59:06.375129Z",
     "iopub.status.idle": "2025-01-30T05:59:06.384875Z",
     "shell.execute_reply": "2025-01-30T05:59:06.384297Z"
    },
    "papermill": {
     "duration": 0.023731,
     "end_time": "2025-01-30T05:59:06.385964",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.362233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19609d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.411029Z",
     "iopub.status.busy": "2025-01-30T05:59:06.410821Z",
     "iopub.status.idle": "2025-01-30T05:59:06.413934Z",
     "shell.execute_reply": "2025-01-30T05:59:06.413356Z"
    },
    "papermill": {
     "duration": 0.016822,
     "end_time": "2025-01-30T05:59:06.415056",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.398234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 7, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26256ff",
   "metadata": {
    "papermill": {
     "duration": 0.012173,
     "end_time": "2025-01-30T05:59:06.439777",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.427604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf608af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T05:59:06.465108Z",
     "iopub.status.busy": "2025-01-30T05:59:06.464914Z",
     "iopub.status.idle": "2025-01-30T11:43:26.065648Z",
     "shell.execute_reply": "2025-01-30T11:43:26.064638Z"
    },
    "papermill": {
     "duration": 20659.615168,
     "end_time": "2025-01-30T11:43:26.067313",
     "exception": false,
     "start_time": "2025-01-30T05:59:06.452145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5517, Accuracy: 0.8265, F1 Micro: 0.0155, F1 Macro: 0.0059\n",
      "Epoch 2/10, Train Loss: 0.4241, Accuracy: 0.8268, F1 Micro: 0.0004, F1 Macro: 0.0002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3823, Accuracy: 0.8302, F1 Micro: 0.042, F1 Macro: 0.0178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3708, Accuracy: 0.8524, F1 Micro: 0.3073, F1 Macro: 0.1027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.35, Accuracy: 0.8693, F1 Micro: 0.4709, F1 Macro: 0.2027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3063, Accuracy: 0.8818, F1 Micro: 0.5906, F1 Macro: 0.3067\n",
      "Epoch 7/10, Train Loss: 0.2676, Accuracy: 0.8832, F1 Micro: 0.5756, F1 Macro: 0.3189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2521, Accuracy: 0.889, F1 Micro: 0.634, F1 Macro: 0.3836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2218, Accuracy: 0.8902, F1 Micro: 0.6449, F1 Macro: 0.3921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1893, Accuracy: 0.89, F1 Micro: 0.6481, F1 Macro: 0.3919\n",
      "Model 1 - Iteration 658: Accuracy: 0.89, F1 Micro: 0.6481, F1 Macro: 0.3919\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.77      0.79      1137\n",
      "      Abusive       0.84      0.77      0.80      1008\n",
      "HS_Individual       0.67      0.60      0.63       729\n",
      "     HS_Group       0.64      0.38      0.47       408\n",
      "  HS_Religion       0.56      0.20      0.29       168\n",
      "      HS_Race       1.00      0.05      0.10       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.68      0.64      0.66       771\n",
      "      HS_Weak       0.63      0.58      0.60       681\n",
      "  HS_Moderate       0.52      0.26      0.35       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.73      0.58      0.65      5589\n",
      "    macro avg       0.53      0.35      0.39      5589\n",
      " weighted avg       0.69      0.58      0.62      5589\n",
      "  samples avg       0.38      0.33      0.33      5589\n",
      "\n",
      "Training completed in 59.294963359832764 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.8341, F1 Micro: 0.1921, F1 Macro: 0.0601\n",
      "Epoch 2/10, Train Loss: 0.4277, Accuracy: 0.8335, F1 Micro: 0.0876, F1 Macro: 0.033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3813, Accuracy: 0.8458, F1 Micro: 0.2494, F1 Macro: 0.0854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3672, Accuracy: 0.8583, F1 Micro: 0.4137, F1 Macro: 0.1329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3474, Accuracy: 0.8698, F1 Micro: 0.4975, F1 Macro: 0.2121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3019, Accuracy: 0.8788, F1 Micro: 0.589, F1 Macro: 0.2859\n",
      "Epoch 7/10, Train Loss: 0.2681, Accuracy: 0.8791, F1 Micro: 0.5536, F1 Macro: 0.2828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2536, Accuracy: 0.8843, F1 Micro: 0.5984, F1 Macro: 0.3202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2307, Accuracy: 0.8866, F1 Micro: 0.631, F1 Macro: 0.3694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1942, Accuracy: 0.8877, F1 Micro: 0.6339, F1 Macro: 0.3739\n",
      "Model 2 - Iteration 658: Accuracy: 0.8877, F1 Micro: 0.6339, F1 Macro: 0.3739\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.78      0.80      1137\n",
      "      Abusive       0.83      0.75      0.79      1008\n",
      "HS_Individual       0.66      0.58      0.62       729\n",
      "     HS_Group       0.61      0.30      0.40       408\n",
      "  HS_Religion       0.55      0.12      0.20       168\n",
      "      HS_Race       1.00      0.06      0.11       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.60      0.65       771\n",
      "      HS_Weak       0.61      0.53      0.57       681\n",
      "  HS_Moderate       0.55      0.26      0.35       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.73      0.56      0.63      5589\n",
      "    macro avg       0.53      0.33      0.37      5589\n",
      " weighted avg       0.69      0.56      0.60      5589\n",
      "  samples avg       0.38      0.32      0.32      5589\n",
      "\n",
      "Training completed in 61.04005217552185 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.58, Accuracy: 0.8362, F1 Micro: 0.2748, F1 Macro: 0.0842\n",
      "Epoch 2/10, Train Loss: 0.4275, Accuracy: 0.8286, F1 Micro: 0.0261, F1 Macro: 0.0113\n",
      "Epoch 3/10, Train Loss: 0.3807, Accuracy: 0.8374, F1 Micro: 0.1411, F1 Macro: 0.0531\n",
      "Epoch 4/10, Train Loss: 0.3712, Accuracy: 0.8484, F1 Micro: 0.2715, F1 Macro: 0.0928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3527, Accuracy: 0.8608, F1 Micro: 0.3932, F1 Macro: 0.1447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3105, Accuracy: 0.8769, F1 Micro: 0.5572, F1 Macro: 0.2737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2779, Accuracy: 0.8805, F1 Micro: 0.5651, F1 Macro: 0.3034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2567, Accuracy: 0.8842, F1 Micro: 0.5902, F1 Macro: 0.3608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2319, Accuracy: 0.8885, F1 Micro: 0.6304, F1 Macro: 0.3777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1985, Accuracy: 0.8908, F1 Micro: 0.6498, F1 Macro: 0.4023\n",
      "Model 3 - Iteration 658: Accuracy: 0.8908, F1 Micro: 0.6498, F1 Macro: 0.4023\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.77      0.79      1137\n",
      "      Abusive       0.84      0.77      0.80      1008\n",
      "HS_Individual       0.67      0.60      0.63       729\n",
      "     HS_Group       0.62      0.35      0.45       408\n",
      "  HS_Religion       0.63      0.26      0.37       168\n",
      "      HS_Race       1.00      0.05      0.10       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.71      0.64      0.67       771\n",
      "      HS_Weak       0.62      0.57      0.60       681\n",
      "  HS_Moderate       0.58      0.26      0.36       359\n",
      "    HS_Strong       1.00      0.03      0.06        97\n",
      "\n",
      "    micro avg       0.73      0.58      0.65      5589\n",
      "    macro avg       0.62      0.36      0.40      5589\n",
      " weighted avg       0.72      0.58      0.62      5589\n",
      "  samples avg       0.40      0.34      0.34      5589\n",
      "\n",
      "Training completed in 57.732555866241455 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8895, F1 Micro: 0.6439, F1 Macro: 0.3894\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 125.21263384819031 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5008, Accuracy: 0.8313, F1 Micro: 0.3992, F1 Macro: 0.1091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4175, Accuracy: 0.8632, F1 Micro: 0.5527, F1 Macro: 0.2479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.362, Accuracy: 0.8902, F1 Micro: 0.6346, F1 Macro: 0.3488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3065, Accuracy: 0.897, F1 Micro: 0.6375, F1 Macro: 0.4287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2599, Accuracy: 0.903, F1 Micro: 0.6971, F1 Macro: 0.5301\n",
      "Epoch 6/10, Train Loss: 0.2167, Accuracy: 0.9035, F1 Micro: 0.6764, F1 Macro: 0.5093\n",
      "Epoch 7/10, Train Loss: 0.1866, Accuracy: 0.9046, F1 Micro: 0.6903, F1 Macro: 0.5165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1546, Accuracy: 0.9061, F1 Micro: 0.7078, F1 Macro: 0.5554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1363, Accuracy: 0.9067, F1 Micro: 0.7119, F1 Macro: 0.5504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.117, Accuracy: 0.906, F1 Micro: 0.7121, F1 Macro: 0.5525\n",
      "Model 1 - Iteration 1646: Accuracy: 0.906, F1 Micro: 0.7121, F1 Macro: 0.5525\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.78      0.81      1137\n",
      "      Abusive       0.83      0.85      0.84      1008\n",
      "HS_Individual       0.68      0.68      0.68       729\n",
      "     HS_Group       0.74      0.49      0.59       408\n",
      "  HS_Religion       0.73      0.54      0.62       168\n",
      "      HS_Race       0.88      0.42      0.57       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       1.00      0.04      0.07        55\n",
      "     HS_Other       0.78      0.69      0.73       771\n",
      "      HS_Weak       0.64      0.67      0.66       681\n",
      "  HS_Moderate       0.65      0.36      0.46       359\n",
      "    HS_Strong       0.88      0.46      0.61        97\n",
      "\n",
      "    micro avg       0.76      0.67      0.71      5589\n",
      "    macro avg       0.72      0.50      0.55      5589\n",
      " weighted avg       0.75      0.67      0.70      5589\n",
      "  samples avg       0.41      0.38      0.38      5589\n",
      "\n",
      "Training completed in 83.87281918525696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5126, Accuracy: 0.8424, F1 Micro: 0.4024, F1 Macro: 0.1127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4186, Accuracy: 0.8621, F1 Micro: 0.5342, F1 Macro: 0.2191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3694, Accuracy: 0.8872, F1 Micro: 0.6215, F1 Macro: 0.3343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3103, Accuracy: 0.8961, F1 Micro: 0.6417, F1 Macro: 0.4273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2645, Accuracy: 0.9016, F1 Micro: 0.6706, F1 Macro: 0.476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2267, Accuracy: 0.9029, F1 Micro: 0.677, F1 Macro: 0.5059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1938, Accuracy: 0.9037, F1 Micro: 0.6881, F1 Macro: 0.5026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.162, Accuracy: 0.906, F1 Micro: 0.7106, F1 Macro: 0.5554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1445, Accuracy: 0.9053, F1 Micro: 0.7138, F1 Macro: 0.5485\n",
      "Epoch 10/10, Train Loss: 0.1226, Accuracy: 0.9055, F1 Micro: 0.7085, F1 Macro: 0.5398\n",
      "Model 2 - Iteration 1646: Accuracy: 0.9053, F1 Micro: 0.7138, F1 Macro: 0.5485\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.82      1137\n",
      "      Abusive       0.86      0.81      0.83      1008\n",
      "HS_Individual       0.66      0.72      0.69       729\n",
      "     HS_Group       0.72      0.49      0.58       408\n",
      "  HS_Religion       0.75      0.47      0.58       168\n",
      "      HS_Race       0.84      0.43      0.57       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       1.00      0.02      0.04        55\n",
      "     HS_Other       0.76      0.71      0.74       771\n",
      "      HS_Weak       0.62      0.70      0.66       681\n",
      "  HS_Moderate       0.63      0.37      0.47       359\n",
      "    HS_Strong       0.79      0.51      0.62        97\n",
      "\n",
      "    micro avg       0.75      0.68      0.71      5589\n",
      "    macro avg       0.70      0.50      0.55      5589\n",
      " weighted avg       0.75      0.68      0.70      5589\n",
      "  samples avg       0.41      0.38      0.38      5589\n",
      "\n",
      "Training completed in 85.62688493728638 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5124, Accuracy: 0.8312, F1 Micro: 0.4032, F1 Macro: 0.1096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4161, Accuracy: 0.8615, F1 Micro: 0.5133, F1 Macro: 0.2157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.367, Accuracy: 0.8895, F1 Micro: 0.632, F1 Macro: 0.3587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3116, Accuracy: 0.8973, F1 Micro: 0.639, F1 Macro: 0.4306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2677, Accuracy: 0.9041, F1 Micro: 0.6788, F1 Macro: 0.4938\n",
      "Epoch 6/10, Train Loss: 0.2274, Accuracy: 0.904, F1 Micro: 0.6784, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1933, Accuracy: 0.9075, F1 Micro: 0.7054, F1 Macro: 0.5319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1603, Accuracy: 0.9069, F1 Micro: 0.7066, F1 Macro: 0.5493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1426, Accuracy: 0.9071, F1 Micro: 0.7118, F1 Macro: 0.5571\n",
      "Epoch 10/10, Train Loss: 0.121, Accuracy: 0.9059, F1 Micro: 0.6997, F1 Macro: 0.5289\n",
      "Model 3 - Iteration 1646: Accuracy: 0.9071, F1 Micro: 0.7118, F1 Macro: 0.5571\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.78      0.81      1137\n",
      "      Abusive       0.86      0.80      0.83      1008\n",
      "HS_Individual       0.71      0.65      0.68       729\n",
      "     HS_Group       0.71      0.55      0.62       408\n",
      "  HS_Religion       0.75      0.58      0.65       168\n",
      "      HS_Race       0.84      0.47      0.60       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.78      0.67      0.72       771\n",
      "      HS_Weak       0.67      0.62      0.65       681\n",
      "  HS_Moderate       0.60      0.45      0.52       359\n",
      "    HS_Strong       0.87      0.47      0.61        97\n",
      "\n",
      "    micro avg       0.77      0.66      0.71      5589\n",
      "    macro avg       0.63      0.50      0.56      5589\n",
      " weighted avg       0.75      0.66      0.70      5589\n",
      "  samples avg       0.40      0.37      0.37      5589\n",
      "\n",
      "Training completed in 83.66229104995728 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8978, F1 Micro: 0.6782, F1 Macro: 0.471\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 112.97828078269958 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4758, Accuracy: 0.8311, F1 Micro: 0.409, F1 Macro: 0.1193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3855, Accuracy: 0.8908, F1 Micro: 0.6388, F1 Macro: 0.3749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3282, Accuracy: 0.9055, F1 Micro: 0.7084, F1 Macro: 0.5318\n",
      "Epoch 4/10, Train Loss: 0.2708, Accuracy: 0.9059, F1 Micro: 0.6984, F1 Macro: 0.5121\n",
      "Epoch 5/10, Train Loss: 0.2231, Accuracy: 0.9097, F1 Micro: 0.6989, F1 Macro: 0.5364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1851, Accuracy: 0.9131, F1 Micro: 0.7283, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1541, Accuracy: 0.913, F1 Micro: 0.733, F1 Macro: 0.5976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1224, Accuracy: 0.9125, F1 Micro: 0.7345, F1 Macro: 0.5966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.103, Accuracy: 0.9137, F1 Micro: 0.7362, F1 Macro: 0.6001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0892, Accuracy: 0.9128, F1 Micro: 0.7375, F1 Macro: 0.6134\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9128, F1 Micro: 0.7375, F1 Macro: 0.6134\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1137\n",
      "      Abusive       0.87      0.83      0.85      1008\n",
      "HS_Individual       0.73      0.67      0.70       729\n",
      "     HS_Group       0.65      0.63      0.64       408\n",
      "  HS_Religion       0.77      0.59      0.67       168\n",
      "      HS_Race       0.81      0.70      0.75       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.67      0.11      0.19        55\n",
      "     HS_Other       0.77      0.75      0.76       771\n",
      "      HS_Weak       0.69      0.63      0.66       681\n",
      "  HS_Moderate       0.60      0.52      0.56       359\n",
      "    HS_Strong       0.78      0.76      0.77        97\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5589\n",
      "    macro avg       0.68      0.58      0.61      5589\n",
      " weighted avg       0.76      0.71      0.73      5589\n",
      "  samples avg       0.42      0.40      0.39      5589\n",
      "\n",
      "Training completed in 105.22380137443542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.485, Accuracy: 0.8386, F1 Micro: 0.4164, F1 Macro: 0.1252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3885, Accuracy: 0.8878, F1 Micro: 0.6102, F1 Macro: 0.3222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3318, Accuracy: 0.905, F1 Micro: 0.708, F1 Macro: 0.5205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2778, Accuracy: 0.908, F1 Micro: 0.7103, F1 Macro: 0.5234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2287, Accuracy: 0.9111, F1 Micro: 0.7178, F1 Macro: 0.5448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1968, Accuracy: 0.9149, F1 Micro: 0.7319, F1 Macro: 0.5814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1632, Accuracy: 0.9145, F1 Micro: 0.7364, F1 Macro: 0.5944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1245, Accuracy: 0.9134, F1 Micro: 0.7401, F1 Macro: 0.6051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1082, Accuracy: 0.9153, F1 Micro: 0.7428, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0925, Accuracy: 0.9146, F1 Micro: 0.7471, F1 Macro: 0.6191\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9146, F1 Micro: 0.7471, F1 Macro: 0.6191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1137\n",
      "      Abusive       0.86      0.86      0.86      1008\n",
      "HS_Individual       0.70      0.71      0.71       729\n",
      "     HS_Group       0.72      0.60      0.65       408\n",
      "  HS_Religion       0.78      0.61      0.68       168\n",
      "      HS_Race       0.78      0.70      0.74       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.53      0.15      0.23        55\n",
      "     HS_Other       0.77      0.75      0.76       771\n",
      "      HS_Weak       0.66      0.70      0.68       681\n",
      "  HS_Moderate       0.66      0.52      0.58       359\n",
      "    HS_Strong       0.78      0.65      0.71        97\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5589\n",
      "    macro avg       0.67      0.59      0.62      5589\n",
      " weighted avg       0.76      0.73      0.74      5589\n",
      "  samples avg       0.43      0.41      0.40      5589\n",
      "\n",
      "Training completed in 108.53104043006897 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4815, Accuracy: 0.8344, F1 Micro: 0.4173, F1 Macro: 0.1296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3885, Accuracy: 0.886, F1 Micro: 0.6002, F1 Macro: 0.3332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3309, Accuracy: 0.9038, F1 Micro: 0.7044, F1 Macro: 0.5299\n",
      "Epoch 4/10, Train Loss: 0.2789, Accuracy: 0.9081, F1 Micro: 0.6985, F1 Macro: 0.5154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2315, Accuracy: 0.9114, F1 Micro: 0.7136, F1 Macro: 0.5528\n",
      "Epoch 6/10, Train Loss: 0.1941, Accuracy: 0.9116, F1 Micro: 0.7109, F1 Macro: 0.5687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1608, Accuracy: 0.9159, F1 Micro: 0.7377, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1251, Accuracy: 0.912, F1 Micro: 0.7432, F1 Macro: 0.6049\n",
      "Epoch 9/10, Train Loss: 0.107, Accuracy: 0.9108, F1 Micro: 0.743, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0892, Accuracy: 0.9139, F1 Micro: 0.7455, F1 Macro: 0.6226\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9139, F1 Micro: 0.7455, F1 Macro: 0.6226\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1137\n",
      "      Abusive       0.84      0.87      0.85      1008\n",
      "HS_Individual       0.70      0.69      0.70       729\n",
      "     HS_Group       0.66      0.62      0.64       408\n",
      "  HS_Religion       0.77      0.59      0.67       168\n",
      "      HS_Race       0.81      0.71      0.75       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.62      0.18      0.28        55\n",
      "     HS_Other       0.79      0.77      0.78       771\n",
      "      HS_Weak       0.66      0.66      0.66       681\n",
      "  HS_Moderate       0.61      0.53      0.57       359\n",
      "    HS_Strong       0.79      0.67      0.73        97\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5589\n",
      "    macro avg       0.68      0.59      0.62      5589\n",
      " weighted avg       0.75      0.73      0.74      5589\n",
      "  samples avg       0.43      0.41      0.40      5589\n",
      "\n",
      "Training completed in 103.0932502746582 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9031, F1 Micro: 0.7, F1 Macro: 0.5201\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 100.96445918083191 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4656, Accuracy: 0.8588, F1 Micro: 0.5205, F1 Macro: 0.2367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3624, Accuracy: 0.8999, F1 Micro: 0.6708, F1 Macro: 0.4103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2949, Accuracy: 0.9072, F1 Micro: 0.708, F1 Macro: 0.4838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2453, Accuracy: 0.9115, F1 Micro: 0.7364, F1 Macro: 0.5545\n",
      "Epoch 5/10, Train Loss: 0.1982, Accuracy: 0.9137, F1 Micro: 0.7146, F1 Macro: 0.5688\n",
      "Epoch 6/10, Train Loss: 0.163, Accuracy: 0.916, F1 Micro: 0.7359, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1346, Accuracy: 0.9133, F1 Micro: 0.7458, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1096, Accuracy: 0.9156, F1 Micro: 0.7504, F1 Macro: 0.6295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.094, Accuracy: 0.9152, F1 Micro: 0.7527, F1 Macro: 0.6354\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9168, F1 Micro: 0.752, F1 Macro: 0.6411\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9152, F1 Micro: 0.7527, F1 Macro: 0.6354\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.83      1137\n",
      "      Abusive       0.83      0.91      0.87      1008\n",
      "HS_Individual       0.70      0.73      0.72       729\n",
      "     HS_Group       0.71      0.58      0.64       408\n",
      "  HS_Religion       0.81      0.61      0.70       168\n",
      "      HS_Race       0.85      0.66      0.74       119\n",
      "  HS_Physical       0.25      0.04      0.06        57\n",
      "    HS_Gender       0.42      0.25      0.32        55\n",
      "     HS_Other       0.77      0.78      0.78       771\n",
      "      HS_Weak       0.67      0.71      0.69       681\n",
      "  HS_Moderate       0.63      0.50      0.55       359\n",
      "    HS_Strong       0.79      0.69      0.74        97\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5589\n",
      "    macro avg       0.69      0.61      0.64      5589\n",
      " weighted avg       0.75      0.74      0.75      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 124.39276766777039 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4713, Accuracy: 0.8638, F1 Micro: 0.5243, F1 Macro: 0.2211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3675, Accuracy: 0.8981, F1 Micro: 0.6709, F1 Macro: 0.3882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3005, Accuracy: 0.9074, F1 Micro: 0.7158, F1 Macro: 0.4773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2542, Accuracy: 0.9132, F1 Micro: 0.7335, F1 Macro: 0.5449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2048, Accuracy: 0.9186, F1 Micro: 0.7422, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1666, Accuracy: 0.9191, F1 Micro: 0.7499, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1405, Accuracy: 0.9142, F1 Micro: 0.7566, F1 Macro: 0.6232\n",
      "Epoch 8/10, Train Loss: 0.1159, Accuracy: 0.9159, F1 Micro: 0.7527, F1 Macro: 0.6335\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9155, F1 Micro: 0.7489, F1 Macro: 0.6274\n",
      "Epoch 10/10, Train Loss: 0.0831, Accuracy: 0.918, F1 Micro: 0.7565, F1 Macro: 0.6335\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9142, F1 Micro: 0.7566, F1 Macro: 0.6232\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1137\n",
      "      Abusive       0.86      0.88      0.87      1008\n",
      "HS_Individual       0.70      0.76      0.73       729\n",
      "     HS_Group       0.66      0.65      0.66       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.76      0.66      0.71       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.86      0.11      0.19        55\n",
      "     HS_Other       0.72      0.83      0.77       771\n",
      "      HS_Weak       0.66      0.73      0.69       681\n",
      "  HS_Moderate       0.60      0.57      0.58       359\n",
      "    HS_Strong       0.80      0.70      0.75        97\n",
      "\n",
      "    micro avg       0.74      0.77      0.76      5589\n",
      "    macro avg       0.68      0.62      0.62      5589\n",
      " weighted avg       0.74      0.77      0.75      5589\n",
      "  samples avg       0.43      0.43      0.42      5589\n",
      "\n",
      "Training completed in 123.6633415222168 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4696, Accuracy: 0.8533, F1 Micro: 0.4934, F1 Macro: 0.2146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3708, Accuracy: 0.8961, F1 Micro: 0.6598, F1 Macro: 0.3934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3023, Accuracy: 0.9072, F1 Micro: 0.7157, F1 Macro: 0.5082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2544, Accuracy: 0.9109, F1 Micro: 0.7379, F1 Macro: 0.5575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2037, Accuracy: 0.9168, F1 Micro: 0.7388, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.167, Accuracy: 0.9169, F1 Micro: 0.7425, F1 Macro: 0.6092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1387, Accuracy: 0.9169, F1 Micro: 0.7532, F1 Macro: 0.6122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1122, Accuracy: 0.9146, F1 Micro: 0.7546, F1 Macro: 0.6315\n",
      "Epoch 9/10, Train Loss: 0.0983, Accuracy: 0.9165, F1 Micro: 0.7523, F1 Macro: 0.6306\n",
      "Epoch 10/10, Train Loss: 0.0822, Accuracy: 0.9139, F1 Micro: 0.7442, F1 Macro: 0.6224\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9146, F1 Micro: 0.7546, F1 Macro: 0.6315\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1137\n",
      "      Abusive       0.85      0.89      0.87      1008\n",
      "HS_Individual       0.70      0.75      0.72       729\n",
      "     HS_Group       0.66      0.60      0.63       408\n",
      "  HS_Religion       0.69      0.68      0.68       168\n",
      "      HS_Race       0.75      0.69      0.72       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.56      0.25      0.35        55\n",
      "     HS_Other       0.79      0.79      0.79       771\n",
      "      HS_Weak       0.65      0.73      0.69       681\n",
      "  HS_Moderate       0.58      0.53      0.55       359\n",
      "    HS_Strong       0.78      0.69      0.73        97\n",
      "\n",
      "    micro avg       0.75      0.76      0.75      5589\n",
      "    macro avg       0.65      0.62      0.63      5589\n",
      " weighted avg       0.74      0.76      0.75      5589\n",
      "  samples avg       0.43      0.43      0.42      5589\n",
      "\n",
      "Training completed in 125.10028409957886 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.906, F1 Micro: 0.7136, F1 Macro: 0.5476\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 91.25682950019836 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4527, Accuracy: 0.8679, F1 Micro: 0.5151, F1 Macro: 0.2235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3507, Accuracy: 0.9058, F1 Micro: 0.7181, F1 Macro: 0.5095\n",
      "Epoch 3/10, Train Loss: 0.282, Accuracy: 0.9096, F1 Micro: 0.7123, F1 Macro: 0.5078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2311, Accuracy: 0.9185, F1 Micro: 0.7456, F1 Macro: 0.5978\n",
      "Epoch 5/10, Train Loss: 0.1882, Accuracy: 0.9175, F1 Micro: 0.7398, F1 Macro: 0.598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1582, Accuracy: 0.9182, F1 Micro: 0.7582, F1 Macro: 0.6501\n",
      "Epoch 7/10, Train Loss: 0.1225, Accuracy: 0.9204, F1 Micro: 0.7558, F1 Macro: 0.6357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1075, Accuracy: 0.9159, F1 Micro: 0.7614, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0869, Accuracy: 0.9188, F1 Micro: 0.7618, F1 Macro: 0.6443\n",
      "Epoch 10/10, Train Loss: 0.0746, Accuracy: 0.9179, F1 Micro: 0.7563, F1 Macro: 0.6547\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9188, F1 Micro: 0.7618, F1 Macro: 0.6443\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1137\n",
      "      Abusive       0.84      0.92      0.88      1008\n",
      "HS_Individual       0.71      0.74      0.73       729\n",
      "     HS_Group       0.72      0.58      0.64       408\n",
      "  HS_Religion       0.78      0.67      0.72       168\n",
      "      HS_Race       0.80      0.69      0.74       119\n",
      "  HS_Physical       0.50      0.04      0.07        57\n",
      "    HS_Gender       0.71      0.22      0.33        55\n",
      "     HS_Other       0.81      0.77      0.79       771\n",
      "      HS_Weak       0.68      0.72      0.70       681\n",
      "  HS_Moderate       0.63      0.48      0.55       359\n",
      "    HS_Strong       0.80      0.72      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5589\n",
      "    macro avg       0.73      0.62      0.64      5589\n",
      " weighted avg       0.77      0.75      0.75      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 139.1214587688446 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.46, Accuracy: 0.8689, F1 Micro: 0.5326, F1 Macro: 0.2302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3565, Accuracy: 0.9017, F1 Micro: 0.7127, F1 Macro: 0.515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2896, Accuracy: 0.9122, F1 Micro: 0.7179, F1 Macro: 0.5099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.238, Accuracy: 0.9201, F1 Micro: 0.7532, F1 Macro: 0.6008\n",
      "Epoch 5/10, Train Loss: 0.1945, Accuracy: 0.919, F1 Micro: 0.7497, F1 Macro: 0.6033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1657, Accuracy: 0.9158, F1 Micro: 0.7575, F1 Macro: 0.6463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.127, Accuracy: 0.9193, F1 Micro: 0.7599, F1 Macro: 0.6492\n",
      "Epoch 8/10, Train Loss: 0.1069, Accuracy: 0.9197, F1 Micro: 0.757, F1 Macro: 0.6427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0911, Accuracy: 0.9158, F1 Micro: 0.7603, F1 Macro: 0.6468\n",
      "Epoch 10/10, Train Loss: 0.0786, Accuracy: 0.9204, F1 Micro: 0.7567, F1 Macro: 0.6475\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9158, F1 Micro: 0.7603, F1 Macro: 0.6468\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1137\n",
      "      Abusive       0.85      0.90      0.87      1008\n",
      "HS_Individual       0.68      0.78      0.73       729\n",
      "     HS_Group       0.71      0.60      0.65       408\n",
      "  HS_Religion       0.66      0.71      0.68       168\n",
      "      HS_Race       0.70      0.82      0.76       119\n",
      "  HS_Physical       0.33      0.02      0.03        57\n",
      "    HS_Gender       0.56      0.27      0.37        55\n",
      "     HS_Other       0.79      0.76      0.77       771\n",
      "      HS_Weak       0.65      0.76      0.70       681\n",
      "  HS_Moderate       0.66      0.51      0.58       359\n",
      "    HS_Strong       0.78      0.78      0.78        97\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5589\n",
      "    macro avg       0.68      0.65      0.65      5589\n",
      " weighted avg       0.75      0.77      0.75      5589\n",
      "  samples avg       0.45      0.44      0.42      5589\n",
      "\n",
      "Training completed in 141.14565229415894 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4582, Accuracy: 0.8606, F1 Micro: 0.5014, F1 Macro: 0.2206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.358, Accuracy: 0.9008, F1 Micro: 0.701, F1 Macro: 0.4993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2906, Accuracy: 0.9099, F1 Micro: 0.7049, F1 Macro: 0.503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2408, Accuracy: 0.9178, F1 Micro: 0.7491, F1 Macro: 0.5943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1939, Accuracy: 0.9201, F1 Micro: 0.7593, F1 Macro: 0.6065\n",
      "Epoch 6/10, Train Loss: 0.1606, Accuracy: 0.9177, F1 Micro: 0.7578, F1 Macro: 0.6399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1262, Accuracy: 0.9204, F1 Micro: 0.7655, F1 Macro: 0.6419\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9193, F1 Micro: 0.7485, F1 Macro: 0.618\n",
      "Epoch 9/10, Train Loss: 0.0912, Accuracy: 0.9202, F1 Micro: 0.7606, F1 Macro: 0.6327\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9187, F1 Micro: 0.7512, F1 Macro: 0.6361\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9204, F1 Micro: 0.7655, F1 Macro: 0.6419\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1137\n",
      "      Abusive       0.86      0.89      0.88      1008\n",
      "HS_Individual       0.74      0.73      0.73       729\n",
      "     HS_Group       0.70      0.62      0.65       408\n",
      "  HS_Religion       0.74      0.65      0.69       168\n",
      "      HS_Race       0.78      0.74      0.76       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.61      0.20      0.30        55\n",
      "     HS_Other       0.80      0.77      0.79       771\n",
      "      HS_Weak       0.70      0.71      0.70       681\n",
      "  HS_Moderate       0.63      0.57      0.60       359\n",
      "    HS_Strong       0.80      0.72      0.76        97\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5589\n",
      "    macro avg       0.68      0.62      0.64      5589\n",
      " weighted avg       0.77      0.75      0.76      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 139.42896938323975 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9085, F1 Micro: 0.7234, F1 Macro: 0.567\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 81.90603423118591 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4403, Accuracy: 0.8727, F1 Micro: 0.6426, F1 Macro: 0.3594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.333, Accuracy: 0.9101, F1 Micro: 0.7333, F1 Macro: 0.5522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2709, Accuracy: 0.9175, F1 Micro: 0.7472, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2236, Accuracy: 0.9212, F1 Micro: 0.7607, F1 Macro: 0.6217\n",
      "Epoch 5/10, Train Loss: 0.1758, Accuracy: 0.9199, F1 Micro: 0.7566, F1 Macro: 0.6296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1477, Accuracy: 0.921, F1 Micro: 0.762, F1 Macro: 0.655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9223, F1 Micro: 0.7636, F1 Macro: 0.6497\n",
      "Epoch 8/10, Train Loss: 0.1019, Accuracy: 0.9219, F1 Micro: 0.7612, F1 Macro: 0.6678\n",
      "Epoch 9/10, Train Loss: 0.0831, Accuracy: 0.9218, F1 Micro: 0.7621, F1 Macro: 0.6707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.92, F1 Micro: 0.7693, F1 Macro: 0.6769\n",
      "Model 1 - Iteration 4703: Accuracy: 0.92, F1 Micro: 0.7693, F1 Macro: 0.6769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.86      0.90      0.88      1008\n",
      "HS_Individual       0.72      0.73      0.73       729\n",
      "     HS_Group       0.67      0.65      0.66       408\n",
      "  HS_Religion       0.75      0.68      0.71       168\n",
      "      HS_Race       0.76      0.81      0.78       119\n",
      "  HS_Physical       0.65      0.19      0.30        57\n",
      "    HS_Gender       0.58      0.27      0.37        55\n",
      "     HS_Other       0.79      0.81      0.80       771\n",
      "      HS_Weak       0.69      0.70      0.70       681\n",
      "  HS_Moderate       0.61      0.58      0.60       359\n",
      "    HS_Strong       0.78      0.73      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5589\n",
      "    macro avg       0.72      0.66      0.68      5589\n",
      " weighted avg       0.77      0.77      0.77      5589\n",
      "  samples avg       0.44      0.44      0.42      5589\n",
      "\n",
      "Training completed in 158.14989495277405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4445, Accuracy: 0.8824, F1 Micro: 0.6301, F1 Macro: 0.328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3364, Accuracy: 0.9092, F1 Micro: 0.7292, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2755, Accuracy: 0.9171, F1 Micro: 0.738, F1 Macro: 0.5849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2276, Accuracy: 0.9209, F1 Micro: 0.7566, F1 Macro: 0.6074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.9217, F1 Micro: 0.7615, F1 Macro: 0.6375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1446, Accuracy: 0.921, F1 Micro: 0.7668, F1 Macro: 0.6568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1213, Accuracy: 0.923, F1 Micro: 0.7703, F1 Macro: 0.6531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1016, Accuracy: 0.9237, F1 Micro: 0.7737, F1 Macro: 0.6724\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9226, F1 Micro: 0.7721, F1 Macro: 0.6782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9227, F1 Micro: 0.775, F1 Macro: 0.6854\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9227, F1 Micro: 0.775, F1 Macro: 0.6854\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1137\n",
      "      Abusive       0.87      0.90      0.88      1008\n",
      "HS_Individual       0.73      0.75      0.74       729\n",
      "     HS_Group       0.71      0.63      0.67       408\n",
      "  HS_Religion       0.78      0.66      0.71       168\n",
      "      HS_Race       0.74      0.80      0.77       119\n",
      "  HS_Physical       0.67      0.21      0.32        57\n",
      "    HS_Gender       0.58      0.33      0.42        55\n",
      "     HS_Other       0.80      0.79      0.80       771\n",
      "      HS_Weak       0.70      0.73      0.71       681\n",
      "  HS_Moderate       0.64      0.55      0.59       359\n",
      "    HS_Strong       0.78      0.74      0.76        97\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5589\n",
      "    macro avg       0.74      0.66      0.69      5589\n",
      " weighted avg       0.78      0.77      0.77      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 161.66425323486328 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4429, Accuracy: 0.8676, F1 Micro: 0.6301, F1 Macro: 0.3635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3379, Accuracy: 0.9087, F1 Micro: 0.7264, F1 Macro: 0.5372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2761, Accuracy: 0.9169, F1 Micro: 0.7497, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2294, Accuracy: 0.9188, F1 Micro: 0.7626, F1 Macro: 0.6114\n",
      "Epoch 5/10, Train Loss: 0.1778, Accuracy: 0.9218, F1 Micro: 0.7612, F1 Macro: 0.6321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1491, Accuracy: 0.9229, F1 Micro: 0.7697, F1 Macro: 0.6512\n",
      "Epoch 7/10, Train Loss: 0.1212, Accuracy: 0.9216, F1 Micro: 0.7637, F1 Macro: 0.6412\n",
      "Epoch 8/10, Train Loss: 0.0996, Accuracy: 0.9227, F1 Micro: 0.7647, F1 Macro: 0.6476\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.9211, F1 Micro: 0.7663, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0736, Accuracy: 0.9209, F1 Micro: 0.7718, F1 Macro: 0.6656\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9209, F1 Micro: 0.7718, F1 Macro: 0.6656\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.86      0.91      0.88      1008\n",
      "HS_Individual       0.72      0.75      0.73       729\n",
      "     HS_Group       0.68      0.65      0.66       408\n",
      "  HS_Religion       0.71      0.70      0.70       168\n",
      "      HS_Race       0.74      0.80      0.77       119\n",
      "  HS_Physical       0.64      0.12      0.21        57\n",
      "    HS_Gender       0.52      0.22      0.31        55\n",
      "     HS_Other       0.81      0.79      0.80       771\n",
      "      HS_Weak       0.70      0.72      0.71       681\n",
      "  HS_Moderate       0.62      0.58      0.60       359\n",
      "    HS_Strong       0.78      0.75      0.77        97\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5589\n",
      "    macro avg       0.72      0.65      0.67      5589\n",
      " weighted avg       0.77      0.77      0.77      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 155.86444306373596 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9106, F1 Micro: 0.7315, F1 Macro: 0.5851\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 74.1010115146637 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4339, Accuracy: 0.8886, F1 Micro: 0.6536, F1 Macro: 0.3856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3244, Accuracy: 0.9107, F1 Micro: 0.7111, F1 Macro: 0.5559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2605, Accuracy: 0.9181, F1 Micro: 0.7511, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2117, Accuracy: 0.9192, F1 Micro: 0.7599, F1 Macro: 0.6163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1695, Accuracy: 0.9238, F1 Micro: 0.7761, F1 Macro: 0.6807\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9223, F1 Micro: 0.7702, F1 Macro: 0.667\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9211, F1 Micro: 0.7749, F1 Macro: 0.6684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9224, F1 Micro: 0.7762, F1 Macro: 0.6833\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9226, F1 Micro: 0.7715, F1 Macro: 0.7037\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9191, F1 Micro: 0.7709, F1 Macro: 0.6888\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9224, F1 Micro: 0.7762, F1 Macro: 0.6833\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.70      0.62      0.66       408\n",
      "  HS_Religion       0.80      0.60      0.68       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.63      0.21      0.32        57\n",
      "    HS_Gender       0.75      0.27      0.40        55\n",
      "     HS_Other       0.78      0.83      0.80       771\n",
      "      HS_Weak       0.69      0.74      0.71       681\n",
      "  HS_Moderate       0.65      0.55      0.60       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.75      0.66      0.68      5589\n",
      " weighted avg       0.77      0.78      0.77      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 169.71092057228088 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.44, Accuracy: 0.8848, F1 Micro: 0.6441, F1 Macro: 0.3566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3293, Accuracy: 0.9121, F1 Micro: 0.712, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2659, Accuracy: 0.9205, F1 Micro: 0.7544, F1 Macro: 0.6057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2183, Accuracy: 0.9207, F1 Micro: 0.7674, F1 Macro: 0.6303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1693, Accuracy: 0.923, F1 Micro: 0.7694, F1 Macro: 0.6645\n",
      "Epoch 6/10, Train Loss: 0.1402, Accuracy: 0.9233, F1 Micro: 0.7681, F1 Macro: 0.6664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1109, Accuracy: 0.9224, F1 Micro: 0.7778, F1 Macro: 0.6848\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9222, F1 Micro: 0.772, F1 Macro: 0.6936\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.922, F1 Micro: 0.7665, F1 Macro: 0.6836\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.92, F1 Micro: 0.7701, F1 Macro: 0.6946\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9224, F1 Micro: 0.7778, F1 Macro: 0.6848\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1137\n",
      "      Abusive       0.87      0.88      0.88      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.71      0.66      0.68       408\n",
      "  HS_Religion       0.74      0.69      0.71       168\n",
      "      HS_Race       0.85      0.71      0.78       119\n",
      "  HS_Physical       0.75      0.16      0.26        57\n",
      "    HS_Gender       0.74      0.25      0.38        55\n",
      "     HS_Other       0.76      0.83      0.79       771\n",
      "      HS_Weak       0.69      0.74      0.71       681\n",
      "  HS_Moderate       0.66      0.59      0.63       359\n",
      "    HS_Strong       0.82      0.77      0.80        97\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5589\n",
      "    macro avg       0.76      0.66      0.68      5589\n",
      " weighted avg       0.77      0.78      0.77      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 169.558030128479 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4394, Accuracy: 0.8821, F1 Micro: 0.62, F1 Macro: 0.336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3313, Accuracy: 0.911, F1 Micro: 0.7115, F1 Macro: 0.5531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2645, Accuracy: 0.919, F1 Micro: 0.7537, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2173, Accuracy: 0.9196, F1 Micro: 0.764, F1 Macro: 0.6167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1735, Accuracy: 0.9247, F1 Micro: 0.7678, F1 Macro: 0.6517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1418, Accuracy: 0.9221, F1 Micro: 0.7687, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9234, F1 Micro: 0.7747, F1 Macro: 0.6644\n",
      "Epoch 8/10, Train Loss: 0.093, Accuracy: 0.9238, F1 Micro: 0.7704, F1 Macro: 0.6785\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9213, F1 Micro: 0.7668, F1 Macro: 0.6834\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9218, F1 Micro: 0.7707, F1 Macro: 0.6887\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9234, F1 Micro: 0.7747, F1 Macro: 0.6644\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1137\n",
      "      Abusive       0.89      0.87      0.88      1008\n",
      "HS_Individual       0.76      0.71      0.73       729\n",
      "     HS_Group       0.68      0.70      0.69       408\n",
      "  HS_Religion       0.73      0.71      0.72       168\n",
      "      HS_Race       0.81      0.71      0.76       119\n",
      "  HS_Physical       0.50      0.07      0.12        57\n",
      "    HS_Gender       0.77      0.18      0.29        55\n",
      "     HS_Other       0.79      0.81      0.80       771\n",
      "      HS_Weak       0.73      0.67      0.70       681\n",
      "  HS_Moderate       0.63      0.63      0.63       359\n",
      "    HS_Strong       0.80      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5589\n",
      "    macro avg       0.75      0.64      0.66      5589\n",
      " weighted avg       0.79      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 171.5702555179596 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9123, F1 Micro: 0.7379, F1 Macro: 0.5983\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 67.37020802497864 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4291, Accuracy: 0.8911, F1 Micro: 0.6466, F1 Macro: 0.4119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3166, Accuracy: 0.9117, F1 Micro: 0.7084, F1 Macro: 0.5299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2505, Accuracy: 0.9191, F1 Micro: 0.7433, F1 Macro: 0.6125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.9184, F1 Micro: 0.7621, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1637, Accuracy: 0.9208, F1 Micro: 0.768, F1 Macro: 0.6563\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.9217, F1 Micro: 0.76, F1 Macro: 0.6298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1049, Accuracy: 0.9223, F1 Micro: 0.7755, F1 Macro: 0.6859\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9227, F1 Micro: 0.7708, F1 Macro: 0.7006\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9233, F1 Micro: 0.7701, F1 Macro: 0.7016\n",
      "Epoch 10/10, Train Loss: 0.0629, Accuracy: 0.9221, F1 Micro: 0.7709, F1 Macro: 0.7137\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9223, F1 Micro: 0.7755, F1 Macro: 0.6859\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1137\n",
      "      Abusive       0.87      0.91      0.89      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.68      0.67      0.68       408\n",
      "  HS_Religion       0.77      0.70      0.73       168\n",
      "      HS_Race       0.73      0.79      0.76       119\n",
      "  HS_Physical       0.62      0.26      0.37        57\n",
      "    HS_Gender       0.85      0.20      0.32        55\n",
      "     HS_Other       0.79      0.81      0.80       771\n",
      "      HS_Weak       0.71      0.70      0.71       681\n",
      "  HS_Moderate       0.63      0.59      0.61       359\n",
      "    HS_Strong       0.76      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5589\n",
      "    macro avg       0.75      0.67      0.69      5589\n",
      " weighted avg       0.77      0.77      0.77      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 182.20384168624878 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4331, Accuracy: 0.8916, F1 Micro: 0.6374, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3194, Accuracy: 0.9132, F1 Micro: 0.7291, F1 Macro: 0.5496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2563, Accuracy: 0.9212, F1 Micro: 0.7523, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2123, Accuracy: 0.9188, F1 Micro: 0.7632, F1 Macro: 0.6241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1645, Accuracy: 0.9187, F1 Micro: 0.7664, F1 Macro: 0.6415\n",
      "Epoch 6/10, Train Loss: 0.1373, Accuracy: 0.9235, F1 Micro: 0.7653, F1 Macro: 0.6478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1087, Accuracy: 0.9249, F1 Micro: 0.7784, F1 Macro: 0.6767\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9244, F1 Micro: 0.7733, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9258, F1 Micro: 0.7792, F1 Macro: 0.703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.924, F1 Micro: 0.7801, F1 Macro: 0.7093\n",
      "Model 2 - Iteration 5812: Accuracy: 0.924, F1 Micro: 0.7801, F1 Macro: 0.7093\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.71      0.76      0.74       729\n",
      "     HS_Group       0.74      0.62      0.67       408\n",
      "  HS_Religion       0.73      0.70      0.71       168\n",
      "      HS_Race       0.74      0.81      0.77       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.66      0.38      0.48        55\n",
      "     HS_Other       0.81      0.80      0.80       771\n",
      "      HS_Weak       0.68      0.74      0.71       681\n",
      "  HS_Moderate       0.66      0.57      0.61       359\n",
      "    HS_Strong       0.83      0.72      0.77        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 185.9480791091919 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4331, Accuracy: 0.8883, F1 Micro: 0.6336, F1 Macro: 0.4031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.32, Accuracy: 0.9117, F1 Micro: 0.7203, F1 Macro: 0.5386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2545, Accuracy: 0.9185, F1 Micro: 0.7384, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2114, Accuracy: 0.9169, F1 Micro: 0.7619, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1679, Accuracy: 0.9222, F1 Micro: 0.7619, F1 Macro: 0.638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9225, F1 Micro: 0.7701, F1 Macro: 0.6452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1094, Accuracy: 0.9245, F1 Micro: 0.7736, F1 Macro: 0.6607\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9203, F1 Micro: 0.7688, F1 Macro: 0.6854\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9243, F1 Micro: 0.7705, F1 Macro: 0.6971\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9244, F1 Micro: 0.7638, F1 Macro: 0.6878\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9245, F1 Micro: 0.7736, F1 Macro: 0.6607\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1137\n",
      "      Abusive       0.88      0.88      0.88      1008\n",
      "HS_Individual       0.76      0.69      0.73       729\n",
      "     HS_Group       0.72      0.67      0.69       408\n",
      "  HS_Religion       0.83      0.59      0.69       168\n",
      "      HS_Race       0.75      0.73      0.74       119\n",
      "  HS_Physical       0.62      0.09      0.15        57\n",
      "    HS_Gender       0.83      0.18      0.30        55\n",
      "     HS_Other       0.81      0.79      0.80       771\n",
      "      HS_Weak       0.74      0.66      0.70       681\n",
      "  HS_Moderate       0.67      0.57      0.62       359\n",
      "    HS_Strong       0.76      0.79      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5589\n",
      "    macro avg       0.77      0.62      0.66      5589\n",
      " weighted avg       0.80      0.74      0.77      5589\n",
      "  samples avg       0.44      0.42      0.42      5589\n",
      "\n",
      "Training completed in 183.74225497245789 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9138, F1 Micro: 0.7427, F1 Macro: 0.6092\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 60.17716026306152 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4229, Accuracy: 0.8952, F1 Micro: 0.6685, F1 Macro: 0.4172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3072, Accuracy: 0.9171, F1 Micro: 0.7476, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2452, Accuracy: 0.9204, F1 Micro: 0.7607, F1 Macro: 0.6007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.9227, F1 Micro: 0.7716, F1 Macro: 0.6469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1617, Accuracy: 0.925, F1 Micro: 0.779, F1 Macro: 0.6797\n",
      "Epoch 6/10, Train Loss: 0.1266, Accuracy: 0.9229, F1 Micro: 0.7756, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.9241, F1 Micro: 0.7822, F1 Macro: 0.7069\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9247, F1 Micro: 0.7721, F1 Macro: 0.6964\n",
      "Epoch 9/10, Train Loss: 0.0708, Accuracy: 0.9229, F1 Micro: 0.7746, F1 Macro: 0.7073\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9241, F1 Micro: 0.78, F1 Macro: 0.7149\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9241, F1 Micro: 0.7822, F1 Macro: 0.7069\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.73      0.76      0.74       729\n",
      "     HS_Group       0.71      0.66      0.69       408\n",
      "  HS_Religion       0.77      0.67      0.72       168\n",
      "      HS_Race       0.77      0.76      0.76       119\n",
      "  HS_Physical       0.59      0.28      0.38        57\n",
      "    HS_Gender       0.72      0.42      0.53        55\n",
      "     HS_Other       0.78      0.82      0.80       771\n",
      "      HS_Weak       0.70      0.74      0.72       681\n",
      "  HS_Moderate       0.64      0.60      0.62       359\n",
      "    HS_Strong       0.81      0.75      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5589\n",
      "    macro avg       0.74      0.69      0.71      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 195.03844380378723 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4268, Accuracy: 0.8956, F1 Micro: 0.6619, F1 Macro: 0.3977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3119, Accuracy: 0.9167, F1 Micro: 0.7457, F1 Macro: 0.5758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2508, Accuracy: 0.9218, F1 Micro: 0.7663, F1 Macro: 0.6045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1989, Accuracy: 0.9229, F1 Micro: 0.7746, F1 Macro: 0.6544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1629, Accuracy: 0.9251, F1 Micro: 0.7818, F1 Macro: 0.679\n",
      "Epoch 6/10, Train Loss: 0.1299, Accuracy: 0.9239, F1 Micro: 0.7752, F1 Macro: 0.6872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.109, Accuracy: 0.9233, F1 Micro: 0.783, F1 Macro: 0.7017\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9226, F1 Micro: 0.7732, F1 Macro: 0.7044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9265, F1 Micro: 0.7844, F1 Macro: 0.711\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9258, F1 Micro: 0.7798, F1 Macro: 0.7148\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9265, F1 Micro: 0.7844, F1 Macro: 0.711\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1137\n",
      "      Abusive       0.88      0.91      0.90      1008\n",
      "HS_Individual       0.73      0.75      0.74       729\n",
      "     HS_Group       0.75      0.64      0.69       408\n",
      "  HS_Religion       0.78      0.65      0.71       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.69      0.32      0.43        57\n",
      "    HS_Gender       0.72      0.38      0.50        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.72      0.53      0.61       359\n",
      "    HS_Strong       0.77      0.85      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5589\n",
      "    macro avg       0.77      0.68      0.71      5589\n",
      " weighted avg       0.79      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 196.53952765464783 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4263, Accuracy: 0.893, F1 Micro: 0.6644, F1 Macro: 0.4259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3116, Accuracy: 0.9154, F1 Micro: 0.7504, F1 Macro: 0.5922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2522, Accuracy: 0.9197, F1 Micro: 0.7602, F1 Macro: 0.597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.9219, F1 Micro: 0.7756, F1 Macro: 0.6362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1654, Accuracy: 0.9215, F1 Micro: 0.7782, F1 Macro: 0.6691\n",
      "Epoch 6/10, Train Loss: 0.132, Accuracy: 0.9231, F1 Micro: 0.7773, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1076, Accuracy: 0.9226, F1 Micro: 0.781, F1 Macro: 0.6973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9269, F1 Micro: 0.7822, F1 Macro: 0.7049\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9207, F1 Micro: 0.7754, F1 Macro: 0.7008\n",
      "Epoch 10/10, Train Loss: 0.0646, Accuracy: 0.9254, F1 Micro: 0.78, F1 Macro: 0.7211\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9269, F1 Micro: 0.7822, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.88      0.89      0.89      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.72      0.67      0.69       408\n",
      "  HS_Religion       0.78      0.65      0.71       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.68      0.26      0.38        57\n",
      "    HS_Gender       0.70      0.38      0.49        55\n",
      "     HS_Other       0.83      0.77      0.80       771\n",
      "      HS_Weak       0.75      0.67      0.71       681\n",
      "  HS_Moderate       0.67      0.57      0.61       359\n",
      "    HS_Strong       0.75      0.88      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5589\n",
      "    macro avg       0.76      0.67      0.70      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 197.61490774154663 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9151, F1 Micro: 0.7472, F1 Macro: 0.6201\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 54.33921527862549 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.415, Accuracy: 0.8971, F1 Micro: 0.6761, F1 Macro: 0.4254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3019, Accuracy: 0.9146, F1 Micro: 0.7514, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.244, Accuracy: 0.922, F1 Micro: 0.7685, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1936, Accuracy: 0.9205, F1 Micro: 0.7731, F1 Macro: 0.648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1586, Accuracy: 0.9259, F1 Micro: 0.7803, F1 Macro: 0.6806\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9218, F1 Micro: 0.7799, F1 Macro: 0.6841\n",
      "Epoch 7/10, Train Loss: 0.1011, Accuracy: 0.9223, F1 Micro: 0.7797, F1 Macro: 0.7066\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.925, F1 Micro: 0.7711, F1 Macro: 0.6988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0689, Accuracy: 0.9266, F1 Micro: 0.783, F1 Macro: 0.7168\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9243, F1 Micro: 0.7725, F1 Macro: 0.7133\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9266, F1 Micro: 0.783, F1 Macro: 0.7168\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1137\n",
      "      Abusive       0.88      0.91      0.90      1008\n",
      "HS_Individual       0.76      0.73      0.74       729\n",
      "     HS_Group       0.72      0.63      0.67       408\n",
      "  HS_Religion       0.73      0.65      0.69       168\n",
      "      HS_Race       0.79      0.77      0.78       119\n",
      "  HS_Physical       0.76      0.39      0.51        57\n",
      "    HS_Gender       0.72      0.42      0.53        55\n",
      "     HS_Other       0.82      0.78      0.80       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.68      0.54      0.60       359\n",
      "    HS_Strong       0.80      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5589\n",
      "    macro avg       0.77      0.68      0.72      5589\n",
      " weighted avg       0.80      0.76      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 202.20330047607422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4193, Accuracy: 0.8971, F1 Micro: 0.646, F1 Macro: 0.3977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3059, Accuracy: 0.9156, F1 Micro: 0.7543, F1 Macro: 0.5971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2508, Accuracy: 0.921, F1 Micro: 0.7676, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1949, Accuracy: 0.9259, F1 Micro: 0.7826, F1 Macro: 0.6533\n",
      "Epoch 5/10, Train Loss: 0.1572, Accuracy: 0.9234, F1 Micro: 0.774, F1 Macro: 0.6643\n",
      "Epoch 6/10, Train Loss: 0.1283, Accuracy: 0.9255, F1 Micro: 0.7816, F1 Macro: 0.6804\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9225, F1 Micro: 0.7789, F1 Macro: 0.6937\n",
      "Epoch 8/10, Train Loss: 0.089, Accuracy: 0.925, F1 Micro: 0.7721, F1 Macro: 0.6974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9273, F1 Micro: 0.7854, F1 Macro: 0.721\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.9263, F1 Micro: 0.7836, F1 Macro: 0.7176\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9273, F1 Micro: 0.7854, F1 Macro: 0.721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.88      0.89      0.89      1008\n",
      "HS_Individual       0.79      0.69      0.74       729\n",
      "     HS_Group       0.69      0.72      0.70       408\n",
      "  HS_Religion       0.77      0.67      0.72       168\n",
      "      HS_Race       0.71      0.82      0.76       119\n",
      "  HS_Physical       0.71      0.35      0.47        57\n",
      "    HS_Gender       0.68      0.45      0.54        55\n",
      "     HS_Other       0.83      0.78      0.81       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.64      0.65      0.65       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 199.4159984588623 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4187, Accuracy: 0.8946, F1 Micro: 0.6441, F1 Macro: 0.396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3073, Accuracy: 0.9157, F1 Micro: 0.7484, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.248, Accuracy: 0.9238, F1 Micro: 0.7702, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1963, Accuracy: 0.9249, F1 Micro: 0.7783, F1 Macro: 0.6506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1606, Accuracy: 0.9244, F1 Micro: 0.7823, F1 Macro: 0.662\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9213, F1 Micro: 0.7744, F1 Macro: 0.6663\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9221, F1 Micro: 0.7781, F1 Macro: 0.6827\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9264, F1 Micro: 0.7817, F1 Macro: 0.7018\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9254, F1 Micro: 0.7799, F1 Macro: 0.7026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0607, Accuracy: 0.9257, F1 Micro: 0.787, F1 Macro: 0.7217\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9257, F1 Micro: 0.787, F1 Macro: 0.7217\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.88      0.91      0.90      1008\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.72      0.65      0.68       408\n",
      "  HS_Religion       0.76      0.67      0.72       168\n",
      "      HS_Race       0.75      0.80      0.77       119\n",
      "  HS_Physical       0.63      0.46      0.53        57\n",
      "    HS_Gender       0.66      0.42      0.51        55\n",
      "     HS_Other       0.80      0.81      0.80       771\n",
      "      HS_Weak       0.69      0.75      0.72       681\n",
      "  HS_Moderate       0.67      0.58      0.62       359\n",
      "    HS_Strong       0.75      0.86      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 201.6579897403717 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9162, F1 Micro: 0.751, F1 Macro: 0.6301\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 51.42888569831848 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4116, Accuracy: 0.8964, F1 Micro: 0.6378, F1 Macro: 0.4011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2907, Accuracy: 0.9137, F1 Micro: 0.7536, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2298, Accuracy: 0.9224, F1 Micro: 0.7773, F1 Macro: 0.6345\n",
      "Epoch 4/10, Train Loss: 0.1868, Accuracy: 0.9203, F1 Micro: 0.7695, F1 Macro: 0.6304\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9247, F1 Micro: 0.7752, F1 Macro: 0.6884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.921, F1 Micro: 0.7788, F1 Macro: 0.7024\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9256, F1 Micro: 0.7773, F1 Macro: 0.6937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9246, F1 Micro: 0.7862, F1 Macro: 0.7131\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.9239, F1 Micro: 0.7775, F1 Macro: 0.7157\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9248, F1 Micro: 0.7755, F1 Macro: 0.7183\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9246, F1 Micro: 0.7862, F1 Macro: 0.7131\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.70      0.81      0.75       729\n",
      "     HS_Group       0.74      0.60      0.66       408\n",
      "  HS_Religion       0.76      0.72      0.74       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.66      0.45      0.54        55\n",
      "     HS_Other       0.79      0.84      0.81       771\n",
      "      HS_Weak       0.67      0.79      0.72       681\n",
      "  HS_Moderate       0.67      0.50      0.57       359\n",
      "    HS_Strong       0.78      0.86      0.81        97\n",
      "\n",
      "    micro avg       0.77      0.80      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.77      0.80      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 211.04653477668762 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4145, Accuracy: 0.8983, F1 Micro: 0.6711, F1 Macro: 0.43\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2946, Accuracy: 0.9116, F1 Micro: 0.749, F1 Macro: 0.5978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.9222, F1 Micro: 0.776, F1 Macro: 0.6268\n",
      "Epoch 4/10, Train Loss: 0.1884, Accuracy: 0.9189, F1 Micro: 0.7702, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1515, Accuracy: 0.9266, F1 Micro: 0.7807, F1 Macro: 0.6952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.925, F1 Micro: 0.7826, F1 Macro: 0.7081\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.9268, F1 Micro: 0.7819, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9281, F1 Micro: 0.79, F1 Macro: 0.723\n",
      "Epoch 9/10, Train Loss: 0.0652, Accuracy: 0.9261, F1 Micro: 0.7839, F1 Macro: 0.7201\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9267, F1 Micro: 0.7783, F1 Macro: 0.7174\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9281, F1 Micro: 0.79, F1 Macro: 0.723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.93      0.88      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.71      0.68      0.70       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.78      0.78      0.78       119\n",
      "  HS_Physical       0.76      0.33      0.46        57\n",
      "    HS_Gender       0.65      0.47      0.55        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.66      0.61      0.63       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.42      5589\n",
      "\n",
      "Training completed in 212.16899943351746 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4135, Accuracy: 0.8975, F1 Micro: 0.6715, F1 Macro: 0.4512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2943, Accuracy: 0.9136, F1 Micro: 0.7515, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2363, Accuracy: 0.9214, F1 Micro: 0.7764, F1 Macro: 0.6265\n",
      "Epoch 4/10, Train Loss: 0.1894, Accuracy: 0.9224, F1 Micro: 0.7692, F1 Macro: 0.6213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.925, F1 Micro: 0.7778, F1 Macro: 0.6876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.925, F1 Micro: 0.7842, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1002, Accuracy: 0.9262, F1 Micro: 0.7859, F1 Macro: 0.7076\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.927, F1 Micro: 0.7795, F1 Macro: 0.7051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9275, F1 Micro: 0.7863, F1 Macro: 0.7208\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.9262, F1 Micro: 0.7771, F1 Macro: 0.7116\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9275, F1 Micro: 0.7863, F1 Macro: 0.7208\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.73      0.64      0.68       408\n",
      "  HS_Religion       0.75      0.65      0.70       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.66      0.40      0.50        57\n",
      "    HS_Gender       0.77      0.42      0.54        55\n",
      "     HS_Other       0.82      0.78      0.80       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.69      0.56      0.62       359\n",
      "    HS_Strong       0.76      0.88      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 213.7735414505005 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9172, F1 Micro: 0.7543, F1 Macro: 0.6382\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 46.11025905609131 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.404, Accuracy: 0.8999, F1 Micro: 0.6688, F1 Macro: 0.4597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2925, Accuracy: 0.9172, F1 Micro: 0.7356, F1 Macro: 0.5763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2305, Accuracy: 0.9228, F1 Micro: 0.768, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1897, Accuracy: 0.9266, F1 Micro: 0.7716, F1 Macro: 0.6395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9237, F1 Micro: 0.7829, F1 Macro: 0.6692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9268, F1 Micro: 0.783, F1 Macro: 0.7106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9265, F1 Micro: 0.7855, F1 Macro: 0.7181\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9283, F1 Micro: 0.784, F1 Macro: 0.7153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.068, Accuracy: 0.9266, F1 Micro: 0.7871, F1 Macro: 0.7176\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.9248, F1 Micro: 0.7844, F1 Macro: 0.7227\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9266, F1 Micro: 0.7871, F1 Macro: 0.7176\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.73      0.61      0.66       408\n",
      "  HS_Religion       0.76      0.66      0.71       168\n",
      "      HS_Race       0.79      0.75      0.77       119\n",
      "  HS_Physical       0.69      0.39      0.49        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.67      0.53      0.59       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 226.41337656974792 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4087, Accuracy: 0.9013, F1 Micro: 0.6797, F1 Macro: 0.4301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2957, Accuracy: 0.9187, F1 Micro: 0.7455, F1 Macro: 0.5804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2329, Accuracy: 0.9237, F1 Micro: 0.7753, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1891, Accuracy: 0.927, F1 Micro: 0.7818, F1 Macro: 0.6508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1544, Accuracy: 0.9252, F1 Micro: 0.7826, F1 Macro: 0.6654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9247, F1 Micro: 0.7827, F1 Macro: 0.6977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0968, Accuracy: 0.9297, F1 Micro: 0.7902, F1 Macro: 0.7001\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9276, F1 Micro: 0.7872, F1 Macro: 0.7107\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9277, F1 Micro: 0.7868, F1 Macro: 0.7189\n",
      "Epoch 10/10, Train Loss: 0.0586, Accuracy: 0.927, F1 Micro: 0.7843, F1 Macro: 0.7151\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9297, F1 Micro: 0.7902, F1 Macro: 0.7001\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.78      0.74      0.76       729\n",
      "     HS_Group       0.75      0.65      0.69       408\n",
      "  HS_Religion       0.86      0.58      0.69       168\n",
      "      HS_Race       0.82      0.63      0.71       119\n",
      "  HS_Physical       0.80      0.28      0.42        57\n",
      "    HS_Gender       0.76      0.29      0.42        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.75      0.73      0.74       681\n",
      "  HS_Moderate       0.70      0.58      0.64       359\n",
      "    HS_Strong       0.84      0.71      0.77        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.80      0.65      0.70      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 224.30131149291992 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4084, Accuracy: 0.8988, F1 Micro: 0.6625, F1 Macro: 0.4374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2971, Accuracy: 0.9168, F1 Micro: 0.7394, F1 Macro: 0.5738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2341, Accuracy: 0.9241, F1 Micro: 0.7745, F1 Macro: 0.6178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1907, Accuracy: 0.927, F1 Micro: 0.7764, F1 Macro: 0.6439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9265, F1 Micro: 0.7853, F1 Macro: 0.6609\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9256, F1 Micro: 0.7805, F1 Macro: 0.6729\n",
      "Epoch 7/10, Train Loss: 0.1009, Accuracy: 0.9282, F1 Micro: 0.7846, F1 Macro: 0.6958\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9288, F1 Micro: 0.7834, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.927, F1 Micro: 0.7869, F1 Macro: 0.7122\n",
      "Epoch 10/10, Train Loss: 0.056, Accuracy: 0.9243, F1 Micro: 0.7828, F1 Macro: 0.721\n",
      "Model 3 - Iteration 7336: Accuracy: 0.927, F1 Micro: 0.7869, F1 Macro: 0.7122\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.76      0.75      0.75       729\n",
      "     HS_Group       0.70      0.67      0.69       408\n",
      "  HS_Religion       0.76      0.62      0.69       168\n",
      "      HS_Race       0.76      0.72      0.74       119\n",
      "  HS_Physical       0.63      0.33      0.44        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.65      0.60      0.63       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 221.59332847595215 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9181, F1 Micro: 0.7571, F1 Macro: 0.6442\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 42.1456618309021 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3971, Accuracy: 0.8993, F1 Micro: 0.6759, F1 Macro: 0.4184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2778, Accuracy: 0.9154, F1 Micro: 0.7211, F1 Macro: 0.5687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2229, Accuracy: 0.9208, F1 Micro: 0.773, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1803, Accuracy: 0.9289, F1 Micro: 0.7813, F1 Macro: 0.6626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1418, Accuracy: 0.9257, F1 Micro: 0.7841, F1 Macro: 0.6862\n",
      "Epoch 6/10, Train Loss: 0.1148, Accuracy: 0.9278, F1 Micro: 0.7766, F1 Macro: 0.6935\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.9278, F1 Micro: 0.7818, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9246, F1 Micro: 0.7866, F1 Macro: 0.719\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.9262, F1 Micro: 0.7845, F1 Macro: 0.7314\n",
      "Epoch 10/10, Train Loss: 0.0545, Accuracy: 0.9256, F1 Micro: 0.7856, F1 Macro: 0.7228\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9246, F1 Micro: 0.7866, F1 Macro: 0.719\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.67      0.67      0.67       408\n",
      "  HS_Religion       0.75      0.69      0.72       168\n",
      "      HS_Race       0.73      0.80      0.76       119\n",
      "  HS_Physical       0.70      0.40      0.51        57\n",
      "    HS_Gender       0.66      0.45      0.54        55\n",
      "     HS_Other       0.78      0.83      0.81       771\n",
      "      HS_Weak       0.70      0.76      0.73       681\n",
      "  HS_Moderate       0.62      0.63      0.62       359\n",
      "    HS_Strong       0.83      0.70      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.80      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.77      0.80      0.78      5589\n",
      "  samples avg       0.46      0.46      0.44      5589\n",
      "\n",
      "Training completed in 228.07916903495789 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4017, Accuracy: 0.8976, F1 Micro: 0.6902, F1 Macro: 0.4343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2793, Accuracy: 0.9127, F1 Micro: 0.701, F1 Macro: 0.5368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2261, Accuracy: 0.9202, F1 Micro: 0.77, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9278, F1 Micro: 0.7825, F1 Macro: 0.6542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1455, Accuracy: 0.9275, F1 Micro: 0.7848, F1 Macro: 0.6952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9272, F1 Micro: 0.7855, F1 Macro: 0.707\n",
      "Epoch 7/10, Train Loss: 0.0963, Accuracy: 0.9269, F1 Micro: 0.7815, F1 Macro: 0.7026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9258, F1 Micro: 0.7874, F1 Macro: 0.719\n",
      "Epoch 9/10, Train Loss: 0.0691, Accuracy: 0.9244, F1 Micro: 0.7828, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.9231, F1 Micro: 0.7824, F1 Macro: 0.7229\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9258, F1 Micro: 0.7874, F1 Macro: 0.719\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.71      0.79      0.75       729\n",
      "     HS_Group       0.76      0.65      0.70       408\n",
      "  HS_Religion       0.77      0.67      0.72       168\n",
      "      HS_Race       0.81      0.71      0.76       119\n",
      "  HS_Physical       0.63      0.33      0.44        57\n",
      "    HS_Gender       0.71      0.49      0.58        55\n",
      "     HS_Other       0.78      0.84      0.81       771\n",
      "      HS_Weak       0.67      0.77      0.72       681\n",
      "  HS_Moderate       0.70      0.58      0.64       359\n",
      "    HS_Strong       0.83      0.72      0.77        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.78      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 230.20764803886414 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8975, F1 Micro: 0.6711, F1 Macro: 0.406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2809, Accuracy: 0.9161, F1 Micro: 0.7257, F1 Macro: 0.5739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2289, Accuracy: 0.9224, F1 Micro: 0.7732, F1 Macro: 0.6208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1838, Accuracy: 0.9261, F1 Micro: 0.7736, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1483, Accuracy: 0.9234, F1 Micro: 0.7774, F1 Macro: 0.6842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.9275, F1 Micro: 0.7825, F1 Macro: 0.6945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9277, F1 Micro: 0.7852, F1 Macro: 0.7047\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9248, F1 Micro: 0.7759, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0687, Accuracy: 0.9265, F1 Micro: 0.7858, F1 Macro: 0.7147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9273, F1 Micro: 0.7873, F1 Macro: 0.7206\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9273, F1 Micro: 0.7873, F1 Macro: 0.7206\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.77      0.71      0.74       729\n",
      "     HS_Group       0.67      0.72      0.69       408\n",
      "  HS_Religion       0.77      0.64      0.70       168\n",
      "      HS_Race       0.79      0.75      0.77       119\n",
      "  HS_Physical       0.69      0.39      0.49        57\n",
      "    HS_Gender       0.65      0.44      0.52        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.75      0.68      0.72       681\n",
      "  HS_Moderate       0.61      0.65      0.63       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 232.85248851776123 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9187, F1 Micro: 0.7594, F1 Macro: 0.65\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 38.27883529663086 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3879, Accuracy: 0.9015, F1 Micro: 0.6793, F1 Macro: 0.4272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2732, Accuracy: 0.9186, F1 Micro: 0.747, F1 Macro: 0.6019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9234, F1 Micro: 0.7757, F1 Macro: 0.6579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9273, F1 Micro: 0.7828, F1 Macro: 0.6653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9286, F1 Micro: 0.7859, F1 Macro: 0.6799\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9284, F1 Micro: 0.7815, F1 Macro: 0.6745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9291, F1 Micro: 0.7873, F1 Macro: 0.7119\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9249, F1 Micro: 0.7848, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9285, F1 Micro: 0.7899, F1 Macro: 0.7313\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9275, F1 Micro: 0.7897, F1 Macro: 0.7264\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9285, F1 Micro: 0.7899, F1 Macro: 0.7313\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.78      0.72      0.74       729\n",
      "     HS_Group       0.70      0.70      0.70       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.74      0.77      0.76       119\n",
      "  HS_Physical       0.66      0.44      0.53        57\n",
      "    HS_Gender       0.67      0.55      0.60        55\n",
      "     HS_Other       0.84      0.76      0.80       771\n",
      "      HS_Weak       0.75      0.69      0.72       681\n",
      "  HS_Moderate       0.64      0.66      0.65       359\n",
      "    HS_Strong       0.85      0.77      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 236.98289561271667 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3926, Accuracy: 0.8998, F1 Micro: 0.6703, F1 Macro: 0.3874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.276, Accuracy: 0.9195, F1 Micro: 0.7538, F1 Macro: 0.5983\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.9234, F1 Micro: 0.7653, F1 Macro: 0.6328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9265, F1 Micro: 0.7837, F1 Macro: 0.6686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.9296, F1 Micro: 0.7851, F1 Macro: 0.6746\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9287, F1 Micro: 0.7791, F1 Macro: 0.6832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0952, Accuracy: 0.9282, F1 Micro: 0.7927, F1 Macro: 0.7267\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9261, F1 Micro: 0.7868, F1 Macro: 0.7217\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9284, F1 Micro: 0.79, F1 Macro: 0.7243\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9278, F1 Micro: 0.7888, F1 Macro: 0.7274\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9282, F1 Micro: 0.7927, F1 Macro: 0.7267\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.73      0.69      0.71       408\n",
      "  HS_Religion       0.73      0.68      0.71       168\n",
      "      HS_Race       0.76      0.78      0.77       119\n",
      "  HS_Physical       0.68      0.33      0.45        57\n",
      "    HS_Gender       0.67      0.55      0.60        55\n",
      "     HS_Other       0.80      0.81      0.81       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.68      0.58      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 234.60240125656128 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3908, Accuracy: 0.8986, F1 Micro: 0.6564, F1 Macro: 0.4122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.9185, F1 Micro: 0.7405, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2199, Accuracy: 0.925, F1 Micro: 0.7761, F1 Macro: 0.6329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1802, Accuracy: 0.9279, F1 Micro: 0.7824, F1 Macro: 0.6485\n",
      "Epoch 5/10, Train Loss: 0.148, Accuracy: 0.929, F1 Micro: 0.7799, F1 Macro: 0.6716\n",
      "Epoch 6/10, Train Loss: 0.118, Accuracy: 0.9269, F1 Micro: 0.7809, F1 Macro: 0.6889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9287, F1 Micro: 0.7886, F1 Macro: 0.7114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0746, Accuracy: 0.9273, F1 Micro: 0.7897, F1 Macro: 0.7165\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.9274, F1 Micro: 0.7895, F1 Macro: 0.7191\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9284, F1 Micro: 0.7886, F1 Macro: 0.7258\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9273, F1 Micro: 0.7897, F1 Macro: 0.7165\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.65      0.72      0.68       408\n",
      "  HS_Religion       0.72      0.71      0.72       168\n",
      "      HS_Race       0.72      0.79      0.76       119\n",
      "  HS_Physical       0.59      0.35      0.44        57\n",
      "    HS_Gender       0.66      0.42      0.51        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.72       681\n",
      "  HS_Moderate       0.61      0.65      0.63       359\n",
      "    HS_Strong       0.77      0.86      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.73      0.71      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 235.8133180141449 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9193, F1 Micro: 0.7617, F1 Macro: 0.6553\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 35.40351724624634 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3849, Accuracy: 0.9052, F1 Micro: 0.7072, F1 Macro: 0.5007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.9154, F1 Micro: 0.7615, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2073, Accuracy: 0.9246, F1 Micro: 0.7696, F1 Macro: 0.6139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9263, F1 Micro: 0.78, F1 Macro: 0.6795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1371, Accuracy: 0.9284, F1 Micro: 0.7811, F1 Macro: 0.6795\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9279, F1 Micro: 0.7802, F1 Macro: 0.702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9305, F1 Micro: 0.7955, F1 Macro: 0.723\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9273, F1 Micro: 0.7875, F1 Macro: 0.7194\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9296, F1 Micro: 0.7877, F1 Macro: 0.7275\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9282, F1 Micro: 0.7888, F1 Macro: 0.724\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9305, F1 Micro: 0.7955, F1 Macro: 0.723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.74      0.64      0.69       408\n",
      "  HS_Religion       0.81      0.62      0.70       168\n",
      "      HS_Race       0.77      0.81      0.79       119\n",
      "  HS_Physical       0.74      0.35      0.48        57\n",
      "    HS_Gender       0.75      0.38      0.51        55\n",
      "     HS_Other       0.83      0.82      0.82       771\n",
      "      HS_Weak       0.74      0.73      0.73       681\n",
      "  HS_Moderate       0.67      0.57      0.62       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.80      5589\n",
      "    macro avg       0.78      0.69      0.72      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 243.1961052417755 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3888, Accuracy: 0.9045, F1 Micro: 0.7028, F1 Macro: 0.4832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9154, F1 Micro: 0.7603, F1 Macro: 0.591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2114, Accuracy: 0.925, F1 Micro: 0.7731, F1 Macro: 0.6098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9283, F1 Micro: 0.7805, F1 Macro: 0.6722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1407, Accuracy: 0.927, F1 Micro: 0.7842, F1 Macro: 0.6615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1102, Accuracy: 0.9254, F1 Micro: 0.785, F1 Macro: 0.7021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9269, F1 Micro: 0.7902, F1 Macro: 0.7232\n",
      "Epoch 8/10, Train Loss: 0.0744, Accuracy: 0.9257, F1 Micro: 0.7834, F1 Macro: 0.715\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9271, F1 Micro: 0.7809, F1 Macro: 0.7141\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9276, F1 Micro: 0.7889, F1 Macro: 0.7256\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9269, F1 Micro: 0.7902, F1 Macro: 0.7232\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.87      0.93      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.75       729\n",
      "     HS_Group       0.71      0.67      0.69       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.67      0.35      0.46        57\n",
      "    HS_Gender       0.69      0.53      0.60        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.67      0.59      0.63       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.78      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 242.33350706100464 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3895, Accuracy: 0.9021, F1 Micro: 0.6952, F1 Macro: 0.4985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.9133, F1 Micro: 0.7601, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9241, F1 Micro: 0.7725, F1 Macro: 0.6043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1656, Accuracy: 0.925, F1 Micro: 0.7804, F1 Macro: 0.6763\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9279, F1 Micro: 0.775, F1 Macro: 0.653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9282, F1 Micro: 0.7863, F1 Macro: 0.6955\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.9297, F1 Micro: 0.7861, F1 Macro: 0.7074\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9282, F1 Micro: 0.7835, F1 Macro: 0.7068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9273, F1 Micro: 0.7873, F1 Macro: 0.7189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9295, F1 Micro: 0.7951, F1 Macro: 0.7259\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9295, F1 Micro: 0.7951, F1 Macro: 0.7259\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.89      0.92      0.91      1008\n",
      "HS_Individual       0.76      0.76      0.76       729\n",
      "     HS_Group       0.72      0.66      0.69       408\n",
      "  HS_Religion       0.76      0.65      0.70       168\n",
      "      HS_Race       0.73      0.85      0.79       119\n",
      "  HS_Physical       0.60      0.42      0.49        57\n",
      "    HS_Gender       0.70      0.42      0.52        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.68      0.59      0.63       359\n",
      "    HS_Strong       0.79      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.80      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 245.05326652526855 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.92, F1 Micro: 0.7638, F1 Macro: 0.6599\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 31.871483325958252 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3756, Accuracy: 0.9048, F1 Micro: 0.6984, F1 Macro: 0.4924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2632, Accuracy: 0.9193, F1 Micro: 0.7416, F1 Macro: 0.563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2128, Accuracy: 0.9248, F1 Micro: 0.7737, F1 Macro: 0.6247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1683, Accuracy: 0.9266, F1 Micro: 0.7838, F1 Macro: 0.6728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.925, F1 Micro: 0.7893, F1 Macro: 0.7142\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9262, F1 Micro: 0.7829, F1 Macro: 0.7027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9285, F1 Micro: 0.7907, F1 Macro: 0.7215\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9275, F1 Micro: 0.7888, F1 Macro: 0.7153\n",
      "Epoch 9/10, Train Loss: 0.0597, Accuracy: 0.9276, F1 Micro: 0.7841, F1 Macro: 0.7207\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9275, F1 Micro: 0.7803, F1 Macro: 0.7153\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9285, F1 Micro: 0.7907, F1 Macro: 0.7215\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.87      0.94      0.90      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.73      0.64      0.69       408\n",
      "  HS_Religion       0.73      0.69      0.71       168\n",
      "      HS_Race       0.73      0.79      0.76       119\n",
      "  HS_Physical       0.71      0.39      0.50        57\n",
      "    HS_Gender       0.79      0.40      0.53        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.70      0.56      0.62       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 248.09486961364746 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.9045, F1 Micro: 0.7003, F1 Macro: 0.4854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2639, Accuracy: 0.9175, F1 Micro: 0.7356, F1 Macro: 0.5391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.214, Accuracy: 0.9227, F1 Micro: 0.7743, F1 Macro: 0.6293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.169, Accuracy: 0.9291, F1 Micro: 0.7861, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9276, F1 Micro: 0.7872, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9287, F1 Micro: 0.7885, F1 Macro: 0.7159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.9292, F1 Micro: 0.7902, F1 Macro: 0.7221\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9189, F1 Micro: 0.775, F1 Macro: 0.7034\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9267, F1 Micro: 0.7868, F1 Macro: 0.7145\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9261, F1 Micro: 0.7787, F1 Macro: 0.7102\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9292, F1 Micro: 0.7902, F1 Macro: 0.7221\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.79      0.70      0.74       729\n",
      "     HS_Group       0.72      0.70      0.71       408\n",
      "  HS_Religion       0.75      0.71      0.73       168\n",
      "      HS_Race       0.78      0.75      0.76       119\n",
      "  HS_Physical       0.70      0.33      0.45        57\n",
      "    HS_Gender       0.76      0.40      0.52        55\n",
      "     HS_Other       0.84      0.77      0.80       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.66      0.63      0.65       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.78      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 249.50714492797852 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3791, Accuracy: 0.904, F1 Micro: 0.6997, F1 Macro: 0.4859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2637, Accuracy: 0.9187, F1 Micro: 0.7388, F1 Macro: 0.5714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2155, Accuracy: 0.9238, F1 Micro: 0.7742, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1701, Accuracy: 0.9255, F1 Micro: 0.7802, F1 Macro: 0.6543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9263, F1 Micro: 0.7871, F1 Macro: 0.6911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.9287, F1 Micro: 0.791, F1 Macro: 0.6957\n",
      "Epoch 7/10, Train Loss: 0.0918, Accuracy: 0.9288, F1 Micro: 0.7876, F1 Macro: 0.7059\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9274, F1 Micro: 0.7879, F1 Macro: 0.7063\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9282, F1 Micro: 0.7892, F1 Macro: 0.7152\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9254, F1 Micro: 0.7862, F1 Macro: 0.7247\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9287, F1 Micro: 0.791, F1 Macro: 0.6957\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.77      0.75      0.76       729\n",
      "     HS_Group       0.71      0.67      0.69       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.64      0.16      0.25        57\n",
      "    HS_Gender       0.62      0.36      0.46        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.74      0.74      0.74       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.82      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.67      0.70      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 247.97119188308716 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9205, F1 Micro: 0.7655, F1 Macro: 0.6632\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 27.476248502731323 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3725, Accuracy: 0.9052, F1 Micro: 0.6938, F1 Macro: 0.5005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.9195, F1 Micro: 0.7479, F1 Macro: 0.5839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.202, Accuracy: 0.9248, F1 Micro: 0.7602, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9272, F1 Micro: 0.7788, F1 Macro: 0.652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.132, Accuracy: 0.9284, F1 Micro: 0.7799, F1 Macro: 0.6963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.9284, F1 Micro: 0.7874, F1 Macro: 0.6974\n",
      "Epoch 7/10, Train Loss: 0.0855, Accuracy: 0.9285, F1 Micro: 0.7855, F1 Macro: 0.7044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9288, F1 Micro: 0.7888, F1 Macro: 0.7239\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9242, F1 Micro: 0.7789, F1 Macro: 0.7189\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.9263, F1 Micro: 0.7834, F1 Macro: 0.7187\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9288, F1 Micro: 0.7888, F1 Macro: 0.7239\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.77      0.71      0.74       729\n",
      "     HS_Group       0.71      0.71      0.71       408\n",
      "  HS_Religion       0.73      0.70      0.72       168\n",
      "      HS_Race       0.74      0.75      0.74       119\n",
      "  HS_Physical       0.66      0.40      0.50        57\n",
      "    HS_Gender       0.72      0.42      0.53        55\n",
      "     HS_Other       0.84      0.77      0.80       771\n",
      "      HS_Weak       0.74      0.68      0.71       681\n",
      "  HS_Moderate       0.67      0.64      0.65       359\n",
      "    HS_Strong       0.85      0.78      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 253.3551630973816 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.375, Accuracy: 0.9044, F1 Micro: 0.6894, F1 Macro: 0.5006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2611, Accuracy: 0.918, F1 Micro: 0.7438, F1 Macro: 0.5758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2047, Accuracy: 0.9241, F1 Micro: 0.7568, F1 Macro: 0.6235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9277, F1 Micro: 0.7832, F1 Macro: 0.6466\n",
      "Epoch 5/10, Train Loss: 0.1378, Accuracy: 0.9283, F1 Micro: 0.7818, F1 Macro: 0.6967\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9246, F1 Micro: 0.7826, F1 Macro: 0.6881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9291, F1 Micro: 0.7901, F1 Macro: 0.7171\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.9287, F1 Micro: 0.7889, F1 Macro: 0.7229\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9264, F1 Micro: 0.7872, F1 Macro: 0.7227\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9276, F1 Micro: 0.7812, F1 Macro: 0.71\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9291, F1 Micro: 0.7901, F1 Macro: 0.7171\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.76      0.75      0.75       729\n",
      "     HS_Group       0.75      0.66      0.70       408\n",
      "  HS_Religion       0.77      0.66      0.71       168\n",
      "      HS_Race       0.77      0.74      0.76       119\n",
      "  HS_Physical       0.75      0.32      0.44        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.70      0.55      0.62       359\n",
      "    HS_Strong       0.78      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.68      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 250.41223573684692 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3761, Accuracy: 0.9037, F1 Micro: 0.6925, F1 Macro: 0.5021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2601, Accuracy: 0.9183, F1 Micro: 0.7434, F1 Macro: 0.5817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2061, Accuracy: 0.9252, F1 Micro: 0.765, F1 Macro: 0.615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.924, F1 Micro: 0.7777, F1 Macro: 0.6416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9281, F1 Micro: 0.7875, F1 Macro: 0.6904\n",
      "Epoch 6/10, Train Loss: 0.1098, Accuracy: 0.9266, F1 Micro: 0.786, F1 Macro: 0.6915\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9282, F1 Micro: 0.786, F1 Macro: 0.7066\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9289, F1 Micro: 0.7777, F1 Macro: 0.7\n",
      "Epoch 9/10, Train Loss: 0.0605, Accuracy: 0.9255, F1 Micro: 0.7861, F1 Macro: 0.7173\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9237, F1 Micro: 0.7814, F1 Macro: 0.7175\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9281, F1 Micro: 0.7875, F1 Macro: 0.6904\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.77      0.74      0.75       729\n",
      "     HS_Group       0.71      0.64      0.67       408\n",
      "  HS_Religion       0.78      0.62      0.69       168\n",
      "      HS_Race       0.81      0.67      0.73       119\n",
      "  HS_Physical       0.55      0.19      0.29        57\n",
      "    HS_Gender       0.75      0.33      0.46        55\n",
      "     HS_Other       0.83      0.80      0.82       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.65      0.57      0.61       359\n",
      "    HS_Strong       0.82      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.65      0.69      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 250.5636248588562 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.921, F1 Micro: 0.7668, F1 Macro: 0.666\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.458019018173218 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3697, Accuracy: 0.9028, F1 Micro: 0.6863, F1 Macro: 0.4814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.9184, F1 Micro: 0.7513, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2043, Accuracy: 0.9236, F1 Micro: 0.7796, F1 Macro: 0.6392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1616, Accuracy: 0.9275, F1 Micro: 0.7907, F1 Macro: 0.6791\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9281, F1 Micro: 0.7883, F1 Macro: 0.6965\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9283, F1 Micro: 0.7797, F1 Macro: 0.7032\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9224, F1 Micro: 0.7809, F1 Macro: 0.7143\n",
      "Epoch 8/10, Train Loss: 0.0676, Accuracy: 0.9245, F1 Micro: 0.7776, F1 Macro: 0.7185\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9275, F1 Micro: 0.7873, F1 Macro: 0.7249\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9271, F1 Micro: 0.7868, F1 Macro: 0.7212\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9275, F1 Micro: 0.7907, F1 Macro: 0.6791\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.71      0.67      0.69       408\n",
      "  HS_Religion       0.76      0.62      0.68       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.55      0.19      0.29        57\n",
      "    HS_Gender       1.00      0.13      0.23        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.73      0.74      0.74       681\n",
      "  HS_Moderate       0.67      0.61      0.64       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.77      0.66      0.68      5589\n",
      " weighted avg       0.79      0.79      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 253.14151906967163 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3759, Accuracy: 0.9052, F1 Micro: 0.6895, F1 Macro: 0.4953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.921, F1 Micro: 0.758, F1 Macro: 0.5901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2056, Accuracy: 0.9235, F1 Micro: 0.7752, F1 Macro: 0.6455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1613, Accuracy: 0.9254, F1 Micro: 0.7839, F1 Macro: 0.6653\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9278, F1 Micro: 0.7832, F1 Macro: 0.7017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1019, Accuracy: 0.9274, F1 Micro: 0.7842, F1 Macro: 0.704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9277, F1 Micro: 0.7855, F1 Macro: 0.7121\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.925, F1 Micro: 0.7791, F1 Macro: 0.7144\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9296, F1 Micro: 0.7837, F1 Macro: 0.7258\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9283, F1 Micro: 0.7834, F1 Macro: 0.7131\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9277, F1 Micro: 0.7855, F1 Macro: 0.7121\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.76      0.72      0.74       729\n",
      "     HS_Group       0.72      0.66      0.69       408\n",
      "  HS_Religion       0.80      0.58      0.68       168\n",
      "      HS_Race       0.75      0.76      0.76       119\n",
      "  HS_Physical       0.65      0.30      0.41        57\n",
      "    HS_Gender       0.77      0.44      0.56        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.69      0.71       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.82      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5589\n",
      "    macro avg       0.77      0.67      0.71      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 257.6666703224182 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3745, Accuracy: 0.9037, F1 Micro: 0.688, F1 Macro: 0.4919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.92, F1 Micro: 0.7603, F1 Macro: 0.5977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2055, Accuracy: 0.9267, F1 Micro: 0.7811, F1 Macro: 0.6292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.9266, F1 Micro: 0.7875, F1 Macro: 0.6586\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9263, F1 Micro: 0.7849, F1 Macro: 0.6801\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.927, F1 Micro: 0.7854, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9283, F1 Micro: 0.7942, F1 Macro: 0.7067\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9278, F1 Micro: 0.7867, F1 Macro: 0.7159\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9297, F1 Micro: 0.7911, F1 Macro: 0.7213\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9247, F1 Micro: 0.7864, F1 Macro: 0.7156\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9283, F1 Micro: 0.7942, F1 Macro: 0.7067\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.87      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.78      0.76       729\n",
      "     HS_Group       0.71      0.69      0.70       408\n",
      "  HS_Religion       0.74      0.67      0.71       168\n",
      "      HS_Race       0.70      0.79      0.74       119\n",
      "  HS_Physical       0.64      0.25      0.35        57\n",
      "    HS_Gender       0.79      0.35      0.48        55\n",
      "     HS_Other       0.80      0.83      0.82       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.67      0.60      0.63       359\n",
      "    HS_Strong       0.77      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 256.31845569610596 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9214, F1 Micro: 0.7681, F1 Macro: 0.6678\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.322997570037842 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3646, Accuracy: 0.9033, F1 Micro: 0.6948, F1 Macro: 0.4367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9186, F1 Micro: 0.7538, F1 Macro: 0.6078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.196, Accuracy: 0.9254, F1 Micro: 0.775, F1 Macro: 0.6547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9275, F1 Micro: 0.7895, F1 Macro: 0.6734\n",
      "Epoch 5/10, Train Loss: 0.1249, Accuracy: 0.9256, F1 Micro: 0.7853, F1 Macro: 0.7094\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9253, F1 Micro: 0.7874, F1 Macro: 0.7101\n",
      "Epoch 7/10, Train Loss: 0.0807, Accuracy: 0.9288, F1 Micro: 0.7886, F1 Macro: 0.7209\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9283, F1 Micro: 0.7858, F1 Macro: 0.7246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9298, F1 Micro: 0.7901, F1 Macro: 0.7267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9287, F1 Micro: 0.7922, F1 Macro: 0.7297\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9287, F1 Micro: 0.7922, F1 Macro: 0.7297\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.91      0.92      0.92      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.69      0.71      0.70       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.70      0.86      0.77       119\n",
      "  HS_Physical       0.57      0.46      0.50        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.82      0.78      0.80       771\n",
      "      HS_Weak       0.74      0.70      0.72       681\n",
      "  HS_Moderate       0.64      0.63      0.63       359\n",
      "    HS_Strong       0.85      0.82      0.84        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 261.6364653110504 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3689, Accuracy: 0.9029, F1 Micro: 0.6911, F1 Macro: 0.4222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2469, Accuracy: 0.92, F1 Micro: 0.7541, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.925, F1 Micro: 0.7621, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.927, F1 Micro: 0.7861, F1 Macro: 0.6676\n",
      "Epoch 5/10, Train Loss: 0.1299, Accuracy: 0.9195, F1 Micro: 0.7711, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9271, F1 Micro: 0.789, F1 Macro: 0.6847\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9251, F1 Micro: 0.7812, F1 Macro: 0.7102\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9273, F1 Micro: 0.7818, F1 Macro: 0.713\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.925, F1 Micro: 0.7794, F1 Macro: 0.7119\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9228, F1 Micro: 0.7825, F1 Macro: 0.7197\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9271, F1 Micro: 0.789, F1 Macro: 0.6847\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.72      0.66      0.69       408\n",
      "  HS_Religion       0.73      0.68      0.70       168\n",
      "      HS_Race       0.81      0.68      0.74       119\n",
      "  HS_Physical       0.67      0.11      0.18        57\n",
      "    HS_Gender       0.76      0.29      0.42        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.68      0.59      0.63       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.66      0.68      5589\n",
      " weighted avg       0.79      0.79      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 260.07903027534485 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3686, Accuracy: 0.9026, F1 Micro: 0.6829, F1 Macro: 0.4162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2484, Accuracy: 0.9191, F1 Micro: 0.7483, F1 Macro: 0.5996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1975, Accuracy: 0.9252, F1 Micro: 0.7736, F1 Macro: 0.6294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.924, F1 Micro: 0.7825, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1302, Accuracy: 0.927, F1 Micro: 0.7878, F1 Macro: 0.6844\n",
      "Epoch 6/10, Train Loss: 0.1013, Accuracy: 0.9259, F1 Micro: 0.7848, F1 Macro: 0.6905\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.9258, F1 Micro: 0.7854, F1 Macro: 0.7147\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9281, F1 Micro: 0.7732, F1 Macro: 0.7004\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.9285, F1 Micro: 0.7845, F1 Macro: 0.7214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9293, F1 Micro: 0.7904, F1 Macro: 0.7218\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9293, F1 Micro: 0.7904, F1 Macro: 0.7218\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.76      0.65      0.70       408\n",
      "  HS_Religion       0.74      0.66      0.70       168\n",
      "      HS_Race       0.70      0.79      0.74       119\n",
      "  HS_Physical       0.65      0.42      0.51        57\n",
      "    HS_Gender       0.66      0.42      0.51        55\n",
      "     HS_Other       0.83      0.77      0.80       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.70      0.60      0.65       359\n",
      "    HS_Strong       0.83      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 262.1483952999115 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9218, F1 Micro: 0.7693, F1 Macro: 0.6702\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.515422344207764 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3581, Accuracy: 0.9039, F1 Micro: 0.6915, F1 Macro: 0.4219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9187, F1 Micro: 0.7654, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.9264, F1 Micro: 0.7674, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9247, F1 Micro: 0.785, F1 Macro: 0.6777\n",
      "Epoch 5/10, Train Loss: 0.1231, Accuracy: 0.9266, F1 Micro: 0.7837, F1 Macro: 0.6881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9296, F1 Micro: 0.7871, F1 Macro: 0.7028\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9286, F1 Micro: 0.785, F1 Macro: 0.7147\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9274, F1 Micro: 0.7854, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.929, F1 Micro: 0.7928, F1 Macro: 0.7296\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9252, F1 Micro: 0.7804, F1 Macro: 0.722\n",
      "Model 1 - Iteration 9216: Accuracy: 0.929, F1 Micro: 0.7928, F1 Macro: 0.7296\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.70      0.67      0.68       408\n",
      "  HS_Religion       0.80      0.62      0.70       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.60      0.46      0.52        57\n",
      "    HS_Gender       0.65      0.51      0.57        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.82      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 268.84632635116577 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3622, Accuracy: 0.9032, F1 Micro: 0.6816, F1 Macro: 0.4027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2488, Accuracy: 0.9202, F1 Micro: 0.7701, F1 Macro: 0.61\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9267, F1 Micro: 0.7675, F1 Macro: 0.6297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1565, Accuracy: 0.9212, F1 Micro: 0.7768, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.924, F1 Micro: 0.7813, F1 Macro: 0.6722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.9293, F1 Micro: 0.7878, F1 Macro: 0.7125\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9268, F1 Micro: 0.7844, F1 Macro: 0.7125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0674, Accuracy: 0.9289, F1 Micro: 0.7881, F1 Macro: 0.7233\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9287, F1 Micro: 0.7878, F1 Macro: 0.7312\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9261, F1 Micro: 0.7856, F1 Macro: 0.7214\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9289, F1 Micro: 0.7881, F1 Macro: 0.7233\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.72      0.67      0.69       408\n",
      "  HS_Religion       0.78      0.64      0.70       168\n",
      "      HS_Race       0.73      0.77      0.75       119\n",
      "  HS_Physical       0.73      0.33      0.46        57\n",
      "    HS_Gender       0.69      0.53      0.60        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.75      0.68      0.71       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.76      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 266.28612327575684 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3616, Accuracy: 0.9022, F1 Micro: 0.6704, F1 Macro: 0.3944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2465, Accuracy: 0.9214, F1 Micro: 0.7685, F1 Macro: 0.6044\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9258, F1 Micro: 0.7657, F1 Macro: 0.6174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1552, Accuracy: 0.9255, F1 Micro: 0.7854, F1 Macro: 0.6839\n",
      "Epoch 5/10, Train Loss: 0.1243, Accuracy: 0.9255, F1 Micro: 0.7829, F1 Macro: 0.6719\n",
      "Epoch 6/10, Train Loss: 0.102, Accuracy: 0.9254, F1 Micro: 0.783, F1 Macro: 0.6825\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9241, F1 Micro: 0.7797, F1 Macro: 0.6994\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9186, F1 Micro: 0.774, F1 Macro: 0.7053\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9276, F1 Micro: 0.7846, F1 Macro: 0.7142\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9275, F1 Micro: 0.7831, F1 Macro: 0.7172\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9255, F1 Micro: 0.7854, F1 Macro: 0.6839\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.71      0.79      0.75       729\n",
      "     HS_Group       0.75      0.62      0.68       408\n",
      "  HS_Religion       0.77      0.68      0.72       168\n",
      "      HS_Race       0.75      0.73      0.74       119\n",
      "  HS_Physical       0.57      0.21      0.31        57\n",
      "    HS_Gender       0.62      0.24      0.34        55\n",
      "     HS_Other       0.80      0.81      0.81       771\n",
      "      HS_Weak       0.68      0.78      0.73       681\n",
      "  HS_Moderate       0.71      0.55      0.62       359\n",
      "    HS_Strong       0.78      0.73      0.76        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.66      0.68      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 261.63357639312744 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9221, F1 Micro: 0.7703, F1 Macro: 0.6723\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 17.287165880203247 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.359, Accuracy: 0.9059, F1 Micro: 0.6931, F1 Macro: 0.4779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2446, Accuracy: 0.9184, F1 Micro: 0.7461, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1922, Accuracy: 0.9253, F1 Micro: 0.773, F1 Macro: 0.6306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1578, Accuracy: 0.9265, F1 Micro: 0.7736, F1 Macro: 0.6786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9294, F1 Micro: 0.7919, F1 Macro: 0.6903\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9282, F1 Micro: 0.7895, F1 Macro: 0.7204\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9282, F1 Micro: 0.7835, F1 Macro: 0.7126\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9281, F1 Micro: 0.7825, F1 Macro: 0.7201\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9247, F1 Micro: 0.785, F1 Macro: 0.7227\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9278, F1 Micro: 0.7902, F1 Macro: 0.7293\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9294, F1 Micro: 0.7919, F1 Macro: 0.6903\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.92      0.87      0.89      1008\n",
      "HS_Individual       0.75      0.76      0.76       729\n",
      "     HS_Group       0.76      0.64      0.70       408\n",
      "  HS_Religion       0.82      0.63      0.71       168\n",
      "      HS_Race       0.78      0.78      0.78       119\n",
      "  HS_Physical       0.79      0.19      0.31        57\n",
      "    HS_Gender       0.83      0.18      0.30        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.71      0.58      0.64       359\n",
      "    HS_Strong       0.81      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.80      0.65      0.69      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 266.6790587902069 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3635, Accuracy: 0.9059, F1 Micro: 0.703, F1 Macro: 0.5003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2465, Accuracy: 0.9193, F1 Micro: 0.7556, F1 Macro: 0.6058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1928, Accuracy: 0.9262, F1 Micro: 0.7789, F1 Macro: 0.6348\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9272, F1 Micro: 0.7753, F1 Macro: 0.6644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9287, F1 Micro: 0.7874, F1 Macro: 0.694\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.928, F1 Micro: 0.7872, F1 Macro: 0.7084\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9243, F1 Micro: 0.7836, F1 Macro: 0.7146\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.924, F1 Micro: 0.7843, F1 Macro: 0.7194\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9238, F1 Micro: 0.7841, F1 Macro: 0.7154\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9257, F1 Micro: 0.7828, F1 Macro: 0.7108\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9287, F1 Micro: 0.7874, F1 Macro: 0.694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.92      0.86      0.89      1008\n",
      "HS_Individual       0.75      0.76      0.75       729\n",
      "     HS_Group       0.78      0.62      0.69       408\n",
      "  HS_Religion       0.80      0.63      0.70       168\n",
      "      HS_Race       0.81      0.71      0.75       119\n",
      "  HS_Physical       0.78      0.25      0.37        57\n",
      "    HS_Gender       0.70      0.25      0.37        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.73      0.54      0.62       359\n",
      "    HS_Strong       0.81      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5589\n",
      "    macro avg       0.79      0.65      0.69      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 265.98096084594727 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3637, Accuracy: 0.9059, F1 Micro: 0.7021, F1 Macro: 0.5011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2465, Accuracy: 0.9189, F1 Micro: 0.7512, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1961, Accuracy: 0.9252, F1 Micro: 0.7761, F1 Macro: 0.6334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9275, F1 Micro: 0.7785, F1 Macro: 0.6667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.929, F1 Micro: 0.7893, F1 Macro: 0.6906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.9262, F1 Micro: 0.7911, F1 Macro: 0.6979\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9242, F1 Micro: 0.783, F1 Macro: 0.7096\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9268, F1 Micro: 0.7805, F1 Macro: 0.6999\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9265, F1 Micro: 0.7835, F1 Macro: 0.7203\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.924, F1 Micro: 0.7789, F1 Macro: 0.7086\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9262, F1 Micro: 0.7911, F1 Macro: 0.6979\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.70      0.71      0.70       408\n",
      "  HS_Religion       0.73      0.68      0.71       168\n",
      "      HS_Race       0.74      0.77      0.75       119\n",
      "  HS_Physical       0.77      0.18      0.29        57\n",
      "    HS_Gender       0.77      0.31      0.44        55\n",
      "     HS_Other       0.77      0.84      0.80       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.64      0.64      0.64       359\n",
      "    HS_Strong       0.78      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.81      0.79      5589\n",
      "    macro avg       0.75      0.69      0.70      5589\n",
      " weighted avg       0.78      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 268.4018952846527 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9223, F1 Micro: 0.7712, F1 Macro: 0.6733\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.687793016433716 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3531, Accuracy: 0.9063, F1 Micro: 0.7088, F1 Macro: 0.4924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2384, Accuracy: 0.9189, F1 Micro: 0.7365, F1 Macro: 0.5532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1903, Accuracy: 0.9251, F1 Micro: 0.7798, F1 Macro: 0.6625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9218, F1 Micro: 0.7828, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1212, Accuracy: 0.9288, F1 Micro: 0.7868, F1 Macro: 0.6944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9278, F1 Micro: 0.7885, F1 Macro: 0.6998\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.9262, F1 Micro: 0.7848, F1 Macro: 0.7105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9297, F1 Micro: 0.7889, F1 Macro: 0.725\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.927, F1 Micro: 0.7879, F1 Macro: 0.7174\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9247, F1 Micro: 0.7881, F1 Macro: 0.7272\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9297, F1 Micro: 0.7889, F1 Macro: 0.725\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.93      0.84      0.89      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.76      0.61      0.68       408\n",
      "  HS_Religion       0.76      0.70      0.73       168\n",
      "      HS_Race       0.74      0.85      0.79       119\n",
      "  HS_Physical       0.71      0.42      0.53        57\n",
      "    HS_Gender       0.80      0.36      0.50        55\n",
      "     HS_Other       0.85      0.78      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.72      0.53      0.61       359\n",
      "    HS_Strong       0.82      0.81      0.82        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.79      0.69      0.73      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.44      0.42      0.42      5589\n",
      "\n",
      "Training completed in 275.07447481155396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3546, Accuracy: 0.9045, F1 Micro: 0.6881, F1 Macro: 0.471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2415, Accuracy: 0.9185, F1 Micro: 0.7345, F1 Macro: 0.5491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1926, Accuracy: 0.925, F1 Micro: 0.776, F1 Macro: 0.6609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9216, F1 Micro: 0.7794, F1 Macro: 0.6677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9288, F1 Micro: 0.7865, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9273, F1 Micro: 0.7908, F1 Macro: 0.6913\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.9279, F1 Micro: 0.7885, F1 Macro: 0.7165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.929, F1 Micro: 0.7912, F1 Macro: 0.7158\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9262, F1 Micro: 0.789, F1 Macro: 0.7281\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.9275, F1 Micro: 0.786, F1 Macro: 0.7248\n",
      "Model 2 - Iteration 9418: Accuracy: 0.929, F1 Micro: 0.7912, F1 Macro: 0.7158\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.74      0.64      0.69       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.68      0.84      0.75       119\n",
      "  HS_Physical       0.80      0.28      0.42        57\n",
      "    HS_Gender       0.73      0.44      0.55        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.69      0.57      0.62       359\n",
      "    HS_Strong       0.81      0.80      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 274.7282290458679 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3561, Accuracy: 0.9036, F1 Micro: 0.6774, F1 Macro: 0.4623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2412, Accuracy: 0.9186, F1 Micro: 0.736, F1 Macro: 0.5609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.193, Accuracy: 0.926, F1 Micro: 0.7797, F1 Macro: 0.6549\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9216, F1 Micro: 0.7791, F1 Macro: 0.6512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9291, F1 Micro: 0.7806, F1 Macro: 0.6782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1028, Accuracy: 0.9265, F1 Micro: 0.7888, F1 Macro: 0.6834\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9245, F1 Micro: 0.7805, F1 Macro: 0.7026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.929, F1 Micro: 0.7907, F1 Macro: 0.7154\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.9269, F1 Micro: 0.7863, F1 Macro: 0.7127\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9282, F1 Micro: 0.7865, F1 Macro: 0.7183\n",
      "Model 3 - Iteration 9418: Accuracy: 0.929, F1 Micro: 0.7907, F1 Macro: 0.7154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.92      0.88      0.90      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.74      0.66      0.70       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.70      0.80      0.75       119\n",
      "  HS_Physical       0.74      0.35      0.48        57\n",
      "    HS_Gender       0.70      0.42      0.52        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.74      0.73       681\n",
      "  HS_Moderate       0.69      0.59      0.64       359\n",
      "    HS_Strong       0.78      0.72      0.75        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 272.6553659439087 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9227, F1 Micro: 0.7721, F1 Macro: 0.6754\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.884777545928955 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3502, Accuracy: 0.9015, F1 Micro: 0.6783, F1 Macro: 0.4977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.236, Accuracy: 0.9218, F1 Micro: 0.7567, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1823, Accuracy: 0.9271, F1 Micro: 0.7812, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.928, F1 Micro: 0.7891, F1 Macro: 0.6761\n",
      "Epoch 5/10, Train Loss: 0.1163, Accuracy: 0.9275, F1 Micro: 0.785, F1 Macro: 0.6951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9271, F1 Micro: 0.7923, F1 Macro: 0.717\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9286, F1 Micro: 0.7904, F1 Macro: 0.7147\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9289, F1 Micro: 0.7877, F1 Macro: 0.7245\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9244, F1 Micro: 0.7838, F1 Macro: 0.7245\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9243, F1 Micro: 0.7827, F1 Macro: 0.7181\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9271, F1 Micro: 0.7923, F1 Macro: 0.717\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.73      0.78      0.76       729\n",
      "     HS_Group       0.69      0.69      0.69       408\n",
      "  HS_Religion       0.69      0.75      0.72       168\n",
      "      HS_Race       0.71      0.84      0.77       119\n",
      "  HS_Physical       0.57      0.35      0.43        57\n",
      "    HS_Gender       0.70      0.38      0.49        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.71      0.74      0.72       681\n",
      "  HS_Moderate       0.65      0.63      0.64       359\n",
      "    HS_Strong       0.77      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.73      0.72      0.72      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 276.6022913455963 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3509, Accuracy: 0.9037, F1 Micro: 0.6902, F1 Macro: 0.5129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2376, Accuracy: 0.9232, F1 Micro: 0.7597, F1 Macro: 0.5936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1869, Accuracy: 0.9284, F1 Micro: 0.7829, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.9287, F1 Micro: 0.7845, F1 Macro: 0.6737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9282, F1 Micro: 0.7899, F1 Macro: 0.709\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9281, F1 Micro: 0.7879, F1 Macro: 0.702\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.927, F1 Micro: 0.7893, F1 Macro: 0.7191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9292, F1 Micro: 0.7926, F1 Macro: 0.7318\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9266, F1 Micro: 0.7884, F1 Macro: 0.721\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9286, F1 Micro: 0.7902, F1 Macro: 0.7252\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9292, F1 Micro: 0.7926, F1 Macro: 0.7318\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.72      0.74       729\n",
      "     HS_Group       0.71      0.71      0.71       408\n",
      "  HS_Religion       0.73      0.68      0.71       168\n",
      "      HS_Race       0.69      0.86      0.77       119\n",
      "  HS_Physical       0.71      0.39      0.50        57\n",
      "    HS_Gender       0.70      0.55      0.61        55\n",
      "     HS_Other       0.84      0.79      0.81       771\n",
      "      HS_Weak       0.73      0.69      0.71       681\n",
      "  HS_Moderate       0.67      0.64      0.65       359\n",
      "    HS_Strong       0.79      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.72      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 277.9275166988373 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3509, Accuracy: 0.9044, F1 Micro: 0.6921, F1 Macro: 0.5123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2371, Accuracy: 0.9212, F1 Micro: 0.753, F1 Macro: 0.594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1878, Accuracy: 0.9277, F1 Micro: 0.783, F1 Macro: 0.6491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1525, Accuracy: 0.9291, F1 Micro: 0.79, F1 Macro: 0.6698\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.925, F1 Micro: 0.7845, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9265, F1 Micro: 0.7915, F1 Macro: 0.7083\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9286, F1 Micro: 0.7885, F1 Macro: 0.7041\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9262, F1 Micro: 0.7902, F1 Macro: 0.7174\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9269, F1 Micro: 0.7863, F1 Macro: 0.7191\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9296, F1 Micro: 0.7909, F1 Macro: 0.7248\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9265, F1 Micro: 0.7915, F1 Macro: 0.7083\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.67      0.73      0.70       408\n",
      "  HS_Religion       0.70      0.72      0.71       168\n",
      "      HS_Race       0.71      0.83      0.76       119\n",
      "  HS_Physical       0.58      0.26      0.36        57\n",
      "    HS_Gender       0.65      0.36      0.47        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.63      0.66      0.65       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.81      0.79      5589\n",
      "    macro avg       0.73      0.71      0.71      5589\n",
      " weighted avg       0.78      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 278.17098093032837 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9229, F1 Micro: 0.773, F1 Macro: 0.6773\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.887296199798584 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3472, Accuracy: 0.8977, F1 Micro: 0.634, F1 Macro: 0.3573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2297, Accuracy: 0.9197, F1 Micro: 0.7525, F1 Macro: 0.5764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1789, Accuracy: 0.9251, F1 Micro: 0.7724, F1 Macro: 0.6387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9223, F1 Micro: 0.7768, F1 Macro: 0.665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9277, F1 Micro: 0.7918, F1 Macro: 0.6927\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9295, F1 Micro: 0.7892, F1 Macro: 0.7063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9295, F1 Micro: 0.7928, F1 Macro: 0.7289\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9276, F1 Micro: 0.7921, F1 Macro: 0.7246\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9275, F1 Micro: 0.7917, F1 Macro: 0.7265\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9254, F1 Micro: 0.7841, F1 Macro: 0.7199\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9295, F1 Micro: 0.7928, F1 Macro: 0.7289\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.92      0.89      0.91      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.71      0.70      0.71       408\n",
      "  HS_Religion       0.71      0.69      0.70       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.72      0.40      0.52        57\n",
      "    HS_Gender       0.60      0.53      0.56        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.66      0.63      0.64       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 283.4447865486145 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3499, Accuracy: 0.897, F1 Micro: 0.6286, F1 Macro: 0.3661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2317, Accuracy: 0.9173, F1 Micro: 0.7417, F1 Macro: 0.556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9254, F1 Micro: 0.7743, F1 Macro: 0.6358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.145, Accuracy: 0.9235, F1 Micro: 0.7796, F1 Macro: 0.6575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1217, Accuracy: 0.9269, F1 Micro: 0.7848, F1 Macro: 0.6772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.9269, F1 Micro: 0.7911, F1 Macro: 0.7104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9279, F1 Micro: 0.7912, F1 Macro: 0.7193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9264, F1 Micro: 0.7932, F1 Macro: 0.7297\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9281, F1 Micro: 0.7907, F1 Macro: 0.725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9309, F1 Micro: 0.7936, F1 Macro: 0.7265\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9309, F1 Micro: 0.7936, F1 Macro: 0.7265\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.79      0.72      0.75       729\n",
      "     HS_Group       0.73      0.71      0.72       408\n",
      "  HS_Religion       0.72      0.66      0.69       168\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.63      0.39      0.48        57\n",
      "    HS_Gender       0.64      0.49      0.56        55\n",
      "     HS_Other       0.85      0.77      0.81       771\n",
      "      HS_Weak       0.78      0.68      0.73       681\n",
      "  HS_Moderate       0.67      0.65      0.66       359\n",
      "    HS_Strong       0.81      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.82      0.77      0.79      5589\n",
      "    macro avg       0.77      0.70      0.73      5589\n",
      " weighted avg       0.82      0.77      0.79      5589\n",
      "  samples avg       0.44      0.43      0.43      5589\n",
      "\n",
      "Training completed in 287.092577457428 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3493, Accuracy: 0.898, F1 Micro: 0.6354, F1 Macro: 0.3722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2309, Accuracy: 0.9194, F1 Micro: 0.7572, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1821, Accuracy: 0.9277, F1 Micro: 0.7843, F1 Macro: 0.6468\n",
      "Epoch 4/10, Train Loss: 0.1465, Accuracy: 0.9232, F1 Micro: 0.7811, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1248, Accuracy: 0.9277, F1 Micro: 0.7892, F1 Macro: 0.6745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9282, F1 Micro: 0.7912, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9277, F1 Micro: 0.7918, F1 Macro: 0.7145\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9245, F1 Micro: 0.786, F1 Macro: 0.7216\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9303, F1 Micro: 0.7874, F1 Macro: 0.7133\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.926, F1 Micro: 0.7881, F1 Macro: 0.7202\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9277, F1 Micro: 0.7918, F1 Macro: 0.7145\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.71      0.68      0.70       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.58      0.32      0.41        57\n",
      "    HS_Gender       0.64      0.42      0.51        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.78      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 283.96432065963745 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9231, F1 Micro: 0.7738, F1 Macro: 0.6792\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.066842555999756 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3371, Accuracy: 0.9055, F1 Micro: 0.71, F1 Macro: 0.4716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2216, Accuracy: 0.9196, F1 Micro: 0.7513, F1 Macro: 0.6032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1792, Accuracy: 0.9263, F1 Micro: 0.7793, F1 Macro: 0.6399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1419, Accuracy: 0.9274, F1 Micro: 0.7925, F1 Macro: 0.6913\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9284, F1 Micro: 0.7798, F1 Macro: 0.6842\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9274, F1 Micro: 0.7889, F1 Macro: 0.7067\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9277, F1 Micro: 0.7859, F1 Macro: 0.7159\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9271, F1 Micro: 0.7884, F1 Macro: 0.7192\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.927, F1 Micro: 0.7886, F1 Macro: 0.7183\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.9233, F1 Micro: 0.7817, F1 Macro: 0.7116\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9274, F1 Micro: 0.7925, F1 Macro: 0.6913\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.90      0.87      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.70      0.82      0.76       729\n",
      "     HS_Group       0.76      0.60      0.67       408\n",
      "  HS_Religion       0.80      0.68      0.74       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.47      0.25      0.32        57\n",
      "    HS_Gender       0.71      0.22      0.33        55\n",
      "     HS_Other       0.82      0.84      0.83       771\n",
      "      HS_Weak       0.68      0.80      0.74       681\n",
      "  HS_Moderate       0.73      0.50      0.59       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.74      0.67      0.69      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 283.3568615913391 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3393, Accuracy: 0.9032, F1 Micro: 0.6997, F1 Macro: 0.4514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2269, Accuracy: 0.921, F1 Micro: 0.7565, F1 Macro: 0.6049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1827, Accuracy: 0.9235, F1 Micro: 0.7768, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1461, Accuracy: 0.9279, F1 Micro: 0.7811, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9261, F1 Micro: 0.7853, F1 Macro: 0.6844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9286, F1 Micro: 0.793, F1 Macro: 0.7073\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.9268, F1 Micro: 0.7889, F1 Macro: 0.7202\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9244, F1 Micro: 0.7871, F1 Macro: 0.7169\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.9242, F1 Micro: 0.7866, F1 Macro: 0.7192\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9266, F1 Micro: 0.7889, F1 Macro: 0.7187\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9286, F1 Micro: 0.793, F1 Macro: 0.7073\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.75      0.76      0.76       729\n",
      "     HS_Group       0.72      0.69      0.70       408\n",
      "  HS_Religion       0.78      0.66      0.72       168\n",
      "      HS_Race       0.74      0.77      0.76       119\n",
      "  HS_Physical       0.81      0.23      0.36        57\n",
      "    HS_Gender       0.71      0.36      0.48        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.69      0.60      0.64       359\n",
      "    HS_Strong       0.74      0.82      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.77      0.69      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 287.3526360988617 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3399, Accuracy: 0.9055, F1 Micro: 0.6971, F1 Macro: 0.464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2245, Accuracy: 0.9202, F1 Micro: 0.7618, F1 Macro: 0.6072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1827, Accuracy: 0.9275, F1 Micro: 0.7814, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1465, Accuracy: 0.9267, F1 Micro: 0.7844, F1 Macro: 0.6699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9291, F1 Micro: 0.7889, F1 Macro: 0.6896\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9281, F1 Micro: 0.7842, F1 Macro: 0.7001\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9263, F1 Micro: 0.7873, F1 Macro: 0.7101\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9268, F1 Micro: 0.7866, F1 Macro: 0.7185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9296, F1 Micro: 0.7914, F1 Macro: 0.7237\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9259, F1 Micro: 0.786, F1 Macro: 0.7154\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9296, F1 Micro: 0.7914, F1 Macro: 0.7237\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.92      0.89      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.74      0.64      0.69       408\n",
      "  HS_Religion       0.75      0.68      0.72       168\n",
      "      HS_Race       0.70      0.77      0.74       119\n",
      "  HS_Physical       0.69      0.44      0.54        57\n",
      "    HS_Gender       0.66      0.42      0.51        55\n",
      "     HS_Other       0.84      0.78      0.80       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.69      0.58      0.63       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 287.0776319503784 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9234, F1 Micro: 0.7745, F1 Macro: 0.6803\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.728107452392578 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3339, Accuracy: 0.905, F1 Micro: 0.6788, F1 Macro: 0.47\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2258, Accuracy: 0.9192, F1 Micro: 0.7642, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1765, Accuracy: 0.9235, F1 Micro: 0.7809, F1 Macro: 0.6505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1432, Accuracy: 0.9249, F1 Micro: 0.7866, F1 Macro: 0.6761\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.9228, F1 Micro: 0.7864, F1 Macro: 0.6794\n",
      "Epoch 6/10, Train Loss: 0.0895, Accuracy: 0.9271, F1 Micro: 0.7861, F1 Macro: 0.6921\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.927, F1 Micro: 0.7834, F1 Macro: 0.7201\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9271, F1 Micro: 0.786, F1 Macro: 0.7208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9274, F1 Micro: 0.7875, F1 Macro: 0.7195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9271, F1 Micro: 0.79, F1 Macro: 0.7275\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9271, F1 Micro: 0.79, F1 Macro: 0.7275\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.73      0.76      0.74       729\n",
      "     HS_Group       0.71      0.66      0.69       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.76      0.78      0.77       119\n",
      "  HS_Physical       0.62      0.44      0.52        57\n",
      "    HS_Gender       0.70      0.51      0.59        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.70      0.74      0.72       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.79      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 291.90575313568115 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3358, Accuracy: 0.9054, F1 Micro: 0.6825, F1 Macro: 0.4838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2294, Accuracy: 0.9182, F1 Micro: 0.7576, F1 Macro: 0.578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1767, Accuracy: 0.9217, F1 Micro: 0.7772, F1 Macro: 0.6261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9275, F1 Micro: 0.7898, F1 Macro: 0.7004\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.9237, F1 Micro: 0.7843, F1 Macro: 0.6859\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9235, F1 Micro: 0.7823, F1 Macro: 0.6879\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.9259, F1 Micro: 0.7846, F1 Macro: 0.7201\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9262, F1 Micro: 0.7863, F1 Macro: 0.7199\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9282, F1 Micro: 0.7845, F1 Macro: 0.7179\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9181, F1 Micro: 0.7783, F1 Macro: 0.7037\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9275, F1 Micro: 0.7898, F1 Macro: 0.7004\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.70      0.82      0.76       729\n",
      "     HS_Group       0.78      0.61      0.69       408\n",
      "  HS_Religion       0.75      0.65      0.70       168\n",
      "      HS_Race       0.84      0.73      0.78       119\n",
      "  HS_Physical       0.55      0.28      0.37        57\n",
      "    HS_Gender       0.83      0.27      0.41        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.68      0.79      0.73       681\n",
      "  HS_Moderate       0.74      0.53      0.62       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.77      0.67      0.70      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 288.4979727268219 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3374, Accuracy: 0.9068, F1 Micro: 0.6928, F1 Macro: 0.4982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2271, Accuracy: 0.9183, F1 Micro: 0.7581, F1 Macro: 0.5877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9242, F1 Micro: 0.7795, F1 Macro: 0.6249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1459, Accuracy: 0.926, F1 Micro: 0.7874, F1 Macro: 0.6888\n",
      "Epoch 5/10, Train Loss: 0.1116, Accuracy: 0.9243, F1 Micro: 0.7844, F1 Macro: 0.6555\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9261, F1 Micro: 0.7857, F1 Macro: 0.6799\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9261, F1 Micro: 0.784, F1 Macro: 0.7191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.9246, F1 Micro: 0.7894, F1 Macro: 0.7295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9287, F1 Micro: 0.7928, F1 Macro: 0.7199\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9275, F1 Micro: 0.784, F1 Macro: 0.7217\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9287, F1 Micro: 0.7928, F1 Macro: 0.7199\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.71      0.67      0.69       408\n",
      "  HS_Religion       0.78      0.67      0.72       168\n",
      "      HS_Race       0.75      0.80      0.77       119\n",
      "  HS_Physical       0.70      0.33      0.45        57\n",
      "    HS_Gender       0.69      0.40      0.51        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.65      0.60      0.62       359\n",
      "    HS_Strong       0.79      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 291.66532850265503 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9235, F1 Micro: 0.7752, F1 Macro: 0.6817\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.715036630630493 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3297, Accuracy: 0.9075, F1 Micro: 0.7116, F1 Macro: 0.4972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2246, Accuracy: 0.9229, F1 Micro: 0.7693, F1 Macro: 0.6064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.926, F1 Micro: 0.7757, F1 Macro: 0.644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1407, Accuracy: 0.9279, F1 Micro: 0.7931, F1 Macro: 0.6769\n",
      "Epoch 5/10, Train Loss: 0.1093, Accuracy: 0.9259, F1 Micro: 0.7887, F1 Macro: 0.6766\n",
      "Epoch 6/10, Train Loss: 0.0835, Accuracy: 0.9262, F1 Micro: 0.7895, F1 Macro: 0.7073\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9276, F1 Micro: 0.7771, F1 Macro: 0.7048\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9292, F1 Micro: 0.788, F1 Macro: 0.7132\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9271, F1 Micro: 0.7865, F1 Macro: 0.7265\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9265, F1 Micro: 0.7901, F1 Macro: 0.7299\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9279, F1 Micro: 0.7931, F1 Macro: 0.6769\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.73      0.80      0.77       729\n",
      "     HS_Group       0.76      0.66      0.71       408\n",
      "  HS_Religion       0.76      0.68      0.72       168\n",
      "      HS_Race       0.79      0.73      0.76       119\n",
      "  HS_Physical       0.57      0.14      0.23        57\n",
      "    HS_Gender       0.80      0.15      0.25        55\n",
      "     HS_Other       0.78      0.85      0.81       771\n",
      "      HS_Weak       0.69      0.78      0.73       681\n",
      "  HS_Moderate       0.72      0.57      0.63       359\n",
      "    HS_Strong       0.79      0.73      0.76        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5589\n",
      "    macro avg       0.76      0.66      0.68      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 294.20728516578674 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3312, Accuracy: 0.9069, F1 Micro: 0.7139, F1 Macro: 0.5008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.9218, F1 Micro: 0.7688, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1785, Accuracy: 0.925, F1 Micro: 0.7767, F1 Macro: 0.6376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1406, Accuracy: 0.9269, F1 Micro: 0.7862, F1 Macro: 0.6713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1085, Accuracy: 0.9261, F1 Micro: 0.787, F1 Macro: 0.6771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9261, F1 Micro: 0.7893, F1 Macro: 0.7103\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9257, F1 Micro: 0.7769, F1 Macro: 0.6989\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9249, F1 Micro: 0.7843, F1 Macro: 0.7203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9299, F1 Micro: 0.7909, F1 Macro: 0.7254\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9262, F1 Micro: 0.7898, F1 Macro: 0.7282\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9299, F1 Micro: 0.7909, F1 Macro: 0.7254\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.74      0.67      0.71       408\n",
      "  HS_Religion       0.72      0.68      0.70       168\n",
      "      HS_Race       0.76      0.79      0.78       119\n",
      "  HS_Physical       0.58      0.46      0.51        57\n",
      "    HS_Gender       0.65      0.47      0.55        55\n",
      "     HS_Other       0.85      0.76      0.80       771\n",
      "      HS_Weak       0.76      0.69      0.72       681\n",
      "  HS_Moderate       0.69      0.55      0.61       359\n",
      "    HS_Strong       0.77      0.87      0.82        97\n",
      "\n",
      "    micro avg       0.82      0.77      0.79      5589\n",
      "    macro avg       0.76      0.70      0.73      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 299.0373682975769 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3322, Accuracy: 0.9058, F1 Micro: 0.6968, F1 Macro: 0.4802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.9214, F1 Micro: 0.7684, F1 Macro: 0.6033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.178, Accuracy: 0.9245, F1 Micro: 0.7785, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1439, Accuracy: 0.9291, F1 Micro: 0.7887, F1 Macro: 0.6607\n",
      "Epoch 5/10, Train Loss: 0.1096, Accuracy: 0.9276, F1 Micro: 0.7815, F1 Macro: 0.6543\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.9265, F1 Micro: 0.7849, F1 Macro: 0.7071\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9276, F1 Micro: 0.7852, F1 Macro: 0.707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9284, F1 Micro: 0.7906, F1 Macro: 0.7228\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9293, F1 Micro: 0.7868, F1 Macro: 0.7212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.9298, F1 Micro: 0.7965, F1 Macro: 0.7266\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9298, F1 Micro: 0.7965, F1 Macro: 0.7266\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.74      0.65      0.69       168\n",
      "      HS_Race       0.74      0.82      0.78       119\n",
      "  HS_Physical       0.67      0.42      0.52        57\n",
      "    HS_Gender       0.65      0.44      0.52        55\n",
      "     HS_Other       0.83      0.82      0.82       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.67      0.59      0.63       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.80      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 297.802353143692 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9237, F1 Micro: 0.7758, F1 Macro: 0.6827\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.6407511234283447 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3293, Accuracy: 0.9031, F1 Micro: 0.7184, F1 Macro: 0.4909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2128, Accuracy: 0.921, F1 Micro: 0.767, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1692, Accuracy: 0.9238, F1 Micro: 0.7749, F1 Macro: 0.6324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.9274, F1 Micro: 0.7894, F1 Macro: 0.6957\n",
      "Epoch 5/10, Train Loss: 0.1116, Accuracy: 0.9255, F1 Micro: 0.7886, F1 Macro: 0.6939\n",
      "Epoch 6/10, Train Loss: 0.0842, Accuracy: 0.9258, F1 Micro: 0.7876, F1 Macro: 0.7081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.9302, F1 Micro: 0.7942, F1 Macro: 0.7273\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9293, F1 Micro: 0.7898, F1 Macro: 0.7244\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9284, F1 Micro: 0.7892, F1 Macro: 0.7273\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.926, F1 Micro: 0.786, F1 Macro: 0.7223\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9302, F1 Micro: 0.7942, F1 Macro: 0.7273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.73      0.71      0.72       408\n",
      "  HS_Religion       0.75      0.63      0.68       168\n",
      "      HS_Race       0.80      0.74      0.77       119\n",
      "  HS_Physical       0.78      0.37      0.50        57\n",
      "    HS_Gender       0.75      0.44      0.55        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.75      0.69      0.72       681\n",
      "  HS_Moderate       0.67      0.60      0.63       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.78      0.69      0.73      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 298.2085962295532 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3324, Accuracy: 0.9042, F1 Micro: 0.7197, F1 Macro: 0.4895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2149, Accuracy: 0.9223, F1 Micro: 0.7671, F1 Macro: 0.5999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1728, Accuracy: 0.9231, F1 Micro: 0.7715, F1 Macro: 0.621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1406, Accuracy: 0.928, F1 Micro: 0.7897, F1 Macro: 0.693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1154, Accuracy: 0.9293, F1 Micro: 0.7933, F1 Macro: 0.7126\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9278, F1 Micro: 0.7896, F1 Macro: 0.701\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9264, F1 Micro: 0.7912, F1 Macro: 0.7203\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9306, F1 Micro: 0.7905, F1 Macro: 0.7202\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.9294, F1 Micro: 0.7888, F1 Macro: 0.728\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.924, F1 Micro: 0.7895, F1 Macro: 0.7262\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9293, F1 Micro: 0.7933, F1 Macro: 0.7126\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.72      0.69      0.71       408\n",
      "  HS_Religion       0.82      0.60      0.69       168\n",
      "      HS_Race       0.80      0.68      0.74       119\n",
      "  HS_Physical       0.77      0.30      0.43        57\n",
      "    HS_Gender       0.80      0.36      0.50        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.75      0.70      0.73       681\n",
      "  HS_Moderate       0.67      0.62      0.64       359\n",
      "    HS_Strong       0.82      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.79      0.67      0.71      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 299.28396582603455 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3336, Accuracy: 0.9025, F1 Micro: 0.7138, F1 Macro: 0.4713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.214, Accuracy: 0.9221, F1 Micro: 0.7684, F1 Macro: 0.6075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1725, Accuracy: 0.9251, F1 Micro: 0.781, F1 Macro: 0.6216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1391, Accuracy: 0.9279, F1 Micro: 0.7854, F1 Macro: 0.6841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1146, Accuracy: 0.9294, F1 Micro: 0.7967, F1 Macro: 0.6965\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.9267, F1 Micro: 0.7853, F1 Macro: 0.6872\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9268, F1 Micro: 0.7902, F1 Macro: 0.7215\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9297, F1 Micro: 0.7795, F1 Macro: 0.7094\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9295, F1 Micro: 0.7901, F1 Macro: 0.7229\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9281, F1 Micro: 0.7912, F1 Macro: 0.7239\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9294, F1 Micro: 0.7967, F1 Macro: 0.6965\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.75      0.78      0.76       729\n",
      "     HS_Group       0.74      0.68      0.71       408\n",
      "  HS_Religion       0.77      0.65      0.71       168\n",
      "      HS_Race       0.69      0.79      0.74       119\n",
      "  HS_Physical       0.56      0.16      0.25        57\n",
      "    HS_Gender       0.75      0.33      0.46        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.73      0.75      0.74       681\n",
      "  HS_Moderate       0.69      0.61      0.64       359\n",
      "    HS_Strong       0.77      0.77      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.80      0.80      5589\n",
      "    macro avg       0.75      0.68      0.70      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 298.2632396221161 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9239, F1 Micro: 0.7765, F1 Macro: 0.6838\n",
      "Total sampling time: 1193.21 seconds\n",
      "Total runtime: 20658.540759325027 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44687a7",
   "metadata": {
    "papermill": {
     "duration": 0.215707,
     "end_time": "2025-01-30T11:43:26.489218",
     "exception": false,
     "start_time": "2025-01-30T11:43:26.273511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3e680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.8263, F1 Micro: 0.2896, F1 Macro: 0.0616\n",
      "Epoch 2/10, Train Loss: 0.4179, Accuracy: 0.8282, F1 Micro: 0.0243, F1 Macro: 0.0105\n",
      "Epoch 3/10, Train Loss: 0.3769, Accuracy: 0.8377, F1 Micro: 0.1431, F1 Macro: 0.0548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3632, Accuracy: 0.8542, F1 Micro: 0.3403, F1 Macro: 0.1159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3417, Accuracy: 0.8717, F1 Micro: 0.4938, F1 Macro: 0.2223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2995, Accuracy: 0.8823, F1 Micro: 0.5895, F1 Macro: 0.2882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2634, Accuracy: 0.886, F1 Micro: 0.6182, F1 Macro: 0.3134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2462, Accuracy: 0.8873, F1 Micro: 0.6237, F1 Macro: 0.3587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2127, Accuracy: 0.89, F1 Micro: 0.6317, F1 Macro: 0.3787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1837, Accuracy: 0.8897, F1 Micro: 0.6321, F1 Macro: 0.3839\n",
      "Model 1 - Iteration 658: Accuracy: 0.8897, F1 Micro: 0.6321, F1 Macro: 0.3839\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.75      0.79      1137\n",
      "      Abusive       0.85      0.73      0.78      1008\n",
      "HS_Individual       0.70      0.55      0.62       729\n",
      "     HS_Group       0.61      0.37      0.46       408\n",
      "  HS_Religion       0.70      0.19      0.30       168\n",
      "      HS_Race       0.75      0.05      0.09       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.72      0.57      0.63       771\n",
      "      HS_Weak       0.65      0.52      0.58       681\n",
      "  HS_Moderate       0.55      0.26      0.36       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.75      0.55      0.63      5589\n",
      "    macro avg       0.53      0.33      0.38      5589\n",
      " weighted avg       0.71      0.55      0.61      5589\n",
      "  samples avg       0.39      0.32      0.32      5589\n",
      "\n",
      "Training completed in 60.00955057144165 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5496, Accuracy: 0.8306, F1 Micro: 0.1731, F1 Macro: 0.06\n",
      "Epoch 2/10, Train Loss: 0.4186, Accuracy: 0.8311, F1 Micro: 0.0568, F1 Macro: 0.0229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.375, Accuracy: 0.8478, F1 Micro: 0.2581, F1 Macro: 0.0895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3589, Accuracy: 0.8582, F1 Micro: 0.3871, F1 Macro: 0.1215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3376, Accuracy: 0.8704, F1 Micro: 0.4795, F1 Macro: 0.2006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2961, Accuracy: 0.879, F1 Micro: 0.5587, F1 Macro: 0.2789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2586, Accuracy: 0.8827, F1 Micro: 0.5789, F1 Macro: 0.3154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2424, Accuracy: 0.8877, F1 Micro: 0.6226, F1 Macro: 0.3637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2175, Accuracy: 0.8897, F1 Micro: 0.6498, F1 Macro: 0.3975\n",
      "Epoch 10/10, Train Loss: 0.1845, Accuracy: 0.8891, F1 Micro: 0.6482, F1 Macro: 0.3891\n",
      "Model 2 - Iteration 658: Accuracy: 0.8897, F1 Micro: 0.6498, F1 Macro: 0.3975\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.77      0.79      1137\n",
      "      Abusive       0.82      0.78      0.80      1008\n",
      "HS_Individual       0.65      0.62      0.63       729\n",
      "     HS_Group       0.62      0.34      0.44       408\n",
      "  HS_Religion       0.65      0.30      0.41       168\n",
      "      HS_Race       1.00      0.03      0.07       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.64      0.67       771\n",
      "      HS_Weak       0.62      0.58      0.60       681\n",
      "  HS_Moderate       0.55      0.26      0.36       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.72      0.59      0.65      5589\n",
      "    macro avg       0.53      0.36      0.40      5589\n",
      " weighted avg       0.69      0.59      0.62      5589\n",
      "  samples avg       0.39      0.34      0.33      5589\n",
      "\n",
      "Training completed in 59.632203340530396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5569, Accuracy: 0.8269, F1 Micro: 0.0396, F1 Macro: 0.0186\n",
      "Epoch 2/10, Train Loss: 0.4196, Accuracy: 0.8275, F1 Micro: 0.0089, F1 Macro: 0.004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3774, Accuracy: 0.8362, F1 Micro: 0.1188, F1 Macro: 0.046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3697, Accuracy: 0.8451, F1 Micro: 0.2465, F1 Macro: 0.0833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3528, Accuracy: 0.859, F1 Micro: 0.3944, F1 Macro: 0.1369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3121, Accuracy: 0.8744, F1 Micro: 0.5122, F1 Macro: 0.2371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2777, Accuracy: 0.8784, F1 Micro: 0.5484, F1 Macro: 0.285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2545, Accuracy: 0.8875, F1 Micro: 0.6283, F1 Macro: 0.3757\n",
      "Epoch 9/10, Train Loss: 0.2275, Accuracy: 0.8873, F1 Micro: 0.6105, F1 Macro: 0.3622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1962, Accuracy: 0.8896, F1 Micro: 0.6346, F1 Macro: 0.373\n",
      "Model 3 - Iteration 658: Accuracy: 0.8896, F1 Micro: 0.6346, F1 Macro: 0.373\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.73      0.78      1137\n",
      "      Abusive       0.84      0.75      0.80      1008\n",
      "HS_Individual       0.69      0.58      0.63       729\n",
      "     HS_Group       0.63      0.29      0.40       408\n",
      "  HS_Religion       0.78      0.12      0.22       168\n",
      "      HS_Race       1.00      0.04      0.08       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.61      0.65       771\n",
      "      HS_Weak       0.63      0.56      0.59       681\n",
      "  HS_Moderate       0.54      0.24      0.33       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.74      0.55      0.63      5589\n",
      "    macro avg       0.55      0.33      0.37      5589\n",
      " weighted avg       0.71      0.55      0.60      5589\n",
      "  samples avg       0.38      0.32      0.32      5589\n",
      "\n",
      "Training completed in 59.76804280281067 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8897, F1 Micro: 0.6388, F1 Macro: 0.3848\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 124.33015036582947 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.495, Accuracy: 0.8355, F1 Micro: 0.3998, F1 Macro: 0.1098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4133, Accuracy: 0.8631, F1 Micro: 0.5826, F1 Macro: 0.29\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3577, Accuracy: 0.8937, F1 Micro: 0.6715, F1 Macro: 0.4107\n",
      "Epoch 4/10, Train Loss: 0.301, Accuracy: 0.9016, F1 Micro: 0.668, F1 Macro: 0.4615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2563, Accuracy: 0.9039, F1 Micro: 0.7035, F1 Macro: 0.5163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2128, Accuracy: 0.9062, F1 Micro: 0.7171, F1 Macro: 0.5374\n",
      "Epoch 7/10, Train Loss: 0.1968, Accuracy: 0.9088, F1 Micro: 0.7012, F1 Macro: 0.5253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1552, Accuracy: 0.9083, F1 Micro: 0.7268, F1 Macro: 0.5588\n",
      "Epoch 9/10, Train Loss: 0.1345, Accuracy: 0.9084, F1 Micro: 0.7218, F1 Macro: 0.5629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1128, Accuracy: 0.9059, F1 Micro: 0.7284, F1 Macro: 0.5772\n",
      "Model 1 - Iteration 1646: Accuracy: 0.9059, F1 Micro: 0.7284, F1 Macro: 0.5772\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1137\n",
      "      Abusive       0.83      0.84      0.84      1008\n",
      "HS_Individual       0.68      0.72      0.70       729\n",
      "     HS_Group       0.63      0.62      0.62       408\n",
      "  HS_Religion       0.64      0.64      0.64       168\n",
      "      HS_Race       0.76      0.69      0.72       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.67      0.07      0.13        55\n",
      "     HS_Other       0.75      0.75      0.75       771\n",
      "      HS_Weak       0.63      0.70      0.66       681\n",
      "  HS_Moderate       0.56      0.53      0.55       359\n",
      "    HS_Strong       0.85      0.34      0.49        97\n",
      "\n",
      "    micro avg       0.73      0.73      0.73      5589\n",
      "    macro avg       0.65      0.56      0.58      5589\n",
      " weighted avg       0.72      0.73      0.72      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 82.25495505332947 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5023, Accuracy: 0.8361, F1 Micro: 0.394, F1 Macro: 0.1089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4153, Accuracy: 0.8724, F1 Micro: 0.5601, F1 Macro: 0.2504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3578, Accuracy: 0.8897, F1 Micro: 0.6635, F1 Macro: 0.3958\n",
      "Epoch 4/10, Train Loss: 0.3041, Accuracy: 0.9012, F1 Micro: 0.6606, F1 Macro: 0.4615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2601, Accuracy: 0.9056, F1 Micro: 0.7071, F1 Macro: 0.5152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2203, Accuracy: 0.9077, F1 Micro: 0.7197, F1 Macro: 0.5429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1956, Accuracy: 0.9089, F1 Micro: 0.7221, F1 Macro: 0.5614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1542, Accuracy: 0.9083, F1 Micro: 0.7279, F1 Macro: 0.5795\n",
      "Epoch 9/10, Train Loss: 0.1334, Accuracy: 0.9097, F1 Micro: 0.712, F1 Macro: 0.5477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1101, Accuracy: 0.9071, F1 Micro: 0.7284, F1 Macro: 0.5976\n",
      "Model 2 - Iteration 1646: Accuracy: 0.9071, F1 Micro: 0.7284, F1 Macro: 0.5976\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1137\n",
      "      Abusive       0.84      0.84      0.84      1008\n",
      "HS_Individual       0.69      0.70      0.70       729\n",
      "     HS_Group       0.63      0.60      0.61       408\n",
      "  HS_Religion       0.62      0.66      0.64       168\n",
      "      HS_Race       0.74      0.72      0.73       119\n",
      "  HS_Physical       1.00      0.02      0.03        57\n",
      "    HS_Gender       0.70      0.13      0.22        55\n",
      "     HS_Other       0.78      0.71      0.74       771\n",
      "      HS_Weak       0.64      0.67      0.66       681\n",
      "  HS_Moderate       0.57      0.53      0.55       359\n",
      "    HS_Strong       0.84      0.49      0.62        97\n",
      "\n",
      "    micro avg       0.74      0.72      0.73      5589\n",
      "    macro avg       0.74      0.58      0.60      5589\n",
      " weighted avg       0.74      0.72      0.72      5589\n",
      "  samples avg       0.42      0.40      0.40      5589\n",
      "\n",
      "Training completed in 83.48812365531921 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5041, Accuracy: 0.8355, F1 Micro: 0.372, F1 Macro: 0.103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4204, Accuracy: 0.8582, F1 Micro: 0.557, F1 Macro: 0.2672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3643, Accuracy: 0.8909, F1 Micro: 0.654, F1 Macro: 0.3835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3043, Accuracy: 0.8996, F1 Micro: 0.6589, F1 Macro: 0.477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2577, Accuracy: 0.9042, F1 Micro: 0.7114, F1 Macro: 0.5198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.217, Accuracy: 0.9066, F1 Micro: 0.7238, F1 Macro: 0.5493\n",
      "Epoch 7/10, Train Loss: 0.1972, Accuracy: 0.9064, F1 Micro: 0.6973, F1 Macro: 0.5348\n",
      "Epoch 8/10, Train Loss: 0.1493, Accuracy: 0.9055, F1 Micro: 0.7237, F1 Macro: 0.5647\n",
      "Epoch 9/10, Train Loss: 0.1331, Accuracy: 0.9086, F1 Micro: 0.7141, F1 Macro: 0.5647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1094, Accuracy: 0.909, F1 Micro: 0.7305, F1 Macro: 0.5958\n",
      "Model 3 - Iteration 1646: Accuracy: 0.909, F1 Micro: 0.7305, F1 Macro: 0.5958\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.83      1137\n",
      "      Abusive       0.83      0.84      0.84      1008\n",
      "HS_Individual       0.70      0.70      0.70       729\n",
      "     HS_Group       0.65      0.59      0.61       408\n",
      "  HS_Religion       0.66      0.57      0.61       168\n",
      "      HS_Race       0.75      0.65      0.70       119\n",
      "  HS_Physical       1.00      0.02      0.03        57\n",
      "    HS_Gender       0.89      0.15      0.25        55\n",
      "     HS_Other       0.79      0.71      0.75       771\n",
      "      HS_Weak       0.65      0.69      0.67       681\n",
      "  HS_Moderate       0.61      0.54      0.57       359\n",
      "    HS_Strong       0.88      0.44      0.59        97\n",
      "\n",
      "    micro avg       0.75      0.71      0.73      5589\n",
      "    macro avg       0.77      0.56      0.60      5589\n",
      " weighted avg       0.75      0.71      0.72      5589\n",
      "  samples avg       0.42      0.40      0.39      5589\n",
      "\n",
      "Training completed in 81.23208999633789 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8985, F1 Micro: 0.684, F1 Macro: 0.4875\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 112.20459866523743 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.475, Accuracy: 0.8413, F1 Micro: 0.4597, F1 Macro: 0.1721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3896, Accuracy: 0.893, F1 Micro: 0.6463, F1 Macro: 0.3533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.318, Accuracy: 0.9049, F1 Micro: 0.7066, F1 Macro: 0.5102\n",
      "Epoch 4/10, Train Loss: 0.2683, Accuracy: 0.9089, F1 Micro: 0.698, F1 Macro: 0.5261\n",
      "Epoch 5/10, Train Loss: 0.2261, Accuracy: 0.9099, F1 Micro: 0.706, F1 Macro: 0.5343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1925, Accuracy: 0.9152, F1 Micro: 0.7409, F1 Macro: 0.5833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1569, Accuracy: 0.912, F1 Micro: 0.7449, F1 Macro: 0.6168\n",
      "Epoch 8/10, Train Loss: 0.1272, Accuracy: 0.9162, F1 Micro: 0.7437, F1 Macro: 0.6189\n",
      "Epoch 9/10, Train Loss: 0.1084, Accuracy: 0.9141, F1 Micro: 0.7448, F1 Macro: 0.6363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0916, Accuracy: 0.9145, F1 Micro: 0.7485, F1 Macro: 0.6549\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9145, F1 Micro: 0.7485, F1 Macro: 0.6549\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1137\n",
      "      Abusive       0.82      0.90      0.86      1008\n",
      "HS_Individual       0.72      0.70      0.71       729\n",
      "     HS_Group       0.68      0.61      0.64       408\n",
      "  HS_Religion       0.70      0.61      0.65       168\n",
      "      HS_Race       0.74      0.73      0.73       119\n",
      "  HS_Physical       0.58      0.19      0.29        57\n",
      "    HS_Gender       0.50      0.27      0.35        55\n",
      "     HS_Other       0.80      0.72      0.76       771\n",
      "      HS_Weak       0.69      0.69      0.69       681\n",
      "  HS_Moderate       0.63      0.51      0.56       359\n",
      "    HS_Strong       0.82      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5589\n",
      "    macro avg       0.71      0.63      0.65      5589\n",
      " weighted avg       0.76      0.73      0.74      5589\n",
      "  samples avg       0.43      0.42      0.41      5589\n",
      "\n",
      "Training completed in 102.55645561218262 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4786, Accuracy: 0.8418, F1 Micro: 0.4203, F1 Macro: 0.1265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3862, Accuracy: 0.8911, F1 Micro: 0.6146, F1 Macro: 0.3277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3179, Accuracy: 0.906, F1 Micro: 0.7025, F1 Macro: 0.5087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2657, Accuracy: 0.9113, F1 Micro: 0.709, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2249, Accuracy: 0.9114, F1 Micro: 0.7282, F1 Macro: 0.5604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1909, Accuracy: 0.9144, F1 Micro: 0.7389, F1 Macro: 0.6101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1536, Accuracy: 0.9138, F1 Micro: 0.7443, F1 Macro: 0.634\n",
      "Epoch 8/10, Train Loss: 0.1266, Accuracy: 0.9144, F1 Micro: 0.744, F1 Macro: 0.6335\n",
      "Epoch 9/10, Train Loss: 0.106, Accuracy: 0.9128, F1 Micro: 0.7403, F1 Macro: 0.6385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0901, Accuracy: 0.9128, F1 Micro: 0.7454, F1 Macro: 0.6477\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9128, F1 Micro: 0.7454, F1 Macro: 0.6477\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1137\n",
      "      Abusive       0.84      0.87      0.86      1008\n",
      "HS_Individual       0.71      0.70      0.71       729\n",
      "     HS_Group       0.67      0.66      0.66       408\n",
      "  HS_Religion       0.68      0.62      0.65       168\n",
      "      HS_Race       0.73      0.68      0.70       119\n",
      "  HS_Physical       0.62      0.18      0.27        57\n",
      "    HS_Gender       0.48      0.22      0.30        55\n",
      "     HS_Other       0.78      0.74      0.76       771\n",
      "      HS_Weak       0.68      0.66      0.67       681\n",
      "  HS_Moderate       0.61      0.58      0.59       359\n",
      "    HS_Strong       0.80      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5589\n",
      "    macro avg       0.70      0.62      0.65      5589\n",
      " weighted avg       0.75      0.74      0.74      5589\n",
      "  samples avg       0.43      0.42      0.41      5589\n",
      "\n",
      "Training completed in 106.31096863746643 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4803, Accuracy: 0.8339, F1 Micro: 0.4232, F1 Macro: 0.133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3925, Accuracy: 0.8908, F1 Micro: 0.6377, F1 Macro: 0.3837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3219, Accuracy: 0.9035, F1 Micro: 0.6997, F1 Macro: 0.4895\n",
      "Epoch 4/10, Train Loss: 0.2689, Accuracy: 0.9064, F1 Micro: 0.6813, F1 Macro: 0.5043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2274, Accuracy: 0.9125, F1 Micro: 0.7334, F1 Macro: 0.5797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.189, Accuracy: 0.9132, F1 Micro: 0.7426, F1 Macro: 0.6196\n",
      "Epoch 7/10, Train Loss: 0.1531, Accuracy: 0.9114, F1 Micro: 0.7408, F1 Macro: 0.618\n",
      "Epoch 8/10, Train Loss: 0.1248, Accuracy: 0.9171, F1 Micro: 0.7423, F1 Macro: 0.6365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1069, Accuracy: 0.9133, F1 Micro: 0.7439, F1 Macro: 0.6483\n",
      "Epoch 10/10, Train Loss: 0.0906, Accuracy: 0.9114, F1 Micro: 0.7434, F1 Macro: 0.6554\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9133, F1 Micro: 0.7439, F1 Macro: 0.6483\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1137\n",
      "      Abusive       0.87      0.84      0.86      1008\n",
      "HS_Individual       0.70      0.71      0.70       729\n",
      "     HS_Group       0.67      0.61      0.63       408\n",
      "  HS_Religion       0.66      0.64      0.65       168\n",
      "      HS_Race       0.79      0.71      0.74       119\n",
      "  HS_Physical       0.83      0.18      0.29        57\n",
      "    HS_Gender       0.69      0.20      0.31        55\n",
      "     HS_Other       0.79      0.73      0.76       771\n",
      "      HS_Weak       0.67      0.69      0.68       681\n",
      "  HS_Moderate       0.62      0.52      0.56       359\n",
      "    HS_Strong       0.80      0.72      0.76        97\n",
      "\n",
      "    micro avg       0.76      0.73      0.74      5589\n",
      "    macro avg       0.74      0.61      0.65      5589\n",
      " weighted avg       0.76      0.73      0.74      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 101.77719640731812 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9035, F1 Micro: 0.7046, F1 Macro: 0.5418\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 101.5166130065918 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.453, Accuracy: 0.8534, F1 Micro: 0.5809, F1 Macro: 0.2855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3617, Accuracy: 0.9, F1 Micro: 0.6939, F1 Macro: 0.4925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2969, Accuracy: 0.911, F1 Micro: 0.7305, F1 Macro: 0.5557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2451, Accuracy: 0.9127, F1 Micro: 0.7401, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1993, Accuracy: 0.9135, F1 Micro: 0.7403, F1 Macro: 0.5887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1655, Accuracy: 0.9167, F1 Micro: 0.7498, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1359, Accuracy: 0.9177, F1 Micro: 0.7582, F1 Macro: 0.6293\n",
      "Epoch 8/10, Train Loss: 0.1114, Accuracy: 0.9179, F1 Micro: 0.7547, F1 Macro: 0.6473\n",
      "Epoch 9/10, Train Loss: 0.1014, Accuracy: 0.9167, F1 Micro: 0.7397, F1 Macro: 0.6472\n",
      "Epoch 10/10, Train Loss: 0.0809, Accuracy: 0.9182, F1 Micro: 0.7487, F1 Macro: 0.6438\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9177, F1 Micro: 0.7582, F1 Macro: 0.6293\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1137\n",
      "      Abusive       0.87      0.85      0.86      1008\n",
      "HS_Individual       0.71      0.74      0.73       729\n",
      "     HS_Group       0.71      0.61      0.66       408\n",
      "  HS_Religion       0.72      0.64      0.68       168\n",
      "      HS_Race       0.76      0.71      0.74       119\n",
      "  HS_Physical       1.00      0.05      0.10        57\n",
      "    HS_Gender       1.00      0.07      0.14        55\n",
      "     HS_Other       0.78      0.79      0.79       771\n",
      "      HS_Weak       0.68      0.72      0.70       681\n",
      "  HS_Moderate       0.64      0.53      0.58       359\n",
      "    HS_Strong       0.80      0.72      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5589\n",
      "    macro avg       0.79      0.61      0.63      5589\n",
      " weighted avg       0.78      0.74      0.75      5589\n",
      "  samples avg       0.43      0.42      0.41      5589\n",
      "\n",
      "Training completed in 124.19418501853943 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4581, Accuracy: 0.8601, F1 Micro: 0.4997, F1 Macro: 0.1955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3586, Accuracy: 0.8983, F1 Micro: 0.6876, F1 Macro: 0.5056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2992, Accuracy: 0.9099, F1 Micro: 0.7325, F1 Macro: 0.5676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2462, Accuracy: 0.9132, F1 Micro: 0.7419, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2025, Accuracy: 0.9144, F1 Micro: 0.745, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1678, Accuracy: 0.9138, F1 Micro: 0.7535, F1 Macro: 0.6257\n",
      "Epoch 7/10, Train Loss: 0.1337, Accuracy: 0.916, F1 Micro: 0.7492, F1 Macro: 0.6333\n",
      "Epoch 8/10, Train Loss: 0.1178, Accuracy: 0.9154, F1 Micro: 0.7471, F1 Macro: 0.651\n",
      "Epoch 9/10, Train Loss: 0.0974, Accuracy: 0.9156, F1 Micro: 0.7412, F1 Macro: 0.6472\n",
      "Epoch 10/10, Train Loss: 0.0811, Accuracy: 0.9102, F1 Micro: 0.748, F1 Macro: 0.663\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9138, F1 Micro: 0.7535, F1 Macro: 0.6257\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.83      1137\n",
      "      Abusive       0.82      0.88      0.85      1008\n",
      "HS_Individual       0.72      0.72      0.72       729\n",
      "     HS_Group       0.66      0.70      0.68       408\n",
      "  HS_Religion       0.76      0.55      0.64       168\n",
      "      HS_Race       0.77      0.74      0.75       119\n",
      "  HS_Physical       1.00      0.09      0.16        57\n",
      "    HS_Gender       0.67      0.04      0.07        55\n",
      "     HS_Other       0.73      0.81      0.77       771\n",
      "      HS_Weak       0.69      0.67      0.68       681\n",
      "  HS_Moderate       0.60      0.64      0.62       359\n",
      "    HS_Strong       0.82      0.66      0.73        97\n",
      "\n",
      "    micro avg       0.75      0.76      0.75      5589\n",
      "    macro avg       0.75      0.61      0.63      5589\n",
      " weighted avg       0.75      0.76      0.75      5589\n",
      "  samples avg       0.43      0.43      0.41      5589\n",
      "\n",
      "Training completed in 122.76857662200928 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4611, Accuracy: 0.8541, F1 Micro: 0.5373, F1 Macro: 0.2484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3662, Accuracy: 0.8987, F1 Micro: 0.6892, F1 Macro: 0.476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2992, Accuracy: 0.9098, F1 Micro: 0.7309, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2446, Accuracy: 0.9128, F1 Micro: 0.74, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1985, Accuracy: 0.9129, F1 Micro: 0.741, F1 Macro: 0.605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1633, Accuracy: 0.9147, F1 Micro: 0.7481, F1 Macro: 0.6471\n",
      "Epoch 7/10, Train Loss: 0.1365, Accuracy: 0.9185, F1 Micro: 0.7465, F1 Macro: 0.6445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1151, Accuracy: 0.915, F1 Micro: 0.7504, F1 Macro: 0.6558\n",
      "Epoch 9/10, Train Loss: 0.0958, Accuracy: 0.9161, F1 Micro: 0.7455, F1 Macro: 0.6568\n",
      "Epoch 10/10, Train Loss: 0.0779, Accuracy: 0.9145, F1 Micro: 0.747, F1 Macro: 0.6622\n",
      "Model 3 - Iteration 3335: Accuracy: 0.915, F1 Micro: 0.7504, F1 Macro: 0.6558\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1137\n",
      "      Abusive       0.84      0.90      0.87      1008\n",
      "HS_Individual       0.71      0.69      0.70       729\n",
      "     HS_Group       0.68      0.64      0.66       408\n",
      "  HS_Religion       0.75      0.61      0.68       168\n",
      "      HS_Race       0.74      0.71      0.72       119\n",
      "  HS_Physical       0.83      0.18      0.29        57\n",
      "    HS_Gender       0.73      0.20      0.31        55\n",
      "     HS_Other       0.79      0.75      0.77       771\n",
      "      HS_Weak       0.68      0.64      0.66       681\n",
      "  HS_Moderate       0.62      0.57      0.59       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5589\n",
      "    macro avg       0.75      0.63      0.66      5589\n",
      " weighted avg       0.76      0.74      0.74      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 124.37359738349915 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9065, F1 Micro: 0.717, F1 Macro: 0.5656\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 91.93676161766052 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4457, Accuracy: 0.8685, F1 Micro: 0.5568, F1 Macro: 0.26\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3467, Accuracy: 0.9079, F1 Micro: 0.7233, F1 Macro: 0.5393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2823, Accuracy: 0.9142, F1 Micro: 0.7243, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2336, Accuracy: 0.9167, F1 Micro: 0.7419, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1842, Accuracy: 0.9141, F1 Micro: 0.7544, F1 Macro: 0.6131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1492, Accuracy: 0.9182, F1 Micro: 0.7544, F1 Macro: 0.6431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1167, Accuracy: 0.9178, F1 Micro: 0.7608, F1 Macro: 0.636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9202, F1 Micro: 0.7627, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9227, F1 Micro: 0.7684, F1 Macro: 0.6747\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9218, F1 Micro: 0.7656, F1 Macro: 0.6885\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9227, F1 Micro: 0.7684, F1 Macro: 0.6747\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1137\n",
      "      Abusive       0.88      0.85      0.87      1008\n",
      "HS_Individual       0.73      0.74      0.73       729\n",
      "     HS_Group       0.74      0.57      0.65       408\n",
      "  HS_Religion       0.80      0.57      0.67       168\n",
      "      HS_Race       0.77      0.76      0.76       119\n",
      "  HS_Physical       0.87      0.23      0.36        57\n",
      "    HS_Gender       0.71      0.22      0.33        55\n",
      "     HS_Other       0.81      0.78      0.80       771\n",
      "      HS_Weak       0.71      0.72      0.71       681\n",
      "  HS_Moderate       0.69      0.49      0.57       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5589\n",
      "    macro avg       0.78      0.63      0.67      5589\n",
      " weighted avg       0.80      0.74      0.76      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 145.9232475757599 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4476, Accuracy: 0.875, F1 Micro: 0.5692, F1 Macro: 0.273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3421, Accuracy: 0.9068, F1 Micro: 0.7215, F1 Macro: 0.5375\n",
      "Epoch 3/10, Train Loss: 0.2808, Accuracy: 0.912, F1 Micro: 0.7108, F1 Macro: 0.5668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2345, Accuracy: 0.9158, F1 Micro: 0.7468, F1 Macro: 0.5946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1875, Accuracy: 0.9172, F1 Micro: 0.7612, F1 Macro: 0.6273\n",
      "Epoch 6/10, Train Loss: 0.1559, Accuracy: 0.9199, F1 Micro: 0.7609, F1 Macro: 0.6432\n",
      "Epoch 7/10, Train Loss: 0.1202, Accuracy: 0.9168, F1 Micro: 0.7565, F1 Macro: 0.6347\n",
      "Epoch 8/10, Train Loss: 0.1072, Accuracy: 0.9201, F1 Micro: 0.7513, F1 Macro: 0.6251\n",
      "Epoch 9/10, Train Loss: 0.0874, Accuracy: 0.9187, F1 Micro: 0.7544, F1 Macro: 0.6598\n",
      "Epoch 10/10, Train Loss: 0.0746, Accuracy: 0.92, F1 Micro: 0.7607, F1 Macro: 0.6752\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9172, F1 Micro: 0.7612, F1 Macro: 0.6273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1137\n",
      "      Abusive       0.87      0.86      0.86      1008\n",
      "HS_Individual       0.71      0.75      0.73       729\n",
      "     HS_Group       0.69      0.64      0.67       408\n",
      "  HS_Religion       0.71      0.68      0.69       168\n",
      "      HS_Race       0.71      0.80      0.75       119\n",
      "  HS_Physical       1.00      0.02      0.03        57\n",
      "    HS_Gender       0.80      0.07      0.13        55\n",
      "     HS_Other       0.77      0.80      0.78       771\n",
      "      HS_Weak       0.67      0.72      0.70       681\n",
      "  HS_Moderate       0.64      0.57      0.60       359\n",
      "    HS_Strong       0.78      0.69      0.73        97\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5589\n",
      "    macro avg       0.76      0.62      0.63      5589\n",
      " weighted avg       0.76      0.76      0.75      5589\n",
      "  samples avg       0.43      0.43      0.41      5589\n",
      "\n",
      "Training completed in 136.47885537147522 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4505, Accuracy: 0.8686, F1 Micro: 0.5279, F1 Macro: 0.2402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3459, Accuracy: 0.9054, F1 Micro: 0.717, F1 Macro: 0.5289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2781, Accuracy: 0.9155, F1 Micro: 0.7289, F1 Macro: 0.5821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2305, Accuracy: 0.9177, F1 Micro: 0.7468, F1 Macro: 0.6001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1848, Accuracy: 0.9168, F1 Micro: 0.7564, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9163, F1 Micro: 0.7603, F1 Macro: 0.6484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.9186, F1 Micro: 0.7634, F1 Macro: 0.6565\n",
      "Epoch 8/10, Train Loss: 0.1005, Accuracy: 0.9218, F1 Micro: 0.7591, F1 Macro: 0.6634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9186, F1 Micro: 0.7662, F1 Macro: 0.685\n",
      "Epoch 10/10, Train Loss: 0.0728, Accuracy: 0.9195, F1 Micro: 0.7626, F1 Macro: 0.6814\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9186, F1 Micro: 0.7662, F1 Macro: 0.685\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1137\n",
      "      Abusive       0.88      0.87      0.87      1008\n",
      "HS_Individual       0.69      0.75      0.72       729\n",
      "     HS_Group       0.70      0.62      0.66       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.75      0.76      0.75       119\n",
      "  HS_Physical       0.79      0.26      0.39        57\n",
      "    HS_Gender       0.64      0.29      0.40        55\n",
      "     HS_Other       0.76      0.82      0.79       771\n",
      "      HS_Weak       0.67      0.74      0.70       681\n",
      "  HS_Moderate       0.66      0.55      0.60       359\n",
      "    HS_Strong       0.82      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5589\n",
      "    macro avg       0.74      0.66      0.69      5589\n",
      " weighted avg       0.76      0.77      0.76      5589\n",
      "  samples avg       0.43      0.43      0.41      5589\n",
      "\n",
      "Training completed in 143.43730115890503 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9091, F1 Micro: 0.7266, F1 Macro: 0.5849\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 83.12953591346741 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4363, Accuracy: 0.8827, F1 Micro: 0.6086, F1 Macro: 0.3046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3308, Accuracy: 0.9081, F1 Micro: 0.6969, F1 Macro: 0.4793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2769, Accuracy: 0.9108, F1 Micro: 0.7522, F1 Macro: 0.5882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2193, Accuracy: 0.9195, F1 Micro: 0.7674, F1 Macro: 0.6257\n",
      "Epoch 5/10, Train Loss: 0.1792, Accuracy: 0.9146, F1 Micro: 0.7619, F1 Macro: 0.6175\n",
      "Epoch 6/10, Train Loss: 0.145, Accuracy: 0.9142, F1 Micro: 0.7608, F1 Macro: 0.644\n",
      "Epoch 7/10, Train Loss: 0.1142, Accuracy: 0.9206, F1 Micro: 0.7667, F1 Macro: 0.6356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0952, Accuracy: 0.9207, F1 Micro: 0.7706, F1 Macro: 0.6739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9247, F1 Micro: 0.7734, F1 Macro: 0.6875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9225, F1 Micro: 0.7742, F1 Macro: 0.669\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9225, F1 Micro: 0.7742, F1 Macro: 0.669\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1137\n",
      "      Abusive       0.87      0.89      0.88      1008\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.73      0.59      0.65       408\n",
      "  HS_Religion       0.82      0.58      0.68       168\n",
      "      HS_Race       0.76      0.66      0.71       119\n",
      "  HS_Physical       0.77      0.18      0.29        57\n",
      "    HS_Gender       0.76      0.24      0.36        55\n",
      "     HS_Other       0.77      0.84      0.80       771\n",
      "      HS_Weak       0.69      0.76      0.72       681\n",
      "  HS_Moderate       0.67      0.50      0.57       359\n",
      "    HS_Strong       0.79      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5589\n",
      "    macro avg       0.77      0.63      0.67      5589\n",
      " weighted avg       0.78      0.77      0.77      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 159.13934206962585 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4378, Accuracy: 0.8838, F1 Micro: 0.6229, F1 Macro: 0.299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3265, Accuracy: 0.908, F1 Micro: 0.7047, F1 Macro: 0.4835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2778, Accuracy: 0.9145, F1 Micro: 0.7565, F1 Macro: 0.6003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2227, Accuracy: 0.9193, F1 Micro: 0.7675, F1 Macro: 0.6388\n",
      "Epoch 5/10, Train Loss: 0.1845, Accuracy: 0.914, F1 Micro: 0.761, F1 Macro: 0.6284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1452, Accuracy: 0.921, F1 Micro: 0.7696, F1 Macro: 0.6686\n",
      "Epoch 7/10, Train Loss: 0.1164, Accuracy: 0.9164, F1 Micro: 0.7648, F1 Macro: 0.6485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9233, F1 Micro: 0.7707, F1 Macro: 0.6776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.083, Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6835\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9217, F1 Micro: 0.7718, F1 Macro: 0.6689\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9222, F1 Micro: 0.7729, F1 Macro: 0.6835\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1137\n",
      "      Abusive       0.87      0.89      0.88      1008\n",
      "HS_Individual       0.74      0.73      0.73       729\n",
      "     HS_Group       0.67      0.66      0.67       408\n",
      "  HS_Religion       0.73      0.64      0.68       168\n",
      "      HS_Race       0.72      0.74      0.73       119\n",
      "  HS_Physical       0.82      0.25      0.38        57\n",
      "    HS_Gender       0.79      0.27      0.41        55\n",
      "     HS_Other       0.80      0.81      0.81       771\n",
      "      HS_Weak       0.72      0.69      0.70       681\n",
      "  HS_Moderate       0.62      0.57      0.59       359\n",
      "    HS_Strong       0.75      0.78      0.77        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.76      0.66      0.68      5589\n",
      " weighted avg       0.78      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 158.82291173934937 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4425, Accuracy: 0.8834, F1 Micro: 0.6055, F1 Macro: 0.2936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3298, Accuracy: 0.907, F1 Micro: 0.6996, F1 Macro: 0.4785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2776, Accuracy: 0.9116, F1 Micro: 0.7524, F1 Macro: 0.6002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2193, Accuracy: 0.9217, F1 Micro: 0.7594, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.9143, F1 Micro: 0.7609, F1 Macro: 0.6313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1443, Accuracy: 0.9199, F1 Micro: 0.7642, F1 Macro: 0.6543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1149, Accuracy: 0.9201, F1 Micro: 0.7677, F1 Macro: 0.6672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9211, F1 Micro: 0.7693, F1 Macro: 0.6813\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9218, F1 Micro: 0.7643, F1 Macro: 0.6819\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9198, F1 Micro: 0.7655, F1 Macro: 0.6752\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9211, F1 Micro: 0.7693, F1 Macro: 0.6813\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1137\n",
      "      Abusive       0.89      0.87      0.88      1008\n",
      "HS_Individual       0.74      0.73      0.74       729\n",
      "     HS_Group       0.67      0.64      0.65       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.73      0.76      0.74       119\n",
      "  HS_Physical       0.87      0.23      0.36        57\n",
      "    HS_Gender       0.70      0.25      0.37        55\n",
      "     HS_Other       0.79      0.79      0.79       771\n",
      "      HS_Weak       0.71      0.71      0.71       681\n",
      "  HS_Moderate       0.61      0.55      0.58       359\n",
      "    HS_Strong       0.83      0.77      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.76      0.65      0.68      5589\n",
      " weighted avg       0.78      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 158.3232023715973 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9112, F1 Micro: 0.7342, F1 Macro: 0.6004\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 73.84336066246033 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4291, Accuracy: 0.8842, F1 Micro: 0.6162, F1 Macro: 0.3643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3242, Accuracy: 0.9073, F1 Micro: 0.6761, F1 Macro: 0.4801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2665, Accuracy: 0.9186, F1 Micro: 0.7557, F1 Macro: 0.5946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2139, Accuracy: 0.923, F1 Micro: 0.7586, F1 Macro: 0.6253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1724, Accuracy: 0.9223, F1 Micro: 0.7728, F1 Macro: 0.6526\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9216, F1 Micro: 0.7648, F1 Macro: 0.6555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9251, F1 Micro: 0.775, F1 Macro: 0.6715\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9196, F1 Micro: 0.7731, F1 Macro: 0.6869\n",
      "Epoch 9/10, Train Loss: 0.0786, Accuracy: 0.9203, F1 Micro: 0.7739, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9229, F1 Micro: 0.7758, F1 Macro: 0.7032\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9229, F1 Micro: 0.7758, F1 Macro: 0.7032\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1137\n",
      "      Abusive       0.87      0.89      0.88      1008\n",
      "HS_Individual       0.76      0.69      0.72       729\n",
      "     HS_Group       0.66      0.72      0.69       408\n",
      "  HS_Religion       0.76      0.67      0.72       168\n",
      "      HS_Race       0.78      0.81      0.79       119\n",
      "  HS_Physical       0.68      0.30      0.41        57\n",
      "    HS_Gender       0.58      0.38      0.46        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.73      0.67      0.70       681\n",
      "  HS_Moderate       0.61      0.67      0.63       359\n",
      "    HS_Strong       0.82      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5589\n",
      "    macro avg       0.74      0.68      0.70      5589\n",
      " weighted avg       0.78      0.77      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 173.08610606193542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4289, Accuracy: 0.8875, F1 Micro: 0.6244, F1 Macro: 0.3382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3226, Accuracy: 0.9084, F1 Micro: 0.6856, F1 Macro: 0.5032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2659, Accuracy: 0.9181, F1 Micro: 0.7557, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.218, Accuracy: 0.9218, F1 Micro: 0.7619, F1 Macro: 0.6314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1793, Accuracy: 0.9185, F1 Micro: 0.7684, F1 Macro: 0.6495\n",
      "Epoch 6/10, Train Loss: 0.14, Accuracy: 0.9226, F1 Micro: 0.7638, F1 Macro: 0.6474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1168, Accuracy: 0.9223, F1 Micro: 0.7702, F1 Macro: 0.6727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.9231, F1 Micro: 0.7749, F1 Macro: 0.6788\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9213, F1 Micro: 0.77, F1 Macro: 0.6908\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9204, F1 Micro: 0.7732, F1 Macro: 0.701\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9231, F1 Micro: 0.7749, F1 Macro: 0.6788\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.89      0.89      1008\n",
      "HS_Individual       0.75      0.72      0.74       729\n",
      "     HS_Group       0.65      0.69      0.67       408\n",
      "  HS_Religion       0.76      0.58      0.66       168\n",
      "      HS_Race       0.78      0.78      0.78       119\n",
      "  HS_Physical       0.60      0.11      0.18        57\n",
      "    HS_Gender       0.70      0.38      0.49        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.73      0.69      0.71       681\n",
      "  HS_Moderate       0.60      0.57      0.59       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5589\n",
      "    macro avg       0.74      0.66      0.68      5589\n",
      " weighted avg       0.78      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 172.53922176361084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4328, Accuracy: 0.8846, F1 Micro: 0.6065, F1 Macro: 0.3526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.325, Accuracy: 0.9051, F1 Micro: 0.6615, F1 Macro: 0.479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2637, Accuracy: 0.919, F1 Micro: 0.7577, F1 Macro: 0.606\n",
      "Epoch 4/10, Train Loss: 0.2128, Accuracy: 0.9216, F1 Micro: 0.7572, F1 Macro: 0.6455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1776, Accuracy: 0.9218, F1 Micro: 0.7728, F1 Macro: 0.6695\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9236, F1 Micro: 0.7671, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1133, Accuracy: 0.9238, F1 Micro: 0.7779, F1 Macro: 0.6878\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9241, F1 Micro: 0.7761, F1 Macro: 0.686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9244, F1 Micro: 0.7796, F1 Macro: 0.7015\n",
      "Epoch 10/10, Train Loss: 0.0655, Accuracy: 0.9189, F1 Micro: 0.7709, F1 Macro: 0.7012\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9244, F1 Micro: 0.7796, F1 Macro: 0.7015\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.68      0.65      0.66       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.79      0.74      0.76       119\n",
      "  HS_Physical       0.80      0.28      0.42        57\n",
      "    HS_Gender       0.65      0.40      0.49        55\n",
      "     HS_Other       0.78      0.83      0.80       771\n",
      "      HS_Weak       0.72      0.70      0.71       681\n",
      "  HS_Moderate       0.63      0.57      0.60       359\n",
      "    HS_Strong       0.79      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5589\n",
      "    macro avg       0.76      0.67      0.70      5589\n",
      " weighted avg       0.79      0.77      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 171.6315770149231 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.913, F1 Micro: 0.7403, F1 Macro: 0.6139\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 67.0762197971344 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4213, Accuracy: 0.8883, F1 Micro: 0.6529, F1 Macro: 0.3798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3204, Accuracy: 0.9139, F1 Micro: 0.7248, F1 Macro: 0.5374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2561, Accuracy: 0.9163, F1 Micro: 0.759, F1 Macro: 0.6102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2089, Accuracy: 0.925, F1 Micro: 0.7706, F1 Macro: 0.6352\n",
      "Epoch 5/10, Train Loss: 0.1659, Accuracy: 0.9233, F1 Micro: 0.7681, F1 Macro: 0.6779\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.9248, F1 Micro: 0.7706, F1 Macro: 0.6757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1094, Accuracy: 0.9228, F1 Micro: 0.7766, F1 Macro: 0.6993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9262, F1 Micro: 0.7829, F1 Macro: 0.6893\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9264, F1 Micro: 0.782, F1 Macro: 0.7084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9256, F1 Micro: 0.7847, F1 Macro: 0.7176\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9256, F1 Micro: 0.7847, F1 Macro: 0.7176\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.89      0.89      0.89      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.68      0.67      0.68       408\n",
      "  HS_Religion       0.77      0.61      0.68       168\n",
      "      HS_Race       0.73      0.82      0.77       119\n",
      "  HS_Physical       0.57      0.46      0.50        57\n",
      "    HS_Gender       0.64      0.45      0.53        55\n",
      "     HS_Other       0.80      0.81      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.64      0.59      0.62       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 186.45662379264832 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4234, Accuracy: 0.8897, F1 Micro: 0.67, F1 Macro: 0.4199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3205, Accuracy: 0.9138, F1 Micro: 0.72, F1 Macro: 0.5347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2531, Accuracy: 0.9154, F1 Micro: 0.7605, F1 Macro: 0.6193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2058, Accuracy: 0.9257, F1 Micro: 0.777, F1 Macro: 0.6389\n",
      "Epoch 5/10, Train Loss: 0.1699, Accuracy: 0.9236, F1 Micro: 0.7732, F1 Macro: 0.6653\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.923, F1 Micro: 0.7679, F1 Macro: 0.6791\n",
      "Epoch 7/10, Train Loss: 0.1086, Accuracy: 0.9227, F1 Micro: 0.7695, F1 Macro: 0.69\n",
      "Epoch 8/10, Train Loss: 0.0963, Accuracy: 0.9188, F1 Micro: 0.7724, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9246, F1 Micro: 0.7782, F1 Macro: 0.7023\n",
      "Epoch 10/10, Train Loss: 0.0681, Accuracy: 0.9204, F1 Micro: 0.7767, F1 Macro: 0.7106\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9246, F1 Micro: 0.7782, F1 Macro: 0.7023\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.74      0.71      0.73       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.74      0.69      0.72       168\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.74      0.30      0.42        57\n",
      "    HS_Gender       0.68      0.35      0.46        55\n",
      "     HS_Other       0.83      0.77      0.80       771\n",
      "      HS_Weak       0.72      0.69      0.70       681\n",
      "  HS_Moderate       0.64      0.60      0.62       359\n",
      "    HS_Strong       0.81      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5589\n",
      "    macro avg       0.76      0.67      0.70      5589\n",
      " weighted avg       0.79      0.76      0.77      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 182.44793224334717 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.426, Accuracy: 0.8904, F1 Micro: 0.666, F1 Macro: 0.4384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3196, Accuracy: 0.9134, F1 Micro: 0.7212, F1 Macro: 0.546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2547, Accuracy: 0.9175, F1 Micro: 0.7613, F1 Macro: 0.6152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2077, Accuracy: 0.9225, F1 Micro: 0.7713, F1 Macro: 0.649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1667, Accuracy: 0.9254, F1 Micro: 0.7739, F1 Macro: 0.6813\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9233, F1 Micro: 0.7686, F1 Macro: 0.69\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9264, F1 Micro: 0.7736, F1 Macro: 0.6944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9219, F1 Micro: 0.774, F1 Macro: 0.6862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9267, F1 Micro: 0.7823, F1 Macro: 0.7177\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9255, F1 Micro: 0.7753, F1 Macro: 0.7089\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9267, F1 Micro: 0.7823, F1 Macro: 0.7177\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1137\n",
      "      Abusive       0.90      0.88      0.89      1008\n",
      "HS_Individual       0.75      0.74      0.74       729\n",
      "     HS_Group       0.73      0.62      0.67       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.84      0.74      0.79       119\n",
      "  HS_Physical       0.74      0.35      0.48        57\n",
      "    HS_Gender       0.67      0.51      0.58        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.67      0.57      0.62       359\n",
      "    HS_Strong       0.83      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5589\n",
      "    macro avg       0.77      0.68      0.72      5589\n",
      " weighted avg       0.80      0.76      0.78      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 185.9786491394043 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9146, F1 Micro: 0.7455, F1 Macro: 0.6262\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 60.84578609466553 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4157, Accuracy: 0.8925, F1 Micro: 0.6742, F1 Macro: 0.4456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.305, Accuracy: 0.9147, F1 Micro: 0.7315, F1 Macro: 0.573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2487, Accuracy: 0.9226, F1 Micro: 0.7684, F1 Macro: 0.6205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1934, Accuracy: 0.9228, F1 Micro: 0.774, F1 Macro: 0.6387\n",
      "Epoch 5/10, Train Loss: 0.1565, Accuracy: 0.9244, F1 Micro: 0.771, F1 Macro: 0.6391\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.924, F1 Micro: 0.7736, F1 Macro: 0.6734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1005, Accuracy: 0.9235, F1 Micro: 0.7765, F1 Macro: 0.6966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9203, F1 Micro: 0.779, F1 Macro: 0.7069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.9247, F1 Micro: 0.7851, F1 Macro: 0.7121\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9238, F1 Micro: 0.7831, F1 Macro: 0.7201\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9247, F1 Micro: 0.7851, F1 Macro: 0.7121\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.86      0.91      0.89      1008\n",
      "HS_Individual       0.71      0.80      0.75       729\n",
      "     HS_Group       0.73      0.61      0.66       408\n",
      "  HS_Religion       0.71      0.71      0.71       168\n",
      "      HS_Race       0.74      0.75      0.74       119\n",
      "  HS_Physical       0.64      0.32      0.42        57\n",
      "    HS_Gender       0.74      0.45      0.56        55\n",
      "     HS_Other       0.79      0.82      0.81       771\n",
      "      HS_Weak       0.69      0.78      0.73       681\n",
      "  HS_Moderate       0.68      0.53      0.59       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.77      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 198.17348098754883 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4174, Accuracy: 0.8945, F1 Micro: 0.6796, F1 Macro: 0.4622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3064, Accuracy: 0.9162, F1 Micro: 0.7362, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2477, Accuracy: 0.9207, F1 Micro: 0.7674, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1964, Accuracy: 0.9228, F1 Micro: 0.772, F1 Macro: 0.6265\n",
      "Epoch 5/10, Train Loss: 0.158, Accuracy: 0.9225, F1 Micro: 0.7712, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1313, Accuracy: 0.9218, F1 Micro: 0.7741, F1 Macro: 0.6701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9243, F1 Micro: 0.7757, F1 Macro: 0.6995\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9184, F1 Micro: 0.7738, F1 Macro: 0.6904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9214, F1 Micro: 0.7761, F1 Macro: 0.707\n",
      "Epoch 10/10, Train Loss: 0.0598, Accuracy: 0.9206, F1 Micro: 0.7751, F1 Macro: 0.7084\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9214, F1 Micro: 0.7761, F1 Macro: 0.707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1137\n",
      "      Abusive       0.87      0.91      0.89      1008\n",
      "HS_Individual       0.73      0.75      0.74       729\n",
      "     HS_Group       0.66      0.67      0.66       408\n",
      "  HS_Religion       0.70      0.71      0.71       168\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.69      0.35      0.47        57\n",
      "    HS_Gender       0.77      0.42      0.54        55\n",
      "     HS_Other       0.79      0.82      0.80       771\n",
      "      HS_Weak       0.70      0.73      0.71       681\n",
      "  HS_Moderate       0.60      0.58      0.59       359\n",
      "    HS_Strong       0.77      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5589\n",
      "    macro avg       0.73      0.70      0.71      5589\n",
      " weighted avg       0.76      0.79      0.77      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 198.1238226890564 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4199, Accuracy: 0.8906, F1 Micro: 0.6771, F1 Macro: 0.447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3067, Accuracy: 0.9165, F1 Micro: 0.7386, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.246, Accuracy: 0.9212, F1 Micro: 0.7653, F1 Macro: 0.6194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1928, Accuracy: 0.9225, F1 Micro: 0.7747, F1 Macro: 0.6462\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9244, F1 Micro: 0.7702, F1 Macro: 0.6409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1285, Accuracy: 0.9263, F1 Micro: 0.7778, F1 Macro: 0.7019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.101, Accuracy: 0.9261, F1 Micro: 0.7815, F1 Macro: 0.7081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.922, F1 Micro: 0.7815, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9254, F1 Micro: 0.7842, F1 Macro: 0.7197\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9242, F1 Micro: 0.7787, F1 Macro: 0.7124\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9254, F1 Micro: 0.7842, F1 Macro: 0.7197\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.86      0.92      0.89      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.76      0.78      0.77       119\n",
      "  HS_Physical       0.88      0.37      0.52        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.65      0.59      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 200.70169854164124 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9156, F1 Micro: 0.7495, F1 Macro: 0.6358\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 54.39567828178406 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4128, Accuracy: 0.8973, F1 Micro: 0.6724, F1 Macro: 0.4276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3049, Accuracy: 0.9177, F1 Micro: 0.7481, F1 Macro: 0.578\n",
      "Epoch 3/10, Train Loss: 0.2417, Accuracy: 0.9204, F1 Micro: 0.7451, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1973, Accuracy: 0.9233, F1 Micro: 0.7643, F1 Macro: 0.6271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.9256, F1 Micro: 0.7782, F1 Macro: 0.6732\n",
      "Epoch 6/10, Train Loss: 0.1222, Accuracy: 0.9223, F1 Micro: 0.7705, F1 Macro: 0.665\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9247, F1 Micro: 0.7685, F1 Macro: 0.6779\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9251, F1 Micro: 0.7713, F1 Macro: 0.6903\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9264, F1 Micro: 0.773, F1 Macro: 0.7014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.9277, F1 Micro: 0.7887, F1 Macro: 0.7191\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9277, F1 Micro: 0.7887, F1 Macro: 0.7191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.88      0.89      1008\n",
      "HS_Individual       0.74      0.77      0.75       729\n",
      "     HS_Group       0.73      0.65      0.68       408\n",
      "  HS_Religion       0.77      0.65      0.70       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.56      0.35      0.43        57\n",
      "    HS_Gender       0.74      0.45      0.56        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.55      0.61       359\n",
      "    HS_Strong       0.82      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 201.06664490699768 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4142, Accuracy: 0.8991, F1 Micro: 0.6869, F1 Macro: 0.4926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3023, Accuracy: 0.918, F1 Micro: 0.7466, F1 Macro: 0.5785\n",
      "Epoch 3/10, Train Loss: 0.24, Accuracy: 0.9184, F1 Micro: 0.7352, F1 Macro: 0.5726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.198, Accuracy: 0.9237, F1 Micro: 0.7626, F1 Macro: 0.6323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1623, Accuracy: 0.9248, F1 Micro: 0.7747, F1 Macro: 0.6773\n",
      "Epoch 6/10, Train Loss: 0.1265, Accuracy: 0.9214, F1 Micro: 0.7719, F1 Macro: 0.6636\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.924, F1 Micro: 0.771, F1 Macro: 0.6879\n",
      "Epoch 8/10, Train Loss: 0.0833, Accuracy: 0.9235, F1 Micro: 0.7729, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9235, F1 Micro: 0.7795, F1 Macro: 0.7118\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9253, F1 Micro: 0.7763, F1 Macro: 0.7095\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9235, F1 Micro: 0.7795, F1 Macro: 0.7118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.73      0.74      0.74       729\n",
      "     HS_Group       0.69      0.64      0.66       408\n",
      "  HS_Religion       0.76      0.67      0.71       168\n",
      "      HS_Race       0.72      0.82      0.76       119\n",
      "  HS_Physical       0.72      0.37      0.49        57\n",
      "    HS_Gender       0.72      0.42      0.53        55\n",
      "     HS_Other       0.80      0.81      0.80       771\n",
      "      HS_Weak       0.70      0.72      0.71       681\n",
      "  HS_Moderate       0.63      0.57      0.60       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 200.9692633152008 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4167, Accuracy: 0.8974, F1 Micro: 0.6739, F1 Macro: 0.4663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3051, Accuracy: 0.9167, F1 Micro: 0.7468, F1 Macro: 0.5845\n",
      "Epoch 3/10, Train Loss: 0.2408, Accuracy: 0.9209, F1 Micro: 0.7434, F1 Macro: 0.5769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9247, F1 Micro: 0.7646, F1 Macro: 0.6335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.9256, F1 Micro: 0.7798, F1 Macro: 0.6847\n",
      "Epoch 6/10, Train Loss: 0.1196, Accuracy: 0.9217, F1 Micro: 0.768, F1 Macro: 0.6889\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9245, F1 Micro: 0.7739, F1 Macro: 0.7\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9237, F1 Micro: 0.7707, F1 Macro: 0.7055\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9213, F1 Micro: 0.7792, F1 Macro: 0.7179\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.9247, F1 Micro: 0.7779, F1 Macro: 0.7107\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9256, F1 Micro: 0.7798, F1 Macro: 0.6847\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1137\n",
      "      Abusive       0.89      0.89      0.89      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.74      0.62      0.67       408\n",
      "  HS_Religion       0.76      0.60      0.67       168\n",
      "      HS_Race       0.74      0.76      0.75       119\n",
      "  HS_Physical       0.52      0.21      0.30        57\n",
      "    HS_Gender       0.75      0.27      0.40        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.69      0.52      0.59       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5589\n",
      "    macro avg       0.75      0.65      0.68      5589\n",
      " weighted avg       0.80      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 199.60269832611084 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9166, F1 Micro: 0.7528, F1 Macro: 0.6428\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 50.95392155647278 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4034, Accuracy: 0.8872, F1 Micro: 0.6883, F1 Macro: 0.4718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2958, Accuracy: 0.9134, F1 Micro: 0.7107, F1 Macro: 0.5396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.237, Accuracy: 0.9204, F1 Micro: 0.7695, F1 Macro: 0.621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.9273, F1 Micro: 0.7833, F1 Macro: 0.6501\n",
      "Epoch 5/10, Train Loss: 0.1583, Accuracy: 0.925, F1 Micro: 0.7813, F1 Macro: 0.6695\n",
      "Epoch 6/10, Train Loss: 0.1192, Accuracy: 0.9253, F1 Micro: 0.7819, F1 Macro: 0.7022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9255, F1 Micro: 0.7845, F1 Macro: 0.6887\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.9235, F1 Micro: 0.7764, F1 Macro: 0.7101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9278, F1 Micro: 0.7856, F1 Macro: 0.7084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.9267, F1 Micro: 0.7885, F1 Macro: 0.7175\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9267, F1 Micro: 0.7885, F1 Macro: 0.7175\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.72      0.62      0.66       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.63      0.33      0.44        57\n",
      "    HS_Gender       0.71      0.45      0.56        55\n",
      "     HS_Other       0.80      0.84      0.81       771\n",
      "      HS_Weak       0.70      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 216.04071140289307 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4038, Accuracy: 0.8939, F1 Micro: 0.6966, F1 Macro: 0.4781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2942, Accuracy: 0.9167, F1 Micro: 0.731, F1 Macro: 0.5655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2382, Accuracy: 0.9226, F1 Micro: 0.7723, F1 Macro: 0.6207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1953, Accuracy: 0.9263, F1 Micro: 0.7785, F1 Macro: 0.6565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9249, F1 Micro: 0.7795, F1 Macro: 0.6718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.9238, F1 Micro: 0.78, F1 Macro: 0.7026\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.9203, F1 Micro: 0.7762, F1 Macro: 0.6967\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9241, F1 Micro: 0.7657, F1 Macro: 0.6982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0666, Accuracy: 0.9264, F1 Micro: 0.7843, F1 Macro: 0.7121\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.925, F1 Micro: 0.7809, F1 Macro: 0.7103\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9264, F1 Micro: 0.7843, F1 Macro: 0.7121\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.74      0.59      0.66       408\n",
      "  HS_Religion       0.72      0.64      0.68       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.69      0.42      0.52        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.71      0.74      0.73       681\n",
      "  HS_Moderate       0.70      0.48      0.57       359\n",
      "    HS_Strong       0.73      0.87      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.79      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 216.2699408531189 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4065, Accuracy: 0.8829, F1 Micro: 0.6854, F1 Macro: 0.4805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2946, Accuracy: 0.9159, F1 Micro: 0.7221, F1 Macro: 0.5524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2356, Accuracy: 0.9231, F1 Micro: 0.7702, F1 Macro: 0.6211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.9255, F1 Micro: 0.7768, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1546, Accuracy: 0.9245, F1 Micro: 0.779, F1 Macro: 0.6701\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9229, F1 Micro: 0.7758, F1 Macro: 0.7049\n",
      "Epoch 7/10, Train Loss: 0.0969, Accuracy: 0.9242, F1 Micro: 0.7769, F1 Macro: 0.7\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.9241, F1 Micro: 0.7683, F1 Macro: 0.7082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9227, F1 Micro: 0.7811, F1 Macro: 0.7139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9227, F1 Micro: 0.7819, F1 Macro: 0.721\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9227, F1 Micro: 0.7819, F1 Macro: 0.721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.72      0.75      0.73       729\n",
      "     HS_Group       0.66      0.71      0.68       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.69      0.83      0.75       119\n",
      "  HS_Physical       0.70      0.37      0.48        57\n",
      "    HS_Gender       0.70      0.51      0.59        55\n",
      "     HS_Other       0.77      0.84      0.81       771\n",
      "      HS_Weak       0.69      0.72      0.71       681\n",
      "  HS_Moderate       0.62      0.64      0.63       359\n",
      "    HS_Strong       0.77      0.85      0.81        97\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5589\n",
      "    macro avg       0.73      0.73      0.72      5589\n",
      " weighted avg       0.76      0.80      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 215.433917760849 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9174, F1 Micro: 0.7557, F1 Macro: 0.6495\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 46.42658352851868 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3967, Accuracy: 0.9015, F1 Micro: 0.6622, F1 Macro: 0.4189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2893, Accuracy: 0.916, F1 Micro: 0.7322, F1 Macro: 0.5529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2307, Accuracy: 0.9213, F1 Micro: 0.7722, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1856, Accuracy: 0.9264, F1 Micro: 0.783, F1 Macro: 0.6536\n",
      "Epoch 5/10, Train Loss: 0.148, Accuracy: 0.9261, F1 Micro: 0.775, F1 Macro: 0.6797\n",
      "Epoch 6/10, Train Loss: 0.1181, Accuracy: 0.9258, F1 Micro: 0.7776, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0975, Accuracy: 0.9281, F1 Micro: 0.784, F1 Macro: 0.6903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9284, F1 Micro: 0.7884, F1 Macro: 0.7161\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9236, F1 Micro: 0.7802, F1 Macro: 0.7071\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9265, F1 Micro: 0.7721, F1 Macro: 0.716\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9284, F1 Micro: 0.7884, F1 Macro: 0.7161\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.75      0.59      0.66       408\n",
      "  HS_Religion       0.77      0.68      0.72       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.62      0.37      0.46        57\n",
      "    HS_Gender       0.74      0.42      0.53        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.69      0.53      0.60       359\n",
      "    HS_Strong       0.82      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.68      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 225.25310492515564 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3979, Accuracy: 0.9016, F1 Micro: 0.6623, F1 Macro: 0.4392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2891, Accuracy: 0.9176, F1 Micro: 0.7375, F1 Macro: 0.5688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2312, Accuracy: 0.9228, F1 Micro: 0.7778, F1 Macro: 0.6427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.187, Accuracy: 0.9255, F1 Micro: 0.7806, F1 Macro: 0.6492\n",
      "Epoch 5/10, Train Loss: 0.1516, Accuracy: 0.9233, F1 Micro: 0.7767, F1 Macro: 0.6795\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.9254, F1 Micro: 0.779, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0974, Accuracy: 0.9261, F1 Micro: 0.784, F1 Macro: 0.692\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9245, F1 Micro: 0.7816, F1 Macro: 0.7054\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.9266, F1 Micro: 0.7831, F1 Macro: 0.7109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9275, F1 Micro: 0.7853, F1 Macro: 0.719\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9275, F1 Micro: 0.7853, F1 Macro: 0.719\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.70      0.63      0.66       408\n",
      "  HS_Religion       0.74      0.61      0.67       168\n",
      "      HS_Race       0.73      0.78      0.76       119\n",
      "  HS_Physical       0.61      0.49      0.54        57\n",
      "    HS_Gender       0.71      0.44      0.54        55\n",
      "     HS_Other       0.84      0.79      0.82       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.66      0.56      0.60       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.75      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.43      0.43      0.42      5589\n",
      "\n",
      "Training completed in 225.19029235839844 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4003, Accuracy: 0.9, F1 Micro: 0.659, F1 Macro: 0.4426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2883, Accuracy: 0.9181, F1 Micro: 0.7382, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2282, Accuracy: 0.921, F1 Micro: 0.7694, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9248, F1 Micro: 0.7729, F1 Macro: 0.6486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1451, Accuracy: 0.9268, F1 Micro: 0.7801, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.926, F1 Micro: 0.7811, F1 Macro: 0.6899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0947, Accuracy: 0.9266, F1 Micro: 0.7867, F1 Macro: 0.7042\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.9257, F1 Micro: 0.7856, F1 Macro: 0.7194\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.9286, F1 Micro: 0.7853, F1 Macro: 0.722\n",
      "Epoch 10/10, Train Loss: 0.0563, Accuracy: 0.9261, F1 Micro: 0.7819, F1 Macro: 0.7118\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9266, F1 Micro: 0.7867, F1 Macro: 0.7042\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.89      0.89      1008\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.72      0.65      0.68       408\n",
      "  HS_Religion       0.81      0.64      0.72       168\n",
      "      HS_Race       0.77      0.74      0.76       119\n",
      "  HS_Physical       0.84      0.28      0.42        57\n",
      "    HS_Gender       0.70      0.29      0.41        55\n",
      "     HS_Other       0.78      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.66      0.57      0.61       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.77      0.67      0.70      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 225.0038845539093 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9182, F1 Micro: 0.7583, F1 Macro: 0.6548\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 41.60092520713806 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.391, Accuracy: 0.9004, F1 Micro: 0.693, F1 Macro: 0.4427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2805, Accuracy: 0.9197, F1 Micro: 0.7562, F1 Macro: 0.5861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2214, Accuracy: 0.9211, F1 Micro: 0.7723, F1 Macro: 0.6124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1802, Accuracy: 0.927, F1 Micro: 0.7764, F1 Macro: 0.6569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.9264, F1 Micro: 0.7858, F1 Macro: 0.7019\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9272, F1 Micro: 0.7762, F1 Macro: 0.6892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0912, Accuracy: 0.9281, F1 Micro: 0.7904, F1 Macro: 0.7188\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.9276, F1 Micro: 0.7856, F1 Macro: 0.7063\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9288, F1 Micro: 0.7892, F1 Macro: 0.7137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9294, F1 Micro: 0.794, F1 Macro: 0.7306\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9294, F1 Micro: 0.794, F1 Macro: 0.7306\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.70      0.67      0.68       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.79      0.80      0.79       119\n",
      "  HS_Physical       0.61      0.44      0.51        57\n",
      "    HS_Gender       0.64      0.51      0.57        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.75      0.70      0.73       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 231.84966206550598 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3919, Accuracy: 0.9005, F1 Micro: 0.6997, F1 Macro: 0.4585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2799, Accuracy: 0.919, F1 Micro: 0.7567, F1 Macro: 0.5975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2243, Accuracy: 0.9194, F1 Micro: 0.7685, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1827, Accuracy: 0.9275, F1 Micro: 0.775, F1 Macro: 0.6459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9258, F1 Micro: 0.7779, F1 Macro: 0.6794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9284, F1 Micro: 0.784, F1 Macro: 0.6913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.926, F1 Micro: 0.785, F1 Macro: 0.7077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0761, Accuracy: 0.9271, F1 Micro: 0.7883, F1 Macro: 0.7247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9275, F1 Micro: 0.7883, F1 Macro: 0.7205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9285, F1 Micro: 0.7903, F1 Macro: 0.7273\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9285, F1 Micro: 0.7903, F1 Macro: 0.7273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.78      0.71      0.75       729\n",
      "     HS_Group       0.67      0.72      0.69       408\n",
      "  HS_Religion       0.76      0.67      0.71       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.63      0.46      0.53        57\n",
      "    HS_Gender       0.58      0.47      0.52        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.64      0.65      0.64       359\n",
      "    HS_Strong       0.78      0.87      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 236.2115797996521 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.395, Accuracy: 0.8994, F1 Micro: 0.6991, F1 Macro: 0.4646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9198, F1 Micro: 0.7562, F1 Macro: 0.5922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2223, Accuracy: 0.9222, F1 Micro: 0.7715, F1 Macro: 0.6376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.179, Accuracy: 0.927, F1 Micro: 0.7748, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.143, Accuracy: 0.9247, F1 Micro: 0.7835, F1 Macro: 0.7033\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.9263, F1 Micro: 0.7741, F1 Macro: 0.7019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.927, F1 Micro: 0.7862, F1 Macro: 0.7153\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9274, F1 Micro: 0.7852, F1 Macro: 0.7137\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.9272, F1 Micro: 0.7845, F1 Macro: 0.7172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0545, Accuracy: 0.9279, F1 Micro: 0.7895, F1 Macro: 0.722\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9279, F1 Micro: 0.7895, F1 Macro: 0.722\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.77      0.74      0.75       729\n",
      "     HS_Group       0.70      0.69      0.70       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.72      0.74      0.73       119\n",
      "  HS_Physical       0.63      0.39      0.48        57\n",
      "    HS_Gender       0.63      0.49      0.55        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.73       681\n",
      "  HS_Moderate       0.63      0.61      0.62       359\n",
      "    HS_Strong       0.81      0.86      0.83        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.44      0.43      5589\n",
      "\n",
      "Training completed in 233.08818292617798 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.919, F1 Micro: 0.7609, F1 Macro: 0.6603\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 37.53829836845398 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.9032, F1 Micro: 0.6999, F1 Macro: 0.4775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2763, Accuracy: 0.9177, F1 Micro: 0.756, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2189, Accuracy: 0.9217, F1 Micro: 0.7752, F1 Macro: 0.6332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1738, Accuracy: 0.9261, F1 Micro: 0.7782, F1 Macro: 0.6421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1409, Accuracy: 0.927, F1 Micro: 0.7832, F1 Macro: 0.6564\n",
      "Epoch 6/10, Train Loss: 0.1135, Accuracy: 0.9265, F1 Micro: 0.7822, F1 Macro: 0.6813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.931, F1 Micro: 0.7943, F1 Macro: 0.7097\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9273, F1 Micro: 0.7888, F1 Macro: 0.6969\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9268, F1 Micro: 0.7883, F1 Macro: 0.7228\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.9288, F1 Micro: 0.7902, F1 Macro: 0.7194\n",
      "Model 1 - Iteration 7901: Accuracy: 0.931, F1 Micro: 0.7943, F1 Macro: 0.7097\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.77      0.75      0.76       729\n",
      "     HS_Group       0.75      0.65      0.70       408\n",
      "  HS_Religion       0.79      0.61      0.69       168\n",
      "      HS_Race       0.76      0.72      0.74       119\n",
      "  HS_Physical       0.76      0.28      0.41        57\n",
      "    HS_Gender       0.78      0.33      0.46        55\n",
      "     HS_Other       0.83      0.82      0.82       771\n",
      "      HS_Weak       0.76      0.71      0.73       681\n",
      "  HS_Moderate       0.72      0.57      0.63       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.82      0.77      0.79      5589\n",
      "    macro avg       0.79      0.67      0.71      5589\n",
      " weighted avg       0.82      0.77      0.79      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 235.6322102546692 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3858, Accuracy: 0.9049, F1 Micro: 0.7042, F1 Macro: 0.4887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2733, Accuracy: 0.9173, F1 Micro: 0.7609, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2189, Accuracy: 0.9238, F1 Micro: 0.7802, F1 Macro: 0.6549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9275, F1 Micro: 0.7836, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9285, F1 Micro: 0.7859, F1 Macro: 0.6607\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9256, F1 Micro: 0.7743, F1 Macro: 0.6809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.9269, F1 Micro: 0.7873, F1 Macro: 0.705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.9288, F1 Micro: 0.7901, F1 Macro: 0.7077\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9275, F1 Micro: 0.7831, F1 Macro: 0.7167\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9282, F1 Micro: 0.7901, F1 Macro: 0.7267\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9288, F1 Micro: 0.7901, F1 Macro: 0.7077\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.72      0.67      0.69       408\n",
      "  HS_Religion       0.78      0.66      0.71       168\n",
      "      HS_Race       0.71      0.80      0.75       119\n",
      "  HS_Physical       0.71      0.30      0.42        57\n",
      "    HS_Gender       0.69      0.33      0.44        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.67      0.58      0.62       359\n",
      "    HS_Strong       0.80      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.80      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 238.30602931976318 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3906, Accuracy: 0.9017, F1 Micro: 0.7028, F1 Macro: 0.5041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2751, Accuracy: 0.9164, F1 Micro: 0.7583, F1 Macro: 0.6074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2204, Accuracy: 0.9226, F1 Micro: 0.7784, F1 Macro: 0.665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1745, Accuracy: 0.9269, F1 Micro: 0.7826, F1 Macro: 0.6583\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9274, F1 Micro: 0.7767, F1 Macro: 0.6646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.9294, F1 Micro: 0.7872, F1 Macro: 0.7049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9276, F1 Micro: 0.7876, F1 Macro: 0.7153\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9273, F1 Micro: 0.7778, F1 Macro: 0.6955\n",
      "Epoch 9/10, Train Loss: 0.0642, Accuracy: 0.926, F1 Micro: 0.7847, F1 Macro: 0.7193\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9265, F1 Micro: 0.7838, F1 Macro: 0.7144\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9276, F1 Micro: 0.7876, F1 Macro: 0.7153\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.70      0.68      0.69       408\n",
      "  HS_Religion       0.73      0.72      0.72       168\n",
      "      HS_Race       0.75      0.76      0.76       119\n",
      "  HS_Physical       0.81      0.30      0.44        57\n",
      "    HS_Gender       0.75      0.38      0.51        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.68      0.71       681\n",
      "  HS_Moderate       0.65      0.61      0.63       359\n",
      "    HS_Strong       0.79      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 236.1047022342682 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9198, F1 Micro: 0.763, F1 Macro: 0.6639\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 34.34003281593323 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3859, Accuracy: 0.9028, F1 Micro: 0.6901, F1 Macro: 0.4574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2747, Accuracy: 0.919, F1 Micro: 0.7613, F1 Macro: 0.5912\n",
      "Epoch 3/10, Train Loss: 0.2159, Accuracy: 0.9231, F1 Micro: 0.754, F1 Macro: 0.5962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1744, Accuracy: 0.9225, F1 Micro: 0.7782, F1 Macro: 0.6541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.9254, F1 Micro: 0.7822, F1 Macro: 0.6746\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9268, F1 Micro: 0.7778, F1 Macro: 0.6893\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9252, F1 Micro: 0.7789, F1 Macro: 0.6995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0757, Accuracy: 0.9286, F1 Micro: 0.7878, F1 Macro: 0.7172\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.9272, F1 Micro: 0.7858, F1 Macro: 0.7151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.927, F1 Micro: 0.7892, F1 Macro: 0.7262\n",
      "Model 1 - Iteration 8165: Accuracy: 0.927, F1 Micro: 0.7892, F1 Macro: 0.7262\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.76      0.71      0.74       729\n",
      "     HS_Group       0.67      0.72      0.69       408\n",
      "  HS_Religion       0.72      0.67      0.69       168\n",
      "      HS_Race       0.72      0.82      0.76       119\n",
      "  HS_Physical       0.57      0.51      0.54        57\n",
      "    HS_Gender       0.69      0.45      0.55        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.74      0.70      0.72       681\n",
      "  HS_Moderate       0.61      0.64      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 242.58544993400574 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3847, Accuracy: 0.9036, F1 Micro: 0.6903, F1 Macro: 0.4665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.272, Accuracy: 0.9195, F1 Micro: 0.7623, F1 Macro: 0.601\n",
      "Epoch 3/10, Train Loss: 0.2173, Accuracy: 0.9195, F1 Micro: 0.7332, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1745, Accuracy: 0.9207, F1 Micro: 0.7777, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1431, Accuracy: 0.9267, F1 Micro: 0.7824, F1 Macro: 0.6726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.113, Accuracy: 0.9289, F1 Micro: 0.7869, F1 Macro: 0.7049\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9248, F1 Micro: 0.78, F1 Macro: 0.6941\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9254, F1 Micro: 0.7812, F1 Macro: 0.7014\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9252, F1 Micro: 0.7828, F1 Macro: 0.7158\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9231, F1 Micro: 0.7797, F1 Macro: 0.7141\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9289, F1 Micro: 0.7869, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.73      0.64      0.68       408\n",
      "  HS_Religion       0.79      0.61      0.69       168\n",
      "      HS_Race       0.81      0.69      0.75       119\n",
      "  HS_Physical       0.69      0.32      0.43        57\n",
      "    HS_Gender       0.69      0.33      0.44        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.72       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.78      0.66      0.70      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 240.3809196949005 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3879, Accuracy: 0.9031, F1 Micro: 0.6899, F1 Macro: 0.4713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2715, Accuracy: 0.9181, F1 Micro: 0.7597, F1 Macro: 0.604\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9218, F1 Micro: 0.7412, F1 Macro: 0.5901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1698, Accuracy: 0.9219, F1 Micro: 0.7782, F1 Macro: 0.6712\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.9232, F1 Micro: 0.7704, F1 Macro: 0.6833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9284, F1 Micro: 0.7846, F1 Macro: 0.7073\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9255, F1 Micro: 0.7732, F1 Macro: 0.6973\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9257, F1 Micro: 0.7827, F1 Macro: 0.7162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9265, F1 Micro: 0.7853, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9271, F1 Micro: 0.7826, F1 Macro: 0.7227\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9265, F1 Micro: 0.7853, F1 Macro: 0.7204\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.71      0.68      0.69       408\n",
      "  HS_Religion       0.76      0.62      0.68       168\n",
      "      HS_Race       0.76      0.74      0.75       119\n",
      "  HS_Physical       0.60      0.46      0.52        57\n",
      "    HS_Gender       0.61      0.49      0.55        55\n",
      "     HS_Other       0.81      0.79      0.80       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.64      0.61      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.74      0.70      0.72      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 241.8439130783081 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9203, F1 Micro: 0.7646, F1 Macro: 0.6675\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 31.023977756500244 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.376, Accuracy: 0.9036, F1 Micro: 0.7046, F1 Macro: 0.4751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9199, F1 Micro: 0.7605, F1 Macro: 0.5981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2138, Accuracy: 0.9235, F1 Micro: 0.7734, F1 Macro: 0.6062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1673, Accuracy: 0.9271, F1 Micro: 0.7834, F1 Macro: 0.6562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1352, Accuracy: 0.9282, F1 Micro: 0.7878, F1 Macro: 0.699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9281, F1 Micro: 0.7896, F1 Macro: 0.7089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9282, F1 Micro: 0.7918, F1 Macro: 0.7192\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9277, F1 Micro: 0.7865, F1 Macro: 0.7123\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9266, F1 Micro: 0.7841, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9315, F1 Micro: 0.7983, F1 Macro: 0.7305\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9315, F1 Micro: 0.7983, F1 Macro: 0.7305\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.87      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.80      0.73      0.76       729\n",
      "     HS_Group       0.70      0.69      0.69       408\n",
      "  HS_Religion       0.77      0.67      0.72       168\n",
      "      HS_Race       0.79      0.74      0.77       119\n",
      "  HS_Physical       0.65      0.39      0.48        57\n",
      "    HS_Gender       0.68      0.47      0.56        55\n",
      "     HS_Other       0.84      0.80      0.82       771\n",
      "      HS_Weak       0.77      0.70      0.73       681\n",
      "  HS_Moderate       0.65      0.64      0.65       359\n",
      "    HS_Strong       0.82      0.80      0.81        97\n",
      "\n",
      "    micro avg       0.82      0.78      0.80      5589\n",
      "    macro avg       0.77      0.70      0.73      5589\n",
      " weighted avg       0.81      0.78      0.80      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 251.74331784248352 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3759, Accuracy: 0.9046, F1 Micro: 0.7107, F1 Macro: 0.5107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2606, Accuracy: 0.92, F1 Micro: 0.7599, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2134, Accuracy: 0.9237, F1 Micro: 0.7758, F1 Macro: 0.6274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9258, F1 Micro: 0.7814, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9284, F1 Micro: 0.7838, F1 Macro: 0.6853\n",
      "Epoch 6/10, Train Loss: 0.1036, Accuracy: 0.9284, F1 Micro: 0.7825, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.9282, F1 Micro: 0.787, F1 Macro: 0.7077\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9257, F1 Micro: 0.7822, F1 Macro: 0.7075\n",
      "Epoch 9/10, Train Loss: 0.0628, Accuracy: 0.9293, F1 Micro: 0.7866, F1 Macro: 0.7187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.926, F1 Micro: 0.7871, F1 Macro: 0.7295\n",
      "Model 2 - Iteration 8402: Accuracy: 0.926, F1 Micro: 0.7871, F1 Macro: 0.7295\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.75      0.72      0.74       729\n",
      "     HS_Group       0.67      0.70      0.69       408\n",
      "  HS_Religion       0.70      0.68      0.69       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.64      0.47      0.55        57\n",
      "    HS_Gender       0.67      0.55      0.60        55\n",
      "     HS_Other       0.81      0.80      0.80       771\n",
      "      HS_Weak       0.73      0.70      0.72       681\n",
      "  HS_Moderate       0.62      0.65      0.63       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.78      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 250.72281074523926 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3805, Accuracy: 0.9028, F1 Micro: 0.7044, F1 Macro: 0.4888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.9195, F1 Micro: 0.7571, F1 Macro: 0.5919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2117, Accuracy: 0.9237, F1 Micro: 0.7783, F1 Macro: 0.6339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9262, F1 Micro: 0.7812, F1 Macro: 0.656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.136, Accuracy: 0.9283, F1 Micro: 0.7831, F1 Macro: 0.6989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1003, Accuracy: 0.9276, F1 Micro: 0.784, F1 Macro: 0.7092\n",
      "Epoch 7/10, Train Loss: 0.087, Accuracy: 0.9271, F1 Micro: 0.7828, F1 Macro: 0.7154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.9244, F1 Micro: 0.785, F1 Macro: 0.7192\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.926, F1 Micro: 0.7825, F1 Macro: 0.7203\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9252, F1 Micro: 0.7839, F1 Macro: 0.725\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9244, F1 Micro: 0.785, F1 Macro: 0.7192\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.72      0.64      0.68       408\n",
      "  HS_Religion       0.74      0.70      0.72       168\n",
      "      HS_Race       0.75      0.76      0.76       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.55      0.58      0.57        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.67      0.76      0.71       681\n",
      "  HS_Moderate       0.68      0.56      0.62       359\n",
      "    HS_Strong       0.84      0.73      0.78        97\n",
      "\n",
      "    micro avg       0.77      0.80      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.77      0.80      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 250.07653737068176 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9207, F1 Micro: 0.7662, F1 Macro: 0.6712\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 27.916308164596558 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3673, Accuracy: 0.9056, F1 Micro: 0.7094, F1 Macro: 0.4929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2561, Accuracy: 0.9177, F1 Micro: 0.734, F1 Macro: 0.5617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9237, F1 Micro: 0.7624, F1 Macro: 0.6097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1634, Accuracy: 0.9261, F1 Micro: 0.7847, F1 Macro: 0.6884\n",
      "Epoch 5/10, Train Loss: 0.1352, Accuracy: 0.9268, F1 Micro: 0.7812, F1 Macro: 0.6907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9289, F1 Micro: 0.7894, F1 Macro: 0.7046\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9302, F1 Micro: 0.7851, F1 Macro: 0.7021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9289, F1 Micro: 0.7923, F1 Macro: 0.7233\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9275, F1 Micro: 0.7921, F1 Macro: 0.7258\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.9263, F1 Micro: 0.7874, F1 Macro: 0.7161\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9289, F1 Micro: 0.7923, F1 Macro: 0.7233\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.76      0.76       729\n",
      "     HS_Group       0.74      0.65      0.69       408\n",
      "  HS_Religion       0.74      0.65      0.70       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.63      0.33      0.44        57\n",
      "    HS_Gender       0.69      0.53      0.60        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.73       681\n",
      "  HS_Moderate       0.69      0.57      0.63       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 255.44676685333252 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3675, Accuracy: 0.9043, F1 Micro: 0.7046, F1 Macro: 0.4763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2553, Accuracy: 0.9184, F1 Micro: 0.7375, F1 Macro: 0.5773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2047, Accuracy: 0.9226, F1 Micro: 0.7633, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.923, F1 Micro: 0.7831, F1 Macro: 0.6817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.9265, F1 Micro: 0.7838, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.9273, F1 Micro: 0.787, F1 Macro: 0.7012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9299, F1 Micro: 0.7907, F1 Macro: 0.7159\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9257, F1 Micro: 0.783, F1 Macro: 0.7203\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9279, F1 Micro: 0.7893, F1 Macro: 0.7308\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9227, F1 Micro: 0.7745, F1 Macro: 0.6897\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9299, F1 Micro: 0.7907, F1 Macro: 0.7159\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.92      0.89      0.91      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.74      0.67      0.70       408\n",
      "  HS_Religion       0.79      0.63      0.70       168\n",
      "      HS_Race       0.78      0.75      0.76       119\n",
      "  HS_Physical       0.67      0.28      0.40        57\n",
      "    HS_Gender       0.70      0.47      0.57        55\n",
      "     HS_Other       0.84      0.79      0.81       771\n",
      "      HS_Weak       0.74      0.70      0.72       681\n",
      "  HS_Moderate       0.68      0.61      0.64       359\n",
      "    HS_Strong       0.83      0.73      0.78        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.78      0.67      0.72      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 257.2107172012329 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.372, Accuracy: 0.9052, F1 Micro: 0.7065, F1 Macro: 0.4984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2538, Accuracy: 0.9185, F1 Micro: 0.7399, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2013, Accuracy: 0.9245, F1 Micro: 0.7681, F1 Macro: 0.6255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1623, Accuracy: 0.9272, F1 Micro: 0.7866, F1 Macro: 0.6917\n",
      "Epoch 5/10, Train Loss: 0.1345, Accuracy: 0.9239, F1 Micro: 0.7741, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.9288, F1 Micro: 0.7882, F1 Macro: 0.7067\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9287, F1 Micro: 0.7819, F1 Macro: 0.7081\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9258, F1 Micro: 0.7836, F1 Macro: 0.7173\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9254, F1 Micro: 0.7866, F1 Macro: 0.7243\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9269, F1 Micro: 0.7881, F1 Macro: 0.7147\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9288, F1 Micro: 0.7882, F1 Macro: 0.7067\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.77      0.62      0.69       408\n",
      "  HS_Religion       0.79      0.61      0.69       168\n",
      "      HS_Race       0.77      0.71      0.74       119\n",
      "  HS_Physical       0.75      0.26      0.39        57\n",
      "    HS_Gender       0.72      0.38      0.50        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.71      0.55      0.62       359\n",
      "    HS_Strong       0.82      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5589\n",
      "    macro avg       0.78      0.66      0.71      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 252.0192108154297 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9212, F1 Micro: 0.7676, F1 Macro: 0.6738\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.068854570388794 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3638, Accuracy: 0.904, F1 Micro: 0.6992, F1 Macro: 0.468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2509, Accuracy: 0.9178, F1 Micro: 0.7613, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.925, F1 Micro: 0.7754, F1 Macro: 0.6247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.928, F1 Micro: 0.7854, F1 Macro: 0.6731\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9265, F1 Micro: 0.7849, F1 Macro: 0.6803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9294, F1 Micro: 0.7881, F1 Macro: 0.6761\n",
      "Epoch 7/10, Train Loss: 0.0817, Accuracy: 0.9291, F1 Micro: 0.7862, F1 Macro: 0.7139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.9296, F1 Micro: 0.7932, F1 Macro: 0.7232\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9266, F1 Micro: 0.7855, F1 Macro: 0.7097\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.9284, F1 Micro: 0.788, F1 Macro: 0.7152\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9296, F1 Micro: 0.7932, F1 Macro: 0.7232\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.76      0.75       729\n",
      "     HS_Group       0.72      0.64      0.68       408\n",
      "  HS_Religion       0.81      0.64      0.72       168\n",
      "      HS_Race       0.79      0.75      0.77       119\n",
      "  HS_Physical       0.69      0.35      0.47        57\n",
      "    HS_Gender       0.71      0.45      0.56        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.67      0.57      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.44      5589\n",
      "\n",
      "Training completed in 257.8018379211426 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3644, Accuracy: 0.9037, F1 Micro: 0.7004, F1 Macro: 0.4862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9188, F1 Micro: 0.7621, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2028, Accuracy: 0.9266, F1 Micro: 0.7808, F1 Macro: 0.6329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.165, Accuracy: 0.9296, F1 Micro: 0.7892, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9284, F1 Micro: 0.7893, F1 Macro: 0.6958\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.928, F1 Micro: 0.7856, F1 Macro: 0.6743\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.9279, F1 Micro: 0.7855, F1 Macro: 0.7135\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9289, F1 Micro: 0.7851, F1 Macro: 0.7064\n",
      "Epoch 9/10, Train Loss: 0.0597, Accuracy: 0.9267, F1 Micro: 0.7839, F1 Macro: 0.7198\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9254, F1 Micro: 0.7805, F1 Macro: 0.7198\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9284, F1 Micro: 0.7893, F1 Macro: 0.6958\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.77      0.74      0.75       729\n",
      "     HS_Group       0.71      0.67      0.69       408\n",
      "  HS_Religion       0.67      0.69      0.68       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.57      0.21      0.31        57\n",
      "    HS_Gender       0.57      0.36      0.44        55\n",
      "     HS_Other       0.83      0.78      0.81       771\n",
      "      HS_Weak       0.76      0.72      0.74       681\n",
      "  HS_Moderate       0.67      0.60      0.63       359\n",
      "    HS_Strong       0.76      0.76      0.76        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.74      0.67      0.70      5589\n",
      " weighted avg       0.80      0.77      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 256.59027075767517 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.368, Accuracy: 0.9045, F1 Micro: 0.6847, F1 Macro: 0.4622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9188, F1 Micro: 0.7628, F1 Macro: 0.6085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2012, Accuracy: 0.9258, F1 Micro: 0.7753, F1 Macro: 0.6274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9289, F1 Micro: 0.7798, F1 Macro: 0.6877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1297, Accuracy: 0.9263, F1 Micro: 0.786, F1 Macro: 0.6938\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9288, F1 Micro: 0.7829, F1 Macro: 0.6916\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9265, F1 Micro: 0.7843, F1 Macro: 0.7221\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9286, F1 Micro: 0.7847, F1 Macro: 0.7127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9283, F1 Micro: 0.7878, F1 Macro: 0.7214\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9258, F1 Micro: 0.7835, F1 Macro: 0.7192\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9283, F1 Micro: 0.7878, F1 Macro: 0.7214\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.74      0.61      0.67       408\n",
      "  HS_Religion       0.77      0.68      0.72       168\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.60      0.46      0.52        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.83      0.77      0.80       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.69      0.53      0.60       359\n",
      "    HS_Strong       0.81      0.80      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 258.4379026889801 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9216, F1 Micro: 0.7689, F1 Macro: 0.676\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.423414945602417 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3607, Accuracy: 0.9075, F1 Micro: 0.694, F1 Macro: 0.4704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2438, Accuracy: 0.9167, F1 Micro: 0.7594, F1 Macro: 0.5824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1941, Accuracy: 0.9248, F1 Micro: 0.7792, F1 Macro: 0.6327\n",
      "Epoch 4/10, Train Loss: 0.1552, Accuracy: 0.9275, F1 Micro: 0.7786, F1 Macro: 0.6418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.9278, F1 Micro: 0.7792, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0997, Accuracy: 0.93, F1 Micro: 0.7906, F1 Macro: 0.6907\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9266, F1 Micro: 0.7864, F1 Macro: 0.7083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9283, F1 Micro: 0.7912, F1 Macro: 0.7164\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9272, F1 Micro: 0.7864, F1 Macro: 0.719\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9267, F1 Micro: 0.7905, F1 Macro: 0.7274\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9283, F1 Micro: 0.7912, F1 Macro: 0.7164\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.71      0.66      0.68       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.73      0.81      0.77       119\n",
      "  HS_Physical       0.71      0.30      0.42        57\n",
      "    HS_Gender       0.79      0.42      0.55        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.74      0.71      0.73       681\n",
      "  HS_Moderate       0.67      0.58      0.62       359\n",
      "    HS_Strong       0.74      0.87      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 263.2743196487427 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3599, Accuracy: 0.9065, F1 Micro: 0.6852, F1 Macro: 0.4603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.9173, F1 Micro: 0.76, F1 Macro: 0.5832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.9256, F1 Micro: 0.7771, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1583, Accuracy: 0.9291, F1 Micro: 0.7793, F1 Macro: 0.6443\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9271, F1 Micro: 0.7735, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.928, F1 Micro: 0.786, F1 Macro: 0.686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9261, F1 Micro: 0.7861, F1 Macro: 0.7075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.9273, F1 Micro: 0.7886, F1 Macro: 0.7202\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9257, F1 Micro: 0.784, F1 Macro: 0.7178\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9278, F1 Micro: 0.7847, F1 Macro: 0.7205\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9273, F1 Micro: 0.7886, F1 Macro: 0.7202\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.70      0.68      0.69       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.72      0.37      0.49        57\n",
      "    HS_Gender       0.68      0.42      0.52        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.70      0.71       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.76      0.87      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 264.5351634025574 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3648, Accuracy: 0.908, F1 Micro: 0.7034, F1 Macro: 0.4706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2411, Accuracy: 0.9177, F1 Micro: 0.7586, F1 Macro: 0.5817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1942, Accuracy: 0.9262, F1 Micro: 0.7792, F1 Macro: 0.6476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1557, Accuracy: 0.9278, F1 Micro: 0.7847, F1 Macro: 0.6496\n",
      "Epoch 5/10, Train Loss: 0.1237, Accuracy: 0.9267, F1 Micro: 0.7786, F1 Macro: 0.6915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9272, F1 Micro: 0.7885, F1 Macro: 0.6904\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9274, F1 Micro: 0.7846, F1 Macro: 0.7132\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9254, F1 Micro: 0.7815, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9271, F1 Micro: 0.7886, F1 Macro: 0.7279\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9269, F1 Micro: 0.7849, F1 Macro: 0.7115\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9271, F1 Micro: 0.7886, F1 Macro: 0.7279\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.77      0.70      0.73       729\n",
      "     HS_Group       0.68      0.72      0.70       408\n",
      "  HS_Religion       0.68      0.71      0.70       168\n",
      "      HS_Race       0.67      0.87      0.75       119\n",
      "  HS_Physical       0.66      0.44      0.53        57\n",
      "    HS_Gender       0.66      0.49      0.56        55\n",
      "     HS_Other       0.82      0.78      0.80       771\n",
      "      HS_Weak       0.74      0.68      0.71       681\n",
      "  HS_Moderate       0.63      0.66      0.65       359\n",
      "    HS_Strong       0.81      0.86      0.83        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.73      0.73      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 263.99730110168457 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9219, F1 Micro: 0.77, F1 Macro: 0.6784\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.860284328460693 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3586, Accuracy: 0.9066, F1 Micro: 0.7154, F1 Macro: 0.5296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.9198, F1 Micro: 0.7444, F1 Macro: 0.5649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1947, Accuracy: 0.9248, F1 Micro: 0.7721, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1545, Accuracy: 0.9287, F1 Micro: 0.7826, F1 Macro: 0.648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9237, F1 Micro: 0.7847, F1 Macro: 0.6712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9281, F1 Micro: 0.7915, F1 Macro: 0.6868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9287, F1 Micro: 0.7919, F1 Macro: 0.7095\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9283, F1 Micro: 0.7868, F1 Macro: 0.7236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9293, F1 Micro: 0.7925, F1 Macro: 0.7315\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9268, F1 Micro: 0.7894, F1 Macro: 0.726\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9293, F1 Micro: 0.7925, F1 Macro: 0.7315\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.79      0.71      0.75       729\n",
      "     HS_Group       0.69      0.72      0.70       408\n",
      "  HS_Religion       0.76      0.67      0.72       168\n",
      "      HS_Race       0.76      0.81      0.78       119\n",
      "  HS_Physical       0.58      0.44      0.50        57\n",
      "    HS_Gender       0.64      0.51      0.57        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.76      0.69      0.72       681\n",
      "  HS_Moderate       0.63      0.64      0.63       359\n",
      "    HS_Strong       0.83      0.84      0.83        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 270.81434059143066 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3577, Accuracy: 0.9063, F1 Micro: 0.7158, F1 Macro: 0.5364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2469, Accuracy: 0.9213, F1 Micro: 0.7549, F1 Macro: 0.5778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.927, F1 Micro: 0.7762, F1 Macro: 0.6455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.9282, F1 Micro: 0.7786, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.925, F1 Micro: 0.7823, F1 Macro: 0.6798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9263, F1 Micro: 0.7861, F1 Macro: 0.7057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9277, F1 Micro: 0.7869, F1 Macro: 0.703\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9277, F1 Micro: 0.7843, F1 Macro: 0.712\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9249, F1 Micro: 0.7823, F1 Macro: 0.7131\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9284, F1 Micro: 0.7816, F1 Macro: 0.7073\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9277, F1 Micro: 0.7869, F1 Macro: 0.703\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.93      0.89      0.91      1008\n",
      "HS_Individual       0.76      0.73      0.74       729\n",
      "     HS_Group       0.72      0.65      0.68       408\n",
      "  HS_Religion       0.75      0.64      0.69       168\n",
      "      HS_Race       0.72      0.75      0.73       119\n",
      "  HS_Physical       0.68      0.26      0.38        57\n",
      "    HS_Gender       0.74      0.36      0.49        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.65      0.60      0.63       359\n",
      "    HS_Strong       0.82      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.67      0.70      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 271.48080587387085 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3605, Accuracy: 0.906, F1 Micro: 0.7201, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.246, Accuracy: 0.922, F1 Micro: 0.7576, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9256, F1 Micro: 0.7739, F1 Macro: 0.6483\n",
      "Epoch 4/10, Train Loss: 0.1503, Accuracy: 0.9267, F1 Micro: 0.7721, F1 Macro: 0.6609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9274, F1 Micro: 0.7885, F1 Macro: 0.6845\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9254, F1 Micro: 0.7874, F1 Macro: 0.704\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9257, F1 Micro: 0.7808, F1 Macro: 0.7077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9266, F1 Micro: 0.7897, F1 Macro: 0.7241\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.927, F1 Micro: 0.7863, F1 Macro: 0.7188\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9267, F1 Micro: 0.7838, F1 Macro: 0.7148\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9266, F1 Micro: 0.7897, F1 Macro: 0.7241\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.88      0.94      0.91      1008\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.74      0.63      0.68       408\n",
      "  HS_Religion       0.74      0.69      0.72       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.59      0.40      0.48        57\n",
      "    HS_Gender       0.68      0.49      0.57        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.69      0.76      0.72       681\n",
      "  HS_Moderate       0.69      0.55      0.61       359\n",
      "    HS_Strong       0.82      0.81      0.82        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 266.37234926223755 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9222, F1 Micro: 0.7709, F1 Macro: 0.6804\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 17.22645092010498 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3543, Accuracy: 0.9075, F1 Micro: 0.7058, F1 Macro: 0.4891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2407, Accuracy: 0.9219, F1 Micro: 0.7657, F1 Macro: 0.6059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9257, F1 Micro: 0.7747, F1 Macro: 0.6176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9263, F1 Micro: 0.7865, F1 Macro: 0.6622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1243, Accuracy: 0.9307, F1 Micro: 0.7889, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9306, F1 Micro: 0.7948, F1 Macro: 0.7044\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9298, F1 Micro: 0.7847, F1 Macro: 0.7096\n",
      "Epoch 8/10, Train Loss: 0.0694, Accuracy: 0.9294, F1 Micro: 0.7933, F1 Macro: 0.7181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9302, F1 Micro: 0.7974, F1 Macro: 0.7219\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9277, F1 Micro: 0.7944, F1 Macro: 0.7294\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9302, F1 Micro: 0.7974, F1 Macro: 0.7219\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.92      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.75       729\n",
      "     HS_Group       0.71      0.69      0.70       408\n",
      "  HS_Religion       0.75      0.67      0.71       168\n",
      "      HS_Race       0.75      0.76      0.75       119\n",
      "  HS_Physical       0.62      0.32      0.42        57\n",
      "    HS_Gender       0.72      0.47      0.57        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.73      0.74      0.73       681\n",
      "  HS_Moderate       0.65      0.65      0.65       359\n",
      "    HS_Strong       0.83      0.72      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.80      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.79      0.80      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 273.3552944660187 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3542, Accuracy: 0.908, F1 Micro: 0.7124, F1 Macro: 0.4964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2408, Accuracy: 0.9218, F1 Micro: 0.7547, F1 Macro: 0.5976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1909, Accuracy: 0.9264, F1 Micro: 0.7753, F1 Macro: 0.6235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1564, Accuracy: 0.928, F1 Micro: 0.7914, F1 Macro: 0.662\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9293, F1 Micro: 0.783, F1 Macro: 0.6733\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.93, F1 Micro: 0.7908, F1 Macro: 0.7081\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9271, F1 Micro: 0.7856, F1 Macro: 0.711\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9297, F1 Micro: 0.7884, F1 Macro: 0.7195\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9258, F1 Micro: 0.7884, F1 Macro: 0.7206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9282, F1 Micro: 0.7942, F1 Macro: 0.741\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9282, F1 Micro: 0.7942, F1 Macro: 0.741\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.70      0.70      0.70       408\n",
      "  HS_Religion       0.73      0.70      0.71       168\n",
      "      HS_Race       0.71      0.84      0.77       119\n",
      "  HS_Physical       0.66      0.47      0.55        57\n",
      "    HS_Gender       0.70      0.55      0.61        55\n",
      "     HS_Other       0.79      0.82      0.80       771\n",
      "      HS_Weak       0.72      0.73      0.73       681\n",
      "  HS_Moderate       0.66      0.64      0.65       359\n",
      "    HS_Strong       0.85      0.86      0.85        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5589\n",
      "    macro avg       0.75      0.74      0.74      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 268.6299321651459 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3582, Accuracy: 0.9062, F1 Micro: 0.6815, F1 Macro: 0.4778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2397, Accuracy: 0.9227, F1 Micro: 0.7574, F1 Macro: 0.6038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1918, Accuracy: 0.9266, F1 Micro: 0.7782, F1 Macro: 0.6292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1532, Accuracy: 0.9269, F1 Micro: 0.7859, F1 Macro: 0.6734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1203, Accuracy: 0.9308, F1 Micro: 0.7893, F1 Macro: 0.6989\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.928, F1 Micro: 0.7858, F1 Macro: 0.7159\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9277, F1 Micro: 0.7745, F1 Macro: 0.7057\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9276, F1 Micro: 0.7822, F1 Macro: 0.716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9292, F1 Micro: 0.7899, F1 Macro: 0.7172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9291, F1 Micro: 0.7947, F1 Macro: 0.7307\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9291, F1 Micro: 0.7947, F1 Macro: 0.7307\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.75       729\n",
      "     HS_Group       0.71      0.69      0.70       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.68      0.86      0.76       119\n",
      "  HS_Physical       0.60      0.42      0.49        57\n",
      "    HS_Gender       0.66      0.53      0.59        55\n",
      "     HS_Other       0.82      0.79      0.80       771\n",
      "      HS_Weak       0.74      0.73      0.73       681\n",
      "  HS_Moderate       0.66      0.62      0.64       359\n",
      "    HS_Strong       0.81      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.74      0.73      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 271.3251929283142 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9226, F1 Micro: 0.7721, F1 Macro: 0.6828\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.08033061027527 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3527, Accuracy: 0.9074, F1 Micro: 0.7069, F1 Macro: 0.4817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2359, Accuracy: 0.9206, F1 Micro: 0.7482, F1 Macro: 0.5729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1909, Accuracy: 0.9215, F1 Micro: 0.7492, F1 Macro: 0.6011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1529, Accuracy: 0.9246, F1 Micro: 0.7812, F1 Macro: 0.669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9223, F1 Micro: 0.7845, F1 Macro: 0.6721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.9266, F1 Micro: 0.7882, F1 Macro: 0.6867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9295, F1 Micro: 0.7912, F1 Macro: 0.7163\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.9276, F1 Micro: 0.7836, F1 Macro: 0.7149\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9297, F1 Micro: 0.7876, F1 Macro: 0.7189\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9278, F1 Micro: 0.7818, F1 Macro: 0.7116\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9295, F1 Micro: 0.7912, F1 Macro: 0.7163\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.74      0.67      0.71       408\n",
      "  HS_Religion       0.79      0.64      0.71       168\n",
      "      HS_Race       0.76      0.82      0.79       119\n",
      "  HS_Physical       0.76      0.28      0.41        57\n",
      "    HS_Gender       0.70      0.38      0.49        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.71      0.60      0.65       359\n",
      "    HS_Strong       0.81      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.78      0.68      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 276.0634753704071 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3536, Accuracy: 0.9068, F1 Micro: 0.7119, F1 Macro: 0.5084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2367, Accuracy: 0.9223, F1 Micro: 0.7605, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1898, Accuracy: 0.9248, F1 Micro: 0.7677, F1 Macro: 0.6219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.9249, F1 Micro: 0.7826, F1 Macro: 0.6815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9246, F1 Micro: 0.7858, F1 Macro: 0.6829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9279, F1 Micro: 0.7915, F1 Macro: 0.6992\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9291, F1 Micro: 0.7881, F1 Macro: 0.7031\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.928, F1 Micro: 0.7876, F1 Macro: 0.7175\n",
      "Epoch 9/10, Train Loss: 0.0548, Accuracy: 0.9275, F1 Micro: 0.7863, F1 Macro: 0.7226\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9258, F1 Micro: 0.7871, F1 Macro: 0.725\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9279, F1 Micro: 0.7915, F1 Macro: 0.6992\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.92      0.89      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.69      0.70      0.70       408\n",
      "  HS_Religion       0.69      0.70      0.69       168\n",
      "      HS_Race       0.71      0.78      0.74       119\n",
      "  HS_Physical       0.65      0.26      0.38        57\n",
      "    HS_Gender       0.82      0.25      0.39        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.64      0.66      0.65       359\n",
      "    HS_Strong       0.83      0.73      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.68      0.70      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 275.8656816482544 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3582, Accuracy: 0.907, F1 Micro: 0.7094, F1 Macro: 0.5048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2348, Accuracy: 0.9207, F1 Micro: 0.7515, F1 Macro: 0.5897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1895, Accuracy: 0.9251, F1 Micro: 0.7593, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9257, F1 Micro: 0.7803, F1 Macro: 0.675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9255, F1 Micro: 0.7894, F1 Macro: 0.6962\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9289, F1 Micro: 0.7838, F1 Macro: 0.6881\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.9235, F1 Micro: 0.7809, F1 Macro: 0.7036\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9271, F1 Micro: 0.7814, F1 Macro: 0.7145\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9282, F1 Micro: 0.7865, F1 Macro: 0.7219\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.9262, F1 Micro: 0.7821, F1 Macro: 0.7172\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9255, F1 Micro: 0.7894, F1 Macro: 0.6962\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.68      0.68      0.68       408\n",
      "  HS_Religion       0.66      0.74      0.70       168\n",
      "      HS_Race       0.72      0.78      0.75       119\n",
      "  HS_Physical       0.79      0.19      0.31        57\n",
      "    HS_Gender       0.63      0.31      0.41        55\n",
      "     HS_Other       0.78      0.84      0.81       771\n",
      "      HS_Weak       0.71      0.76      0.73       681\n",
      "  HS_Moderate       0.63      0.61      0.62       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.74      0.70      0.70      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.46      0.46      0.44      5589\n",
      "\n",
      "Training completed in 273.27151560783386 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9228, F1 Micro: 0.773, F1 Macro: 0.6838\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.554666757583618 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3446, Accuracy: 0.9069, F1 Micro: 0.7145, F1 Macro: 0.5236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2375, Accuracy: 0.9211, F1 Micro: 0.7613, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1873, Accuracy: 0.9275, F1 Micro: 0.7818, F1 Macro: 0.652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.153, Accuracy: 0.9294, F1 Micro: 0.7877, F1 Macro: 0.6604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1144, Accuracy: 0.9278, F1 Micro: 0.7896, F1 Macro: 0.6909\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9244, F1 Micro: 0.7855, F1 Macro: 0.7064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9298, F1 Micro: 0.7957, F1 Macro: 0.7219\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9278, F1 Micro: 0.7766, F1 Macro: 0.7014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.932, F1 Micro: 0.7991, F1 Macro: 0.7323\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9295, F1 Micro: 0.7924, F1 Macro: 0.7323\n",
      "Model 1 - Iteration 9618: Accuracy: 0.932, F1 Micro: 0.7991, F1 Macro: 0.7323\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.85      0.87      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.77      0.61      0.68       408\n",
      "  HS_Religion       0.75      0.64      0.69       168\n",
      "      HS_Race       0.79      0.73      0.76       119\n",
      "  HS_Physical       0.73      0.42      0.53        57\n",
      "    HS_Gender       0.71      0.49      0.58        55\n",
      "     HS_Other       0.82      0.83      0.83       771\n",
      "      HS_Weak       0.73      0.75      0.74       681\n",
      "  HS_Moderate       0.73      0.53      0.61       359\n",
      "    HS_Strong       0.80      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.82      0.78      0.80      5589\n",
      "    macro avg       0.78      0.70      0.73      5589\n",
      " weighted avg       0.82      0.78      0.80      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 280.6836359500885 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3445, Accuracy: 0.9054, F1 Micro: 0.7051, F1 Macro: 0.5158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2391, Accuracy: 0.9208, F1 Micro: 0.7666, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1882, Accuracy: 0.9256, F1 Micro: 0.7801, F1 Macro: 0.6486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1552, Accuracy: 0.9282, F1 Micro: 0.7829, F1 Macro: 0.6525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1194, Accuracy: 0.9272, F1 Micro: 0.7888, F1 Macro: 0.6797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9287, F1 Micro: 0.7894, F1 Macro: 0.7021\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.926, F1 Micro: 0.787, F1 Macro: 0.7157\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9276, F1 Micro: 0.7886, F1 Macro: 0.7155\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9246, F1 Micro: 0.7842, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9282, F1 Micro: 0.786, F1 Macro: 0.7196\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9287, F1 Micro: 0.7894, F1 Macro: 0.7021\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.76      0.73      0.75       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.72      0.68      0.70       168\n",
      "      HS_Race       0.74      0.76      0.75       119\n",
      "  HS_Physical       0.79      0.26      0.39        57\n",
      "    HS_Gender       0.80      0.29      0.43        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.67      0.60      0.63       359\n",
      "    HS_Strong       0.78      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.78      0.67      0.70      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 280.77091884613037 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3486, Accuracy: 0.9077, F1 Micro: 0.712, F1 Macro: 0.5197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.235, Accuracy: 0.9222, F1 Micro: 0.7645, F1 Macro: 0.6015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1855, Accuracy: 0.9269, F1 Micro: 0.7803, F1 Macro: 0.6499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1498, Accuracy: 0.9282, F1 Micro: 0.7841, F1 Macro: 0.6639\n",
      "Epoch 5/10, Train Loss: 0.1123, Accuracy: 0.9266, F1 Micro: 0.7837, F1 Macro: 0.687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.091, Accuracy: 0.9286, F1 Micro: 0.7883, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9267, F1 Micro: 0.7883, F1 Macro: 0.7154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0623, Accuracy: 0.9258, F1 Micro: 0.7887, F1 Macro: 0.7186\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9267, F1 Micro: 0.7882, F1 Macro: 0.7175\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9293, F1 Micro: 0.7885, F1 Macro: 0.7222\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9258, F1 Micro: 0.7887, F1 Macro: 0.7186\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.71      0.81      0.75       729\n",
      "     HS_Group       0.75      0.62      0.68       408\n",
      "  HS_Religion       0.76      0.67      0.72       168\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.58      0.39      0.46        57\n",
      "    HS_Gender       0.62      0.51      0.56        55\n",
      "     HS_Other       0.79      0.82      0.81       771\n",
      "      HS_Weak       0.68      0.79      0.73       681\n",
      "  HS_Moderate       0.67      0.54      0.60       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.46      0.46      0.44      5589\n",
      "\n",
      "Training completed in 280.5172076225281 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9231, F1 Micro: 0.7738, F1 Macro: 0.6853\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.087977886199951 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3417, Accuracy: 0.9085, F1 Micro: 0.7129, F1 Macro: 0.5056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2249, Accuracy: 0.9215, F1 Micro: 0.7653, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1831, Accuracy: 0.9272, F1 Micro: 0.7795, F1 Macro: 0.6288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1433, Accuracy: 0.9254, F1 Micro: 0.788, F1 Macro: 0.6653\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9293, F1 Micro: 0.7878, F1 Macro: 0.6668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.9276, F1 Micro: 0.7882, F1 Macro: 0.6985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9289, F1 Micro: 0.7939, F1 Macro: 0.7074\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9281, F1 Micro: 0.7902, F1 Macro: 0.708\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9293, F1 Micro: 0.7851, F1 Macro: 0.7122\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9294, F1 Micro: 0.793, F1 Macro: 0.7325\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9289, F1 Micro: 0.7939, F1 Macro: 0.7074\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.72      0.81      0.76       729\n",
      "     HS_Group       0.76      0.60      0.67       408\n",
      "  HS_Religion       0.76      0.66      0.71       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.70      0.25      0.36        57\n",
      "    HS_Gender       0.77      0.36      0.49        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.70      0.79      0.74       681\n",
      "  HS_Moderate       0.71      0.51      0.60       359\n",
      "    HS_Strong       0.82      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.77      0.68      0.71      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 285.5662167072296 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3428, Accuracy: 0.9082, F1 Micro: 0.7124, F1 Macro: 0.5105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2246, Accuracy: 0.9204, F1 Micro: 0.7609, F1 Macro: 0.6017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9258, F1 Micro: 0.7779, F1 Macro: 0.6381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9283, F1 Micro: 0.7902, F1 Macro: 0.6671\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9298, F1 Micro: 0.7875, F1 Macro: 0.6659\n",
      "Epoch 6/10, Train Loss: 0.0922, Accuracy: 0.9275, F1 Micro: 0.7878, F1 Macro: 0.7006\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9285, F1 Micro: 0.7887, F1 Macro: 0.7156\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9273, F1 Micro: 0.7864, F1 Macro: 0.7204\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.929, F1 Micro: 0.788, F1 Macro: 0.7159\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.9257, F1 Micro: 0.7834, F1 Macro: 0.722\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9283, F1 Micro: 0.7902, F1 Macro: 0.6671\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.71      0.74      0.72       408\n",
      "  HS_Religion       0.71      0.67      0.69       168\n",
      "      HS_Race       0.78      0.73      0.76       119\n",
      "  HS_Physical       0.60      0.05      0.10        57\n",
      "    HS_Gender       0.75      0.16      0.27        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.72       681\n",
      "  HS_Moderate       0.66      0.66      0.66       359\n",
      "    HS_Strong       0.78      0.76      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.65      0.67      5589\n",
      " weighted avg       0.80      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 281.3206374645233 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3472, Accuracy: 0.9082, F1 Micro: 0.7202, F1 Macro: 0.5375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2261, Accuracy: 0.9211, F1 Micro: 0.763, F1 Macro: 0.6082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1806, Accuracy: 0.9268, F1 Micro: 0.7771, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1422, Accuracy: 0.9278, F1 Micro: 0.7855, F1 Macro: 0.6792\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9276, F1 Micro: 0.7848, F1 Macro: 0.6892\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9258, F1 Micro: 0.7849, F1 Macro: 0.7137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.9277, F1 Micro: 0.7926, F1 Macro: 0.7248\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9283, F1 Micro: 0.7877, F1 Macro: 0.7184\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9268, F1 Micro: 0.785, F1 Macro: 0.7145\n",
      "Epoch 10/10, Train Loss: 0.0409, Accuracy: 0.9267, F1 Micro: 0.7853, F1 Macro: 0.7186\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9277, F1 Micro: 0.7926, F1 Macro: 0.7248\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.71      0.81      0.76       729\n",
      "     HS_Group       0.76      0.62      0.69       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.72      0.79      0.76       119\n",
      "  HS_Physical       0.70      0.37      0.48        57\n",
      "    HS_Gender       0.68      0.51      0.58        55\n",
      "     HS_Other       0.81      0.80      0.81       771\n",
      "      HS_Weak       0.69      0.78      0.73       681\n",
      "  HS_Moderate       0.69      0.56      0.62       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 282.83212780952454 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9233, F1 Micro: 0.7746, F1 Macro: 0.6859\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.680635929107666 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3378, Accuracy: 0.9069, F1 Micro: 0.6951, F1 Macro: 0.4916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2261, Accuracy: 0.9186, F1 Micro: 0.7592, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1792, Accuracy: 0.9259, F1 Micro: 0.7768, F1 Macro: 0.6225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1439, Accuracy: 0.9265, F1 Micro: 0.7878, F1 Macro: 0.6556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.93, F1 Micro: 0.7895, F1 Macro: 0.6943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9275, F1 Micro: 0.7898, F1 Macro: 0.6991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9283, F1 Micro: 0.7913, F1 Macro: 0.7185\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9283, F1 Micro: 0.7874, F1 Macro: 0.711\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9232, F1 Micro: 0.7843, F1 Macro: 0.7107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9264, F1 Micro: 0.7915, F1 Macro: 0.7262\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9264, F1 Micro: 0.7915, F1 Macro: 0.7262\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.90      0.87      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.71      0.80      0.75       729\n",
      "     HS_Group       0.71      0.62      0.66       408\n",
      "  HS_Religion       0.70      0.73      0.71       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.69      0.42      0.52        57\n",
      "    HS_Gender       0.66      0.49      0.56        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.69      0.78      0.73       681\n",
      "  HS_Moderate       0.65      0.56      0.60       359\n",
      "    HS_Strong       0.79      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.78      0.81      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.78      0.81      0.79      5589\n",
      "  samples avg       0.46      0.46      0.45      5589\n",
      "\n",
      "Training completed in 292.6396517753601 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.336, Accuracy: 0.9044, F1 Micro: 0.6682, F1 Macro: 0.4499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2266, Accuracy: 0.9198, F1 Micro: 0.76, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1784, Accuracy: 0.9246, F1 Micro: 0.782, F1 Macro: 0.6397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1456, Accuracy: 0.9228, F1 Micro: 0.7832, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.9249, F1 Micro: 0.7865, F1 Macro: 0.6851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9262, F1 Micro: 0.7865, F1 Macro: 0.7089\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9281, F1 Micro: 0.7856, F1 Macro: 0.7139\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9266, F1 Micro: 0.7841, F1 Macro: 0.7126\n",
      "Epoch 9/10, Train Loss: 0.051, Accuracy: 0.923, F1 Micro: 0.7842, F1 Macro: 0.7159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9253, F1 Micro: 0.7882, F1 Macro: 0.7257\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9253, F1 Micro: 0.7882, F1 Macro: 0.7257\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.70      0.63      0.66       408\n",
      "  HS_Religion       0.70      0.71      0.71       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.62      0.44      0.52        57\n",
      "    HS_Gender       0.67      0.53      0.59        55\n",
      "     HS_Other       0.78      0.83      0.80       771\n",
      "      HS_Weak       0.69      0.77      0.73       681\n",
      "  HS_Moderate       0.65      0.55      0.60       359\n",
      "    HS_Strong       0.74      0.89      0.81        97\n",
      "\n",
      "    micro avg       0.77      0.80      0.79      5589\n",
      "    macro avg       0.73      0.73      0.73      5589\n",
      " weighted avg       0.77      0.80      0.79      5589\n",
      "  samples avg       0.46      0.46      0.44      5589\n",
      "\n",
      "Training completed in 292.1281976699829 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.341, Accuracy: 0.9045, F1 Micro: 0.6732, F1 Macro: 0.4711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2263, Accuracy: 0.9152, F1 Micro: 0.7559, F1 Macro: 0.5828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.178, Accuracy: 0.9241, F1 Micro: 0.7726, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1431, Accuracy: 0.9225, F1 Micro: 0.7795, F1 Macro: 0.6544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1105, Accuracy: 0.9253, F1 Micro: 0.7802, F1 Macro: 0.6947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9272, F1 Micro: 0.7842, F1 Macro: 0.6907\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9271, F1 Micro: 0.784, F1 Macro: 0.7186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.93, F1 Micro: 0.7935, F1 Macro: 0.7246\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9273, F1 Micro: 0.7904, F1 Macro: 0.7256\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9276, F1 Micro: 0.789, F1 Macro: 0.7291\n",
      "Model 3 - Iteration 10018: Accuracy: 0.93, F1 Micro: 0.7935, F1 Macro: 0.7246\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.79      0.70      0.74       729\n",
      "     HS_Group       0.70      0.72      0.71       408\n",
      "  HS_Religion       0.72      0.63      0.67       168\n",
      "      HS_Race       0.76      0.79      0.77       119\n",
      "  HS_Physical       0.68      0.40      0.51        57\n",
      "    HS_Gender       0.67      0.40      0.50        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.77      0.68      0.72       681\n",
      "  HS_Moderate       0.65      0.67      0.66       359\n",
      "    HS_Strong       0.84      0.80      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 290.56488704681396 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9234, F1 Micro: 0.7752, F1 Macro: 0.6875\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.249230623245239 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3286, Accuracy: 0.9084, F1 Micro: 0.713, F1 Macro: 0.4891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2225, Accuracy: 0.9174, F1 Micro: 0.7514, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1731, Accuracy: 0.928, F1 Micro: 0.7832, F1 Macro: 0.6423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1383, Accuracy: 0.9278, F1 Micro: 0.7886, F1 Macro: 0.6684\n",
      "Epoch 5/10, Train Loss: 0.1134, Accuracy: 0.9229, F1 Micro: 0.7828, F1 Macro: 0.6954\n",
      "Epoch 6/10, Train Loss: 0.0876, Accuracy: 0.9276, F1 Micro: 0.7865, F1 Macro: 0.7115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0697, Accuracy: 0.9325, F1 Micro: 0.7988, F1 Macro: 0.727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9315, F1 Micro: 0.8019, F1 Macro: 0.7359\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9301, F1 Micro: 0.7974, F1 Macro: 0.731\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.923, F1 Micro: 0.7852, F1 Macro: 0.7177\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9315, F1 Micro: 0.8019, F1 Macro: 0.7359\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.79      0.73      0.76       729\n",
      "     HS_Group       0.68      0.75      0.71       408\n",
      "  HS_Religion       0.70      0.71      0.71       168\n",
      "      HS_Race       0.71      0.84      0.77       119\n",
      "  HS_Physical       0.69      0.39      0.49        57\n",
      "    HS_Gender       0.72      0.47      0.57        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.77      0.69      0.73       681\n",
      "  HS_Moderate       0.64      0.69      0.67       359\n",
      "    HS_Strong       0.82      0.80      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.80      0.80      5589\n",
      "    macro avg       0.76      0.73      0.74      5589\n",
      " weighted avg       0.80      0.80      0.80      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 294.40787625312805 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3277, Accuracy: 0.9076, F1 Micro: 0.7087, F1 Macro: 0.4819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2243, Accuracy: 0.921, F1 Micro: 0.7638, F1 Macro: 0.5889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1729, Accuracy: 0.9293, F1 Micro: 0.7831, F1 Macro: 0.655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9232, F1 Micro: 0.784, F1 Macro: 0.6691\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.9267, F1 Micro: 0.7816, F1 Macro: 0.6889\n",
      "Epoch 6/10, Train Loss: 0.0887, Accuracy: 0.9285, F1 Micro: 0.7788, F1 Macro: 0.6893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0705, Accuracy: 0.9296, F1 Micro: 0.785, F1 Macro: 0.7035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9305, F1 Micro: 0.791, F1 Macro: 0.7243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9296, F1 Micro: 0.7963, F1 Macro: 0.7311\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9272, F1 Micro: 0.7863, F1 Macro: 0.7126\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9296, F1 Micro: 0.7963, F1 Macro: 0.7311\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.73      0.79      0.76       729\n",
      "     HS_Group       0.77      0.63      0.69       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.72      0.75      0.73       119\n",
      "  HS_Physical       0.67      0.42      0.52        57\n",
      "    HS_Gender       0.71      0.55      0.62        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.71      0.77      0.74       681\n",
      "  HS_Moderate       0.73      0.54      0.62       359\n",
      "    HS_Strong       0.77      0.86      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.80      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 295.8353695869446 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3319, Accuracy: 0.908, F1 Micro: 0.7139, F1 Macro: 0.5031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2225, Accuracy: 0.92, F1 Micro: 0.7642, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1732, Accuracy: 0.9257, F1 Micro: 0.7765, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1387, Accuracy: 0.9261, F1 Micro: 0.7848, F1 Macro: 0.6813\n",
      "Epoch 5/10, Train Loss: 0.1112, Accuracy: 0.9242, F1 Micro: 0.7828, F1 Macro: 0.6902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9301, F1 Micro: 0.7898, F1 Macro: 0.7139\n",
      "Epoch 7/10, Train Loss: 0.0686, Accuracy: 0.9275, F1 Micro: 0.7821, F1 Macro: 0.7177\n",
      "Epoch 8/10, Train Loss: 0.0561, Accuracy: 0.9287, F1 Micro: 0.7836, F1 Macro: 0.723\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9246, F1 Micro: 0.7867, F1 Macro: 0.7184\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.9257, F1 Micro: 0.7872, F1 Macro: 0.714\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9301, F1 Micro: 0.7898, F1 Macro: 0.7139\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1137\n",
      "      Abusive       0.92      0.88      0.90      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.76      0.66      0.71       408\n",
      "  HS_Religion       0.77      0.65      0.70       168\n",
      "      HS_Race       0.80      0.71      0.76       119\n",
      "  HS_Physical       0.73      0.28      0.41        57\n",
      "    HS_Gender       0.83      0.36      0.51        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.70      0.60      0.65       359\n",
      "    HS_Strong       0.84      0.77      0.81        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.80      0.66      0.71      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 292.9447753429413 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9237, F1 Micro: 0.776, F1 Macro: 0.689\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.138025760650635 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3247, Accuracy: 0.9074, F1 Micro: 0.7172, F1 Macro: 0.5103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2159, Accuracy: 0.9215, F1 Micro: 0.757, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9275, F1 Micro: 0.7799, F1 Macro: 0.6358\n",
      "Epoch 4/10, Train Loss: 0.1355, Accuracy: 0.9258, F1 Micro: 0.7754, F1 Macro: 0.6324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1075, Accuracy: 0.9286, F1 Micro: 0.7823, F1 Macro: 0.662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0859, Accuracy: 0.9299, F1 Micro: 0.7922, F1 Macro: 0.7044\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9277, F1 Micro: 0.7842, F1 Macro: 0.7134\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9273, F1 Micro: 0.7871, F1 Macro: 0.7211\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.9253, F1 Micro: 0.7871, F1 Macro: 0.7183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0418, Accuracy: 0.9307, F1 Micro: 0.7975, F1 Macro: 0.7239\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9307, F1 Micro: 0.7975, F1 Macro: 0.7239\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.78      0.76       729\n",
      "     HS_Group       0.76      0.63      0.69       408\n",
      "  HS_Religion       0.72      0.68      0.70       168\n",
      "      HS_Race       0.79      0.74      0.76       119\n",
      "  HS_Physical       0.57      0.44      0.50        57\n",
      "    HS_Gender       0.64      0.42      0.51        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.73      0.75      0.74       681\n",
      "  HS_Moderate       0.73      0.56      0.63       359\n",
      "    HS_Strong       0.77      0.86      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.79      0.80      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.81      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 298.109539270401 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3259, Accuracy: 0.9071, F1 Micro: 0.7148, F1 Macro: 0.4976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2172, Accuracy: 0.9219, F1 Micro: 0.7565, F1 Macro: 0.5911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1771, Accuracy: 0.9271, F1 Micro: 0.7784, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.138, Accuracy: 0.9272, F1 Micro: 0.7828, F1 Macro: 0.6634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1089, Accuracy: 0.9271, F1 Micro: 0.7834, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.088, Accuracy: 0.9295, F1 Micro: 0.789, F1 Macro: 0.7106\n",
      "Epoch 7/10, Train Loss: 0.0697, Accuracy: 0.9268, F1 Micro: 0.7871, F1 Macro: 0.7153\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9258, F1 Micro: 0.7882, F1 Macro: 0.7197\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9229, F1 Micro: 0.7812, F1 Macro: 0.722\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.926, F1 Micro: 0.7826, F1 Macro: 0.7159\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9295, F1 Micro: 0.789, F1 Macro: 0.7106\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.92      0.89      0.91      1008\n",
      "HS_Individual       0.76      0.73      0.74       729\n",
      "     HS_Group       0.76      0.64      0.70       408\n",
      "  HS_Religion       0.80      0.58      0.68       168\n",
      "      HS_Race       0.78      0.71      0.74       119\n",
      "  HS_Physical       0.81      0.30      0.44        57\n",
      "    HS_Gender       0.73      0.40      0.52        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.73      0.70      0.72       681\n",
      "  HS_Moderate       0.72      0.56      0.63       359\n",
      "    HS_Strong       0.79      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.79      0.66      0.71      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 301.0733251571655 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3301, Accuracy: 0.9077, F1 Micro: 0.7169, F1 Macro: 0.5013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2156, Accuracy: 0.9225, F1 Micro: 0.7647, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1733, Accuracy: 0.9258, F1 Micro: 0.7802, F1 Macro: 0.6505\n",
      "Epoch 4/10, Train Loss: 0.1348, Accuracy: 0.9279, F1 Micro: 0.7787, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1062, Accuracy: 0.9276, F1 Micro: 0.7845, F1 Macro: 0.6871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0854, Accuracy: 0.9277, F1 Micro: 0.7856, F1 Macro: 0.7122\n",
      "Epoch 7/10, Train Loss: 0.0663, Accuracy: 0.9299, F1 Micro: 0.7852, F1 Macro: 0.7111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0537, Accuracy: 0.9289, F1 Micro: 0.7907, F1 Macro: 0.7215\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.928, F1 Micro: 0.7901, F1 Macro: 0.723\n",
      "Epoch 10/10, Train Loss: 0.038, Accuracy: 0.9271, F1 Micro: 0.7864, F1 Macro: 0.719\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9289, F1 Micro: 0.7907, F1 Macro: 0.7215\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.92      0.91      0.91      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.73      0.64      0.68       408\n",
      "  HS_Religion       0.76      0.66      0.71       168\n",
      "      HS_Race       0.72      0.80      0.76       119\n",
      "  HS_Physical       0.70      0.37      0.48        57\n",
      "    HS_Gender       0.69      0.45      0.55        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.69      0.55      0.62       359\n",
      "    HS_Strong       0.79      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 299.5600049495697 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9239, F1 Micro: 0.7766, F1 Macro: 0.6901\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.828982353210449 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3295, Accuracy: 0.906, F1 Micro: 0.7138, F1 Macro: 0.4818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2154, Accuracy: 0.9151, F1 Micro: 0.7632, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1717, Accuracy: 0.9264, F1 Micro: 0.7694, F1 Macro: 0.6124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1367, Accuracy: 0.9216, F1 Micro: 0.775, F1 Macro: 0.6419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9289, F1 Micro: 0.7918, F1 Macro: 0.6979\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.9243, F1 Micro: 0.7865, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9289, F1 Micro: 0.7937, F1 Macro: 0.7214\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9287, F1 Micro: 0.7844, F1 Macro: 0.7127\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9303, F1 Micro: 0.791, F1 Macro: 0.7222\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.925, F1 Micro: 0.7854, F1 Macro: 0.7207\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9289, F1 Micro: 0.7937, F1 Macro: 0.7214\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.67      0.75      0.71       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.74      0.35      0.48        57\n",
      "    HS_Gender       0.60      0.44      0.51        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.76      0.69      0.72       681\n",
      "  HS_Moderate       0.64      0.69      0.66       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 303.4928619861603 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.329, Accuracy: 0.9027, F1 Micro: 0.6998, F1 Macro: 0.4266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2179, Accuracy: 0.9169, F1 Micro: 0.7652, F1 Macro: 0.6031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9252, F1 Micro: 0.7666, F1 Macro: 0.5988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1385, Accuracy: 0.9192, F1 Micro: 0.7727, F1 Macro: 0.6347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1147, Accuracy: 0.9304, F1 Micro: 0.7899, F1 Macro: 0.6998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9265, F1 Micro: 0.7932, F1 Macro: 0.7079\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9268, F1 Micro: 0.7917, F1 Macro: 0.7172\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9282, F1 Micro: 0.7852, F1 Macro: 0.7128\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9289, F1 Micro: 0.7906, F1 Macro: 0.7299\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9225, F1 Micro: 0.7844, F1 Macro: 0.7232\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9265, F1 Micro: 0.7932, F1 Macro: 0.7079\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.87      1137\n",
      "      Abusive       0.87      0.94      0.90      1008\n",
      "HS_Individual       0.70      0.82      0.75       729\n",
      "     HS_Group       0.76      0.65      0.70       408\n",
      "  HS_Religion       0.78      0.64      0.70       168\n",
      "      HS_Race       0.70      0.79      0.74       119\n",
      "  HS_Physical       0.59      0.28      0.38        57\n",
      "    HS_Gender       0.74      0.36      0.49        55\n",
      "     HS_Other       0.78      0.85      0.81       771\n",
      "      HS_Weak       0.67      0.79      0.73       681\n",
      "  HS_Moderate       0.70      0.57      0.63       359\n",
      "    HS_Strong       0.82      0.75      0.78        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.75      0.70      0.71      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 301.652574300766 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.333, Accuracy: 0.9062, F1 Micro: 0.7137, F1 Macro: 0.4818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2145, Accuracy: 0.9135, F1 Micro: 0.7562, F1 Macro: 0.5837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1713, Accuracy: 0.9244, F1 Micro: 0.7635, F1 Macro: 0.6195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.136, Accuracy: 0.9185, F1 Micro: 0.7694, F1 Macro: 0.6401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.9266, F1 Micro: 0.7853, F1 Macro: 0.7092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0873, Accuracy: 0.9289, F1 Micro: 0.787, F1 Macro: 0.7084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9294, F1 Micro: 0.7908, F1 Macro: 0.728\n",
      "Epoch 8/10, Train Loss: 0.0563, Accuracy: 0.9253, F1 Micro: 0.7808, F1 Macro: 0.7099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.048, Accuracy: 0.9293, F1 Micro: 0.7923, F1 Macro: 0.7336\n",
      "Epoch 10/10, Train Loss: 0.0401, Accuracy: 0.928, F1 Micro: 0.789, F1 Macro: 0.7289\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9293, F1 Micro: 0.7923, F1 Macro: 0.7336\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.76      0.72      0.74       729\n",
      "     HS_Group       0.70      0.69      0.70       408\n",
      "  HS_Religion       0.77      0.66      0.71       168\n",
      "      HS_Race       0.77      0.80      0.79       119\n",
      "  HS_Physical       0.66      0.40      0.50        57\n",
      "    HS_Gender       0.67      0.55      0.60        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.70      0.72       681\n",
      "  HS_Moderate       0.66      0.65      0.65       359\n",
      "    HS_Strong       0.83      0.80      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 306.0452392101288 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9241, F1 Micro: 0.7772, F1 Macro: 0.6912\n",
      "Total sampling time: 1192.28 seconds\n",
      "Total runtime: 20263.94561958313 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd9154",
   "metadata": {
    "papermill": {
     "duration": 0.18386,
     "end_time": "2025-01-30T11:43:27.256444",
     "exception": false,
     "start_time": "2025-01-30T11:43:27.072584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 7, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5356, Accuracy: 0.8282, F1 Micro: 0.0362, F1 Macro: 0.0134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4166, Accuracy: 0.8319, F1 Micro: 0.0681, F1 Macro: 0.0278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3751, Accuracy: 0.8415, F1 Micro: 0.1821, F1 Macro: 0.0677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3581, Accuracy: 0.8578, F1 Micro: 0.3857, F1 Macro: 0.1235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.332, Accuracy: 0.8682, F1 Micro: 0.4815, F1 Macro: 0.2321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2859, Accuracy: 0.8781, F1 Micro: 0.5652, F1 Macro: 0.3008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2566, Accuracy: 0.8836, F1 Micro: 0.6098, F1 Macro: 0.338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2353, Accuracy: 0.8861, F1 Micro: 0.6177, F1 Macro: 0.3756\n",
      "Epoch 9/10, Train Loss: 0.2121, Accuracy: 0.887, F1 Micro: 0.6122, F1 Macro: 0.3727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1803, Accuracy: 0.8878, F1 Micro: 0.6323, F1 Macro: 0.3868\n",
      "Model 1 - Iteration 658: Accuracy: 0.8878, F1 Micro: 0.6323, F1 Macro: 0.3868\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.76      0.79      1137\n",
      "      Abusive       0.85      0.73      0.78      1008\n",
      "HS_Individual       0.67      0.56      0.61       729\n",
      "     HS_Group       0.58      0.37      0.45       408\n",
      "  HS_Religion       0.59      0.20      0.30       168\n",
      "      HS_Race       1.00      0.06      0.11       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.71      0.59      0.65       771\n",
      "      HS_Weak       0.62      0.52      0.57       681\n",
      "  HS_Moderate       0.54      0.29      0.38       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.73      0.56      0.63      5589\n",
      "    macro avg       0.53      0.34      0.39      5589\n",
      " weighted avg       0.70      0.56      0.61      5589\n",
      "  samples avg       0.38      0.31      0.32      5589\n",
      "\n",
      "Training completed in 64.36910605430603 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5536, Accuracy: 0.8278, F1 Micro: 0.0173, F1 Macro: 0.0076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4234, Accuracy: 0.8307, F1 Micro: 0.0554, F1 Macro: 0.0223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3784, Accuracy: 0.8389, F1 Micro: 0.1535, F1 Macro: 0.0583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3671, Accuracy: 0.8559, F1 Micro: 0.359, F1 Macro: 0.1169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3443, Accuracy: 0.8661, F1 Micro: 0.4565, F1 Macro: 0.1811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2995, Accuracy: 0.8787, F1 Micro: 0.5536, F1 Macro: 0.2818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2651, Accuracy: 0.8836, F1 Micro: 0.5869, F1 Macro: 0.3338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2444, Accuracy: 0.8863, F1 Micro: 0.6088, F1 Macro: 0.3586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2181, Accuracy: 0.8868, F1 Micro: 0.611, F1 Macro: 0.3657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1836, Accuracy: 0.8893, F1 Micro: 0.6362, F1 Macro: 0.3754\n",
      "Model 2 - Iteration 658: Accuracy: 0.8893, F1 Micro: 0.6362, F1 Macro: 0.3754\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.73      0.78      1137\n",
      "      Abusive       0.85      0.75      0.79      1008\n",
      "HS_Individual       0.67      0.61      0.64       729\n",
      "     HS_Group       0.64      0.29      0.40       408\n",
      "  HS_Religion       0.68      0.17      0.27       168\n",
      "      HS_Race       1.00      0.03      0.07       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.63      0.66       771\n",
      "      HS_Weak       0.63      0.56      0.59       681\n",
      "  HS_Moderate       0.55      0.22      0.31       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.74      0.56      0.64      5589\n",
      "    macro avg       0.55      0.33      0.38      5589\n",
      " weighted avg       0.71      0.56      0.61      5589\n",
      "  samples avg       0.37      0.32      0.32      5589\n",
      "\n",
      "Training completed in 64.94477844238281 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5564, Accuracy: 0.8306, F1 Micro: 0.1013, F1 Macro: 0.0348\n",
      "Epoch 2/10, Train Loss: 0.4228, Accuracy: 0.8299, F1 Micro: 0.0419, F1 Macro: 0.0174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3793, Accuracy: 0.8449, F1 Micro: 0.2306, F1 Macro: 0.0817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3638, Accuracy: 0.8535, F1 Micro: 0.3461, F1 Macro: 0.1102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3389, Accuracy: 0.8684, F1 Micro: 0.4603, F1 Macro: 0.2077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2998, Accuracy: 0.8801, F1 Micro: 0.5717, F1 Macro: 0.3078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2586, Accuracy: 0.8866, F1 Micro: 0.6362, F1 Macro: 0.3766\n",
      "Epoch 8/10, Train Loss: 0.2428, Accuracy: 0.8879, F1 Micro: 0.6247, F1 Macro: 0.3817\n",
      "Epoch 9/10, Train Loss: 0.2118, Accuracy: 0.888, F1 Micro: 0.6162, F1 Macro: 0.3657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1825, Accuracy: 0.8884, F1 Micro: 0.6606, F1 Macro: 0.3957\n",
      "Model 3 - Iteration 658: Accuracy: 0.8884, F1 Micro: 0.6606, F1 Macro: 0.3957\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.82      0.80      1137\n",
      "      Abusive       0.82      0.77      0.80      1008\n",
      "HS_Individual       0.64      0.70      0.67       729\n",
      "     HS_Group       0.62      0.36      0.46       408\n",
      "  HS_Religion       0.58      0.21      0.31       168\n",
      "      HS_Race       1.00      0.01      0.02       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.64      0.70      0.67       771\n",
      "      HS_Weak       0.60      0.67      0.63       681\n",
      "  HS_Moderate       0.54      0.31      0.39       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.70      0.63      0.66      5589\n",
      "    macro avg       0.52      0.38      0.40      5589\n",
      " weighted avg       0.67      0.63      0.63      5589\n",
      "  samples avg       0.39      0.36      0.35      5589\n",
      "\n",
      "Training completed in 58.898388147354126 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8885, F1 Micro: 0.643, F1 Macro: 0.386\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 112.55525326728821 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4937, Accuracy: 0.831, F1 Micro: 0.4129, F1 Macro: 0.1109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4059, Accuracy: 0.8679, F1 Micro: 0.4774, F1 Macro: 0.1921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3543, Accuracy: 0.8908, F1 Micro: 0.6707, F1 Macro: 0.4077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2992, Accuracy: 0.9005, F1 Micro: 0.6722, F1 Macro: 0.4638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2545, Accuracy: 0.9024, F1 Micro: 0.7, F1 Macro: 0.5384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.209, Accuracy: 0.9023, F1 Micro: 0.7119, F1 Macro: 0.5245\n",
      "Epoch 7/10, Train Loss: 0.1879, Accuracy: 0.9045, F1 Micro: 0.6861, F1 Macro: 0.513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1553, Accuracy: 0.9047, F1 Micro: 0.7193, F1 Macro: 0.5623\n",
      "Epoch 9/10, Train Loss: 0.1313, Accuracy: 0.9059, F1 Micro: 0.7184, F1 Macro: 0.562\n",
      "Epoch 10/10, Train Loss: 0.1088, Accuracy: 0.9053, F1 Micro: 0.7169, F1 Macro: 0.5616\n",
      "Model 1 - Iteration 1646: Accuracy: 0.9047, F1 Micro: 0.7193, F1 Macro: 0.5623\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.84      0.82      1137\n",
      "      Abusive       0.84      0.83      0.84      1008\n",
      "HS_Individual       0.70      0.68      0.69       729\n",
      "     HS_Group       0.63      0.62      0.63       408\n",
      "  HS_Religion       0.62      0.60      0.61       168\n",
      "      HS_Race       0.67      0.69      0.68       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.74      0.71      0.72       771\n",
      "      HS_Weak       0.66      0.65      0.65       681\n",
      "  HS_Moderate       0.57      0.52      0.54       359\n",
      "    HS_Strong       0.78      0.44      0.57        97\n",
      "\n",
      "    micro avg       0.73      0.70      0.72      5589\n",
      "    macro avg       0.59      0.55      0.56      5589\n",
      " weighted avg       0.72      0.70      0.71      5589\n",
      "  samples avg       0.41      0.40      0.39      5589\n",
      "\n",
      "Training completed in 82.17232584953308 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5021, Accuracy: 0.8254, F1 Micro: 0.3939, F1 Macro: 0.1062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4117, Accuracy: 0.8633, F1 Micro: 0.4912, F1 Macro: 0.1947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3616, Accuracy: 0.8919, F1 Micro: 0.6575, F1 Macro: 0.3887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3061, Accuracy: 0.9011, F1 Micro: 0.6823, F1 Macro: 0.4759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2589, Accuracy: 0.8991, F1 Micro: 0.6868, F1 Macro: 0.5314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2184, Accuracy: 0.9008, F1 Micro: 0.7127, F1 Macro: 0.5398\n",
      "Epoch 7/10, Train Loss: 0.1921, Accuracy: 0.9031, F1 Micro: 0.7013, F1 Macro: 0.5189\n",
      "Epoch 8/10, Train Loss: 0.1614, Accuracy: 0.9041, F1 Micro: 0.7065, F1 Macro: 0.5548\n",
      "Epoch 9/10, Train Loss: 0.1351, Accuracy: 0.9045, F1 Micro: 0.705, F1 Macro: 0.5482\n",
      "Epoch 10/10, Train Loss: 0.1109, Accuracy: 0.906, F1 Micro: 0.7127, F1 Macro: 0.5633\n",
      "Model 2 - Iteration 1646: Accuracy: 0.9008, F1 Micro: 0.7127, F1 Macro: 0.5398\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.86      0.82      1137\n",
      "      Abusive       0.79      0.88      0.83      1008\n",
      "HS_Individual       0.66      0.70      0.68       729\n",
      "     HS_Group       0.64      0.59      0.62       408\n",
      "  HS_Religion       0.68      0.46      0.55       168\n",
      "      HS_Race       0.77      0.56      0.65       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.74      0.72       771\n",
      "      HS_Weak       0.62      0.68      0.65       681\n",
      "  HS_Moderate       0.61      0.40      0.48       359\n",
      "    HS_Strong       0.79      0.34      0.47        97\n",
      "\n",
      "    micro avg       0.71      0.71      0.71      5589\n",
      "    macro avg       0.59      0.52      0.54      5589\n",
      " weighted avg       0.70      0.71      0.70      5589\n",
      "  samples avg       0.41      0.40      0.39      5589\n",
      "\n",
      "Training completed in 81.27638983726501 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4996, Accuracy: 0.8271, F1 Micro: 0.4077, F1 Macro: 0.1093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4113, Accuracy: 0.858, F1 Micro: 0.4692, F1 Macro: 0.1691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3638, Accuracy: 0.892, F1 Micro: 0.6585, F1 Macro: 0.3792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3075, Accuracy: 0.9003, F1 Micro: 0.683, F1 Macro: 0.4397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2653, Accuracy: 0.8997, F1 Micro: 0.6939, F1 Macro: 0.5261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2174, Accuracy: 0.9005, F1 Micro: 0.7059, F1 Macro: 0.5177\n",
      "Epoch 7/10, Train Loss: 0.1887, Accuracy: 0.904, F1 Micro: 0.6873, F1 Macro: 0.511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1609, Accuracy: 0.9052, F1 Micro: 0.7148, F1 Macro: 0.5584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1387, Accuracy: 0.9018, F1 Micro: 0.7201, F1 Macro: 0.56\n",
      "Epoch 10/10, Train Loss: 0.1145, Accuracy: 0.9048, F1 Micro: 0.7136, F1 Macro: 0.5492\n",
      "Model 3 - Iteration 1646: Accuracy: 0.9018, F1 Micro: 0.7201, F1 Macro: 0.56\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.85      0.82      1137\n",
      "      Abusive       0.82      0.86      0.84      1008\n",
      "HS_Individual       0.66      0.73      0.69       729\n",
      "     HS_Group       0.63      0.58      0.61       408\n",
      "  HS_Religion       0.68      0.54      0.60       168\n",
      "      HS_Race       0.62      0.71      0.67       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.75      0.73       771\n",
      "      HS_Weak       0.62      0.72      0.66       681\n",
      "  HS_Moderate       0.59      0.50      0.54       359\n",
      "    HS_Strong       0.80      0.44      0.57        97\n",
      "\n",
      "    micro avg       0.71      0.73      0.72      5589\n",
      "    macro avg       0.58      0.56      0.56      5589\n",
      " weighted avg       0.70      0.73      0.71      5589\n",
      "  samples avg       0.41      0.41      0.39      5589\n",
      "\n",
      "Training completed in 85.57261514663696 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8955, F1 Micro: 0.6802, F1 Macro: 0.47\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 100.59902477264404 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4708, Accuracy: 0.8457, F1 Micro: 0.4242, F1 Macro: 0.1225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3771, Accuracy: 0.8834, F1 Micro: 0.5509, F1 Macro: 0.2911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3117, Accuracy: 0.9034, F1 Micro: 0.7044, F1 Macro: 0.4885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2629, Accuracy: 0.9089, F1 Micro: 0.7168, F1 Macro: 0.5427\n",
      "Epoch 5/10, Train Loss: 0.223, Accuracy: 0.91, F1 Micro: 0.7108, F1 Macro: 0.5378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1774, Accuracy: 0.9096, F1 Micro: 0.7197, F1 Macro: 0.5424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1449, Accuracy: 0.9103, F1 Micro: 0.7224, F1 Macro: 0.567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1166, Accuracy: 0.912, F1 Micro: 0.7312, F1 Macro: 0.5891\n",
      "Epoch 9/10, Train Loss: 0.1018, Accuracy: 0.9097, F1 Micro: 0.7261, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.087, Accuracy: 0.9104, F1 Micro: 0.732, F1 Macro: 0.5879\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9104, F1 Micro: 0.732, F1 Macro: 0.5879\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.81      0.82      1137\n",
      "      Abusive       0.86      0.85      0.85      1008\n",
      "HS_Individual       0.66      0.74      0.70       729\n",
      "     HS_Group       0.79      0.47      0.59       408\n",
      "  HS_Religion       0.73      0.57      0.64       168\n",
      "      HS_Race       0.77      0.67      0.72       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.50      0.05      0.10        55\n",
      "     HS_Other       0.77      0.74      0.75       771\n",
      "      HS_Weak       0.63      0.73      0.67       681\n",
      "  HS_Moderate       0.71      0.36      0.47       359\n",
      "    HS_Strong       0.78      0.69      0.73        97\n",
      "\n",
      "    micro avg       0.76      0.71      0.73      5589\n",
      "    macro avg       0.67      0.56      0.59      5589\n",
      " weighted avg       0.75      0.71      0.72      5589\n",
      "  samples avg       0.42      0.40      0.39      5589\n",
      "\n",
      "Training completed in 105.4891083240509 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4758, Accuracy: 0.8453, F1 Micro: 0.4174, F1 Macro: 0.122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3829, Accuracy: 0.8805, F1 Micro: 0.5354, F1 Macro: 0.28\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3178, Accuracy: 0.9037, F1 Micro: 0.7086, F1 Macro: 0.5011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2667, Accuracy: 0.9089, F1 Micro: 0.7181, F1 Macro: 0.5448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2249, Accuracy: 0.9098, F1 Micro: 0.7187, F1 Macro: 0.5399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1841, Accuracy: 0.9118, F1 Micro: 0.7263, F1 Macro: 0.5448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1462, Accuracy: 0.912, F1 Micro: 0.7385, F1 Macro: 0.5791\n",
      "Epoch 8/10, Train Loss: 0.1187, Accuracy: 0.9118, F1 Micro: 0.716, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1072, Accuracy: 0.9134, F1 Micro: 0.7436, F1 Macro: 0.6064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0897, Accuracy: 0.9132, F1 Micro: 0.7458, F1 Macro: 0.6026\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9132, F1 Micro: 0.7458, F1 Macro: 0.6026\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.83      1137\n",
      "      Abusive       0.86      0.86      0.86      1008\n",
      "HS_Individual       0.68      0.76      0.72       729\n",
      "     HS_Group       0.74      0.53      0.62       408\n",
      "  HS_Religion       0.73      0.56      0.64       168\n",
      "      HS_Race       0.72      0.71      0.72       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.67      0.07      0.13        55\n",
      "     HS_Other       0.76      0.78      0.77       771\n",
      "      HS_Weak       0.64      0.74      0.69       681\n",
      "  HS_Moderate       0.69      0.41      0.51       359\n",
      "    HS_Strong       0.77      0.73      0.75        97\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5589\n",
      "    macro avg       0.67      0.58      0.60      5589\n",
      " weighted avg       0.75      0.73      0.74      5589\n",
      "  samples avg       0.43      0.41      0.40      5589\n",
      "\n",
      "Training completed in 107.56635689735413 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4738, Accuracy: 0.8412, F1 Micro: 0.4021, F1 Macro: 0.1247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3821, Accuracy: 0.8842, F1 Micro: 0.5665, F1 Macro: 0.2888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3159, Accuracy: 0.9005, F1 Micro: 0.7094, F1 Macro: 0.4766\n",
      "Epoch 4/10, Train Loss: 0.2642, Accuracy: 0.908, F1 Micro: 0.7049, F1 Macro: 0.5317\n",
      "Epoch 5/10, Train Loss: 0.2257, Accuracy: 0.9087, F1 Micro: 0.7045, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1817, Accuracy: 0.9097, F1 Micro: 0.7166, F1 Macro: 0.5434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1459, Accuracy: 0.9098, F1 Micro: 0.7264, F1 Macro: 0.571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1177, Accuracy: 0.9098, F1 Micro: 0.7298, F1 Macro: 0.5843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1046, Accuracy: 0.9111, F1 Micro: 0.7337, F1 Macro: 0.5946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0934, Accuracy: 0.9109, F1 Micro: 0.7389, F1 Macro: 0.5939\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9109, F1 Micro: 0.7389, F1 Macro: 0.5939\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.83      0.82      1137\n",
      "      Abusive       0.85      0.86      0.86      1008\n",
      "HS_Individual       0.67      0.75      0.71       729\n",
      "     HS_Group       0.73      0.53      0.61       408\n",
      "  HS_Religion       0.78      0.52      0.62       168\n",
      "      HS_Race       0.77      0.61      0.68       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.57      0.07      0.13        55\n",
      "     HS_Other       0.73      0.78      0.75       771\n",
      "      HS_Weak       0.65      0.74      0.69       681\n",
      "  HS_Moderate       0.66      0.44      0.53       359\n",
      "    HS_Strong       0.80      0.66      0.72        97\n",
      "\n",
      "    micro avg       0.75      0.73      0.74      5589\n",
      "    macro avg       0.67      0.57      0.59      5589\n",
      " weighted avg       0.74      0.73      0.73      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 105.74816036224365 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9008, F1 Micro: 0.6998, F1 Macro: 0.5116\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 90.5636613368988 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4546, Accuracy: 0.8608, F1 Micro: 0.4723, F1 Macro: 0.1767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3623, Accuracy: 0.8991, F1 Micro: 0.6634, F1 Macro: 0.4436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2991, Accuracy: 0.9077, F1 Micro: 0.7342, F1 Macro: 0.5618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2456, Accuracy: 0.9126, F1 Micro: 0.7456, F1 Macro: 0.592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2011, Accuracy: 0.9154, F1 Micro: 0.7464, F1 Macro: 0.5879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1595, Accuracy: 0.911, F1 Micro: 0.7486, F1 Macro: 0.6003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1299, Accuracy: 0.9123, F1 Micro: 0.75, F1 Macro: 0.6048\n",
      "Epoch 8/10, Train Loss: 0.1109, Accuracy: 0.9167, F1 Micro: 0.7483, F1 Macro: 0.6205\n",
      "Epoch 9/10, Train Loss: 0.0941, Accuracy: 0.916, F1 Micro: 0.7405, F1 Macro: 0.619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9173, F1 Micro: 0.76, F1 Macro: 0.6368\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9173, F1 Micro: 0.76, F1 Macro: 0.6368\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1137\n",
      "      Abusive       0.86      0.90      0.88      1008\n",
      "HS_Individual       0.69      0.75      0.72       729\n",
      "     HS_Group       0.72      0.57      0.64       408\n",
      "  HS_Religion       0.73      0.62      0.67       168\n",
      "      HS_Race       0.77      0.71      0.74       119\n",
      "  HS_Physical       0.75      0.05      0.10        57\n",
      "    HS_Gender       0.57      0.15      0.23        55\n",
      "     HS_Other       0.77      0.80      0.79       771\n",
      "      HS_Weak       0.66      0.73      0.69       681\n",
      "  HS_Moderate       0.67      0.49      0.57       359\n",
      "    HS_Strong       0.78      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5589\n",
      "    macro avg       0.73      0.62      0.64      5589\n",
      " weighted avg       0.76      0.76      0.75      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 125.52891302108765 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.46, Accuracy: 0.8542, F1 Micro: 0.4831, F1 Macro: 0.1942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3692, Accuracy: 0.8972, F1 Micro: 0.6632, F1 Macro: 0.4716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3034, Accuracy: 0.9095, F1 Micro: 0.7263, F1 Macro: 0.5466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2452, Accuracy: 0.9127, F1 Micro: 0.7382, F1 Macro: 0.5858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2019, Accuracy: 0.9158, F1 Micro: 0.7468, F1 Macro: 0.5898\n",
      "Epoch 6/10, Train Loss: 0.1626, Accuracy: 0.9148, F1 Micro: 0.7425, F1 Macro: 0.5993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1363, Accuracy: 0.9156, F1 Micro: 0.7495, F1 Macro: 0.6003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1102, Accuracy: 0.916, F1 Micro: 0.7502, F1 Macro: 0.6319\n",
      "Epoch 9/10, Train Loss: 0.093, Accuracy: 0.9167, F1 Micro: 0.7427, F1 Macro: 0.6223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.9162, F1 Micro: 0.7518, F1 Macro: 0.6338\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9162, F1 Micro: 0.7518, F1 Macro: 0.6338\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1137\n",
      "      Abusive       0.88      0.86      0.87      1008\n",
      "HS_Individual       0.72      0.68      0.70       729\n",
      "     HS_Group       0.65      0.64      0.65       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.75      0.72      0.74       119\n",
      "  HS_Physical       1.00      0.04      0.07        57\n",
      "    HS_Gender       0.67      0.18      0.29        55\n",
      "     HS_Other       0.79      0.78      0.78       771\n",
      "      HS_Weak       0.69      0.65      0.67       681\n",
      "  HS_Moderate       0.62      0.55      0.58       359\n",
      "    HS_Strong       0.76      0.72      0.74        97\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5589\n",
      "    macro avg       0.76      0.61      0.63      5589\n",
      " weighted avg       0.77      0.73      0.75      5589\n",
      "  samples avg       0.43      0.41      0.41      5589\n",
      "\n",
      "Training completed in 126.24452805519104 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4602, Accuracy: 0.8475, F1 Micro: 0.5118, F1 Macro: 0.2244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3702, Accuracy: 0.898, F1 Micro: 0.6806, F1 Macro: 0.4837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3033, Accuracy: 0.9088, F1 Micro: 0.732, F1 Macro: 0.5473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2469, Accuracy: 0.9128, F1 Micro: 0.7449, F1 Macro: 0.5927\n",
      "Epoch 5/10, Train Loss: 0.2009, Accuracy: 0.9146, F1 Micro: 0.743, F1 Macro: 0.5809\n",
      "Epoch 6/10, Train Loss: 0.1637, Accuracy: 0.9109, F1 Micro: 0.7446, F1 Macro: 0.5911\n",
      "Epoch 7/10, Train Loss: 0.1358, Accuracy: 0.9139, F1 Micro: 0.736, F1 Macro: 0.5811\n",
      "Epoch 8/10, Train Loss: 0.1121, Accuracy: 0.9132, F1 Micro: 0.7438, F1 Macro: 0.6104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0955, Accuracy: 0.9142, F1 Micro: 0.7501, F1 Macro: 0.6149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0855, Accuracy: 0.9138, F1 Micro: 0.7519, F1 Macro: 0.6219\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9138, F1 Micro: 0.7519, F1 Macro: 0.6219\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.84      1137\n",
      "      Abusive       0.85      0.90      0.88      1008\n",
      "HS_Individual       0.68      0.74      0.71       729\n",
      "     HS_Group       0.69      0.58      0.63       408\n",
      "  HS_Religion       0.68      0.68      0.68       168\n",
      "      HS_Race       0.73      0.70      0.71       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.73      0.15      0.24        55\n",
      "     HS_Other       0.76      0.79      0.77       771\n",
      "      HS_Weak       0.65      0.72      0.68       681\n",
      "  HS_Moderate       0.63      0.50      0.56       359\n",
      "    HS_Strong       0.76      0.75      0.76        97\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5589\n",
      "    macro avg       0.67      0.61      0.62      5589\n",
      " weighted avg       0.74      0.75      0.74      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 123.58691549301147 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9046, F1 Micro: 0.7135, F1 Macro: 0.5414\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 81.50728964805603 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4467, Accuracy: 0.877, F1 Micro: 0.5814, F1 Macro: 0.2846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3349, Accuracy: 0.907, F1 Micro: 0.7164, F1 Macro: 0.5265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2806, Accuracy: 0.9147, F1 Micro: 0.7267, F1 Macro: 0.5348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2353, Accuracy: 0.9163, F1 Micro: 0.7282, F1 Macro: 0.5687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1798, Accuracy: 0.9201, F1 Micro: 0.763, F1 Macro: 0.6199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1537, Accuracy: 0.9167, F1 Micro: 0.7661, F1 Macro: 0.6331\n",
      "Epoch 7/10, Train Loss: 0.1187, Accuracy: 0.9186, F1 Micro: 0.7566, F1 Macro: 0.6275\n",
      "Epoch 8/10, Train Loss: 0.1071, Accuracy: 0.9202, F1 Micro: 0.7592, F1 Macro: 0.6371\n",
      "Epoch 9/10, Train Loss: 0.0911, Accuracy: 0.9137, F1 Micro: 0.7627, F1 Macro: 0.6603\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9212, F1 Micro: 0.7633, F1 Macro: 0.6481\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9167, F1 Micro: 0.7661, F1 Macro: 0.6331\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1137\n",
      "      Abusive       0.87      0.86      0.87      1008\n",
      "HS_Individual       0.69      0.78      0.73       729\n",
      "     HS_Group       0.68      0.67      0.68       408\n",
      "  HS_Religion       0.66      0.71      0.68       168\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.45      0.09      0.15        55\n",
      "     HS_Other       0.73      0.84      0.78       771\n",
      "      HS_Weak       0.66      0.76      0.71       681\n",
      "  HS_Moderate       0.65      0.57      0.61       359\n",
      "    HS_Strong       0.75      0.78      0.77        97\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5589\n",
      "    macro avg       0.64      0.65      0.63      5589\n",
      " weighted avg       0.74      0.79      0.76      5589\n",
      "  samples avg       0.43      0.44      0.42      5589\n",
      "\n",
      "Training completed in 140.9108064174652 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4532, Accuracy: 0.8668, F1 Micro: 0.5594, F1 Macro: 0.2638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3435, Accuracy: 0.9063, F1 Micro: 0.7064, F1 Macro: 0.497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2858, Accuracy: 0.9138, F1 Micro: 0.7331, F1 Macro: 0.5418\n",
      "Epoch 4/10, Train Loss: 0.2399, Accuracy: 0.9167, F1 Micro: 0.7283, F1 Macro: 0.5655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1848, Accuracy: 0.9204, F1 Micro: 0.7638, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1566, Accuracy: 0.915, F1 Micro: 0.7646, F1 Macro: 0.6436\n",
      "Epoch 7/10, Train Loss: 0.1226, Accuracy: 0.9209, F1 Micro: 0.7593, F1 Macro: 0.6371\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.9211, F1 Micro: 0.7604, F1 Macro: 0.6375\n",
      "Epoch 9/10, Train Loss: 0.089, Accuracy: 0.9181, F1 Micro: 0.7639, F1 Macro: 0.6603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 0.9214, F1 Micro: 0.7655, F1 Macro: 0.6513\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9214, F1 Micro: 0.7655, F1 Macro: 0.6513\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1137\n",
      "      Abusive       0.89      0.88      0.89      1008\n",
      "HS_Individual       0.73      0.72      0.72       729\n",
      "     HS_Group       0.74      0.61      0.67       408\n",
      "  HS_Religion       0.69      0.63      0.66       168\n",
      "      HS_Race       0.83      0.68      0.75       119\n",
      "  HS_Physical       0.62      0.09      0.15        57\n",
      "    HS_Gender       0.48      0.20      0.28        55\n",
      "     HS_Other       0.81      0.77      0.79       771\n",
      "      HS_Weak       0.70      0.69      0.69       681\n",
      "  HS_Moderate       0.67      0.54      0.60       359\n",
      "    HS_Strong       0.81      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5589\n",
      "    macro avg       0.74      0.61      0.65      5589\n",
      " weighted avg       0.79      0.74      0.76      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 140.4195122718811 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4516, Accuracy: 0.8631, F1 Micro: 0.5068, F1 Macro: 0.2225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3441, Accuracy: 0.905, F1 Micro: 0.7085, F1 Macro: 0.4838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.288, Accuracy: 0.9129, F1 Micro: 0.7344, F1 Macro: 0.5415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2409, Accuracy: 0.9168, F1 Micro: 0.7348, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1859, Accuracy: 0.9198, F1 Micro: 0.7631, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1533, Accuracy: 0.9183, F1 Micro: 0.767, F1 Macro: 0.6327\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.9193, F1 Micro: 0.7584, F1 Macro: 0.6297\n",
      "Epoch 8/10, Train Loss: 0.1106, Accuracy: 0.9204, F1 Micro: 0.7597, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9221, F1 Micro: 0.7675, F1 Macro: 0.646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0784, Accuracy: 0.922, F1 Micro: 0.7681, F1 Macro: 0.6532\n",
      "Model 3 - Iteration 4055: Accuracy: 0.922, F1 Micro: 0.7681, F1 Macro: 0.6532\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1137\n",
      "      Abusive       0.89      0.88      0.89      1008\n",
      "HS_Individual       0.72      0.74      0.73       729\n",
      "     HS_Group       0.74      0.59      0.66       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.80      0.71      0.75       119\n",
      "  HS_Physical       0.50      0.07      0.12        57\n",
      "    HS_Gender       0.73      0.20      0.31        55\n",
      "     HS_Other       0.82      0.77      0.80       771\n",
      "      HS_Weak       0.68      0.72      0.70       681\n",
      "  HS_Moderate       0.69      0.54      0.60       359\n",
      "    HS_Strong       0.79      0.69      0.74        97\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5589\n",
      "    macro avg       0.75      0.62      0.65      5589\n",
      " weighted avg       0.79      0.75      0.76      5589\n",
      "  samples avg       0.44      0.42      0.42      5589\n",
      "\n",
      "Training completed in 143.67948389053345 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9077, F1 Micro: 0.7241, F1 Macro: 0.5623\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 73.37940096855164 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4349, Accuracy: 0.8772, F1 Micro: 0.6456, F1 Macro: 0.3394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3298, Accuracy: 0.9082, F1 Micro: 0.7341, F1 Macro: 0.5624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2772, Accuracy: 0.9146, F1 Micro: 0.7533, F1 Macro: 0.5953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.224, Accuracy: 0.9188, F1 Micro: 0.7569, F1 Macro: 0.6048\n",
      "Epoch 5/10, Train Loss: 0.178, Accuracy: 0.9207, F1 Micro: 0.7536, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1403, Accuracy: 0.9185, F1 Micro: 0.7612, F1 Macro: 0.6429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1153, Accuracy: 0.9174, F1 Micro: 0.7633, F1 Macro: 0.6597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0963, Accuracy: 0.9226, F1 Micro: 0.7672, F1 Macro: 0.6516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.082, Accuracy: 0.9213, F1 Micro: 0.7691, F1 Macro: 0.6782\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9225, F1 Micro: 0.7682, F1 Macro: 0.672\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9213, F1 Micro: 0.7691, F1 Macro: 0.6782\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1137\n",
      "      Abusive       0.87      0.90      0.89      1008\n",
      "HS_Individual       0.73      0.70      0.72       729\n",
      "     HS_Group       0.69      0.66      0.68       408\n",
      "  HS_Religion       0.71      0.65      0.68       168\n",
      "      HS_Race       0.72      0.78      0.75       119\n",
      "  HS_Physical       0.48      0.19      0.27        57\n",
      "    HS_Gender       0.67      0.29      0.41        55\n",
      "     HS_Other       0.82      0.77      0.79       771\n",
      "      HS_Weak       0.71      0.69      0.70       681\n",
      "  HS_Moderate       0.64      0.60      0.62       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.72      0.66      0.68      5589\n",
      " weighted avg       0.78      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 159.36865186691284 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4416, Accuracy: 0.8759, F1 Micro: 0.6266, F1 Macro: 0.3321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3343, Accuracy: 0.9061, F1 Micro: 0.7333, F1 Macro: 0.5669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2773, Accuracy: 0.9148, F1 Micro: 0.7558, F1 Macro: 0.5942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2295, Accuracy: 0.9189, F1 Micro: 0.7642, F1 Macro: 0.6309\n",
      "Epoch 5/10, Train Loss: 0.1812, Accuracy: 0.9229, F1 Micro: 0.761, F1 Macro: 0.639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1437, Accuracy: 0.9213, F1 Micro: 0.7676, F1 Macro: 0.6655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1138, Accuracy: 0.9206, F1 Micro: 0.7705, F1 Macro: 0.6717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0975, Accuracy: 0.9225, F1 Micro: 0.7735, F1 Macro: 0.6741\n",
      "Epoch 9/10, Train Loss: 0.0817, Accuracy: 0.9205, F1 Micro: 0.7677, F1 Macro: 0.6903\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.922, F1 Micro: 0.7585, F1 Macro: 0.6696\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9225, F1 Micro: 0.7735, F1 Macro: 0.6741\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1137\n",
      "      Abusive       0.88      0.89      0.89      1008\n",
      "HS_Individual       0.72      0.74      0.73       729\n",
      "     HS_Group       0.72      0.63      0.67       408\n",
      "  HS_Religion       0.81      0.55      0.66       168\n",
      "      HS_Race       0.73      0.76      0.74       119\n",
      "  HS_Physical       0.57      0.14      0.23        57\n",
      "    HS_Gender       0.53      0.35      0.42        55\n",
      "     HS_Other       0.79      0.82      0.80       771\n",
      "      HS_Weak       0.70      0.72      0.71       681\n",
      "  HS_Moderate       0.66      0.56      0.61       359\n",
      "    HS_Strong       0.80      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.73      0.65      0.67      5589\n",
      " weighted avg       0.78      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 158.64456152915955 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4413, Accuracy: 0.8805, F1 Micro: 0.603, F1 Macro: 0.3112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.333, Accuracy: 0.9075, F1 Micro: 0.7283, F1 Macro: 0.5422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2797, Accuracy: 0.9108, F1 Micro: 0.7498, F1 Macro: 0.5875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2293, Accuracy: 0.9174, F1 Micro: 0.7549, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1824, Accuracy: 0.9207, F1 Micro: 0.7553, F1 Macro: 0.6099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1413, Accuracy: 0.9183, F1 Micro: 0.7671, F1 Macro: 0.6463\n",
      "Epoch 7/10, Train Loss: 0.1167, Accuracy: 0.9138, F1 Micro: 0.7646, F1 Macro: 0.6465\n",
      "Epoch 8/10, Train Loss: 0.0979, Accuracy: 0.9222, F1 Micro: 0.766, F1 Macro: 0.6391\n",
      "Epoch 9/10, Train Loss: 0.0858, Accuracy: 0.9202, F1 Micro: 0.7618, F1 Macro: 0.6578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.9229, F1 Micro: 0.77, F1 Macro: 0.6517\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9229, F1 Micro: 0.77, F1 Macro: 0.6517\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1137\n",
      "      Abusive       0.89      0.88      0.89      1008\n",
      "HS_Individual       0.75      0.68      0.72       729\n",
      "     HS_Group       0.71      0.67      0.69       408\n",
      "  HS_Religion       0.86      0.54      0.66       168\n",
      "      HS_Race       0.82      0.68      0.74       119\n",
      "  HS_Physical       0.75      0.05      0.10        57\n",
      "    HS_Gender       0.77      0.18      0.29        55\n",
      "     HS_Other       0.78      0.82      0.80       771\n",
      "      HS_Weak       0.73      0.67      0.70       681\n",
      "  HS_Moderate       0.65      0.61      0.63       359\n",
      "    HS_Strong       0.79      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5589\n",
      "    macro avg       0.78      0.61      0.65      5589\n",
      " weighted avg       0.80      0.75      0.76      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 159.26140117645264 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9101, F1 Micro: 0.7319, F1 Macro: 0.5799\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 66.3070456981659 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4273, Accuracy: 0.889, F1 Micro: 0.6232, F1 Macro: 0.365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3206, Accuracy: 0.9091, F1 Micro: 0.6928, F1 Macro: 0.5007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2599, Accuracy: 0.9152, F1 Micro: 0.7556, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2032, Accuracy: 0.9179, F1 Micro: 0.765, F1 Macro: 0.6166\n",
      "Epoch 5/10, Train Loss: 0.1699, Accuracy: 0.9221, F1 Micro: 0.7532, F1 Macro: 0.6301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1316, Accuracy: 0.9236, F1 Micro: 0.7677, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1034, Accuracy: 0.9224, F1 Micro: 0.7692, F1 Macro: 0.6664\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9228, F1 Micro: 0.7676, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9235, F1 Micro: 0.7783, F1 Macro: 0.6929\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.9235, F1 Micro: 0.7707, F1 Macro: 0.6939\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9235, F1 Micro: 0.7783, F1 Macro: 0.6929\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.74      0.73      0.74       729\n",
      "     HS_Group       0.69      0.68      0.68       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.76      0.78      0.77       119\n",
      "  HS_Physical       0.62      0.18      0.27        57\n",
      "    HS_Gender       0.68      0.38      0.49        55\n",
      "     HS_Other       0.79      0.81      0.80       771\n",
      "      HS_Weak       0.72      0.71      0.71       681\n",
      "  HS_Moderate       0.63      0.62      0.62       359\n",
      "    HS_Strong       0.82      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5589\n",
      "    macro avg       0.74      0.67      0.69      5589\n",
      " weighted avg       0.78      0.77      0.78      5589\n",
      "  samples avg       0.44      0.44      0.42      5589\n",
      "\n",
      "Training completed in 171.83005475997925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4318, Accuracy: 0.8852, F1 Micro: 0.6215, F1 Macro: 0.3791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.325, Accuracy: 0.9107, F1 Micro: 0.7066, F1 Macro: 0.5208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2611, Accuracy: 0.9152, F1 Micro: 0.7525, F1 Macro: 0.5932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2081, Accuracy: 0.9177, F1 Micro: 0.7703, F1 Macro: 0.6379\n",
      "Epoch 5/10, Train Loss: 0.1736, Accuracy: 0.9223, F1 Micro: 0.7522, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1345, Accuracy: 0.9242, F1 Micro: 0.7739, F1 Macro: 0.6657\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.921, F1 Micro: 0.7727, F1 Macro: 0.6853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9219, F1 Micro: 0.7739, F1 Macro: 0.6909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.9233, F1 Micro: 0.7811, F1 Macro: 0.7006\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9234, F1 Micro: 0.7725, F1 Macro: 0.6845\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9233, F1 Micro: 0.7811, F1 Macro: 0.7006\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1137\n",
      "      Abusive       0.87      0.91      0.89      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.71      0.64      0.68       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.72      0.76      0.74       119\n",
      "  HS_Physical       0.67      0.21      0.32        57\n",
      "    HS_Gender       0.58      0.53      0.55        55\n",
      "     HS_Other       0.79      0.82      0.81       771\n",
      "      HS_Weak       0.69      0.75      0.72       681\n",
      "  HS_Moderate       0.65      0.60      0.62       359\n",
      "    HS_Strong       0.86      0.70      0.77        97\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5589\n",
      "    macro avg       0.74      0.69      0.70      5589\n",
      " weighted avg       0.77      0.79      0.78      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 172.4673879146576 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4323, Accuracy: 0.8833, F1 Micro: 0.6429, F1 Macro: 0.3758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.325, Accuracy: 0.9097, F1 Micro: 0.7041, F1 Macro: 0.5129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2623, Accuracy: 0.9143, F1 Micro: 0.7538, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2132, Accuracy: 0.9198, F1 Micro: 0.7708, F1 Macro: 0.6311\n",
      "Epoch 5/10, Train Loss: 0.1712, Accuracy: 0.9208, F1 Micro: 0.7604, F1 Macro: 0.6281\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.9221, F1 Micro: 0.7581, F1 Macro: 0.6287\n",
      "Epoch 7/10, Train Loss: 0.1075, Accuracy: 0.9233, F1 Micro: 0.7697, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9214, F1 Micro: 0.7758, F1 Macro: 0.6777\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.9238, F1 Micro: 0.7675, F1 Macro: 0.6709\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9234, F1 Micro: 0.7735, F1 Macro: 0.6961\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9214, F1 Micro: 0.7758, F1 Macro: 0.6777\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1137\n",
      "      Abusive       0.86      0.92      0.89      1008\n",
      "HS_Individual       0.72      0.73      0.72       729\n",
      "     HS_Group       0.67      0.70      0.68       408\n",
      "  HS_Religion       0.75      0.68      0.72       168\n",
      "      HS_Race       0.74      0.71      0.72       119\n",
      "  HS_Physical       0.64      0.12      0.21        57\n",
      "    HS_Gender       0.62      0.33      0.43        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.69      0.70      0.70       681\n",
      "  HS_Moderate       0.62      0.61      0.61       359\n",
      "    HS_Strong       0.77      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5589\n",
      "    macro avg       0.72      0.67      0.68      5589\n",
      " weighted avg       0.76      0.78      0.77      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 169.38183736801147 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9119, F1 Micro: 0.7385, F1 Macro: 0.5957\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 59.694204330444336 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.422, Accuracy: 0.8956, F1 Micro: 0.6624, F1 Macro: 0.4297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.31, Accuracy: 0.9134, F1 Micro: 0.7372, F1 Macro: 0.5566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2503, Accuracy: 0.9213, F1 Micro: 0.754, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2031, Accuracy: 0.9139, F1 Micro: 0.7594, F1 Macro: 0.6101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1642, Accuracy: 0.9239, F1 Micro: 0.7703, F1 Macro: 0.6581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9242, F1 Micro: 0.7743, F1 Macro: 0.668\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9261, F1 Micro: 0.7725, F1 Macro: 0.6591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9234, F1 Micro: 0.7769, F1 Macro: 0.6982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9247, F1 Micro: 0.7832, F1 Macro: 0.7184\n",
      "Epoch 10/10, Train Loss: 0.0661, Accuracy: 0.9214, F1 Micro: 0.7799, F1 Macro: 0.7166\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9247, F1 Micro: 0.7832, F1 Macro: 0.7184\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.72      0.63      0.68       408\n",
      "  HS_Religion       0.73      0.73      0.73       168\n",
      "      HS_Race       0.74      0.77      0.76       119\n",
      "  HS_Physical       0.66      0.37      0.47        57\n",
      "    HS_Gender       0.61      0.51      0.55        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.69      0.74      0.71       681\n",
      "  HS_Moderate       0.67      0.56      0.61       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 186.32782578468323 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4262, Accuracy: 0.8934, F1 Micro: 0.6463, F1 Macro: 0.4261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3147, Accuracy: 0.9143, F1 Micro: 0.7416, F1 Macro: 0.5647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.253, Accuracy: 0.9213, F1 Micro: 0.7565, F1 Macro: 0.6205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.914, F1 Micro: 0.7618, F1 Macro: 0.6311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.9193, F1 Micro: 0.7663, F1 Macro: 0.6431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1287, Accuracy: 0.9232, F1 Micro: 0.7698, F1 Macro: 0.6657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1049, Accuracy: 0.9225, F1 Micro: 0.7737, F1 Macro: 0.68\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0847, Accuracy: 0.9228, F1 Micro: 0.7749, F1 Macro: 0.6958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9251, F1 Micro: 0.7755, F1 Macro: 0.707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9231, F1 Micro: 0.7759, F1 Macro: 0.7137\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9231, F1 Micro: 0.7759, F1 Macro: 0.7137\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.91      0.87      0.89      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.68      0.64      0.66       408\n",
      "  HS_Religion       0.74      0.65      0.69       168\n",
      "      HS_Race       0.73      0.77      0.75       119\n",
      "  HS_Physical       0.67      0.42      0.52        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.80      0.81      0.80       771\n",
      "      HS_Weak       0.71      0.71      0.71       681\n",
      "  HS_Moderate       0.63      0.58      0.60       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5589\n",
      "    macro avg       0.74      0.69      0.71      5589\n",
      " weighted avg       0.78      0.77      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 189.57660818099976 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4268, Accuracy: 0.8916, F1 Micro: 0.6223, F1 Macro: 0.3709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.316, Accuracy: 0.9125, F1 Micro: 0.7349, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.254, Accuracy: 0.9213, F1 Micro: 0.7526, F1 Macro: 0.603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2053, Accuracy: 0.918, F1 Micro: 0.7662, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1664, Accuracy: 0.9209, F1 Micro: 0.7704, F1 Macro: 0.6402\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9235, F1 Micro: 0.7664, F1 Macro: 0.6407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9232, F1 Micro: 0.7715, F1 Macro: 0.6647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9219, F1 Micro: 0.7725, F1 Macro: 0.6712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0752, Accuracy: 0.9244, F1 Micro: 0.7776, F1 Macro: 0.7055\n",
      "Epoch 10/10, Train Loss: 0.067, Accuracy: 0.9227, F1 Micro: 0.7772, F1 Macro: 0.708\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9244, F1 Micro: 0.7776, F1 Macro: 0.7055\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1137\n",
      "      Abusive       0.87      0.91      0.89      1008\n",
      "HS_Individual       0.72      0.74      0.73       729\n",
      "     HS_Group       0.75      0.61      0.67       408\n",
      "  HS_Religion       0.75      0.68      0.71       168\n",
      "      HS_Race       0.82      0.66      0.73       119\n",
      "  HS_Physical       0.65      0.30      0.41        57\n",
      "    HS_Gender       0.69      0.45      0.55        55\n",
      "     HS_Other       0.81      0.78      0.80       771\n",
      "      HS_Weak       0.70      0.72      0.71       681\n",
      "  HS_Moderate       0.69      0.55      0.61       359\n",
      "    HS_Strong       0.81      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5589\n",
      "    macro avg       0.76      0.67      0.71      5589\n",
      " weighted avg       0.79      0.76      0.77      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 186.5980625152588 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9134, F1 Micro: 0.7436, F1 Macro: 0.6103\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 53.78933119773865 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4133, Accuracy: 0.8963, F1 Micro: 0.6502, F1 Macro: 0.385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3049, Accuracy: 0.912, F1 Micro: 0.7166, F1 Macro: 0.4832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2455, Accuracy: 0.9207, F1 Micro: 0.7531, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2019, Accuracy: 0.9143, F1 Micro: 0.7648, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9245, F1 Micro: 0.7802, F1 Macro: 0.666\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9265, F1 Micro: 0.7786, F1 Macro: 0.6668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.926, F1 Micro: 0.7807, F1 Macro: 0.7056\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0854, Accuracy: 0.9292, F1 Micro: 0.7876, F1 Macro: 0.7134\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9242, F1 Micro: 0.7818, F1 Macro: 0.7098\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9263, F1 Micro: 0.7772, F1 Macro: 0.6982\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9292, F1 Micro: 0.7876, F1 Macro: 0.7134\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.90      0.89      0.89      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.76      0.64      0.69       408\n",
      "  HS_Religion       0.75      0.68      0.71       168\n",
      "      HS_Race       0.85      0.66      0.75       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.74      0.42      0.53        55\n",
      "     HS_Other       0.85      0.79      0.82       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.69      0.57      0.62       359\n",
      "    HS_Strong       0.86      0.72      0.79        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.79      0.66      0.71      5589\n",
      " weighted avg       0.82      0.76      0.78      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 197.50155663490295 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4181, Accuracy: 0.8944, F1 Micro: 0.6462, F1 Macro: 0.3871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3071, Accuracy: 0.9104, F1 Micro: 0.7068, F1 Macro: 0.4794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2488, Accuracy: 0.9216, F1 Micro: 0.7524, F1 Macro: 0.5907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2023, Accuracy: 0.9173, F1 Micro: 0.7733, F1 Macro: 0.6586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9268, F1 Micro: 0.7803, F1 Macro: 0.6711\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9262, F1 Micro: 0.7747, F1 Macro: 0.673\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9252, F1 Micro: 0.7716, F1 Macro: 0.6937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9274, F1 Micro: 0.7858, F1 Macro: 0.7121\n",
      "Epoch 9/10, Train Loss: 0.0783, Accuracy: 0.9264, F1 Micro: 0.7831, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0623, Accuracy: 0.9258, F1 Micro: 0.7872, F1 Macro: 0.7215\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9258, F1 Micro: 0.7872, F1 Macro: 0.7215\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.71      0.79      0.74       729\n",
      "     HS_Group       0.74      0.61      0.67       408\n",
      "  HS_Religion       0.74      0.69      0.72       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.70      0.37      0.48        57\n",
      "    HS_Gender       0.68      0.49      0.57        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.68      0.76      0.72       681\n",
      "  HS_Moderate       0.67      0.53      0.59       359\n",
      "    HS_Strong       0.83      0.80      0.82        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 197.21773886680603 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4167, Accuracy: 0.8957, F1 Micro: 0.6532, F1 Macro: 0.3929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3078, Accuracy: 0.9111, F1 Micro: 0.7211, F1 Macro: 0.5012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2481, Accuracy: 0.921, F1 Micro: 0.7571, F1 Macro: 0.5919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2046, Accuracy: 0.9167, F1 Micro: 0.769, F1 Macro: 0.6433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1628, Accuracy: 0.9227, F1 Micro: 0.7753, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1259, Accuracy: 0.9253, F1 Micro: 0.778, F1 Macro: 0.6647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1071, Accuracy: 0.9263, F1 Micro: 0.7781, F1 Macro: 0.6991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.9281, F1 Micro: 0.7828, F1 Macro: 0.7107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0773, Accuracy: 0.9255, F1 Micro: 0.7832, F1 Macro: 0.7175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0641, Accuracy: 0.926, F1 Micro: 0.786, F1 Macro: 0.7185\n",
      "Model 3 - Iteration 6285: Accuracy: 0.926, F1 Micro: 0.786, F1 Macro: 0.7185\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.72      0.63      0.67       408\n",
      "  HS_Religion       0.75      0.66      0.70       168\n",
      "      HS_Race       0.76      0.78      0.77       119\n",
      "  HS_Physical       0.74      0.35      0.48        57\n",
      "    HS_Gender       0.64      0.53      0.58        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.70      0.75      0.73       681\n",
      "  HS_Moderate       0.66      0.54      0.60       359\n",
      "    HS_Strong       0.76      0.80      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 201.5803427696228 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9149, F1 Micro: 0.7484, F1 Macro: 0.6222\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 48.47545337677002 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4068, Accuracy: 0.8982, F1 Micro: 0.6628, F1 Macro: 0.4246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2988, Accuracy: 0.9149, F1 Micro: 0.7498, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2369, Accuracy: 0.9219, F1 Micro: 0.7649, F1 Macro: 0.6084\n",
      "Epoch 4/10, Train Loss: 0.1971, Accuracy: 0.9241, F1 Micro: 0.7612, F1 Macro: 0.6202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.9236, F1 Micro: 0.7737, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1269, Accuracy: 0.9236, F1 Micro: 0.7756, F1 Macro: 0.6674\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9266, F1 Micro: 0.7756, F1 Macro: 0.6878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.925, F1 Micro: 0.7782, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9266, F1 Micro: 0.7844, F1 Macro: 0.715\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9207, F1 Micro: 0.7775, F1 Macro: 0.7103\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9266, F1 Micro: 0.7844, F1 Macro: 0.715\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.73      0.61      0.66       408\n",
      "  HS_Religion       0.76      0.66      0.70       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.68      0.37      0.48        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.68      0.52      0.59       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 202.85667896270752 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4126, Accuracy: 0.8957, F1 Micro: 0.6589, F1 Macro: 0.433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3022, Accuracy: 0.9162, F1 Micro: 0.7483, F1 Macro: 0.5877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.9229, F1 Micro: 0.7655, F1 Macro: 0.6282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.198, Accuracy: 0.9248, F1 Micro: 0.7719, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1601, Accuracy: 0.9227, F1 Micro: 0.7748, F1 Macro: 0.6769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1293, Accuracy: 0.9269, F1 Micro: 0.7773, F1 Macro: 0.6815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1024, Accuracy: 0.9286, F1 Micro: 0.7879, F1 Macro: 0.7048\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9276, F1 Micro: 0.7834, F1 Macro: 0.709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.9281, F1 Micro: 0.7924, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.923, F1 Micro: 0.7847, F1 Macro: 0.7123\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9281, F1 Micro: 0.7924, F1 Macro: 0.7204\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.79      0.76       729\n",
      "     HS_Group       0.73      0.63      0.68       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.66      0.37      0.47        57\n",
      "    HS_Gender       0.68      0.45      0.54        55\n",
      "     HS_Other       0.81      0.82      0.82       771\n",
      "      HS_Weak       0.72      0.77      0.74       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.82      0.77      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 205.10761666297913 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4105, Accuracy: 0.8952, F1 Micro: 0.653, F1 Macro: 0.3731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.302, Accuracy: 0.9148, F1 Micro: 0.7474, F1 Macro: 0.5859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2424, Accuracy: 0.921, F1 Micro: 0.7589, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.199, Accuracy: 0.924, F1 Micro: 0.7713, F1 Macro: 0.6267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1588, Accuracy: 0.9239, F1 Micro: 0.7758, F1 Macro: 0.6551\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.9197, F1 Micro: 0.7676, F1 Macro: 0.6498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.9225, F1 Micro: 0.7788, F1 Macro: 0.6878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9274, F1 Micro: 0.7818, F1 Macro: 0.7059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0702, Accuracy: 0.9252, F1 Micro: 0.7839, F1 Macro: 0.7117\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9265, F1 Micro: 0.7809, F1 Macro: 0.7125\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9252, F1 Micro: 0.7839, F1 Macro: 0.7117\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.72      0.77      0.75       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.73      0.68      0.70       168\n",
      "      HS_Race       0.72      0.72      0.72       119\n",
      "  HS_Physical       0.76      0.28      0.41        57\n",
      "    HS_Gender       0.69      0.53      0.60        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.71      0.73      0.72       681\n",
      "  HS_Moderate       0.65      0.55      0.60       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 205.34431982040405 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9161, F1 Micro: 0.7522, F1 Macro: 0.6316\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 45.24028015136719 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.8998, F1 Micro: 0.6825, F1 Macro: 0.4454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2928, Accuracy: 0.918, F1 Micro: 0.7525, F1 Macro: 0.5851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.9243, F1 Micro: 0.7662, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1911, Accuracy: 0.924, F1 Micro: 0.7678, F1 Macro: 0.635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9219, F1 Micro: 0.7748, F1 Macro: 0.6606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1179, Accuracy: 0.9283, F1 Micro: 0.7845, F1 Macro: 0.6991\n",
      "Epoch 7/10, Train Loss: 0.0968, Accuracy: 0.9274, F1 Micro: 0.7834, F1 Macro: 0.7002\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.9271, F1 Micro: 0.7824, F1 Macro: 0.7038\n",
      "Epoch 9/10, Train Loss: 0.0683, Accuracy: 0.9272, F1 Micro: 0.7824, F1 Macro: 0.7112\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9222, F1 Micro: 0.7768, F1 Macro: 0.7114\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9283, F1 Micro: 0.7845, F1 Macro: 0.6991\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.76      0.63      0.69       408\n",
      "  HS_Religion       0.79      0.63      0.70       168\n",
      "      HS_Race       0.78      0.68      0.73       119\n",
      "  HS_Physical       0.78      0.25      0.37        57\n",
      "    HS_Gender       0.54      0.40      0.46        55\n",
      "     HS_Other       0.84      0.77      0.80       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.71      0.58      0.64       359\n",
      "    HS_Strong       0.85      0.72      0.78        97\n",
      "\n",
      "    micro avg       0.82      0.75      0.78      5589\n",
      "    macro avg       0.78      0.65      0.70      5589\n",
      " weighted avg       0.81      0.75      0.78      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 212.36075401306152 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4077, Accuracy: 0.8979, F1 Micro: 0.679, F1 Macro: 0.4339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2951, Accuracy: 0.9165, F1 Micro: 0.744, F1 Macro: 0.5691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2372, Accuracy: 0.9236, F1 Micro: 0.7655, F1 Macro: 0.6139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1919, Accuracy: 0.9261, F1 Micro: 0.7825, F1 Macro: 0.6558\n",
      "Epoch 5/10, Train Loss: 0.156, Accuracy: 0.9228, F1 Micro: 0.7794, F1 Macro: 0.6835\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.9262, F1 Micro: 0.7823, F1 Macro: 0.6968\n",
      "Epoch 7/10, Train Loss: 0.0987, Accuracy: 0.9269, F1 Micro: 0.7823, F1 Macro: 0.7017\n",
      "Epoch 8/10, Train Loss: 0.078, Accuracy: 0.9236, F1 Micro: 0.7814, F1 Macro: 0.7022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9244, F1 Micro: 0.784, F1 Macro: 0.7109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0583, Accuracy: 0.9267, F1 Micro: 0.7883, F1 Macro: 0.7252\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9267, F1 Micro: 0.7883, F1 Macro: 0.7252\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.69      0.68      0.68       408\n",
      "  HS_Religion       0.69      0.70      0.69       168\n",
      "      HS_Race       0.73      0.79      0.76       119\n",
      "  HS_Physical       0.64      0.44      0.52        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.80      0.81      0.80       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.71      0.73      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 212.5484766960144 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4073, Accuracy: 0.8967, F1 Micro: 0.669, F1 Macro: 0.4077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2942, Accuracy: 0.9176, F1 Micro: 0.7462, F1 Macro: 0.5577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2348, Accuracy: 0.9199, F1 Micro: 0.7618, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.9248, F1 Micro: 0.7772, F1 Macro: 0.6471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1566, Accuracy: 0.9244, F1 Micro: 0.7781, F1 Macro: 0.6601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.926, F1 Micro: 0.7784, F1 Macro: 0.6788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1022, Accuracy: 0.9285, F1 Micro: 0.7875, F1 Macro: 0.6857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9268, F1 Micro: 0.7903, F1 Macro: 0.7196\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9266, F1 Micro: 0.7842, F1 Macro: 0.7105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0619, Accuracy: 0.9291, F1 Micro: 0.7906, F1 Macro: 0.7271\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9291, F1 Micro: 0.7906, F1 Macro: 0.7271\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.74      0.67      0.70       408\n",
      "  HS_Religion       0.72      0.68      0.70       168\n",
      "      HS_Race       0.71      0.82      0.76       119\n",
      "  HS_Physical       0.62      0.46      0.53        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.83      0.78      0.81       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.69      0.60      0.64       359\n",
      "    HS_Strong       0.79      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 219.99541568756104 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9172, F1 Micro: 0.7555, F1 Macro: 0.6394\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 40.95566368103027 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4005, Accuracy: 0.8952, F1 Micro: 0.6205, F1 Macro: 0.3999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2842, Accuracy: 0.916, F1 Micro: 0.7517, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9239, F1 Micro: 0.7723, F1 Macro: 0.6296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1821, Accuracy: 0.9254, F1 Micro: 0.7785, F1 Macro: 0.6574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9261, F1 Micro: 0.7827, F1 Macro: 0.6778\n",
      "Epoch 6/10, Train Loss: 0.1213, Accuracy: 0.9219, F1 Micro: 0.781, F1 Macro: 0.6906\n",
      "Epoch 7/10, Train Loss: 0.1016, Accuracy: 0.9251, F1 Micro: 0.7803, F1 Macro: 0.6948\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.9225, F1 Micro: 0.7741, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0662, Accuracy: 0.9259, F1 Micro: 0.7837, F1 Macro: 0.7116\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9261, F1 Micro: 0.7808, F1 Macro: 0.7217\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9259, F1 Micro: 0.7837, F1 Macro: 0.7116\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.75      0.58      0.65       408\n",
      "  HS_Religion       0.69      0.66      0.67       168\n",
      "      HS_Race       0.74      0.82      0.78       119\n",
      "  HS_Physical       0.70      0.40      0.51        57\n",
      "    HS_Gender       0.62      0.47      0.54        55\n",
      "     HS_Other       0.83      0.77      0.80       771\n",
      "      HS_Weak       0.69      0.77      0.73       681\n",
      "  HS_Moderate       0.69      0.50      0.58       359\n",
      "    HS_Strong       0.85      0.70      0.77        97\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.79      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 221.4537057876587 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4064, Accuracy: 0.8885, F1 Micro: 0.5824, F1 Macro: 0.3801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2877, Accuracy: 0.9156, F1 Micro: 0.7477, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2323, Accuracy: 0.9231, F1 Micro: 0.7765, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1836, Accuracy: 0.9271, F1 Micro: 0.7786, F1 Macro: 0.6516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.9283, F1 Micro: 0.7882, F1 Macro: 0.6805\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.924, F1 Micro: 0.7838, F1 Macro: 0.6927\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9255, F1 Micro: 0.784, F1 Macro: 0.7135\n",
      "Epoch 8/10, Train Loss: 0.0796, Accuracy: 0.9266, F1 Micro: 0.7845, F1 Macro: 0.7101\n",
      "Epoch 9/10, Train Loss: 0.068, Accuracy: 0.9272, F1 Micro: 0.7819, F1 Macro: 0.7124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9278, F1 Micro: 0.7884, F1 Macro: 0.7263\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9278, F1 Micro: 0.7884, F1 Macro: 0.7263\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.78      0.71      0.74       729\n",
      "     HS_Group       0.70      0.73      0.71       408\n",
      "  HS_Religion       0.73      0.68      0.71       168\n",
      "      HS_Race       0.73      0.86      0.79       119\n",
      "  HS_Physical       0.64      0.40      0.49        57\n",
      "    HS_Gender       0.70      0.47      0.57        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.75      0.67      0.71       681\n",
      "  HS_Moderate       0.64      0.65      0.65       359\n",
      "    HS_Strong       0.76      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 222.45082306861877 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4058, Accuracy: 0.8919, F1 Micro: 0.6032, F1 Macro: 0.3646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.286, Accuracy: 0.9141, F1 Micro: 0.747, F1 Macro: 0.5705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2346, Accuracy: 0.9203, F1 Micro: 0.7719, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.9253, F1 Micro: 0.7782, F1 Macro: 0.6473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.9285, F1 Micro: 0.7835, F1 Macro: 0.6713\n",
      "Epoch 6/10, Train Loss: 0.1246, Accuracy: 0.9243, F1 Micro: 0.7813, F1 Macro: 0.6679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1029, Accuracy: 0.9268, F1 Micro: 0.7884, F1 Macro: 0.712\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9229, F1 Micro: 0.7809, F1 Macro: 0.706\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9279, F1 Micro: 0.7873, F1 Macro: 0.7159\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9266, F1 Micro: 0.7833, F1 Macro: 0.7157\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9268, F1 Micro: 0.7884, F1 Macro: 0.712\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.73      0.76      0.75       729\n",
      "     HS_Group       0.73      0.66      0.69       408\n",
      "  HS_Religion       0.72      0.73      0.72       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.73      0.28      0.41        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.82      0.79      0.80       771\n",
      "      HS_Weak       0.69      0.75      0.72       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.80      0.71      0.75        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.69      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 221.97408032417297 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.918, F1 Micro: 0.7581, F1 Macro: 0.6458\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 36.84303045272827 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3929, Accuracy: 0.8973, F1 Micro: 0.71, F1 Macro: 0.5032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2785, Accuracy: 0.9168, F1 Micro: 0.7566, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2211, Accuracy: 0.9242, F1 Micro: 0.772, F1 Macro: 0.6232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9282, F1 Micro: 0.7769, F1 Macro: 0.6459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.9275, F1 Micro: 0.787, F1 Macro: 0.6826\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9234, F1 Micro: 0.7828, F1 Macro: 0.6861\n",
      "Epoch 7/10, Train Loss: 0.094, Accuracy: 0.9266, F1 Micro: 0.7826, F1 Macro: 0.6891\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.9231, F1 Micro: 0.7821, F1 Macro: 0.7111\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9284, F1 Micro: 0.7844, F1 Macro: 0.7211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9294, F1 Micro: 0.7892, F1 Macro: 0.7226\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9294, F1 Micro: 0.7892, F1 Macro: 0.7226\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.76      0.73      0.75       729\n",
      "     HS_Group       0.74      0.62      0.67       408\n",
      "  HS_Religion       0.78      0.63      0.70       168\n",
      "      HS_Race       0.79      0.70      0.74       119\n",
      "  HS_Physical       0.65      0.46      0.54        57\n",
      "    HS_Gender       0.68      0.51      0.58        55\n",
      "     HS_Other       0.85      0.80      0.82       771\n",
      "      HS_Weak       0.73      0.70      0.71       681\n",
      "  HS_Moderate       0.69      0.55      0.61       359\n",
      "    HS_Strong       0.78      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.77      0.68      0.72      5589\n",
      " weighted avg       0.81      0.76      0.79      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 229.11242842674255 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3974, Accuracy: 0.8959, F1 Micro: 0.7031, F1 Macro: 0.5089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2791, Accuracy: 0.9163, F1 Micro: 0.7567, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.223, Accuracy: 0.9248, F1 Micro: 0.7745, F1 Macro: 0.6243\n",
      "Epoch 4/10, Train Loss: 0.1847, Accuracy: 0.9281, F1 Micro: 0.7736, F1 Macro: 0.6411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.9297, F1 Micro: 0.787, F1 Macro: 0.6965\n",
      "Epoch 6/10, Train Loss: 0.113, Accuracy: 0.9248, F1 Micro: 0.7865, F1 Macro: 0.6847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9262, F1 Micro: 0.7873, F1 Macro: 0.6945\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9242, F1 Micro: 0.7842, F1 Macro: 0.6994\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9285, F1 Micro: 0.7841, F1 Macro: 0.7154\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9279, F1 Micro: 0.7866, F1 Macro: 0.7221\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9262, F1 Micro: 0.7873, F1 Macro: 0.6945\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.89      0.87      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.69      0.62      0.65       408\n",
      "  HS_Religion       0.74      0.66      0.70       168\n",
      "      HS_Race       0.75      0.78      0.77       119\n",
      "  HS_Physical       0.79      0.26      0.39        57\n",
      "    HS_Gender       0.74      0.31      0.44        55\n",
      "     HS_Other       0.81      0.85      0.83       771\n",
      "      HS_Weak       0.70      0.76      0.72       681\n",
      "  HS_Moderate       0.63      0.55      0.58       359\n",
      "    HS_Strong       0.77      0.69      0.73        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.67      0.69      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.44      0.44      5589\n",
      "\n",
      "Training completed in 229.77509188652039 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3982, Accuracy: 0.8987, F1 Micro: 0.7091, F1 Macro: 0.4996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2796, Accuracy: 0.9156, F1 Micro: 0.7582, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.9241, F1 Micro: 0.7761, F1 Macro: 0.6159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.9282, F1 Micro: 0.7804, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9234, F1 Micro: 0.783, F1 Macro: 0.6854\n",
      "Epoch 6/10, Train Loss: 0.1155, Accuracy: 0.9194, F1 Micro: 0.7791, F1 Macro: 0.6803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.9248, F1 Micro: 0.7856, F1 Macro: 0.6881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9245, F1 Micro: 0.7871, F1 Macro: 0.7135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9274, F1 Micro: 0.7884, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.9291, F1 Micro: 0.7918, F1 Macro: 0.7241\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9291, F1 Micro: 0.7918, F1 Macro: 0.7241\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.72      0.70      0.71       408\n",
      "  HS_Religion       0.75      0.68      0.71       168\n",
      "      HS_Race       0.79      0.69      0.74       119\n",
      "  HS_Physical       0.62      0.42      0.50        57\n",
      "    HS_Gender       0.68      0.45      0.54        55\n",
      "     HS_Other       0.82      0.78      0.80       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.68      0.63      0.65       359\n",
      "    HS_Strong       0.79      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.44      0.43      5589\n",
      "\n",
      "Training completed in 234.49461770057678 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9188, F1 Micro: 0.7605, F1 Macro: 0.651\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 33.31398367881775 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3844, Accuracy: 0.9005, F1 Micro: 0.704, F1 Macro: 0.4546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2747, Accuracy: 0.9176, F1 Micro: 0.7632, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2154, Accuracy: 0.9244, F1 Micro: 0.7717, F1 Macro: 0.64\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9264, F1 Micro: 0.782, F1 Macro: 0.6428\n",
      "Epoch 5/10, Train Loss: 0.1396, Accuracy: 0.9274, F1 Micro: 0.7809, F1 Macro: 0.6609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.9266, F1 Micro: 0.7847, F1 Macro: 0.6864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.093, Accuracy: 0.9278, F1 Micro: 0.7896, F1 Macro: 0.7035\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.9287, F1 Micro: 0.7848, F1 Macro: 0.7151\n",
      "Epoch 9/10, Train Loss: 0.0679, Accuracy: 0.929, F1 Micro: 0.7883, F1 Macro: 0.7201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.927, F1 Micro: 0.7901, F1 Macro: 0.7266\n",
      "Model 1 - Iteration 7901: Accuracy: 0.927, F1 Micro: 0.7901, F1 Macro: 0.7266\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.70      0.65      0.68       408\n",
      "  HS_Religion       0.67      0.73      0.70       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.61      0.47      0.53        57\n",
      "    HS_Gender       0.68      0.49      0.57        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.71      0.74      0.72       681\n",
      "  HS_Moderate       0.65      0.61      0.63       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.78      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 236.84646582603455 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3884, Accuracy: 0.8985, F1 Micro: 0.6993, F1 Macro: 0.4344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.919, F1 Micro: 0.7628, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2176, Accuracy: 0.9241, F1 Micro: 0.7739, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1742, Accuracy: 0.9265, F1 Micro: 0.7816, F1 Macro: 0.6506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1392, Accuracy: 0.9271, F1 Micro: 0.7851, F1 Macro: 0.6879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9273, F1 Micro: 0.7898, F1 Macro: 0.6851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9254, F1 Micro: 0.7898, F1 Macro: 0.7127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9305, F1 Micro: 0.7961, F1 Macro: 0.7245\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.9288, F1 Micro: 0.7856, F1 Macro: 0.7205\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9275, F1 Micro: 0.7913, F1 Macro: 0.7208\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9305, F1 Micro: 0.7961, F1 Macro: 0.7245\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.79      0.74      0.76       729\n",
      "     HS_Group       0.68      0.70      0.69       408\n",
      "  HS_Religion       0.73      0.72      0.73       168\n",
      "      HS_Race       0.75      0.75      0.75       119\n",
      "  HS_Physical       0.68      0.33      0.45        57\n",
      "    HS_Gender       0.75      0.44      0.55        55\n",
      "     HS_Other       0.83      0.80      0.82       771\n",
      "      HS_Weak       0.77      0.71      0.74       681\n",
      "  HS_Moderate       0.65      0.65      0.65       359\n",
      "    HS_Strong       0.79      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.80      5589\n",
      "    macro avg       0.77      0.70      0.72      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 238.37586784362793 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3886, Accuracy: 0.8965, F1 Micro: 0.705, F1 Macro: 0.4612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2793, Accuracy: 0.9173, F1 Micro: 0.7584, F1 Macro: 0.5949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2203, Accuracy: 0.9249, F1 Micro: 0.7686, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1772, Accuracy: 0.9232, F1 Micro: 0.7763, F1 Macro: 0.6437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1414, Accuracy: 0.9268, F1 Micro: 0.7819, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9256, F1 Micro: 0.784, F1 Macro: 0.7042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.9255, F1 Micro: 0.786, F1 Macro: 0.7036\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9277, F1 Micro: 0.7836, F1 Macro: 0.7106\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9275, F1 Micro: 0.7834, F1 Macro: 0.7074\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.9268, F1 Micro: 0.7852, F1 Macro: 0.7185\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9255, F1 Micro: 0.786, F1 Macro: 0.7036\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.71      0.78      0.75       729\n",
      "     HS_Group       0.71      0.61      0.66       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.72      0.79      0.75       119\n",
      "  HS_Physical       0.68      0.23      0.34        57\n",
      "    HS_Gender       0.71      0.45      0.56        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.69      0.77      0.73       681\n",
      "  HS_Moderate       0.67      0.55      0.60       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.75      0.69      0.70      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 236.6568009853363 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9194, F1 Micro: 0.7627, F1 Macro: 0.6558\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 30.506413221359253 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3777, Accuracy: 0.8995, F1 Micro: 0.7124, F1 Macro: 0.5194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9204, F1 Micro: 0.7654, F1 Macro: 0.6032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2104, Accuracy: 0.9265, F1 Micro: 0.7783, F1 Macro: 0.632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9272, F1 Micro: 0.7836, F1 Macro: 0.6504\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9267, F1 Micro: 0.7808, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9298, F1 Micro: 0.7845, F1 Macro: 0.7057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9299, F1 Micro: 0.7899, F1 Macro: 0.7022\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9277, F1 Micro: 0.7883, F1 Macro: 0.7111\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9265, F1 Micro: 0.7838, F1 Macro: 0.7034\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9257, F1 Micro: 0.7894, F1 Macro: 0.7222\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9299, F1 Micro: 0.7899, F1 Macro: 0.7022\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.73      0.63      0.68       408\n",
      "  HS_Religion       0.75      0.69      0.72       168\n",
      "      HS_Race       0.80      0.69      0.74       119\n",
      "  HS_Physical       0.79      0.26      0.39        57\n",
      "    HS_Gender       0.76      0.29      0.42        55\n",
      "     HS_Other       0.85      0.79      0.82       771\n",
      "      HS_Weak       0.75      0.71      0.73       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.82      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.79      0.66      0.70      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 241.3137857913971 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3828, Accuracy: 0.8992, F1 Micro: 0.7075, F1 Macro: 0.5099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2688, Accuracy: 0.9209, F1 Micro: 0.7597, F1 Macro: 0.5921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2112, Accuracy: 0.9251, F1 Micro: 0.778, F1 Macro: 0.6356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9292, F1 Micro: 0.782, F1 Macro: 0.6512\n",
      "Epoch 5/10, Train Loss: 0.1381, Accuracy: 0.9252, F1 Micro: 0.7819, F1 Macro: 0.6871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9264, F1 Micro: 0.7875, F1 Macro: 0.6996\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9253, F1 Micro: 0.7854, F1 Macro: 0.7017\n",
      "Epoch 8/10, Train Loss: 0.0737, Accuracy: 0.9269, F1 Micro: 0.7873, F1 Macro: 0.7136\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9214, F1 Micro: 0.7809, F1 Macro: 0.7137\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9258, F1 Micro: 0.7826, F1 Macro: 0.716\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9264, F1 Micro: 0.7875, F1 Macro: 0.6996\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.67      0.73      0.70       408\n",
      "  HS_Religion       0.76      0.65      0.70       168\n",
      "      HS_Race       0.69      0.83      0.76       119\n",
      "  HS_Physical       0.58      0.19      0.29        57\n",
      "    HS_Gender       0.71      0.36      0.48        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.73      0.70      0.72       681\n",
      "  HS_Moderate       0.64      0.67      0.65       359\n",
      "    HS_Strong       0.78      0.78      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.69      0.70      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 240.16251587867737 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.382, Accuracy: 0.9027, F1 Micro: 0.7122, F1 Macro: 0.5148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2675, Accuracy: 0.9198, F1 Micro: 0.7614, F1 Macro: 0.6037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2105, Accuracy: 0.9256, F1 Micro: 0.7765, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9268, F1 Micro: 0.7809, F1 Macro: 0.6404\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9249, F1 Micro: 0.7789, F1 Macro: 0.6697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9291, F1 Micro: 0.7819, F1 Macro: 0.6926\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9242, F1 Micro: 0.7784, F1 Macro: 0.6889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.9269, F1 Micro: 0.7853, F1 Macro: 0.7169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.9272, F1 Micro: 0.7884, F1 Macro: 0.7213\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.9278, F1 Micro: 0.7869, F1 Macro: 0.7228\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9272, F1 Micro: 0.7884, F1 Macro: 0.7213\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.73      0.70      0.72       168\n",
      "      HS_Race       0.70      0.82      0.75       119\n",
      "  HS_Physical       0.88      0.37      0.52        57\n",
      "    HS_Gender       0.68      0.42      0.52        55\n",
      "     HS_Other       0.80      0.81      0.81       771\n",
      "      HS_Weak       0.71      0.72      0.72       681\n",
      "  HS_Moderate       0.67      0.61      0.64       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 242.72884345054626 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.92, F1 Micro: 0.7644, F1 Macro: 0.6593\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 27.42550563812256 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3711, Accuracy: 0.9009, F1 Micro: 0.701, F1 Macro: 0.4298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2605, Accuracy: 0.9204, F1 Micro: 0.7477, F1 Macro: 0.5783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2079, Accuracy: 0.926, F1 Micro: 0.7672, F1 Macro: 0.6147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.9255, F1 Micro: 0.7843, F1 Macro: 0.6587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.9259, F1 Micro: 0.7871, F1 Macro: 0.68\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9253, F1 Micro: 0.7859, F1 Macro: 0.7028\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9249, F1 Micro: 0.7857, F1 Macro: 0.7097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0759, Accuracy: 0.9275, F1 Micro: 0.7886, F1 Macro: 0.7131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9287, F1 Micro: 0.7896, F1 Macro: 0.7219\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9254, F1 Micro: 0.7848, F1 Macro: 0.7198\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9287, F1 Micro: 0.7896, F1 Macro: 0.7219\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.75      0.62      0.68       408\n",
      "  HS_Religion       0.70      0.69      0.69       168\n",
      "      HS_Race       0.80      0.74      0.77       119\n",
      "  HS_Physical       0.75      0.37      0.49        57\n",
      "    HS_Gender       0.66      0.45      0.54        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.71      0.56      0.62       359\n",
      "    HS_Strong       0.86      0.78      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 249.3336443901062 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.376, Accuracy: 0.8997, F1 Micro: 0.697, F1 Macro: 0.4391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9201, F1 Micro: 0.7524, F1 Macro: 0.5735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2106, Accuracy: 0.9256, F1 Micro: 0.7673, F1 Macro: 0.6321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9284, F1 Micro: 0.7869, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1299, Accuracy: 0.9296, F1 Micro: 0.7885, F1 Macro: 0.6922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9267, F1 Micro: 0.7885, F1 Macro: 0.7019\n",
      "Epoch 7/10, Train Loss: 0.0874, Accuracy: 0.9241, F1 Micro: 0.7816, F1 Macro: 0.7042\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9244, F1 Micro: 0.783, F1 Macro: 0.7083\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9273, F1 Micro: 0.7787, F1 Macro: 0.7114\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9225, F1 Micro: 0.7836, F1 Macro: 0.7216\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9267, F1 Micro: 0.7885, F1 Macro: 0.7019\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.66      0.74      0.70       408\n",
      "  HS_Religion       0.71      0.67      0.69       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.75      0.26      0.39        57\n",
      "    HS_Gender       0.68      0.31      0.42        55\n",
      "     HS_Other       0.80      0.83      0.82       771\n",
      "      HS_Weak       0.75      0.69      0.72       681\n",
      "  HS_Moderate       0.60      0.67      0.64       359\n",
      "    HS_Strong       0.78      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.75      0.69      0.70      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 248.47779965400696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3755, Accuracy: 0.902, F1 Micro: 0.7006, F1 Macro: 0.4523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2628, Accuracy: 0.9218, F1 Micro: 0.7632, F1 Macro: 0.5914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.925, F1 Micro: 0.7693, F1 Macro: 0.6063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9241, F1 Micro: 0.7807, F1 Macro: 0.6534\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9276, F1 Micro: 0.7807, F1 Macro: 0.6685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9253, F1 Micro: 0.7837, F1 Macro: 0.6999\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9265, F1 Micro: 0.7834, F1 Macro: 0.6928\n",
      "Epoch 8/10, Train Loss: 0.0755, Accuracy: 0.9256, F1 Micro: 0.7833, F1 Macro: 0.7123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9247, F1 Micro: 0.788, F1 Macro: 0.7262\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9229, F1 Micro: 0.7809, F1 Macro: 0.7135\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9247, F1 Micro: 0.788, F1 Macro: 0.7262\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.72      0.76      0.74       729\n",
      "     HS_Group       0.67      0.71      0.69       408\n",
      "  HS_Religion       0.65      0.74      0.69       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.78      0.37      0.50        57\n",
      "    HS_Gender       0.72      0.47      0.57        55\n",
      "     HS_Other       0.79      0.84      0.81       771\n",
      "      HS_Weak       0.70      0.74      0.72       681\n",
      "  HS_Moderate       0.62      0.65      0.63       359\n",
      "    HS_Strong       0.80      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.74      0.73      0.73      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 247.70815300941467 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9204, F1 Micro: 0.7659, F1 Macro: 0.6629\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 25.412517786026 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3704, Accuracy: 0.9053, F1 Micro: 0.7084, F1 Macro: 0.4791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2574, Accuracy: 0.918, F1 Micro: 0.7653, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2015, Accuracy: 0.9253, F1 Micro: 0.7768, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1599, Accuracy: 0.9286, F1 Micro: 0.7845, F1 Macro: 0.6675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9247, F1 Micro: 0.7875, F1 Macro: 0.6892\n",
      "Epoch 6/10, Train Loss: 0.1085, Accuracy: 0.9283, F1 Micro: 0.7849, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9285, F1 Micro: 0.7921, F1 Macro: 0.7174\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9254, F1 Micro: 0.7762, F1 Macro: 0.7075\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9272, F1 Micro: 0.7854, F1 Macro: 0.7155\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.9241, F1 Micro: 0.7856, F1 Macro: 0.7222\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9285, F1 Micro: 0.7921, F1 Macro: 0.7174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.75      0.76      0.75       729\n",
      "     HS_Group       0.72      0.65      0.69       408\n",
      "  HS_Religion       0.73      0.69      0.71       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.67      0.35      0.46        57\n",
      "    HS_Gender       0.63      0.40      0.49        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.72      0.73      0.73       681\n",
      "  HS_Moderate       0.66      0.58      0.62       359\n",
      "    HS_Strong       0.80      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.75      0.69      0.72      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 253.06770157814026 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.9038, F1 Micro: 0.7025, F1 Macro: 0.4924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.9202, F1 Micro: 0.7675, F1 Macro: 0.6124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.9251, F1 Micro: 0.7816, F1 Macro: 0.6516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9288, F1 Micro: 0.7877, F1 Macro: 0.6665\n",
      "Epoch 5/10, Train Loss: 0.1314, Accuracy: 0.9245, F1 Micro: 0.7823, F1 Macro: 0.6939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9302, F1 Micro: 0.7941, F1 Macro: 0.7047\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.9296, F1 Micro: 0.7872, F1 Macro: 0.7146\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9264, F1 Micro: 0.775, F1 Macro: 0.7095\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9254, F1 Micro: 0.7816, F1 Macro: 0.7162\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9285, F1 Micro: 0.7891, F1 Macro: 0.7209\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9302, F1 Micro: 0.7941, F1 Macro: 0.7047\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.92      0.87      0.89      1008\n",
      "HS_Individual       0.76      0.77      0.76       729\n",
      "     HS_Group       0.75      0.66      0.70       408\n",
      "  HS_Religion       0.73      0.70      0.72       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.81      0.23      0.36        57\n",
      "    HS_Gender       0.84      0.29      0.43        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.74      0.74       681\n",
      "  HS_Moderate       0.69      0.58      0.63       359\n",
      "    HS_Strong       0.78      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.79      0.67      0.70      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.44      0.43      0.43      5589\n",
      "\n",
      "Training completed in 250.91238617897034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3753, Accuracy: 0.9018, F1 Micro: 0.7063, F1 Macro: 0.4696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2627, Accuracy: 0.9211, F1 Micro: 0.7667, F1 Macro: 0.6063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.9234, F1 Micro: 0.7804, F1 Macro: 0.644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1666, Accuracy: 0.9293, F1 Micro: 0.787, F1 Macro: 0.666\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9235, F1 Micro: 0.7838, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9279, F1 Micro: 0.7901, F1 Macro: 0.6808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0848, Accuracy: 0.9291, F1 Micro: 0.793, F1 Macro: 0.7222\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9296, F1 Micro: 0.7913, F1 Macro: 0.7229\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9275, F1 Micro: 0.7888, F1 Macro: 0.7125\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9261, F1 Micro: 0.7856, F1 Macro: 0.7221\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9291, F1 Micro: 0.793, F1 Macro: 0.7222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.71      0.69      0.70       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.77      0.30      0.43        57\n",
      "    HS_Gender       0.74      0.47      0.58        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.74      0.71      0.73       681\n",
      "  HS_Moderate       0.65      0.61      0.63       359\n",
      "    HS_Strong       0.79      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 252.8347532749176 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9209, F1 Micro: 0.7675, F1 Macro: 0.6659\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.025927543640137 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3632, Accuracy: 0.9028, F1 Micro: 0.6961, F1 Macro: 0.463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.253, Accuracy: 0.9187, F1 Micro: 0.7458, F1 Macro: 0.5636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1986, Accuracy: 0.9253, F1 Micro: 0.7785, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1573, Accuracy: 0.9248, F1 Micro: 0.7805, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9272, F1 Micro: 0.7834, F1 Macro: 0.6731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1032, Accuracy: 0.9268, F1 Micro: 0.7875, F1 Macro: 0.6988\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.9263, F1 Micro: 0.7829, F1 Macro: 0.7134\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9257, F1 Micro: 0.7873, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9298, F1 Micro: 0.7926, F1 Macro: 0.7287\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.929, F1 Micro: 0.7906, F1 Macro: 0.7256\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9298, F1 Micro: 0.7926, F1 Macro: 0.7287\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.74      0.75      0.74       729\n",
      "     HS_Group       0.75      0.63      0.69       408\n",
      "  HS_Religion       0.77      0.65      0.70       168\n",
      "      HS_Race       0.79      0.74      0.77       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.67      0.56      0.61        55\n",
      "     HS_Other       0.83      0.80      0.82       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.71      0.57      0.63       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.70      0.73      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 259.41272926330566 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3685, Accuracy: 0.9033, F1 Micro: 0.7, F1 Macro: 0.4851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2526, Accuracy: 0.9204, F1 Micro: 0.7546, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2031, Accuracy: 0.926, F1 Micro: 0.7841, F1 Macro: 0.68\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1606, Accuracy: 0.9244, F1 Micro: 0.7841, F1 Macro: 0.6788\n",
      "Epoch 5/10, Train Loss: 0.132, Accuracy: 0.9286, F1 Micro: 0.7805, F1 Macro: 0.6733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.9271, F1 Micro: 0.7871, F1 Macro: 0.702\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9267, F1 Micro: 0.7857, F1 Macro: 0.7197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0719, Accuracy: 0.9288, F1 Micro: 0.7872, F1 Macro: 0.7147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9258, F1 Micro: 0.7879, F1 Macro: 0.7179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9274, F1 Micro: 0.7892, F1 Macro: 0.723\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9274, F1 Micro: 0.7892, F1 Macro: 0.723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.68      0.69      0.68       408\n",
      "  HS_Religion       0.73      0.74      0.73       168\n",
      "      HS_Race       0.68      0.86      0.76       119\n",
      "  HS_Physical       0.63      0.39      0.48        57\n",
      "    HS_Gender       0.70      0.47      0.57        55\n",
      "     HS_Other       0.83      0.78      0.80       771\n",
      "      HS_Weak       0.74      0.70      0.72       681\n",
      "  HS_Moderate       0.63      0.62      0.63       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 261.2213144302368 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3702, Accuracy: 0.9052, F1 Micro: 0.7018, F1 Macro: 0.4749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2529, Accuracy: 0.9194, F1 Micro: 0.7551, F1 Macro: 0.5808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2006, Accuracy: 0.9239, F1 Micro: 0.7765, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1577, Accuracy: 0.9284, F1 Micro: 0.7843, F1 Macro: 0.6717\n",
      "Epoch 5/10, Train Loss: 0.1306, Accuracy: 0.9259, F1 Micro: 0.7742, F1 Macro: 0.6554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9282, F1 Micro: 0.7888, F1 Macro: 0.7044\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.9261, F1 Micro: 0.7872, F1 Macro: 0.7217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9304, F1 Micro: 0.7927, F1 Macro: 0.7149\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9266, F1 Micro: 0.7791, F1 Macro: 0.716\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9273, F1 Micro: 0.7893, F1 Macro: 0.7222\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9304, F1 Micro: 0.7927, F1 Macro: 0.7149\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.91      0.92      0.91      1008\n",
      "HS_Individual       0.80      0.71      0.75       729\n",
      "     HS_Group       0.70      0.70      0.70       408\n",
      "  HS_Religion       0.78      0.65      0.71       168\n",
      "      HS_Race       0.85      0.66      0.75       119\n",
      "  HS_Physical       0.77      0.30      0.43        57\n",
      "    HS_Gender       0.71      0.40      0.51        55\n",
      "     HS_Other       0.83      0.80      0.82       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.63      0.65      0.64       359\n",
      "    HS_Strong       0.84      0.73      0.78        97\n",
      "\n",
      "    micro avg       0.82      0.77      0.79      5589\n",
      "    macro avg       0.79      0.67      0.71      5589\n",
      " weighted avg       0.82      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 257.98681592941284 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9214, F1 Micro: 0.7688, F1 Macro: 0.6691\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.789849281311035 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3595, Accuracy: 0.8995, F1 Micro: 0.7151, F1 Macro: 0.4915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.9203, F1 Micro: 0.7658, F1 Macro: 0.5972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1968, Accuracy: 0.9272, F1 Micro: 0.7724, F1 Macro: 0.6259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.9284, F1 Micro: 0.79, F1 Macro: 0.6677\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9245, F1 Micro: 0.7819, F1 Macro: 0.6743\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.926, F1 Micro: 0.7893, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.9302, F1 Micro: 0.7955, F1 Macro: 0.7201\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9278, F1 Micro: 0.7892, F1 Macro: 0.7186\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9295, F1 Micro: 0.7926, F1 Macro: 0.7293\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9284, F1 Micro: 0.7947, F1 Macro: 0.7164\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9302, F1 Micro: 0.7955, F1 Macro: 0.7201\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.70      0.73      0.72       408\n",
      "  HS_Religion       0.72      0.68      0.70       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.68      0.33      0.45        57\n",
      "    HS_Gender       0.66      0.38      0.48        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.75      0.71      0.73       681\n",
      "  HS_Moderate       0.65      0.66      0.66       359\n",
      "    HS_Strong       0.83      0.77      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.80      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 261.04747223854065 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3651, Accuracy: 0.9, F1 Micro: 0.7149, F1 Macro: 0.4869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9201, F1 Micro: 0.7651, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9278, F1 Micro: 0.778, F1 Macro: 0.6488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1584, Accuracy: 0.9269, F1 Micro: 0.7864, F1 Macro: 0.6737\n",
      "Epoch 5/10, Train Loss: 0.1306, Accuracy: 0.9257, F1 Micro: 0.7747, F1 Macro: 0.6625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9244, F1 Micro: 0.7868, F1 Macro: 0.7075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0845, Accuracy: 0.9269, F1 Micro: 0.7911, F1 Macro: 0.7156\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9275, F1 Micro: 0.787, F1 Macro: 0.7123\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9273, F1 Micro: 0.7902, F1 Macro: 0.7245\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9291, F1 Micro: 0.7904, F1 Macro: 0.7196\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9269, F1 Micro: 0.7911, F1 Macro: 0.7156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.89      0.92      0.91      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.66      0.74      0.70       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.74      0.78      0.76       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.63      0.44      0.52        55\n",
      "     HS_Other       0.81      0.84      0.82       771\n",
      "      HS_Weak       0.73      0.70      0.71       681\n",
      "  HS_Moderate       0.61      0.68      0.64       359\n",
      "    HS_Strong       0.82      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 262.359712600708 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3646, Accuracy: 0.8933, F1 Micro: 0.7052, F1 Macro: 0.4633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.249, Accuracy: 0.9184, F1 Micro: 0.7624, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9278, F1 Micro: 0.7833, F1 Macro: 0.6327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1601, Accuracy: 0.9287, F1 Micro: 0.7866, F1 Macro: 0.6644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.9274, F1 Micro: 0.7876, F1 Macro: 0.6815\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9269, F1 Micro: 0.787, F1 Macro: 0.7031\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9265, F1 Micro: 0.7848, F1 Macro: 0.7162\n",
      "Epoch 8/10, Train Loss: 0.0716, Accuracy: 0.9289, F1 Micro: 0.7866, F1 Macro: 0.7152\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9271, F1 Micro: 0.7834, F1 Macro: 0.7124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9286, F1 Micro: 0.7881, F1 Macro: 0.7133\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9286, F1 Micro: 0.7881, F1 Macro: 0.7133\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.76      0.73      0.74       729\n",
      "     HS_Group       0.73      0.67      0.69       408\n",
      "  HS_Religion       0.71      0.71      0.71       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.84      0.28      0.42        57\n",
      "    HS_Gender       0.71      0.44      0.54        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.73      0.70      0.71       681\n",
      "  HS_Moderate       0.67      0.60      0.63       359\n",
      "    HS_Strong       0.81      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.78      0.68      0.71      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 262.5747137069702 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9218, F1 Micro: 0.77, F1 Macro: 0.6715\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.531474590301514 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3514, Accuracy: 0.9079, F1 Micro: 0.7052, F1 Macro: 0.5017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2441, Accuracy: 0.9214, F1 Micro: 0.7529, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9274, F1 Micro: 0.7856, F1 Macro: 0.6591\n",
      "Epoch 4/10, Train Loss: 0.157, Accuracy: 0.9282, F1 Micro: 0.7817, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9281, F1 Micro: 0.7885, F1 Macro: 0.6897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9287, F1 Micro: 0.7911, F1 Macro: 0.6925\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9286, F1 Micro: 0.7894, F1 Macro: 0.7157\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9259, F1 Micro: 0.7823, F1 Macro: 0.6992\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9264, F1 Micro: 0.7902, F1 Macro: 0.7215\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9271, F1 Micro: 0.785, F1 Macro: 0.7109\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9287, F1 Micro: 0.7911, F1 Macro: 0.6925\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.88      0.94      0.91      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.72      0.69      0.71       408\n",
      "  HS_Religion       0.84      0.61      0.70       168\n",
      "      HS_Race       0.80      0.62      0.70       119\n",
      "  HS_Physical       0.72      0.23      0.35        57\n",
      "    HS_Gender       0.81      0.24      0.37        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.82      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.78      0.65      0.69      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 265.1873598098755 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3582, Accuracy: 0.906, F1 Micro: 0.7128, F1 Macro: 0.5135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2464, Accuracy: 0.9203, F1 Micro: 0.7484, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1922, Accuracy: 0.9259, F1 Micro: 0.777, F1 Macro: 0.6554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1545, Accuracy: 0.9266, F1 Micro: 0.7843, F1 Macro: 0.6588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9302, F1 Micro: 0.7921, F1 Macro: 0.7111\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9292, F1 Micro: 0.7828, F1 Macro: 0.6952\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.9271, F1 Micro: 0.7853, F1 Macro: 0.7169\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9256, F1 Micro: 0.7873, F1 Macro: 0.7129\n",
      "Epoch 9/10, Train Loss: 0.0597, Accuracy: 0.9254, F1 Micro: 0.7888, F1 Macro: 0.7192\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9253, F1 Micro: 0.7821, F1 Macro: 0.7055\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9302, F1 Micro: 0.7921, F1 Macro: 0.7111\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.74      0.65      0.69       168\n",
      "      HS_Race       0.81      0.69      0.75       119\n",
      "  HS_Physical       0.64      0.28      0.39        57\n",
      "    HS_Gender       0.76      0.40      0.52        55\n",
      "     HS_Other       0.85      0.78      0.81       771\n",
      "      HS_Weak       0.76      0.70      0.73       681\n",
      "  HS_Moderate       0.69      0.58      0.63       359\n",
      "    HS_Strong       0.82      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.82      0.77      0.79      5589\n",
      "    macro avg       0.78      0.67      0.71      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 265.5002772808075 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3576, Accuracy: 0.9055, F1 Micro: 0.7166, F1 Macro: 0.4885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9205, F1 Micro: 0.7502, F1 Macro: 0.5874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1914, Accuracy: 0.9251, F1 Micro: 0.7758, F1 Macro: 0.6478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1571, Accuracy: 0.9281, F1 Micro: 0.7824, F1 Macro: 0.6518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9273, F1 Micro: 0.7862, F1 Macro: 0.6812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.9297, F1 Micro: 0.7907, F1 Macro: 0.6961\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.928, F1 Micro: 0.7897, F1 Macro: 0.7155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9284, F1 Micro: 0.7909, F1 Macro: 0.724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9289, F1 Micro: 0.7942, F1 Macro: 0.7246\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9304, F1 Micro: 0.7899, F1 Macro: 0.7128\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9289, F1 Micro: 0.7942, F1 Macro: 0.7246\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.77      0.71      0.74       729\n",
      "     HS_Group       0.68      0.75      0.71       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.71      0.76      0.74       119\n",
      "  HS_Physical       0.64      0.37      0.47        57\n",
      "    HS_Gender       0.74      0.45      0.56        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.75      0.69      0.72       681\n",
      "  HS_Moderate       0.63      0.69      0.66       359\n",
      "    HS_Strong       0.76      0.82      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 270.5874514579773 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9221, F1 Micro: 0.7712, F1 Macro: 0.6734\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 15.34531569480896 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3543, Accuracy: 0.9058, F1 Micro: 0.717, F1 Macro: 0.5082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2407, Accuracy: 0.9139, F1 Micro: 0.7631, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1935, Accuracy: 0.9244, F1 Micro: 0.7733, F1 Macro: 0.6125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9294, F1 Micro: 0.7878, F1 Macro: 0.654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1207, Accuracy: 0.9289, F1 Micro: 0.789, F1 Macro: 0.6765\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9246, F1 Micro: 0.784, F1 Macro: 0.7032\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9278, F1 Micro: 0.7858, F1 Macro: 0.7008\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9266, F1 Micro: 0.7852, F1 Macro: 0.7144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9289, F1 Micro: 0.7896, F1 Macro: 0.7252\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9245, F1 Micro: 0.7859, F1 Macro: 0.7182\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9289, F1 Micro: 0.7896, F1 Macro: 0.7252\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.76      0.73      0.75       729\n",
      "     HS_Group       0.72      0.64      0.68       408\n",
      "  HS_Religion       0.74      0.66      0.70       168\n",
      "      HS_Race       0.80      0.70      0.74       119\n",
      "  HS_Physical       0.56      0.47      0.51        57\n",
      "    HS_Gender       0.74      0.51      0.60        55\n",
      "     HS_Other       0.84      0.79      0.82       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.70      0.73      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 268.14018750190735 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3606, Accuracy: 0.9034, F1 Micro: 0.7139, F1 Macro: 0.5011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.242, Accuracy: 0.9132, F1 Micro: 0.7614, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9243, F1 Micro: 0.7698, F1 Macro: 0.6204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1548, Accuracy: 0.927, F1 Micro: 0.7766, F1 Macro: 0.638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9254, F1 Micro: 0.7819, F1 Macro: 0.6657\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9266, F1 Micro: 0.7812, F1 Macro: 0.7042\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9273, F1 Micro: 0.7812, F1 Macro: 0.7018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9279, F1 Micro: 0.7825, F1 Macro: 0.7112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9288, F1 Micro: 0.7855, F1 Macro: 0.7163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9251, F1 Micro: 0.7857, F1 Macro: 0.7144\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9251, F1 Micro: 0.7857, F1 Macro: 0.7144\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.70      0.80      0.75       729\n",
      "     HS_Group       0.72      0.56      0.63       408\n",
      "  HS_Religion       0.69      0.74      0.71       168\n",
      "      HS_Race       0.69      0.82      0.75       119\n",
      "  HS_Physical       0.67      0.42      0.52        57\n",
      "    HS_Gender       0.68      0.47      0.56        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.67      0.79      0.72       681\n",
      "  HS_Moderate       0.67      0.50      0.57       359\n",
      "    HS_Strong       0.82      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 272.1646480560303 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3589, Accuracy: 0.9033, F1 Micro: 0.7129, F1 Macro: 0.4945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2443, Accuracy: 0.9133, F1 Micro: 0.7606, F1 Macro: 0.5984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1961, Accuracy: 0.9228, F1 Micro: 0.7705, F1 Macro: 0.6022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1518, Accuracy: 0.9272, F1 Micro: 0.7798, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.929, F1 Micro: 0.7878, F1 Macro: 0.6783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9273, F1 Micro: 0.7894, F1 Macro: 0.7134\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.928, F1 Micro: 0.7826, F1 Macro: 0.7032\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9278, F1 Micro: 0.7891, F1 Macro: 0.7193\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9293, F1 Micro: 0.7863, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9262, F1 Micro: 0.7858, F1 Macro: 0.7181\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9273, F1 Micro: 0.7894, F1 Macro: 0.7134\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.74      0.73      0.74       729\n",
      "     HS_Group       0.70      0.72      0.71       408\n",
      "  HS_Religion       0.76      0.66      0.71       168\n",
      "      HS_Race       0.75      0.80      0.78       119\n",
      "  HS_Physical       0.67      0.21      0.32        57\n",
      "    HS_Gender       0.67      0.51      0.58        55\n",
      "     HS_Other       0.82      0.79      0.80       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.63      0.65      0.64       359\n",
      "    HS_Strong       0.85      0.76      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.75      0.70      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 268.5394608974457 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9224, F1 Micro: 0.772, F1 Macro: 0.6755\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.796151399612427 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3473, Accuracy: 0.9052, F1 Micro: 0.7218, F1 Macro: 0.509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2393, Accuracy: 0.9208, F1 Micro: 0.7608, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9261, F1 Micro: 0.7678, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9273, F1 Micro: 0.7779, F1 Macro: 0.6467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.9275, F1 Micro: 0.7779, F1 Macro: 0.6756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9262, F1 Micro: 0.7861, F1 Macro: 0.6954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9268, F1 Micro: 0.7873, F1 Macro: 0.7091\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9281, F1 Micro: 0.7864, F1 Macro: 0.7133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9285, F1 Micro: 0.7914, F1 Macro: 0.7213\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9264, F1 Micro: 0.7903, F1 Macro: 0.7193\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9285, F1 Micro: 0.7914, F1 Macro: 0.7213\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.73      0.67      0.70       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.75      0.82      0.78       119\n",
      "  HS_Physical       0.60      0.42      0.49        57\n",
      "    HS_Gender       0.62      0.42      0.50        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.67      0.63      0.65       359\n",
      "    HS_Strong       0.83      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 277.0840759277344 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3511, Accuracy: 0.9062, F1 Micro: 0.7142, F1 Macro: 0.5057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2401, Accuracy: 0.9201, F1 Micro: 0.7544, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9253, F1 Micro: 0.762, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.9272, F1 Micro: 0.7798, F1 Macro: 0.645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9295, F1 Micro: 0.7825, F1 Macro: 0.694\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9258, F1 Micro: 0.7796, F1 Macro: 0.7027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9254, F1 Micro: 0.785, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9287, F1 Micro: 0.7872, F1 Macro: 0.7208\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.925, F1 Micro: 0.7818, F1 Macro: 0.718\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9263, F1 Micro: 0.785, F1 Macro: 0.7178\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9287, F1 Micro: 0.7872, F1 Macro: 0.7208\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.91      0.92      0.91      1008\n",
      "HS_Individual       0.78      0.69      0.73       729\n",
      "     HS_Group       0.71      0.70      0.70       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.72      0.37      0.49        57\n",
      "    HS_Gender       0.65      0.47      0.55        55\n",
      "     HS_Other       0.86      0.75      0.80       771\n",
      "      HS_Weak       0.75      0.67      0.70       681\n",
      "  HS_Moderate       0.65      0.63      0.64       359\n",
      "    HS_Strong       0.81      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.76      0.79      5589\n",
      "  samples avg       0.44      0.43      0.43      5589\n",
      "\n",
      "Training completed in 275.27935671806335 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3507, Accuracy: 0.9036, F1 Micro: 0.717, F1 Macro: 0.4931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2427, Accuracy: 0.9198, F1 Micro: 0.7553, F1 Macro: 0.5879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1844, Accuracy: 0.9258, F1 Micro: 0.7637, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9265, F1 Micro: 0.7783, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9296, F1 Micro: 0.7894, F1 Macro: 0.6952\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9257, F1 Micro: 0.7759, F1 Macro: 0.6751\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.9275, F1 Micro: 0.7889, F1 Macro: 0.7072\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9271, F1 Micro: 0.7862, F1 Macro: 0.7175\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9273, F1 Micro: 0.7838, F1 Macro: 0.7162\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9242, F1 Micro: 0.784, F1 Macro: 0.7207\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9296, F1 Micro: 0.7894, F1 Macro: 0.6952\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.75      0.66      0.71       408\n",
      "  HS_Religion       0.77      0.65      0.71       168\n",
      "      HS_Race       0.79      0.68      0.73       119\n",
      "  HS_Physical       0.60      0.16      0.25        57\n",
      "    HS_Gender       0.66      0.38      0.48        55\n",
      "     HS_Other       0.85      0.78      0.81       771\n",
      "      HS_Weak       0.76      0.70      0.73       681\n",
      "  HS_Moderate       0.71      0.58      0.64       359\n",
      "    HS_Strong       0.78      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.77      0.65      0.70      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 271.6911518573761 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9227, F1 Micro: 0.7728, F1 Macro: 0.6772\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.254543781280518 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3419, Accuracy: 0.909, F1 Micro: 0.7152, F1 Macro: 0.4966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2347, Accuracy: 0.9201, F1 Micro: 0.7615, F1 Macro: 0.5856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.9261, F1 Micro: 0.7791, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1451, Accuracy: 0.9257, F1 Micro: 0.7865, F1 Macro: 0.6638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9243, F1 Micro: 0.7869, F1 Macro: 0.6951\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9255, F1 Micro: 0.7868, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9271, F1 Micro: 0.7882, F1 Macro: 0.6962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9291, F1 Micro: 0.7925, F1 Macro: 0.718\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.929, F1 Micro: 0.7845, F1 Macro: 0.7163\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9275, F1 Micro: 0.7782, F1 Macro: 0.7142\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9291, F1 Micro: 0.7925, F1 Macro: 0.718\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.92      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.72      0.74       729\n",
      "     HS_Group       0.70      0.71      0.70       408\n",
      "  HS_Religion       0.79      0.64      0.71       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.69      0.32      0.43        57\n",
      "    HS_Gender       0.73      0.40      0.52        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.74      0.70      0.72       681\n",
      "  HS_Moderate       0.65      0.66      0.65       359\n",
      "    HS_Strong       0.81      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 279.75583577156067 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3458, Accuracy: 0.908, F1 Micro: 0.7002, F1 Macro: 0.4998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2338, Accuracy: 0.9205, F1 Micro: 0.756, F1 Macro: 0.5685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1859, Accuracy: 0.9254, F1 Micro: 0.7726, F1 Macro: 0.6281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1461, Accuracy: 0.9266, F1 Micro: 0.7896, F1 Macro: 0.6651\n",
      "Epoch 5/10, Train Loss: 0.1151, Accuracy: 0.9255, F1 Micro: 0.7882, F1 Macro: 0.696\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9285, F1 Micro: 0.7875, F1 Macro: 0.6977\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9266, F1 Micro: 0.7861, F1 Macro: 0.7054\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9259, F1 Micro: 0.7749, F1 Macro: 0.704\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.9283, F1 Micro: 0.7839, F1 Macro: 0.7146\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9278, F1 Micro: 0.7778, F1 Macro: 0.7146\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9266, F1 Micro: 0.7896, F1 Macro: 0.6651\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.68      0.74      0.71       408\n",
      "  HS_Religion       0.74      0.66      0.70       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.75      0.05      0.10        57\n",
      "    HS_Gender       0.73      0.15      0.24        55\n",
      "     HS_Other       0.79      0.84      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.65      0.67      0.66       359\n",
      "    HS_Strong       0.77      0.79      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.75      0.66      0.67      5589\n",
      " weighted avg       0.78      0.80      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 275.08863830566406 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3487, Accuracy: 0.9071, F1 Micro: 0.6987, F1 Macro: 0.4705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2356, Accuracy: 0.9176, F1 Micro: 0.7578, F1 Macro: 0.5622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1857, Accuracy: 0.9247, F1 Micro: 0.7711, F1 Macro: 0.6317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9279, F1 Micro: 0.7906, F1 Macro: 0.663\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9244, F1 Micro: 0.782, F1 Macro: 0.6888\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.924, F1 Micro: 0.7841, F1 Macro: 0.6958\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9256, F1 Micro: 0.787, F1 Macro: 0.7102\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9282, F1 Micro: 0.7846, F1 Macro: 0.7144\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9284, F1 Micro: 0.7847, F1 Macro: 0.72\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9289, F1 Micro: 0.7828, F1 Macro: 0.7216\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9279, F1 Micro: 0.7906, F1 Macro: 0.663\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.70      0.71      0.71       408\n",
      "  HS_Religion       0.73      0.64      0.68       168\n",
      "      HS_Race       0.75      0.73      0.74       119\n",
      "  HS_Physical       0.67      0.04      0.07        57\n",
      "    HS_Gender       0.75      0.16      0.27        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.73      0.73       681\n",
      "  HS_Moderate       0.66      0.61      0.63       359\n",
      "    HS_Strong       0.79      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.76      0.65      0.66      5589\n",
      " weighted avg       0.79      0.79      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 274.7206840515137 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9229, F1 Micro: 0.7736, F1 Macro: 0.6774\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 11.125627040863037 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3384, Accuracy: 0.9023, F1 Micro: 0.6614, F1 Macro: 0.4866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2274, Accuracy: 0.9216, F1 Micro: 0.7664, F1 Macro: 0.5929\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9237, F1 Micro: 0.7567, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9262, F1 Micro: 0.7864, F1 Macro: 0.6679\n",
      "Epoch 5/10, Train Loss: 0.1175, Accuracy: 0.9287, F1 Micro: 0.7861, F1 Macro: 0.6746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9293, F1 Micro: 0.7914, F1 Macro: 0.7183\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9258, F1 Micro: 0.7826, F1 Macro: 0.7015\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9252, F1 Micro: 0.7852, F1 Macro: 0.712\n",
      "Epoch 9/10, Train Loss: 0.0498, Accuracy: 0.9258, F1 Micro: 0.7818, F1 Macro: 0.7178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9297, F1 Micro: 0.7955, F1 Macro: 0.7321\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9297, F1 Micro: 0.7955, F1 Macro: 0.7321\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.91      0.93      0.92      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.69      0.70      0.70       408\n",
      "  HS_Religion       0.68      0.73      0.71       168\n",
      "      HS_Race       0.73      0.83      0.78       119\n",
      "  HS_Physical       0.64      0.44      0.52        57\n",
      "    HS_Gender       0.74      0.47      0.58        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.75      0.69      0.72       681\n",
      "  HS_Moderate       0.65      0.65      0.65       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.80      5589\n",
      "    macro avg       0.75      0.72      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 281.28283619880676 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3433, Accuracy: 0.898, F1 Micro: 0.6296, F1 Macro: 0.459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2287, Accuracy: 0.9178, F1 Micro: 0.7631, F1 Macro: 0.5852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1857, Accuracy: 0.9264, F1 Micro: 0.7698, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1411, Accuracy: 0.9287, F1 Micro: 0.7903, F1 Macro: 0.6708\n",
      "Epoch 5/10, Train Loss: 0.1175, Accuracy: 0.9278, F1 Micro: 0.7825, F1 Macro: 0.6808\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.9283, F1 Micro: 0.7898, F1 Macro: 0.7114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.9276, F1 Micro: 0.7944, F1 Macro: 0.7197\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.928, F1 Micro: 0.7794, F1 Macro: 0.7072\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9261, F1 Micro: 0.7739, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9285, F1 Micro: 0.795, F1 Macro: 0.7274\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9285, F1 Micro: 0.795, F1 Macro: 0.7274\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.74      0.79      0.76       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.70      0.76      0.73       119\n",
      "  HS_Physical       0.69      0.42      0.52        57\n",
      "    HS_Gender       0.68      0.51      0.58        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.72      0.76      0.74       681\n",
      "  HS_Moderate       0.67      0.57      0.61       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.80      5589\n",
      "    macro avg       0.75      0.72      0.73      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 281.5005977153778 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3425, Accuracy: 0.8978, F1 Micro: 0.6277, F1 Macro: 0.4494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2298, Accuracy: 0.9195, F1 Micro: 0.7594, F1 Macro: 0.5824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1872, Accuracy: 0.9268, F1 Micro: 0.771, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1406, Accuracy: 0.9271, F1 Micro: 0.7785, F1 Macro: 0.648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9288, F1 Micro: 0.7828, F1 Macro: 0.6717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.928, F1 Micro: 0.7892, F1 Macro: 0.7046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9289, F1 Micro: 0.793, F1 Macro: 0.7173\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9259, F1 Micro: 0.7899, F1 Macro: 0.7234\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9254, F1 Micro: 0.7768, F1 Macro: 0.7053\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.927, F1 Micro: 0.7872, F1 Macro: 0.7192\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9289, F1 Micro: 0.793, F1 Macro: 0.7173\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.74      0.77      0.75       729\n",
      "     HS_Group       0.71      0.64      0.67       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.79      0.71      0.75       119\n",
      "  HS_Physical       0.77      0.30      0.43        57\n",
      "    HS_Gender       0.69      0.45      0.55        55\n",
      "     HS_Other       0.82      0.83      0.82       771\n",
      "      HS_Weak       0.71      0.74      0.73       681\n",
      "  HS_Moderate       0.67      0.57      0.61       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 283.2945272922516 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9231, F1 Micro: 0.7744, F1 Macro: 0.6794\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.711833953857422 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3328, Accuracy: 0.9071, F1 Micro: 0.7078, F1 Macro: 0.4828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2218, Accuracy: 0.9205, F1 Micro: 0.7456, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1823, Accuracy: 0.9253, F1 Micro: 0.778, F1 Macro: 0.6407\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9273, F1 Micro: 0.7726, F1 Macro: 0.6538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1134, Accuracy: 0.928, F1 Micro: 0.7891, F1 Macro: 0.6794\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9279, F1 Micro: 0.7873, F1 Macro: 0.7015\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9244, F1 Micro: 0.7859, F1 Macro: 0.7153\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9274, F1 Micro: 0.7887, F1 Macro: 0.7145\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9263, F1 Micro: 0.7771, F1 Macro: 0.7134\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9246, F1 Micro: 0.7861, F1 Macro: 0.7182\n",
      "Model 1 - Iteration 10018: Accuracy: 0.928, F1 Micro: 0.7891, F1 Macro: 0.6794\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.72      0.68      0.70       408\n",
      "  HS_Religion       0.83      0.60      0.70       168\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.80      0.14      0.24        57\n",
      "    HS_Gender       0.85      0.20      0.32        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.67      0.58      0.62       359\n",
      "    HS_Strong       0.76      0.74      0.75        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.79      0.65      0.68      5589\n",
      " weighted avg       0.80      0.78      0.78      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 282.2961165904999 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3364, Accuracy: 0.9054, F1 Micro: 0.7048, F1 Macro: 0.4739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2215, Accuracy: 0.9183, F1 Micro: 0.7387, F1 Macro: 0.5872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1832, Accuracy: 0.926, F1 Micro: 0.777, F1 Macro: 0.6402\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9269, F1 Micro: 0.7753, F1 Macro: 0.6554\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9269, F1 Micro: 0.7737, F1 Macro: 0.6583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.9281, F1 Micro: 0.7852, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9282, F1 Micro: 0.7882, F1 Macro: 0.7168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9308, F1 Micro: 0.7905, F1 Macro: 0.7151\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9264, F1 Micro: 0.7838, F1 Macro: 0.7181\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.9256, F1 Micro: 0.7851, F1 Macro: 0.7154\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9308, F1 Micro: 0.7905, F1 Macro: 0.7151\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.83      0.86      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.77      0.63      0.69       408\n",
      "  HS_Religion       0.81      0.58      0.67       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.68      0.33      0.45        57\n",
      "    HS_Gender       0.61      0.45      0.52        55\n",
      "     HS_Other       0.86      0.77      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.73       681\n",
      "  HS_Moderate       0.71      0.55      0.62       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.83      0.75      0.79      5589\n",
      "    macro avg       0.78      0.67      0.72      5589\n",
      " weighted avg       0.83      0.75      0.79      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 285.42676162719727 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3374, Accuracy: 0.9072, F1 Micro: 0.7084, F1 Macro: 0.4933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2233, Accuracy: 0.9212, F1 Micro: 0.7498, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9255, F1 Micro: 0.7762, F1 Macro: 0.6342\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1464, Accuracy: 0.9275, F1 Micro: 0.7764, F1 Macro: 0.6576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1167, Accuracy: 0.9296, F1 Micro: 0.7903, F1 Macro: 0.6693\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9273, F1 Micro: 0.7881, F1 Macro: 0.7003\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9284, F1 Micro: 0.7889, F1 Macro: 0.7082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.928, F1 Micro: 0.791, F1 Macro: 0.7202\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9283, F1 Micro: 0.7862, F1 Macro: 0.7206\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.926, F1 Micro: 0.7875, F1 Macro: 0.7198\n",
      "Model 3 - Iteration 10018: Accuracy: 0.928, F1 Micro: 0.791, F1 Macro: 0.7202\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.77      0.62      0.69       408\n",
      "  HS_Religion       0.75      0.65      0.70       168\n",
      "      HS_Race       0.77      0.72      0.75       119\n",
      "  HS_Physical       0.70      0.37      0.48        57\n",
      "    HS_Gender       0.60      0.47      0.53        55\n",
      "     HS_Other       0.81      0.79      0.80       771\n",
      "      HS_Weak       0.71      0.77      0.74       681\n",
      "  HS_Moderate       0.72      0.54      0.62       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 285.84730219841003 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9234, F1 Micro: 0.7751, F1 Macro: 0.6805\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.184401988983154 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.33, Accuracy: 0.9061, F1 Micro: 0.695, F1 Macro: 0.4797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2255, Accuracy: 0.9191, F1 Micro: 0.7334, F1 Macro: 0.5671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.175, Accuracy: 0.9265, F1 Micro: 0.7776, F1 Macro: 0.6407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1391, Accuracy: 0.9244, F1 Micro: 0.7854, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1119, Accuracy: 0.9273, F1 Micro: 0.7874, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0881, Accuracy: 0.9289, F1 Micro: 0.7926, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9285, F1 Micro: 0.7939, F1 Macro: 0.7209\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9259, F1 Micro: 0.7835, F1 Macro: 0.7162\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.9286, F1 Micro: 0.7906, F1 Macro: 0.7185\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.926, F1 Micro: 0.7854, F1 Macro: 0.72\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9285, F1 Micro: 0.7939, F1 Macro: 0.7209\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.76      0.75      0.75       729\n",
      "     HS_Group       0.68      0.69      0.68       408\n",
      "  HS_Religion       0.75      0.66      0.70       168\n",
      "      HS_Race       0.76      0.75      0.75       119\n",
      "  HS_Physical       0.59      0.33      0.43        57\n",
      "    HS_Gender       0.66      0.53      0.59        55\n",
      "     HS_Other       0.82      0.83      0.83       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.63      0.62      0.63       359\n",
      "    HS_Strong       0.81      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 295.0107777118683 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3364, Accuracy: 0.9055, F1 Micro: 0.6828, F1 Macro: 0.4637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2274, Accuracy: 0.9193, F1 Micro: 0.7372, F1 Macro: 0.5686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1769, Accuracy: 0.9257, F1 Micro: 0.7784, F1 Macro: 0.648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9285, F1 Micro: 0.7907, F1 Macro: 0.6653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1119, Accuracy: 0.9299, F1 Micro: 0.7941, F1 Macro: 0.6961\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9282, F1 Micro: 0.7868, F1 Macro: 0.6948\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9254, F1 Micro: 0.7867, F1 Macro: 0.7183\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9271, F1 Micro: 0.7885, F1 Macro: 0.7118\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9284, F1 Micro: 0.7913, F1 Macro: 0.7212\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9253, F1 Micro: 0.7834, F1 Macro: 0.7224\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9299, F1 Micro: 0.7941, F1 Macro: 0.6961\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.72      0.70      0.71       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.63      0.21      0.32        57\n",
      "    HS_Gender       0.65      0.27      0.38        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.73       681\n",
      "  HS_Moderate       0.70      0.61      0.65       359\n",
      "    HS_Strong       0.77      0.76      0.77        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.67      0.70      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 293.012775182724 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3367, Accuracy: 0.9045, F1 Micro: 0.6934, F1 Macro: 0.4502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2283, Accuracy: 0.9191, F1 Micro: 0.7337, F1 Macro: 0.5698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1767, Accuracy: 0.9269, F1 Micro: 0.7831, F1 Macro: 0.6522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1395, Accuracy: 0.9262, F1 Micro: 0.7895, F1 Macro: 0.6696\n",
      "Epoch 5/10, Train Loss: 0.1122, Accuracy: 0.9286, F1 Micro: 0.7869, F1 Macro: 0.6862\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9275, F1 Micro: 0.7846, F1 Macro: 0.7011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9273, F1 Micro: 0.7899, F1 Macro: 0.7161\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9284, F1 Micro: 0.7867, F1 Macro: 0.7191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9273, F1 Micro: 0.7928, F1 Macro: 0.7227\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9298, F1 Micro: 0.7922, F1 Macro: 0.73\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9273, F1 Micro: 0.7928, F1 Macro: 0.7227\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.74      0.68      0.71       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.61      0.40      0.48        57\n",
      "    HS_Gender       0.74      0.45      0.56        55\n",
      "     HS_Other       0.79      0.84      0.81       771\n",
      "      HS_Weak       0.69      0.77      0.73       681\n",
      "  HS_Moderate       0.68      0.59      0.63       359\n",
      "    HS_Strong       0.80      0.70      0.75        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 293.71599864959717 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9236, F1 Micro: 0.7758, F1 Macro: 0.6817\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.452123403549194 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3264, Accuracy: 0.9086, F1 Micro: 0.7213, F1 Macro: 0.5272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2168, Accuracy: 0.9223, F1 Micro: 0.7629, F1 Macro: 0.6108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1735, Accuracy: 0.9272, F1 Micro: 0.7725, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.138, Accuracy: 0.9286, F1 Micro: 0.7912, F1 Macro: 0.6725\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.9277, F1 Micro: 0.7859, F1 Macro: 0.6945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9318, F1 Micro: 0.7986, F1 Macro: 0.7205\n",
      "Epoch 7/10, Train Loss: 0.07, Accuracy: 0.9274, F1 Micro: 0.7882, F1 Macro: 0.7089\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9295, F1 Micro: 0.7947, F1 Macro: 0.7198\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9271, F1 Micro: 0.7919, F1 Macro: 0.7276\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9245, F1 Micro: 0.7858, F1 Macro: 0.7252\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9318, F1 Micro: 0.7986, F1 Macro: 0.7205\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.87      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.77      0.74      0.76       729\n",
      "     HS_Group       0.74      0.68      0.71       408\n",
      "  HS_Religion       0.76      0.68      0.72       168\n",
      "      HS_Race       0.79      0.80      0.79       119\n",
      "  HS_Physical       0.72      0.32      0.44        57\n",
      "    HS_Gender       0.57      0.38      0.46        55\n",
      "     HS_Other       0.85      0.79      0.82       771\n",
      "      HS_Weak       0.75      0.72      0.74       681\n",
      "  HS_Moderate       0.69      0.62      0.65       359\n",
      "    HS_Strong       0.79      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.82      0.78      0.80      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.78      0.80      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 297.9549491405487 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3318, Accuracy: 0.9057, F1 Micro: 0.7303, F1 Macro: 0.5381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.219, Accuracy: 0.92, F1 Micro: 0.7617, F1 Macro: 0.6035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1757, Accuracy: 0.9266, F1 Micro: 0.7721, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1396, Accuracy: 0.9263, F1 Micro: 0.7869, F1 Macro: 0.6584\n",
      "Epoch 5/10, Train Loss: 0.1101, Accuracy: 0.927, F1 Micro: 0.7848, F1 Macro: 0.6881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9313, F1 Micro: 0.7949, F1 Macro: 0.7137\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.9233, F1 Micro: 0.7823, F1 Macro: 0.6958\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9252, F1 Micro: 0.7807, F1 Macro: 0.7051\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9277, F1 Micro: 0.788, F1 Macro: 0.7248\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9233, F1 Micro: 0.7857, F1 Macro: 0.723\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9313, F1 Micro: 0.7949, F1 Macro: 0.7137\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.79      0.72      0.76       729\n",
      "     HS_Group       0.73      0.70      0.72       408\n",
      "  HS_Religion       0.71      0.68      0.69       168\n",
      "      HS_Race       0.77      0.76      0.77       119\n",
      "  HS_Physical       0.70      0.25      0.36        57\n",
      "    HS_Gender       0.73      0.40      0.52        55\n",
      "     HS_Other       0.84      0.79      0.82       771\n",
      "      HS_Weak       0.77      0.70      0.73       681\n",
      "  HS_Moderate       0.69      0.61      0.65       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.82      0.77      0.79      5589\n",
      "    macro avg       0.78      0.68      0.71      5589\n",
      " weighted avg       0.82      0.77      0.79      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 297.97872400283813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3315, Accuracy: 0.9063, F1 Micro: 0.7268, F1 Macro: 0.523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2189, Accuracy: 0.9216, F1 Micro: 0.7658, F1 Macro: 0.6061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1776, Accuracy: 0.9272, F1 Micro: 0.7741, F1 Macro: 0.6227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1391, Accuracy: 0.9233, F1 Micro: 0.7849, F1 Macro: 0.655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1113, Accuracy: 0.9282, F1 Micro: 0.7904, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.9267, F1 Micro: 0.7911, F1 Macro: 0.7192\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9297, F1 Micro: 0.787, F1 Macro: 0.715\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9264, F1 Micro: 0.7837, F1 Macro: 0.7092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9288, F1 Micro: 0.7926, F1 Macro: 0.7253\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9249, F1 Micro: 0.7879, F1 Macro: 0.7214\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9288, F1 Micro: 0.7926, F1 Macro: 0.7253\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.67      0.71      0.69       408\n",
      "  HS_Religion       0.71      0.70      0.70       168\n",
      "      HS_Race       0.74      0.78      0.76       119\n",
      "  HS_Physical       0.68      0.33      0.45        57\n",
      "    HS_Gender       0.71      0.55      0.62        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.76      0.69      0.72       681\n",
      "  HS_Moderate       0.62      0.64      0.63       359\n",
      "    HS_Strong       0.76      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 300.4273920059204 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9238, F1 Micro: 0.7765, F1 Macro: 0.6831\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.2115111351013184 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3234, Accuracy: 0.907, F1 Micro: 0.7028, F1 Macro: 0.4901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2119, Accuracy: 0.9153, F1 Micro: 0.7573, F1 Macro: 0.5739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1714, Accuracy: 0.9232, F1 Micro: 0.7744, F1 Macro: 0.6153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9257, F1 Micro: 0.7865, F1 Macro: 0.6774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1098, Accuracy: 0.9275, F1 Micro: 0.7887, F1 Macro: 0.6863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0856, Accuracy: 0.9296, F1 Micro: 0.7899, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9289, F1 Micro: 0.7937, F1 Macro: 0.7228\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9244, F1 Micro: 0.7866, F1 Macro: 0.7188\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.9239, F1 Micro: 0.784, F1 Macro: 0.7244\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9266, F1 Micro: 0.7848, F1 Macro: 0.72\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9289, F1 Micro: 0.7937, F1 Macro: 0.7228\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.69      0.67      0.68       408\n",
      "  HS_Religion       0.73      0.70      0.71       168\n",
      "      HS_Race       0.74      0.76      0.75       119\n",
      "  HS_Physical       0.70      0.33      0.45        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.66      0.59      0.62       359\n",
      "    HS_Strong       0.79      0.86      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 301.6770911216736 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3261, Accuracy: 0.9039, F1 Micro: 0.6717, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2123, Accuracy: 0.9152, F1 Micro: 0.7557, F1 Macro: 0.5601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1699, Accuracy: 0.9238, F1 Micro: 0.7771, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9296, F1 Micro: 0.7908, F1 Macro: 0.68\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.11, Accuracy: 0.9288, F1 Micro: 0.7944, F1 Macro: 0.7072\n",
      "Epoch 6/10, Train Loss: 0.0866, Accuracy: 0.9274, F1 Micro: 0.79, F1 Macro: 0.6919\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9293, F1 Micro: 0.7833, F1 Macro: 0.7169\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9267, F1 Micro: 0.7916, F1 Macro: 0.7241\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9289, F1 Micro: 0.7828, F1 Macro: 0.7195\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9282, F1 Micro: 0.7854, F1 Macro: 0.7242\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9288, F1 Micro: 0.7944, F1 Macro: 0.7072\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.89      0.87      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.73      0.65      0.69       408\n",
      "  HS_Religion       0.74      0.66      0.70       168\n",
      "      HS_Race       0.71      0.76      0.73       119\n",
      "  HS_Physical       0.60      0.26      0.37        57\n",
      "    HS_Gender       0.61      0.45      0.52        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.67      0.59      0.63       359\n",
      "    HS_Strong       0.81      0.72      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.74      0.69      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 297.43653202056885 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.329, Accuracy: 0.9059, F1 Micro: 0.6994, F1 Macro: 0.4773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2149, Accuracy: 0.9164, F1 Micro: 0.7587, F1 Macro: 0.5675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1721, Accuracy: 0.9235, F1 Micro: 0.7723, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.926, F1 Micro: 0.787, F1 Macro: 0.6634\n",
      "Epoch 5/10, Train Loss: 0.1118, Accuracy: 0.9257, F1 Micro: 0.7841, F1 Macro: 0.6686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0842, Accuracy: 0.929, F1 Micro: 0.7924, F1 Macro: 0.6917\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9266, F1 Micro: 0.7902, F1 Macro: 0.7232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9281, F1 Micro: 0.793, F1 Macro: 0.7262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9308, F1 Micro: 0.7966, F1 Macro: 0.7367\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9305, F1 Micro: 0.7932, F1 Macro: 0.7263\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9308, F1 Micro: 0.7966, F1 Macro: 0.7367\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.92      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.75      0.76       729\n",
      "     HS_Group       0.73      0.66      0.69       408\n",
      "  HS_Religion       0.76      0.68      0.72       168\n",
      "      HS_Race       0.71      0.85      0.77       119\n",
      "  HS_Physical       0.63      0.42      0.51        57\n",
      "    HS_Gender       0.74      0.53      0.62        55\n",
      "     HS_Other       0.84      0.77      0.80       771\n",
      "      HS_Weak       0.74      0.74      0.74       681\n",
      "  HS_Moderate       0.67      0.61      0.64       359\n",
      "    HS_Strong       0.82      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.80      5589\n",
      "    macro avg       0.77      0.72      0.74      5589\n",
      " weighted avg       0.81      0.78      0.80      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 300.7065727710724 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.924, F1 Micro: 0.7772, F1 Macro: 0.6845\n",
      "Total sampling time: 1064.0 seconds\n",
      "Total runtime: 20094.074107170105 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a83f0b",
   "metadata": {
    "papermill": {
     "duration": 0.180165,
     "end_time": "2025-01-30T11:43:27.993254",
     "exception": false,
     "start_time": "2025-01-30T11:43:27.813089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5512, Accuracy: 0.8273, F1 Micro: 0.0155, F1 Macro: 0.0068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4227, Accuracy: 0.8318, F1 Micro: 0.0699, F1 Macro: 0.0271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3799, Accuracy: 0.8366, F1 Micro: 0.1263, F1 Macro: 0.0477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3674, Accuracy: 0.8553, F1 Micro: 0.3468, F1 Macro: 0.1119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3434, Accuracy: 0.8696, F1 Micro: 0.4666, F1 Macro: 0.2028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2969, Accuracy: 0.8799, F1 Micro: 0.5801, F1 Macro: 0.2942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2628, Accuracy: 0.8847, F1 Micro: 0.5994, F1 Macro: 0.3207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.246, Accuracy: 0.8869, F1 Micro: 0.6208, F1 Macro: 0.3672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2173, Accuracy: 0.8891, F1 Micro: 0.6299, F1 Macro: 0.3663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1865, Accuracy: 0.8893, F1 Micro: 0.6348, F1 Macro: 0.3831\n",
      "Model 1 - Iteration 658: Accuracy: 0.8893, F1 Micro: 0.6348, F1 Macro: 0.3831\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.73      0.78      1137\n",
      "      Abusive       0.85      0.73      0.79      1008\n",
      "HS_Individual       0.69      0.56      0.62       729\n",
      "     HS_Group       0.63      0.34      0.44       408\n",
      "  HS_Religion       0.59      0.19      0.29       168\n",
      "      HS_Race       1.00      0.03      0.07       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.62      0.66       771\n",
      "      HS_Weak       0.62      0.55      0.58       681\n",
      "  HS_Moderate       0.57      0.28      0.38       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.74      0.56      0.63      5589\n",
      "    macro avg       0.54      0.34      0.38      5589\n",
      " weighted avg       0.71      0.56      0.61      5589\n",
      "  samples avg       0.38      0.32      0.32      5589\n",
      "\n",
      "Training completed in 62.422691822052 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5374, Accuracy: 0.8305, F1 Micro: 0.1019, F1 Macro: 0.0326\n",
      "Epoch 2/10, Train Loss: 0.4175, Accuracy: 0.8339, F1 Micro: 0.1013, F1 Macro: 0.0392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3741, Accuracy: 0.841, F1 Micro: 0.1781, F1 Macro: 0.066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3619, Accuracy: 0.8561, F1 Micro: 0.3601, F1 Macro: 0.114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3371, Accuracy: 0.8691, F1 Micro: 0.462, F1 Macro: 0.1986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2938, Accuracy: 0.8806, F1 Micro: 0.5917, F1 Macro: 0.3113\n",
      "Epoch 7/10, Train Loss: 0.2568, Accuracy: 0.8836, F1 Micro: 0.5876, F1 Macro: 0.3214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.242, Accuracy: 0.8886, F1 Micro: 0.6376, F1 Macro: 0.3959\n",
      "Epoch 9/10, Train Loss: 0.2149, Accuracy: 0.8873, F1 Micro: 0.6054, F1 Macro: 0.3649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1846, Accuracy: 0.8901, F1 Micro: 0.6448, F1 Macro: 0.4105\n",
      "Model 2 - Iteration 658: Accuracy: 0.8901, F1 Micro: 0.6448, F1 Macro: 0.4105\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.75      0.79      1137\n",
      "      Abusive       0.84      0.76      0.80      1008\n",
      "HS_Individual       0.70      0.56      0.62       729\n",
      "     HS_Group       0.59      0.45      0.51       408\n",
      "  HS_Religion       0.53      0.20      0.29       168\n",
      "      HS_Race       1.00      0.10      0.18       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.71      0.63      0.67       771\n",
      "      HS_Weak       0.65      0.51      0.57       681\n",
      "  HS_Moderate       0.52      0.36      0.43       359\n",
      "    HS_Strong       1.00      0.04      0.08        97\n",
      "\n",
      "    micro avg       0.73      0.58      0.64      5589\n",
      "    macro avg       0.61      0.36      0.41      5589\n",
      " weighted avg       0.72      0.58      0.62      5589\n",
      "  samples avg       0.38      0.33      0.33      5589\n",
      "\n",
      "Training completed in 60.02549600601196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5412, Accuracy: 0.8273, F1 Micro: 0.0114, F1 Macro: 0.0051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4358, Accuracy: 0.8297, F1 Micro: 0.0419, F1 Macro: 0.0174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3866, Accuracy: 0.8344, F1 Micro: 0.1004, F1 Macro: 0.0387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3736, Accuracy: 0.8464, F1 Micro: 0.2413, F1 Macro: 0.0848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3538, Accuracy: 0.8609, F1 Micro: 0.3858, F1 Macro: 0.1487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3114, Accuracy: 0.8772, F1 Micro: 0.5475, F1 Macro: 0.2626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2704, Accuracy: 0.8818, F1 Micro: 0.5891, F1 Macro: 0.3144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2517, Accuracy: 0.8853, F1 Micro: 0.6134, F1 Macro: 0.3634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2226, Accuracy: 0.8848, F1 Micro: 0.6258, F1 Macro: 0.3923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1869, Accuracy: 0.8879, F1 Micro: 0.6373, F1 Macro: 0.3969\n",
      "Model 3 - Iteration 658: Accuracy: 0.8879, F1 Micro: 0.6373, F1 Macro: 0.3969\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.76      0.79      1137\n",
      "      Abusive       0.82      0.73      0.77      1008\n",
      "HS_Individual       0.67      0.56      0.61       729\n",
      "     HS_Group       0.60      0.42      0.49       408\n",
      "  HS_Religion       0.57      0.28      0.37       168\n",
      "      HS_Race       1.00      0.04      0.08       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.69      0.61      0.65       771\n",
      "      HS_Weak       0.66      0.52      0.58       681\n",
      "  HS_Moderate       0.54      0.34      0.41       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.73      0.57      0.64      5589\n",
      "    macro avg       0.53      0.35      0.40      5589\n",
      " weighted avg       0.69      0.57      0.61      5589\n",
      "  samples avg       0.38      0.33      0.33      5589\n",
      "\n",
      "Training completed in 65.7714467048645 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8891, F1 Micro: 0.639, F1 Macro: 0.3968\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 144.6731469631195 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5031, Accuracy: 0.8266, F1 Micro: 0.3861, F1 Macro: 0.105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4191, Accuracy: 0.8717, F1 Micro: 0.5407, F1 Macro: 0.2309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3545, Accuracy: 0.8889, F1 Micro: 0.6442, F1 Macro: 0.3454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3105, Accuracy: 0.8975, F1 Micro: 0.6522, F1 Macro: 0.4282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2654, Accuracy: 0.9033, F1 Micro: 0.6915, F1 Macro: 0.5213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2182, Accuracy: 0.9048, F1 Micro: 0.702, F1 Macro: 0.5267\n",
      "Epoch 7/10, Train Loss: 0.1912, Accuracy: 0.9014, F1 Micro: 0.6669, F1 Macro: 0.5157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1631, Accuracy: 0.9031, F1 Micro: 0.7049, F1 Macro: 0.5396\n",
      "Epoch 9/10, Train Loss: 0.1343, Accuracy: 0.9057, F1 Micro: 0.6936, F1 Macro: 0.52\n",
      "Epoch 10/10, Train Loss: 0.1146, Accuracy: 0.9054, F1 Micro: 0.7042, F1 Macro: 0.547\n",
      "Model 1 - Iteration 1646: Accuracy: 0.9031, F1 Micro: 0.7049, F1 Macro: 0.5396\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.79      0.82      1137\n",
      "      Abusive       0.86      0.78      0.82      1008\n",
      "HS_Individual       0.67      0.72      0.69       729\n",
      "     HS_Group       0.71      0.48      0.57       408\n",
      "  HS_Religion       0.61      0.57      0.59       168\n",
      "      HS_Race       0.72      0.53      0.61       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.74      0.69      0.72       771\n",
      "      HS_Weak       0.62      0.68      0.65       681\n",
      "  HS_Moderate       0.63      0.38      0.48       359\n",
      "    HS_Strong       0.78      0.40      0.53        97\n",
      "\n",
      "    micro avg       0.75      0.67      0.70      5589\n",
      "    macro avg       0.60      0.50      0.54      5589\n",
      " weighted avg       0.73      0.67      0.69      5589\n",
      "  samples avg       0.39      0.37      0.36      5589\n",
      "\n",
      "Training completed in 84.27952575683594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4906, Accuracy: 0.8301, F1 Micro: 0.4012, F1 Macro: 0.1089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4144, Accuracy: 0.8686, F1 Micro: 0.5511, F1 Macro: 0.2689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3516, Accuracy: 0.8917, F1 Micro: 0.6443, F1 Macro: 0.3752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3085, Accuracy: 0.8998, F1 Micro: 0.6645, F1 Macro: 0.4431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2636, Accuracy: 0.9057, F1 Micro: 0.6971, F1 Macro: 0.5153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2205, Accuracy: 0.9055, F1 Micro: 0.7088, F1 Macro: 0.5482\n",
      "Epoch 7/10, Train Loss: 0.1915, Accuracy: 0.9044, F1 Micro: 0.6722, F1 Macro: 0.5301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1611, Accuracy: 0.9071, F1 Micro: 0.7153, F1 Macro: 0.5706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1367, Accuracy: 0.9087, F1 Micro: 0.7202, F1 Macro: 0.5666\n",
      "Epoch 10/10, Train Loss: 0.1118, Accuracy: 0.9047, F1 Micro: 0.7155, F1 Macro: 0.5597\n",
      "Model 2 - Iteration 1646: Accuracy: 0.9087, F1 Micro: 0.7202, F1 Macro: 0.5666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.80      0.82      1137\n",
      "      Abusive       0.85      0.83      0.84      1008\n",
      "HS_Individual       0.71      0.67      0.69       729\n",
      "     HS_Group       0.69      0.58      0.63       408\n",
      "  HS_Religion       0.66      0.54      0.59       168\n",
      "      HS_Race       0.75      0.55      0.63       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.78      0.66      0.72       771\n",
      "      HS_Weak       0.68      0.63      0.65       681\n",
      "  HS_Moderate       0.62      0.47      0.53       359\n",
      "    HS_Strong       0.82      0.60      0.69        97\n",
      "\n",
      "    micro avg       0.77      0.68      0.72      5589\n",
      "    macro avg       0.62      0.53      0.57      5589\n",
      " weighted avg       0.75      0.68      0.71      5589\n",
      "  samples avg       0.41      0.39      0.38      5589\n",
      "\n",
      "Training completed in 86.67958354949951 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5025, Accuracy: 0.8262, F1 Micro: 0.357, F1 Macro: 0.0963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4245, Accuracy: 0.862, F1 Micro: 0.5164, F1 Macro: 0.2123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3601, Accuracy: 0.8874, F1 Micro: 0.6476, F1 Macro: 0.3503\n",
      "Epoch 4/10, Train Loss: 0.316, Accuracy: 0.8954, F1 Micro: 0.6333, F1 Macro: 0.4376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2674, Accuracy: 0.9014, F1 Micro: 0.7033, F1 Macro: 0.5285\n",
      "Epoch 6/10, Train Loss: 0.2221, Accuracy: 0.9036, F1 Micro: 0.6923, F1 Macro: 0.5295\n",
      "Epoch 7/10, Train Loss: 0.1952, Accuracy: 0.9027, F1 Micro: 0.677, F1 Macro: 0.5151\n",
      "Epoch 8/10, Train Loss: 0.162, Accuracy: 0.9033, F1 Micro: 0.7021, F1 Macro: 0.552\n",
      "Epoch 9/10, Train Loss: 0.1338, Accuracy: 0.9045, F1 Micro: 0.6873, F1 Macro: 0.5388\n",
      "Epoch 10/10, Train Loss: 0.1135, Accuracy: 0.9048, F1 Micro: 0.7032, F1 Macro: 0.5536\n",
      "Model 3 - Iteration 1646: Accuracy: 0.9014, F1 Micro: 0.7033, F1 Macro: 0.5285\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.86      0.82      1137\n",
      "      Abusive       0.85      0.78      0.81      1008\n",
      "HS_Individual       0.69      0.67      0.68       729\n",
      "     HS_Group       0.65      0.59      0.62       408\n",
      "  HS_Religion       0.75      0.43      0.55       168\n",
      "      HS_Race       0.73      0.50      0.59       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.69      0.73      0.71       771\n",
      "      HS_Weak       0.67      0.62      0.64       681\n",
      "  HS_Moderate       0.60      0.39      0.47       359\n",
      "    HS_Strong       0.79      0.32      0.46        97\n",
      "\n",
      "    micro avg       0.73      0.67      0.70      5589\n",
      "    macro avg       0.60      0.49      0.53      5589\n",
      " weighted avg       0.72      0.67      0.69      5589\n",
      "  samples avg       0.40      0.37      0.37      5589\n",
      "\n",
      "Training completed in 78.68964385986328 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8967, F1 Micro: 0.6742, F1 Macro: 0.4709\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 127.6100766658783 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4827, Accuracy: 0.8359, F1 Micro: 0.5034, F1 Macro: 0.2186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3876, Accuracy: 0.8905, F1 Micro: 0.6405, F1 Macro: 0.3659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.324, Accuracy: 0.901, F1 Micro: 0.6699, F1 Macro: 0.4461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2668, Accuracy: 0.9093, F1 Micro: 0.7336, F1 Macro: 0.5614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2177, Accuracy: 0.9116, F1 Micro: 0.7365, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1869, Accuracy: 0.911, F1 Micro: 0.7399, F1 Macro: 0.5987\n",
      "Epoch 7/10, Train Loss: 0.1486, Accuracy: 0.9133, F1 Micro: 0.7366, F1 Macro: 0.5861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.9141, F1 Micro: 0.7472, F1 Macro: 0.6169\n",
      "Epoch 9/10, Train Loss: 0.1068, Accuracy: 0.9137, F1 Micro: 0.7403, F1 Macro: 0.6109\n",
      "Epoch 10/10, Train Loss: 0.0888, Accuracy: 0.9086, F1 Micro: 0.737, F1 Macro: 0.6055\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9141, F1 Micro: 0.7472, F1 Macro: 0.6169\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1137\n",
      "      Abusive       0.85      0.85      0.85      1008\n",
      "HS_Individual       0.71      0.73      0.72       729\n",
      "     HS_Group       0.69      0.63      0.66       408\n",
      "  HS_Religion       0.67      0.64      0.65       168\n",
      "      HS_Race       0.72      0.66      0.69       119\n",
      "  HS_Physical       0.56      0.09      0.15        57\n",
      "    HS_Gender       1.00      0.04      0.07        55\n",
      "     HS_Other       0.77      0.75      0.76       771\n",
      "      HS_Weak       0.69      0.69      0.69       681\n",
      "  HS_Moderate       0.63      0.53      0.58       359\n",
      "    HS_Strong       0.79      0.71      0.75        97\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5589\n",
      "    macro avg       0.74      0.60      0.62      5589\n",
      " weighted avg       0.76      0.73      0.74      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 107.08220863342285 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.472, Accuracy: 0.8403, F1 Micro: 0.5038, F1 Macro: 0.2138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3811, Accuracy: 0.8916, F1 Micro: 0.6298, F1 Macro: 0.3815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3202, Accuracy: 0.9041, F1 Micro: 0.6787, F1 Macro: 0.4816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2653, Accuracy: 0.9089, F1 Micro: 0.7344, F1 Macro: 0.5794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2164, Accuracy: 0.9131, F1 Micro: 0.7345, F1 Macro: 0.5816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1853, Accuracy: 0.9132, F1 Micro: 0.7433, F1 Macro: 0.6003\n",
      "Epoch 7/10, Train Loss: 0.1498, Accuracy: 0.915, F1 Micro: 0.7329, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1184, Accuracy: 0.9157, F1 Micro: 0.7435, F1 Macro: 0.5968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1053, Accuracy: 0.9144, F1 Micro: 0.7436, F1 Macro: 0.6048\n",
      "Epoch 10/10, Train Loss: 0.0896, Accuracy: 0.9148, F1 Micro: 0.7326, F1 Macro: 0.6098\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9144, F1 Micro: 0.7436, F1 Macro: 0.6048\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1137\n",
      "      Abusive       0.88      0.84      0.86      1008\n",
      "HS_Individual       0.70      0.71      0.70       729\n",
      "     HS_Group       0.72      0.58      0.64       408\n",
      "  HS_Religion       0.72      0.60      0.65       168\n",
      "      HS_Race       0.74      0.57      0.64       119\n",
      "  HS_Physical       0.56      0.09      0.15        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.77      0.75      0.76       771\n",
      "      HS_Weak       0.67      0.69      0.68       681\n",
      "  HS_Moderate       0.65      0.48      0.55       359\n",
      "    HS_Strong       0.80      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.77      0.72      0.74      5589\n",
      "    macro avg       0.67      0.57      0.60      5589\n",
      " weighted avg       0.76      0.72      0.74      5589\n",
      "  samples avg       0.41      0.40      0.39      5589\n",
      "\n",
      "Training completed in 108.87863874435425 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4853, Accuracy: 0.8283, F1 Micro: 0.4944, F1 Macro: 0.2159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3941, Accuracy: 0.8892, F1 Micro: 0.6337, F1 Macro: 0.3585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3264, Accuracy: 0.9013, F1 Micro: 0.6756, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2678, Accuracy: 0.9085, F1 Micro: 0.7282, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2133, Accuracy: 0.9124, F1 Micro: 0.7324, F1 Macro: 0.5812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1848, Accuracy: 0.9113, F1 Micro: 0.7329, F1 Macro: 0.5925\n",
      "Epoch 7/10, Train Loss: 0.1426, Accuracy: 0.9134, F1 Micro: 0.7321, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1206, Accuracy: 0.913, F1 Micro: 0.7362, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1056, Accuracy: 0.9125, F1 Micro: 0.7372, F1 Macro: 0.603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0866, Accuracy: 0.9138, F1 Micro: 0.7433, F1 Macro: 0.6151\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9138, F1 Micro: 0.7433, F1 Macro: 0.6151\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1137\n",
      "      Abusive       0.84      0.88      0.86      1008\n",
      "HS_Individual       0.70      0.72      0.71       729\n",
      "     HS_Group       0.70      0.54      0.61       408\n",
      "  HS_Religion       0.74      0.57      0.65       168\n",
      "      HS_Race       0.70      0.67      0.69       119\n",
      "  HS_Physical       0.50      0.05      0.10        57\n",
      "    HS_Gender       0.83      0.09      0.16        55\n",
      "     HS_Other       0.79      0.71      0.75       771\n",
      "      HS_Weak       0.67      0.70      0.68       681\n",
      "  HS_Moderate       0.66      0.49      0.56       359\n",
      "    HS_Strong       0.79      0.76      0.77        97\n",
      "\n",
      "    micro avg       0.77      0.72      0.74      5589\n",
      "    macro avg       0.73      0.59      0.62      5589\n",
      " weighted avg       0.76      0.72      0.73      5589\n",
      "  samples avg       0.43      0.41      0.40      5589\n",
      "\n",
      "Training completed in 109.90858840942383 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9025, F1 Micro: 0.6977, F1 Macro: 0.518\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 113.87849926948547 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4679, Accuracy: 0.8605, F1 Micro: 0.4726, F1 Macro: 0.1714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3605, Accuracy: 0.8958, F1 Micro: 0.6984, F1 Macro: 0.4848\n",
      "Epoch 3/10, Train Loss: 0.2981, Accuracy: 0.9085, F1 Micro: 0.692, F1 Macro: 0.5113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2449, Accuracy: 0.9093, F1 Micro: 0.7255, F1 Macro: 0.5493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1952, Accuracy: 0.9151, F1 Micro: 0.7406, F1 Macro: 0.5936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.162, Accuracy: 0.914, F1 Micro: 0.7511, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1271, Accuracy: 0.9168, F1 Micro: 0.7514, F1 Macro: 0.6194\n",
      "Epoch 8/10, Train Loss: 0.1046, Accuracy: 0.916, F1 Micro: 0.7484, F1 Macro: 0.6274\n",
      "Epoch 9/10, Train Loss: 0.0906, Accuracy: 0.9175, F1 Micro: 0.7502, F1 Macro: 0.6489\n",
      "Epoch 10/10, Train Loss: 0.0765, Accuracy: 0.9125, F1 Micro: 0.7511, F1 Macro: 0.6501\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9168, F1 Micro: 0.7514, F1 Macro: 0.6194\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.84      1137\n",
      "      Abusive       0.87      0.86      0.86      1008\n",
      "HS_Individual       0.70      0.73      0.72       729\n",
      "     HS_Group       0.74      0.57      0.65       408\n",
      "  HS_Religion       0.72      0.63      0.67       168\n",
      "      HS_Race       0.78      0.66      0.72       119\n",
      "  HS_Physical       0.60      0.05      0.10        57\n",
      "    HS_Gender       0.67      0.07      0.13        55\n",
      "     HS_Other       0.80      0.74      0.77       771\n",
      "      HS_Weak       0.67      0.71      0.69       681\n",
      "  HS_Moderate       0.69      0.48      0.57       359\n",
      "    HS_Strong       0.78      0.68      0.73        97\n",
      "\n",
      "    micro avg       0.78      0.73      0.75      5589\n",
      "    macro avg       0.74      0.59      0.62      5589\n",
      " weighted avg       0.78      0.73      0.74      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 126.69723582267761 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4591, Accuracy: 0.8651, F1 Micro: 0.5089, F1 Macro: 0.2128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3577, Accuracy: 0.8977, F1 Micro: 0.7084, F1 Macro: 0.5331\n",
      "Epoch 3/10, Train Loss: 0.2941, Accuracy: 0.9093, F1 Micro: 0.7023, F1 Macro: 0.548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2431, Accuracy: 0.9098, F1 Micro: 0.7416, F1 Macro: 0.5663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1964, Accuracy: 0.9171, F1 Micro: 0.7455, F1 Macro: 0.6034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1603, Accuracy: 0.9171, F1 Micro: 0.7538, F1 Macro: 0.6072\n",
      "Epoch 7/10, Train Loss: 0.1277, Accuracy: 0.914, F1 Micro: 0.7428, F1 Macro: 0.5948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9179, F1 Micro: 0.759, F1 Macro: 0.6243\n",
      "Epoch 9/10, Train Loss: 0.0945, Accuracy: 0.9187, F1 Micro: 0.7553, F1 Macro: 0.6273\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9148, F1 Micro: 0.7529, F1 Macro: 0.6392\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9179, F1 Micro: 0.759, F1 Macro: 0.6243\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1137\n",
      "      Abusive       0.88      0.85      0.87      1008\n",
      "HS_Individual       0.69      0.75      0.72       729\n",
      "     HS_Group       0.75      0.59      0.66       408\n",
      "  HS_Religion       0.77      0.61      0.68       168\n",
      "      HS_Race       0.75      0.62      0.68       119\n",
      "  HS_Physical       0.60      0.05      0.10        57\n",
      "    HS_Gender       1.00      0.05      0.10        55\n",
      "     HS_Other       0.76      0.80      0.78       771\n",
      "      HS_Weak       0.67      0.73      0.70       681\n",
      "  HS_Moderate       0.70      0.53      0.60       359\n",
      "    HS_Strong       0.78      0.74      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5589\n",
      "    macro avg       0.77      0.60      0.62      5589\n",
      " weighted avg       0.77      0.75      0.75      5589\n",
      "  samples avg       0.43      0.42      0.41      5589\n",
      "\n",
      "Training completed in 126.79150485992432 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4724, Accuracy: 0.8521, F1 Micro: 0.4881, F1 Macro: 0.1882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3663, Accuracy: 0.8963, F1 Micro: 0.6995, F1 Macro: 0.489\n",
      "Epoch 3/10, Train Loss: 0.2992, Accuracy: 0.9067, F1 Micro: 0.6836, F1 Macro: 0.5145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2444, Accuracy: 0.9102, F1 Micro: 0.732, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1953, Accuracy: 0.9148, F1 Micro: 0.7519, F1 Macro: 0.5941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1592, Accuracy: 0.9177, F1 Micro: 0.7525, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1271, Accuracy: 0.9176, F1 Micro: 0.7557, F1 Macro: 0.6231\n",
      "Epoch 8/10, Train Loss: 0.1104, Accuracy: 0.9167, F1 Micro: 0.7407, F1 Macro: 0.6173\n",
      "Epoch 9/10, Train Loss: 0.0951, Accuracy: 0.9197, F1 Micro: 0.7547, F1 Macro: 0.6402\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9172, F1 Micro: 0.7503, F1 Macro: 0.632\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9176, F1 Micro: 0.7557, F1 Macro: 0.6231\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1137\n",
      "      Abusive       0.86      0.87      0.86      1008\n",
      "HS_Individual       0.71      0.74      0.72       729\n",
      "     HS_Group       0.73      0.57      0.64       408\n",
      "  HS_Religion       0.71      0.60      0.65       168\n",
      "      HS_Race       0.83      0.65      0.73       119\n",
      "  HS_Physical       1.00      0.04      0.07        57\n",
      "    HS_Gender       0.71      0.09      0.16        55\n",
      "     HS_Other       0.78      0.76      0.77       771\n",
      "      HS_Weak       0.67      0.72      0.70       681\n",
      "  HS_Moderate       0.68      0.49      0.57       359\n",
      "    HS_Strong       0.79      0.73      0.76        97\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5589\n",
      "    macro avg       0.78      0.59      0.62      5589\n",
      " weighted avg       0.78      0.74      0.75      5589\n",
      "  samples avg       0.43      0.42      0.41      5589\n",
      "\n",
      "Training completed in 125.77313566207886 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9062, F1 Micro: 0.7121, F1 Macro: 0.5441\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 101.55936098098755 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4574, Accuracy: 0.875, F1 Micro: 0.6166, F1 Macro: 0.3221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3448, Accuracy: 0.9049, F1 Micro: 0.7204, F1 Macro: 0.5192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.9113, F1 Micro: 0.7397, F1 Macro: 0.5556\n",
      "Epoch 4/10, Train Loss: 0.2298, Accuracy: 0.9177, F1 Micro: 0.7356, F1 Macro: 0.5905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1829, Accuracy: 0.917, F1 Micro: 0.7562, F1 Macro: 0.6031\n",
      "Epoch 6/10, Train Loss: 0.1357, Accuracy: 0.914, F1 Micro: 0.7512, F1 Macro: 0.6147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1159, Accuracy: 0.9159, F1 Micro: 0.7572, F1 Macro: 0.634\n",
      "Epoch 8/10, Train Loss: 0.0994, Accuracy: 0.9164, F1 Micro: 0.7547, F1 Macro: 0.6548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0792, Accuracy: 0.9172, F1 Micro: 0.7573, F1 Macro: 0.6537\n",
      "Epoch 10/10, Train Loss: 0.0733, Accuracy: 0.9175, F1 Micro: 0.757, F1 Macro: 0.6841\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9172, F1 Micro: 0.7573, F1 Macro: 0.6537\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1137\n",
      "      Abusive       0.86      0.89      0.88      1008\n",
      "HS_Individual       0.72      0.68      0.70       729\n",
      "     HS_Group       0.67      0.65      0.66       408\n",
      "  HS_Religion       0.71      0.65      0.68       168\n",
      "      HS_Race       0.74      0.77      0.76       119\n",
      "  HS_Physical       0.70      0.12      0.21        57\n",
      "    HS_Gender       0.69      0.20      0.31        55\n",
      "     HS_Other       0.78      0.79      0.79       771\n",
      "      HS_Weak       0.70      0.66      0.68       681\n",
      "  HS_Moderate       0.63      0.56      0.59       359\n",
      "    HS_Strong       0.72      0.80      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5589\n",
      "    macro avg       0.73      0.63      0.65      5589\n",
      " weighted avg       0.77      0.75      0.75      5589\n",
      "  samples avg       0.44      0.42      0.42      5589\n",
      "\n",
      "Training completed in 145.10232973098755 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.449, Accuracy: 0.8771, F1 Micro: 0.6123, F1 Macro: 0.3421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3401, Accuracy: 0.9083, F1 Micro: 0.7264, F1 Macro: 0.5478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2818, Accuracy: 0.9134, F1 Micro: 0.7377, F1 Macro: 0.561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2277, Accuracy: 0.9195, F1 Micro: 0.7411, F1 Macro: 0.5989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1822, Accuracy: 0.9154, F1 Micro: 0.7501, F1 Macro: 0.5966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1419, Accuracy: 0.9181, F1 Micro: 0.7511, F1 Macro: 0.6209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9195, F1 Micro: 0.7531, F1 Macro: 0.6362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1018, Accuracy: 0.9186, F1 Micro: 0.7607, F1 Macro: 0.6429\n",
      "Epoch 9/10, Train Loss: 0.0814, Accuracy: 0.9182, F1 Micro: 0.7567, F1 Macro: 0.6537\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9209, F1 Micro: 0.7594, F1 Macro: 0.6743\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9186, F1 Micro: 0.7607, F1 Macro: 0.6429\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1137\n",
      "      Abusive       0.86      0.90      0.88      1008\n",
      "HS_Individual       0.68      0.75      0.71       729\n",
      "     HS_Group       0.75      0.58      0.65       408\n",
      "  HS_Religion       0.78      0.60      0.68       168\n",
      "      HS_Race       0.79      0.68      0.73       119\n",
      "  HS_Physical       0.44      0.12      0.19        57\n",
      "    HS_Gender       0.86      0.11      0.19        55\n",
      "     HS_Other       0.80      0.78      0.79       771\n",
      "      HS_Weak       0.67      0.72      0.69       681\n",
      "  HS_Moderate       0.70      0.49      0.58       359\n",
      "    HS_Strong       0.79      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5589\n",
      "    macro avg       0.75      0.61      0.64      5589\n",
      " weighted avg       0.77      0.75      0.75      5589\n",
      "  samples avg       0.43      0.42      0.41      5589\n",
      "\n",
      "Training completed in 147.1787645816803 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4636, Accuracy: 0.8742, F1 Micro: 0.6043, F1 Macro: 0.3076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3504, Accuracy: 0.9042, F1 Micro: 0.7162, F1 Macro: 0.5119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.288, Accuracy: 0.9113, F1 Micro: 0.746, F1 Macro: 0.5798\n",
      "Epoch 4/10, Train Loss: 0.2328, Accuracy: 0.9174, F1 Micro: 0.7334, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.183, Accuracy: 0.9157, F1 Micro: 0.7601, F1 Macro: 0.6147\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9186, F1 Micro: 0.7558, F1 Macro: 0.6236\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9173, F1 Micro: 0.7558, F1 Macro: 0.6365\n",
      "Epoch 8/10, Train Loss: 0.1026, Accuracy: 0.919, F1 Micro: 0.7552, F1 Macro: 0.6454\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9167, F1 Micro: 0.759, F1 Macro: 0.6448\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9187, F1 Micro: 0.7594, F1 Macro: 0.671\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9157, F1 Micro: 0.7601, F1 Macro: 0.6147\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1137\n",
      "      Abusive       0.83      0.88      0.86      1008\n",
      "HS_Individual       0.67      0.81      0.73       729\n",
      "     HS_Group       0.76      0.57      0.65       408\n",
      "  HS_Religion       0.79      0.56      0.66       168\n",
      "      HS_Race       0.78      0.56      0.65       119\n",
      "  HS_Physical       0.50      0.04      0.07        57\n",
      "    HS_Gender       1.00      0.04      0.07        55\n",
      "     HS_Other       0.75      0.82      0.78       771\n",
      "      HS_Weak       0.63      0.79      0.70       681\n",
      "  HS_Moderate       0.71      0.48      0.58       359\n",
      "    HS_Strong       0.82      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5589\n",
      "    macro avg       0.75      0.60      0.61      5589\n",
      " weighted avg       0.75      0.77      0.75      5589\n",
      "  samples avg       0.43      0.43      0.42      5589\n",
      "\n",
      "Training completed in 140.1320059299469 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9084, F1 Micro: 0.7216, F1 Macro: 0.5627\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 91.58627843856812 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4435, Accuracy: 0.8812, F1 Micro: 0.5531, F1 Macro: 0.2762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3384, Accuracy: 0.9027, F1 Micro: 0.7289, F1 Macro: 0.552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.274, Accuracy: 0.9169, F1 Micro: 0.7521, F1 Macro: 0.6068\n",
      "Epoch 4/10, Train Loss: 0.2219, Accuracy: 0.9204, F1 Micro: 0.7485, F1 Macro: 0.6055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1778, Accuracy: 0.9149, F1 Micro: 0.7618, F1 Macro: 0.6517\n",
      "Epoch 6/10, Train Loss: 0.1426, Accuracy: 0.9168, F1 Micro: 0.7598, F1 Macro: 0.6309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.921, F1 Micro: 0.766, F1 Macro: 0.6651\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9164, F1 Micro: 0.7585, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9226, F1 Micro: 0.7734, F1 Macro: 0.6983\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9231, F1 Micro: 0.7719, F1 Macro: 0.7018\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9226, F1 Micro: 0.7734, F1 Macro: 0.6983\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.73      0.74      0.74       729\n",
      "     HS_Group       0.71      0.62      0.66       408\n",
      "  HS_Religion       0.69      0.68      0.68       168\n",
      "      HS_Race       0.73      0.80      0.76       119\n",
      "  HS_Physical       0.69      0.35      0.47        57\n",
      "    HS_Gender       0.67      0.36      0.47        55\n",
      "     HS_Other       0.80      0.79      0.80       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.64      0.53      0.58       359\n",
      "    HS_Strong       0.78      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.74      0.67      0.70      5589\n",
      " weighted avg       0.78      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 160.39978122711182 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4374, Accuracy: 0.8838, F1 Micro: 0.5739, F1 Macro: 0.299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3351, Accuracy: 0.8994, F1 Micro: 0.724, F1 Macro: 0.5512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.273, Accuracy: 0.9186, F1 Micro: 0.7561, F1 Macro: 0.6083\n",
      "Epoch 4/10, Train Loss: 0.2238, Accuracy: 0.9218, F1 Micro: 0.7537, F1 Macro: 0.6103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1804, Accuracy: 0.9136, F1 Micro: 0.7615, F1 Macro: 0.631\n",
      "Epoch 6/10, Train Loss: 0.1482, Accuracy: 0.9146, F1 Micro: 0.761, F1 Macro: 0.6304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1158, Accuracy: 0.9177, F1 Micro: 0.7625, F1 Macro: 0.6494\n",
      "Epoch 8/10, Train Loss: 0.0901, Accuracy: 0.918, F1 Micro: 0.7568, F1 Macro: 0.661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0798, Accuracy: 0.9189, F1 Micro: 0.7639, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9215, F1 Micro: 0.7661, F1 Macro: 0.6781\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9215, F1 Micro: 0.7661, F1 Macro: 0.6781\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1137\n",
      "      Abusive       0.89      0.88      0.89      1008\n",
      "HS_Individual       0.77      0.67      0.72       729\n",
      "     HS_Group       0.65      0.68      0.66       408\n",
      "  HS_Religion       0.75      0.62      0.68       168\n",
      "      HS_Race       0.72      0.76      0.74       119\n",
      "  HS_Physical       0.78      0.25      0.37        57\n",
      "    HS_Gender       0.70      0.25      0.37        55\n",
      "     HS_Other       0.81      0.78      0.79       771\n",
      "      HS_Weak       0.75      0.64      0.69       681\n",
      "  HS_Moderate       0.58      0.60      0.59       359\n",
      "    HS_Strong       0.76      0.80      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5589\n",
      "    macro avg       0.75      0.65      0.68      5589\n",
      " weighted avg       0.79      0.74      0.76      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 160.91967582702637 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4502, Accuracy: 0.8776, F1 Micro: 0.5415, F1 Macro: 0.2492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3433, Accuracy: 0.9056, F1 Micro: 0.7338, F1 Macro: 0.5605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2739, Accuracy: 0.9177, F1 Micro: 0.7525, F1 Macro: 0.607\n",
      "Epoch 4/10, Train Loss: 0.2202, Accuracy: 0.9208, F1 Micro: 0.7504, F1 Macro: 0.6087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1787, Accuracy: 0.9164, F1 Micro: 0.7658, F1 Macro: 0.6391\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9203, F1 Micro: 0.7625, F1 Macro: 0.6334\n",
      "Epoch 7/10, Train Loss: 0.1155, Accuracy: 0.9188, F1 Micro: 0.7561, F1 Macro: 0.638\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9136, F1 Micro: 0.7575, F1 Macro: 0.6597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9193, F1 Micro: 0.7685, F1 Macro: 0.6616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9232, F1 Micro: 0.7739, F1 Macro: 0.6871\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9232, F1 Micro: 0.7739, F1 Macro: 0.6871\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.88      0.89      0.88      1008\n",
      "HS_Individual       0.75      0.71      0.73       729\n",
      "     HS_Group       0.67      0.65      0.66       408\n",
      "  HS_Religion       0.75      0.61      0.68       168\n",
      "      HS_Race       0.73      0.74      0.74       119\n",
      "  HS_Physical       0.71      0.21      0.32        57\n",
      "    HS_Gender       0.68      0.35      0.46        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.74      0.68      0.71       681\n",
      "  HS_Moderate       0.63      0.58      0.61       359\n",
      "    HS_Strong       0.80      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5589\n",
      "    macro avg       0.75      0.66      0.69      5589\n",
      " weighted avg       0.79      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 160.3137457370758 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9108, F1 Micro: 0.7298, F1 Macro: 0.5835\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 82.38398933410645 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4348, Accuracy: 0.8846, F1 Micro: 0.6425, F1 Macro: 0.3593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3238, Accuracy: 0.912, F1 Micro: 0.7248, F1 Macro: 0.5451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2627, Accuracy: 0.9204, F1 Micro: 0.7593, F1 Macro: 0.6101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2076, Accuracy: 0.9209, F1 Micro: 0.7646, F1 Macro: 0.6243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1679, Accuracy: 0.9221, F1 Micro: 0.7691, F1 Macro: 0.6493\n",
      "Epoch 6/10, Train Loss: 0.1307, Accuracy: 0.9233, F1 Micro: 0.7684, F1 Macro: 0.6534\n",
      "Epoch 7/10, Train Loss: 0.11, Accuracy: 0.9232, F1 Micro: 0.7667, F1 Macro: 0.6686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9222, F1 Micro: 0.771, F1 Macro: 0.7001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9228, F1 Micro: 0.7716, F1 Macro: 0.6898\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9248, F1 Micro: 0.7646, F1 Macro: 0.6871\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9228, F1 Micro: 0.7716, F1 Macro: 0.6898\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.85      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.70      0.77      0.73       729\n",
      "     HS_Group       0.76      0.53      0.62       408\n",
      "  HS_Religion       0.79      0.60      0.68       168\n",
      "      HS_Race       0.74      0.76      0.75       119\n",
      "  HS_Physical       0.70      0.28      0.40        57\n",
      "    HS_Gender       0.67      0.40      0.50        55\n",
      "     HS_Other       0.81      0.78      0.80       771\n",
      "      HS_Weak       0.68      0.75      0.71       681\n",
      "  HS_Moderate       0.70      0.47      0.56       359\n",
      "    HS_Strong       0.79      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5589\n",
      "    macro avg       0.76      0.65      0.69      5589\n",
      " weighted avg       0.79      0.75      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 176.95182466506958 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4286, Accuracy: 0.8888, F1 Micro: 0.6647, F1 Macro: 0.4333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3204, Accuracy: 0.9138, F1 Micro: 0.7355, F1 Macro: 0.5806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.263, Accuracy: 0.9204, F1 Micro: 0.7618, F1 Macro: 0.6153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2073, Accuracy: 0.9208, F1 Micro: 0.764, F1 Macro: 0.6218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1692, Accuracy: 0.9223, F1 Micro: 0.7716, F1 Macro: 0.6342\n",
      "Epoch 6/10, Train Loss: 0.1316, Accuracy: 0.919, F1 Micro: 0.764, F1 Macro: 0.6428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1096, Accuracy: 0.925, F1 Micro: 0.7756, F1 Macro: 0.6557\n",
      "Epoch 8/10, Train Loss: 0.0935, Accuracy: 0.9252, F1 Micro: 0.7715, F1 Macro: 0.6747\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9231, F1 Micro: 0.7736, F1 Macro: 0.685\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9227, F1 Micro: 0.7736, F1 Macro: 0.6914\n",
      "Model 2 - Iteration 5287: Accuracy: 0.925, F1 Micro: 0.7756, F1 Macro: 0.6557\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.74      0.72      0.73       729\n",
      "     HS_Group       0.77      0.62      0.68       408\n",
      "  HS_Religion       0.73      0.65      0.69       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.67      0.14      0.23        57\n",
      "    HS_Gender       0.62      0.09      0.16        55\n",
      "     HS_Other       0.83      0.78      0.81       771\n",
      "      HS_Weak       0.71      0.70      0.71       681\n",
      "  HS_Moderate       0.71      0.52      0.60       359\n",
      "    HS_Strong       0.78      0.75      0.77        97\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5589\n",
      "    macro avg       0.75      0.62      0.66      5589\n",
      " weighted avg       0.80      0.75      0.77      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 174.06990504264832 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.439, Accuracy: 0.8856, F1 Micro: 0.6461, F1 Macro: 0.3693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3265, Accuracy: 0.9118, F1 Micro: 0.7257, F1 Macro: 0.557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.264, Accuracy: 0.9197, F1 Micro: 0.7561, F1 Macro: 0.606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9204, F1 Micro: 0.7631, F1 Macro: 0.6211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1705, Accuracy: 0.9168, F1 Micro: 0.7668, F1 Macro: 0.643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1318, Accuracy: 0.9234, F1 Micro: 0.7708, F1 Macro: 0.6493\n",
      "Epoch 7/10, Train Loss: 0.1099, Accuracy: 0.9211, F1 Micro: 0.7612, F1 Macro: 0.6488\n",
      "Epoch 8/10, Train Loss: 0.0961, Accuracy: 0.9188, F1 Micro: 0.7598, F1 Macro: 0.671\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.9222, F1 Micro: 0.7694, F1 Macro: 0.6747\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9236, F1 Micro: 0.7691, F1 Macro: 0.6796\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9234, F1 Micro: 0.7708, F1 Macro: 0.6493\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1137\n",
      "      Abusive       0.88      0.88      0.88      1008\n",
      "HS_Individual       0.77      0.70      0.73       729\n",
      "     HS_Group       0.69      0.69      0.69       408\n",
      "  HS_Religion       0.74      0.57      0.64       168\n",
      "      HS_Race       0.75      0.76      0.75       119\n",
      "  HS_Physical       1.00      0.05      0.10        57\n",
      "    HS_Gender       0.73      0.15      0.24        55\n",
      "     HS_Other       0.80      0.77      0.79       771\n",
      "      HS_Weak       0.75      0.66      0.70       681\n",
      "  HS_Moderate       0.64      0.62      0.63       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5589\n",
      "    macro avg       0.79      0.62      0.65      5589\n",
      " weighted avg       0.80      0.74      0.76      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 173.48208451271057 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9126, F1 Micro: 0.736, F1 Macro: 0.5952\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 73.84879040718079 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4261, Accuracy: 0.8925, F1 Micro: 0.6417, F1 Macro: 0.3925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3116, Accuracy: 0.9136, F1 Micro: 0.7265, F1 Macro: 0.5305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2477, Accuracy: 0.9205, F1 Micro: 0.7555, F1 Macro: 0.61\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2082, Accuracy: 0.9158, F1 Micro: 0.7673, F1 Macro: 0.6332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1637, Accuracy: 0.9249, F1 Micro: 0.7776, F1 Macro: 0.667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.9261, F1 Micro: 0.7795, F1 Macro: 0.686\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9232, F1 Micro: 0.7728, F1 Macro: 0.695\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9219, F1 Micro: 0.7695, F1 Macro: 0.685\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9259, F1 Micro: 0.7794, F1 Macro: 0.7082\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.922, F1 Micro: 0.776, F1 Macro: 0.7073\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9261, F1 Micro: 0.7795, F1 Macro: 0.686\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.76      0.73      0.74       729\n",
      "     HS_Group       0.75      0.63      0.68       408\n",
      "  HS_Religion       0.77      0.63      0.70       168\n",
      "      HS_Race       0.80      0.72      0.76       119\n",
      "  HS_Physical       0.60      0.21      0.31        57\n",
      "    HS_Gender       0.68      0.27      0.39        55\n",
      "     HS_Other       0.83      0.78      0.80       771\n",
      "      HS_Weak       0.71      0.71      0.71       681\n",
      "  HS_Moderate       0.70      0.54      0.61       359\n",
      "    HS_Strong       0.82      0.75      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5589\n",
      "    macro avg       0.76      0.64      0.69      5589\n",
      " weighted avg       0.80      0.75      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 187.21841502189636 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4192, Accuracy: 0.8943, F1 Micro: 0.6474, F1 Macro: 0.4372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3111, Accuracy: 0.9143, F1 Micro: 0.7276, F1 Macro: 0.5374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2484, Accuracy: 0.9207, F1 Micro: 0.7593, F1 Macro: 0.5973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2113, Accuracy: 0.9155, F1 Micro: 0.7636, F1 Macro: 0.6196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9244, F1 Micro: 0.7747, F1 Macro: 0.6572\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9258, F1 Micro: 0.7671, F1 Macro: 0.664\n",
      "Epoch 7/10, Train Loss: 0.1076, Accuracy: 0.9243, F1 Micro: 0.7603, F1 Macro: 0.6528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0892, Accuracy: 0.9219, F1 Micro: 0.7759, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9246, F1 Micro: 0.78, F1 Macro: 0.6959\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.925, F1 Micro: 0.7749, F1 Macro: 0.6963\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9246, F1 Micro: 0.78, F1 Macro: 0.6959\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.72      0.76      0.74       729\n",
      "     HS_Group       0.71      0.64      0.68       408\n",
      "  HS_Religion       0.72      0.65      0.68       168\n",
      "      HS_Race       0.71      0.81      0.76       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.63      0.31      0.41        55\n",
      "     HS_Other       0.82      0.79      0.81       771\n",
      "      HS_Weak       0.70      0.73      0.71       681\n",
      "  HS_Moderate       0.66      0.56      0.60       359\n",
      "    HS_Strong       0.76      0.79      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5589\n",
      "    macro avg       0.74      0.68      0.70      5589\n",
      " weighted avg       0.79      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 187.86303782463074 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4301, Accuracy: 0.8909, F1 Micro: 0.6476, F1 Macro: 0.4172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3157, Accuracy: 0.9109, F1 Micro: 0.7171, F1 Macro: 0.5202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2488, Accuracy: 0.9187, F1 Micro: 0.7597, F1 Macro: 0.6051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9168, F1 Micro: 0.7674, F1 Macro: 0.6249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1649, Accuracy: 0.9246, F1 Micro: 0.7783, F1 Macro: 0.6472\n",
      "Epoch 6/10, Train Loss: 0.1324, Accuracy: 0.9254, F1 Micro: 0.7757, F1 Macro: 0.6682\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.9245, F1 Micro: 0.7733, F1 Macro: 0.6756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9247, F1 Micro: 0.782, F1 Macro: 0.6805\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9238, F1 Micro: 0.7776, F1 Macro: 0.6849\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9257, F1 Micro: 0.7741, F1 Macro: 0.6962\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9247, F1 Micro: 0.782, F1 Macro: 0.6805\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.87      0.92      0.89      1008\n",
      "HS_Individual       0.73      0.76      0.74       729\n",
      "     HS_Group       0.72      0.66      0.69       408\n",
      "  HS_Religion       0.81      0.57      0.67       168\n",
      "      HS_Race       0.78      0.78      0.78       119\n",
      "  HS_Physical       0.55      0.11      0.18        57\n",
      "    HS_Gender       0.71      0.31      0.43        55\n",
      "     HS_Other       0.78      0.82      0.80       771\n",
      "      HS_Weak       0.70      0.73      0.71       681\n",
      "  HS_Moderate       0.67      0.59      0.63       359\n",
      "    HS_Strong       0.78      0.78      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.75      0.66      0.68      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 186.50689482688904 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9142, F1 Micro: 0.7415, F1 Macro: 0.6067\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 68.02481961250305 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4202, Accuracy: 0.8975, F1 Micro: 0.6687, F1 Macro: 0.4018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3065, Accuracy: 0.9137, F1 Micro: 0.7515, F1 Macro: 0.5842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2466, Accuracy: 0.9206, F1 Micro: 0.7551, F1 Macro: 0.6017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2006, Accuracy: 0.9244, F1 Micro: 0.7682, F1 Macro: 0.635\n",
      "Epoch 5/10, Train Loss: 0.1614, Accuracy: 0.9242, F1 Micro: 0.7678, F1 Macro: 0.6457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.924, F1 Micro: 0.7701, F1 Macro: 0.6764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9258, F1 Micro: 0.7745, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.921, F1 Micro: 0.776, F1 Macro: 0.706\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9248, F1 Micro: 0.7756, F1 Macro: 0.7019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9216, F1 Micro: 0.7788, F1 Macro: 0.7135\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9216, F1 Micro: 0.7788, F1 Macro: 0.7135\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.85      0.93      0.89      1008\n",
      "HS_Individual       0.74      0.72      0.73       729\n",
      "     HS_Group       0.64      0.72      0.68       408\n",
      "  HS_Religion       0.68      0.71      0.70       168\n",
      "      HS_Race       0.68      0.84      0.75       119\n",
      "  HS_Physical       0.60      0.42      0.49        57\n",
      "    HS_Gender       0.63      0.49      0.55        55\n",
      "     HS_Other       0.80      0.81      0.80       771\n",
      "      HS_Weak       0.72      0.70      0.71       681\n",
      "  HS_Moderate       0.59      0.63      0.61       359\n",
      "    HS_Strong       0.76      0.82      0.79        97\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5589\n",
      "    macro avg       0.71      0.72      0.71      5589\n",
      " weighted avg       0.76      0.80      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 204.22206449508667 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4137, Accuracy: 0.8982, F1 Micro: 0.678, F1 Macro: 0.4371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3045, Accuracy: 0.9141, F1 Micro: 0.7528, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2465, Accuracy: 0.9217, F1 Micro: 0.764, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1987, Accuracy: 0.9242, F1 Micro: 0.7752, F1 Macro: 0.6312\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9213, F1 Micro: 0.7545, F1 Macro: 0.6263\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9243, F1 Micro: 0.7657, F1 Macro: 0.6712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1014, Accuracy: 0.9235, F1 Micro: 0.7802, F1 Macro: 0.6816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9229, F1 Micro: 0.7803, F1 Macro: 0.7071\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9238, F1 Micro: 0.7699, F1 Macro: 0.6972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.922, F1 Micro: 0.7822, F1 Macro: 0.7157\n",
      "Model 2 - Iteration 6285: Accuracy: 0.922, F1 Micro: 0.7822, F1 Macro: 0.7157\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1137\n",
      "      Abusive       0.86      0.94      0.90      1008\n",
      "HS_Individual       0.68      0.80      0.74       729\n",
      "     HS_Group       0.71      0.61      0.66       408\n",
      "  HS_Religion       0.70      0.72      0.71       168\n",
      "      HS_Race       0.69      0.81      0.74       119\n",
      "  HS_Physical       0.64      0.44      0.52        57\n",
      "    HS_Gender       0.61      0.49      0.55        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.66      0.78      0.71       681\n",
      "  HS_Moderate       0.67      0.54      0.60       359\n",
      "    HS_Strong       0.79      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.76      0.81      0.78      5589\n",
      "    macro avg       0.72      0.72      0.72      5589\n",
      " weighted avg       0.76      0.81      0.78      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 201.63215613365173 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4238, Accuracy: 0.8924, F1 Micro: 0.6674, F1 Macro: 0.3991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3081, Accuracy: 0.9122, F1 Micro: 0.7485, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2457, Accuracy: 0.9204, F1 Micro: 0.7611, F1 Macro: 0.6019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1963, Accuracy: 0.9246, F1 Micro: 0.7778, F1 Macro: 0.6319\n",
      "Epoch 5/10, Train Loss: 0.1559, Accuracy: 0.9244, F1 Micro: 0.7666, F1 Macro: 0.6365\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9216, F1 Micro: 0.7669, F1 Macro: 0.6558\n",
      "Epoch 7/10, Train Loss: 0.1007, Accuracy: 0.9222, F1 Micro: 0.7774, F1 Macro: 0.6758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9241, F1 Micro: 0.7808, F1 Macro: 0.695\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9252, F1 Micro: 0.7795, F1 Macro: 0.6987\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9235, F1 Micro: 0.7802, F1 Macro: 0.6979\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9241, F1 Micro: 0.7808, F1 Macro: 0.695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.86      0.92      0.89      1008\n",
      "HS_Individual       0.72      0.76      0.74       729\n",
      "     HS_Group       0.73      0.63      0.68       408\n",
      "  HS_Religion       0.69      0.67      0.68       168\n",
      "      HS_Race       0.71      0.82      0.76       119\n",
      "  HS_Physical       0.68      0.23      0.34        57\n",
      "    HS_Gender       0.62      0.38      0.47        55\n",
      "     HS_Other       0.81      0.78      0.80       771\n",
      "      HS_Weak       0.69      0.74      0.71       681\n",
      "  HS_Moderate       0.66      0.55      0.60       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.74      0.68      0.69      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 198.5642330646515 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9151, F1 Micro: 0.7459, F1 Macro: 0.618\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 61.19270849227905 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4137, Accuracy: 0.8948, F1 Micro: 0.6276, F1 Macro: 0.3463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2992, Accuracy: 0.9158, F1 Micro: 0.7307, F1 Macro: 0.5542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2356, Accuracy: 0.922, F1 Micro: 0.751, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9258, F1 Micro: 0.7796, F1 Macro: 0.633\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.9238, F1 Micro: 0.7772, F1 Macro: 0.6612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1236, Accuracy: 0.926, F1 Micro: 0.7802, F1 Macro: 0.6884\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.9258, F1 Micro: 0.7722, F1 Macro: 0.6801\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9227, F1 Micro: 0.7771, F1 Macro: 0.7134\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.9239, F1 Micro: 0.7773, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9214, F1 Micro: 0.7724, F1 Macro: 0.7072\n",
      "Model 1 - Iteration 6584: Accuracy: 0.926, F1 Micro: 0.7802, F1 Macro: 0.6884\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.90      0.87      0.89      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.74      0.63      0.68       408\n",
      "  HS_Religion       0.70      0.71      0.71       168\n",
      "      HS_Race       0.81      0.73      0.77       119\n",
      "  HS_Physical       0.78      0.25      0.37        57\n",
      "    HS_Gender       0.70      0.25      0.37        55\n",
      "     HS_Other       0.81      0.79      0.80       771\n",
      "      HS_Weak       0.72      0.71      0.72       681\n",
      "  HS_Moderate       0.69      0.55      0.61       359\n",
      "    HS_Strong       0.80      0.69      0.74        97\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5589\n",
      "    macro avg       0.77      0.65      0.69      5589\n",
      " weighted avg       0.80      0.76      0.78      5589\n",
      "  samples avg       0.44      0.42      0.42      5589\n",
      "\n",
      "Training completed in 206.48460459709167 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4071, Accuracy: 0.897, F1 Micro: 0.6413, F1 Macro: 0.3842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2977, Accuracy: 0.9163, F1 Micro: 0.7356, F1 Macro: 0.5646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2347, Accuracy: 0.9228, F1 Micro: 0.7562, F1 Macro: 0.6027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1941, Accuracy: 0.9256, F1 Micro: 0.7745, F1 Macro: 0.6363\n",
      "Epoch 5/10, Train Loss: 0.1555, Accuracy: 0.9228, F1 Micro: 0.7709, F1 Macro: 0.6467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1273, Accuracy: 0.9243, F1 Micro: 0.7748, F1 Macro: 0.6638\n",
      "Epoch 7/10, Train Loss: 0.0984, Accuracy: 0.9248, F1 Micro: 0.7732, F1 Macro: 0.668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9236, F1 Micro: 0.7764, F1 Macro: 0.6979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0689, Accuracy: 0.9266, F1 Micro: 0.7821, F1 Macro: 0.7119\n",
      "Epoch 10/10, Train Loss: 0.0592, Accuracy: 0.9246, F1 Micro: 0.7763, F1 Macro: 0.7098\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9266, F1 Micro: 0.7821, F1 Macro: 0.7119\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.77      0.70      0.73       729\n",
      "     HS_Group       0.71      0.68      0.69       408\n",
      "  HS_Religion       0.71      0.71      0.71       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.77      0.35      0.48        57\n",
      "    HS_Gender       0.65      0.36      0.47        55\n",
      "     HS_Other       0.84      0.75      0.80       771\n",
      "      HS_Weak       0.74      0.67      0.71       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.78      0.85      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5589\n",
      "    macro avg       0.76      0.69      0.71      5589\n",
      " weighted avg       0.80      0.76      0.78      5589\n",
      "  samples avg       0.45      0.43      0.42      5589\n",
      "\n",
      "Training completed in 208.39576697349548 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4174, Accuracy: 0.8952, F1 Micro: 0.6358, F1 Macro: 0.3594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2994, Accuracy: 0.9163, F1 Micro: 0.7372, F1 Macro: 0.5762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2366, Accuracy: 0.9225, F1 Micro: 0.7595, F1 Macro: 0.605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1973, Accuracy: 0.9237, F1 Micro: 0.7748, F1 Macro: 0.6365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1589, Accuracy: 0.9238, F1 Micro: 0.7776, F1 Macro: 0.6587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9252, F1 Micro: 0.7788, F1 Macro: 0.6538\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9244, F1 Micro: 0.7639, F1 Macro: 0.658\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9249, F1 Micro: 0.7779, F1 Macro: 0.6899\n",
      "Epoch 9/10, Train Loss: 0.0699, Accuracy: 0.9243, F1 Micro: 0.7721, F1 Macro: 0.6998\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9236, F1 Micro: 0.7739, F1 Macro: 0.7006\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9252, F1 Micro: 0.7788, F1 Macro: 0.6538\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.91      0.87      0.89      1008\n",
      "HS_Individual       0.73      0.76      0.75       729\n",
      "     HS_Group       0.76      0.61      0.67       408\n",
      "  HS_Religion       0.71      0.70      0.70       168\n",
      "      HS_Race       0.79      0.74      0.77       119\n",
      "  HS_Physical       0.80      0.07      0.13        57\n",
      "    HS_Gender       0.67      0.15      0.24        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.69      0.74      0.72       681\n",
      "  HS_Moderate       0.70      0.49      0.58       359\n",
      "    HS_Strong       0.80      0.69      0.74        97\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5589\n",
      "    macro avg       0.77      0.62      0.65      5589\n",
      " weighted avg       0.80      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 206.4370231628418 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9162, F1 Micro: 0.7493, F1 Macro: 0.6246\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 57.50019097328186 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4064, Accuracy: 0.8988, F1 Micro: 0.6598, F1 Macro: 0.4112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2962, Accuracy: 0.9142, F1 Micro: 0.7467, F1 Macro: 0.5768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2394, Accuracy: 0.9222, F1 Micro: 0.7661, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.9204, F1 Micro: 0.774, F1 Macro: 0.6673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1494, Accuracy: 0.9239, F1 Micro: 0.7827, F1 Macro: 0.7004\n",
      "Epoch 6/10, Train Loss: 0.1205, Accuracy: 0.9266, F1 Micro: 0.7825, F1 Macro: 0.7051\n",
      "Epoch 7/10, Train Loss: 0.0982, Accuracy: 0.9254, F1 Micro: 0.7769, F1 Macro: 0.7064\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9226, F1 Micro: 0.7785, F1 Macro: 0.7103\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.9245, F1 Micro: 0.775, F1 Macro: 0.7112\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.9192, F1 Micro: 0.7721, F1 Macro: 0.7156\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9239, F1 Micro: 0.7827, F1 Macro: 0.7004\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.87      0.91      0.89      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.69      0.70      0.70       168\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.70      0.28      0.40        57\n",
      "    HS_Gender       0.75      0.33      0.46        55\n",
      "     HS_Other       0.77      0.85      0.81       771\n",
      "      HS_Weak       0.69      0.75      0.72       681\n",
      "  HS_Moderate       0.67      0.56      0.61       359\n",
      "    HS_Strong       0.79      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5589\n",
      "    macro avg       0.75      0.68      0.70      5589\n",
      " weighted avg       0.77      0.79      0.78      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 216.47659921646118 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3989, Accuracy: 0.898, F1 Micro: 0.6607, F1 Macro: 0.4088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2946, Accuracy: 0.9164, F1 Micro: 0.755, F1 Macro: 0.5959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2381, Accuracy: 0.9233, F1 Micro: 0.7701, F1 Macro: 0.6117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1907, Accuracy: 0.9194, F1 Micro: 0.775, F1 Macro: 0.6439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9253, F1 Micro: 0.7839, F1 Macro: 0.6785\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9255, F1 Micro: 0.7833, F1 Macro: 0.6863\n",
      "Epoch 7/10, Train Loss: 0.0975, Accuracy: 0.9252, F1 Micro: 0.7734, F1 Macro: 0.6931\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9252, F1 Micro: 0.7782, F1 Macro: 0.7117\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.9228, F1 Micro: 0.7807, F1 Macro: 0.7214\n",
      "Epoch 10/10, Train Loss: 0.0571, Accuracy: 0.9246, F1 Micro: 0.783, F1 Macro: 0.7236\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9253, F1 Micro: 0.7839, F1 Macro: 0.6785\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.75      0.62      0.68       408\n",
      "  HS_Religion       0.73      0.68      0.70       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.68      0.23      0.34        57\n",
      "    HS_Gender       1.00      0.15      0.25        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.69      0.76      0.72       681\n",
      "  HS_Moderate       0.71      0.52      0.60       359\n",
      "    HS_Strong       0.77      0.77      0.77        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5589\n",
      "    macro avg       0.78      0.66      0.68      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 216.2507472038269 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4122, Accuracy: 0.8976, F1 Micro: 0.6575, F1 Macro: 0.4157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2973, Accuracy: 0.9092, F1 Micro: 0.7494, F1 Macro: 0.5918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2403, Accuracy: 0.9218, F1 Micro: 0.7695, F1 Macro: 0.6091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.919, F1 Micro: 0.7698, F1 Macro: 0.6422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1513, Accuracy: 0.925, F1 Micro: 0.7822, F1 Macro: 0.6696\n",
      "Epoch 6/10, Train Loss: 0.1226, Accuracy: 0.9254, F1 Micro: 0.7772, F1 Macro: 0.6807\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9248, F1 Micro: 0.7709, F1 Macro: 0.6869\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9243, F1 Micro: 0.7762, F1 Macro: 0.6986\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9231, F1 Micro: 0.7764, F1 Macro: 0.7014\n",
      "Epoch 10/10, Train Loss: 0.0582, Accuracy: 0.9256, F1 Micro: 0.7785, F1 Macro: 0.7086\n",
      "Model 3 - Iteration 6980: Accuracy: 0.925, F1 Micro: 0.7822, F1 Macro: 0.6696\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.88      0.91      0.89      1008\n",
      "HS_Individual       0.73      0.76      0.74       729\n",
      "     HS_Group       0.72      0.62      0.67       408\n",
      "  HS_Religion       0.78      0.67      0.72       168\n",
      "      HS_Race       0.75      0.70      0.72       119\n",
      "  HS_Physical       0.55      0.11      0.18        57\n",
      "    HS_Gender       0.75      0.22      0.34        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.71      0.74      0.72       681\n",
      "  HS_Moderate       0.68      0.53      0.59       359\n",
      "    HS_Strong       0.78      0.78      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5589\n",
      "    macro avg       0.75      0.65      0.67      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 216.27842545509338 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.917, F1 Micro: 0.7524, F1 Macro: 0.6299\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 52.23348379135132 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4038, Accuracy: 0.9006, F1 Micro: 0.6888, F1 Macro: 0.4456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.289, Accuracy: 0.9181, F1 Micro: 0.7528, F1 Macro: 0.5869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2306, Accuracy: 0.9229, F1 Micro: 0.7752, F1 Macro: 0.6409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1837, Accuracy: 0.9225, F1 Micro: 0.7777, F1 Macro: 0.6485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1485, Accuracy: 0.9263, F1 Micro: 0.7837, F1 Macro: 0.6927\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.9287, F1 Micro: 0.7835, F1 Macro: 0.6872\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.9272, F1 Micro: 0.7824, F1 Macro: 0.6951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9265, F1 Micro: 0.784, F1 Macro: 0.7157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9248, F1 Micro: 0.7874, F1 Macro: 0.7211\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9266, F1 Micro: 0.7672, F1 Macro: 0.702\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9248, F1 Micro: 0.7874, F1 Macro: 0.7211\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.65      0.72      0.68       408\n",
      "  HS_Religion       0.66      0.71      0.69       168\n",
      "      HS_Race       0.70      0.80      0.75       119\n",
      "  HS_Physical       0.63      0.42      0.51        57\n",
      "    HS_Gender       0.63      0.49      0.55        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.71      0.72       681\n",
      "  HS_Moderate       0.59      0.67      0.63       359\n",
      "    HS_Strong       0.81      0.80      0.81        97\n",
      "\n",
      "    micro avg       0.77      0.80      0.79      5589\n",
      "    macro avg       0.72      0.73      0.72      5589\n",
      " weighted avg       0.77      0.80      0.79      5589\n",
      "  samples avg       0.46      0.46      0.44      5589\n",
      "\n",
      "Training completed in 231.44541358947754 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3985, Accuracy: 0.9004, F1 Micro: 0.6912, F1 Macro: 0.478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2879, Accuracy: 0.918, F1 Micro: 0.7444, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.233, Accuracy: 0.9234, F1 Micro: 0.7787, F1 Macro: 0.6311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1849, Accuracy: 0.9245, F1 Micro: 0.7822, F1 Macro: 0.6607\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.9263, F1 Micro: 0.7821, F1 Macro: 0.681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1198, Accuracy: 0.9292, F1 Micro: 0.7877, F1 Macro: 0.7014\n",
      "Epoch 7/10, Train Loss: 0.0944, Accuracy: 0.9269, F1 Micro: 0.7798, F1 Macro: 0.6967\n",
      "Epoch 8/10, Train Loss: 0.0819, Accuracy: 0.9249, F1 Micro: 0.7826, F1 Macro: 0.7188\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.928, F1 Micro: 0.7822, F1 Macro: 0.7198\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9252, F1 Micro: 0.7805, F1 Macro: 0.7219\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9292, F1 Micro: 0.7877, F1 Macro: 0.7014\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.91      0.87      0.89      1008\n",
      "HS_Individual       0.76      0.75      0.75       729\n",
      "     HS_Group       0.76      0.63      0.69       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.79      0.71      0.75       119\n",
      "  HS_Physical       0.75      0.32      0.44        57\n",
      "    HS_Gender       0.87      0.24      0.37        55\n",
      "     HS_Other       0.85      0.78      0.81       771\n",
      "      HS_Weak       0.74      0.73      0.73       681\n",
      "  HS_Moderate       0.71      0.55      0.62       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.80      0.65      0.70      5589\n",
      " weighted avg       0.82      0.76      0.78      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 226.74069237709045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4069, Accuracy: 0.8986, F1 Micro: 0.6761, F1 Macro: 0.443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2907, Accuracy: 0.9185, F1 Micro: 0.7488, F1 Macro: 0.586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.231, Accuracy: 0.924, F1 Micro: 0.7776, F1 Macro: 0.6266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1826, Accuracy: 0.9237, F1 Micro: 0.7805, F1 Macro: 0.6554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.9273, F1 Micro: 0.7809, F1 Macro: 0.6708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1178, Accuracy: 0.9274, F1 Micro: 0.7887, F1 Macro: 0.6793\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9256, F1 Micro: 0.7723, F1 Macro: 0.6803\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9249, F1 Micro: 0.7826, F1 Macro: 0.7095\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9258, F1 Micro: 0.7822, F1 Macro: 0.7128\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.9259, F1 Micro: 0.7796, F1 Macro: 0.7105\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9274, F1 Micro: 0.7887, F1 Macro: 0.6793\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.70      0.69      0.70       408\n",
      "  HS_Religion       0.77      0.68      0.72       168\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.77      0.18      0.29        57\n",
      "    HS_Gender       0.60      0.16      0.26        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.66      0.63      0.64       359\n",
      "    HS_Strong       0.78      0.75      0.76        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.66      0.68      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 225.45612835884094 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9178, F1 Micro: 0.7553, F1 Macro: 0.6358\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 46.50865435600281 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3957, Accuracy: 0.9019, F1 Micro: 0.6783, F1 Macro: 0.4645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2841, Accuracy: 0.9178, F1 Micro: 0.7458, F1 Macro: 0.576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2215, Accuracy: 0.9278, F1 Micro: 0.7773, F1 Macro: 0.6286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9276, F1 Micro: 0.7809, F1 Macro: 0.6629\n",
      "Epoch 5/10, Train Loss: 0.1434, Accuracy: 0.9272, F1 Micro: 0.7803, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9271, F1 Micro: 0.7828, F1 Macro: 0.7044\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.928, F1 Micro: 0.7777, F1 Macro: 0.698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.927, F1 Micro: 0.7875, F1 Macro: 0.7255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9289, F1 Micro: 0.7888, F1 Macro: 0.7248\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.9271, F1 Micro: 0.7858, F1 Macro: 0.722\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9289, F1 Micro: 0.7888, F1 Macro: 0.7248\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.78      0.69      0.73       729\n",
      "     HS_Group       0.70      0.69      0.69       408\n",
      "  HS_Religion       0.75      0.64      0.69       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.68      0.44      0.53        57\n",
      "    HS_Gender       0.65      0.47      0.55        55\n",
      "     HS_Other       0.85      0.78      0.81       771\n",
      "      HS_Weak       0.75      0.67      0.71       681\n",
      "  HS_Moderate       0.66      0.61      0.63       359\n",
      "    HS_Strong       0.78      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 235.19254732131958 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3888, Accuracy: 0.904, F1 Micro: 0.6998, F1 Macro: 0.5067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2829, Accuracy: 0.9194, F1 Micro: 0.7422, F1 Macro: 0.5761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2217, Accuracy: 0.9257, F1 Micro: 0.77, F1 Macro: 0.6106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1788, Accuracy: 0.9285, F1 Micro: 0.7831, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9283, F1 Micro: 0.7836, F1 Macro: 0.6868\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.9282, F1 Micro: 0.7783, F1 Macro: 0.6917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.9279, F1 Micro: 0.784, F1 Macro: 0.7084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9295, F1 Micro: 0.7906, F1 Macro: 0.7205\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.925, F1 Micro: 0.7883, F1 Macro: 0.7256\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9276, F1 Micro: 0.7862, F1 Macro: 0.7271\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9295, F1 Micro: 0.7906, F1 Macro: 0.7205\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.77      0.74      0.75       729\n",
      "     HS_Group       0.73      0.64      0.68       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.73      0.81      0.76       119\n",
      "  HS_Physical       0.69      0.35      0.47        57\n",
      "    HS_Gender       0.71      0.44      0.54        55\n",
      "     HS_Other       0.86      0.77      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.73       681\n",
      "  HS_Moderate       0.69      0.57      0.63       359\n",
      "    HS_Strong       0.78      0.86      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 234.51266860961914 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3995, Accuracy: 0.9014, F1 Micro: 0.6967, F1 Macro: 0.4923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2836, Accuracy: 0.9186, F1 Micro: 0.7421, F1 Macro: 0.5831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2208, Accuracy: 0.9257, F1 Micro: 0.7678, F1 Macro: 0.6019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9271, F1 Micro: 0.7824, F1 Macro: 0.6521\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.9268, F1 Micro: 0.7812, F1 Macro: 0.6647\n",
      "Epoch 6/10, Train Loss: 0.115, Accuracy: 0.9244, F1 Micro: 0.7735, F1 Macro: 0.6688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0931, Accuracy: 0.9273, F1 Micro: 0.7842, F1 Macro: 0.6819\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9279, F1 Micro: 0.7841, F1 Macro: 0.7106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9272, F1 Micro: 0.787, F1 Macro: 0.7165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.9264, F1 Micro: 0.7879, F1 Macro: 0.7143\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9264, F1 Micro: 0.7879, F1 Macro: 0.7143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.69      0.62      0.65       408\n",
      "  HS_Religion       0.73      0.64      0.68       168\n",
      "      HS_Race       0.70      0.83      0.76       119\n",
      "  HS_Physical       0.67      0.35      0.46        57\n",
      "    HS_Gender       0.65      0.47      0.55        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.70      0.77      0.73       681\n",
      "  HS_Moderate       0.64      0.53      0.58       359\n",
      "    HS_Strong       0.82      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.78      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 235.96838474273682 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9186, F1 Micro: 0.7579, F1 Macro: 0.6423\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 42.31501221656799 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3864, Accuracy: 0.9045, F1 Micro: 0.6927, F1 Macro: 0.4853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2804, Accuracy: 0.9157, F1 Micro: 0.7586, F1 Macro: 0.5827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.9253, F1 Micro: 0.7702, F1 Macro: 0.6366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9272, F1 Micro: 0.7882, F1 Macro: 0.6724\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9276, F1 Micro: 0.777, F1 Macro: 0.6916\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.9284, F1 Micro: 0.788, F1 Macro: 0.7125\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9243, F1 Micro: 0.7854, F1 Macro: 0.7172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0745, Accuracy: 0.929, F1 Micro: 0.7923, F1 Macro: 0.7264\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9263, F1 Micro: 0.7844, F1 Macro: 0.7244\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.927, F1 Micro: 0.7871, F1 Macro: 0.7318\n",
      "Model 1 - Iteration 7901: Accuracy: 0.929, F1 Micro: 0.7923, F1 Macro: 0.7264\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.77      0.72      0.75       729\n",
      "     HS_Group       0.70      0.70      0.70       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.72      0.76      0.74       119\n",
      "  HS_Physical       0.73      0.33      0.46        57\n",
      "    HS_Gender       0.71      0.53      0.60        55\n",
      "     HS_Other       0.84      0.80      0.82       771\n",
      "      HS_Weak       0.75      0.68      0.71       681\n",
      "  HS_Moderate       0.63      0.64      0.64       359\n",
      "    HS_Strong       0.80      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 239.758154630661 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3808, Accuracy: 0.903, F1 Micro: 0.6837, F1 Macro: 0.4552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2791, Accuracy: 0.9176, F1 Micro: 0.7593, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2198, Accuracy: 0.9258, F1 Micro: 0.7691, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9279, F1 Micro: 0.7811, F1 Macro: 0.6392\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.929, F1 Micro: 0.78, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1096, Accuracy: 0.9275, F1 Micro: 0.7868, F1 Macro: 0.7108\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9261, F1 Micro: 0.7862, F1 Macro: 0.7101\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.9272, F1 Micro: 0.7824, F1 Macro: 0.7121\n",
      "Epoch 9/10, Train Loss: 0.0637, Accuracy: 0.9276, F1 Micro: 0.7831, F1 Macro: 0.7173\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9256, F1 Micro: 0.7813, F1 Macro: 0.7195\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9275, F1 Micro: 0.7868, F1 Macro: 0.7108\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.78      0.71      0.74       729\n",
      "     HS_Group       0.67      0.72      0.69       408\n",
      "  HS_Religion       0.78      0.64      0.70       168\n",
      "      HS_Race       0.71      0.82      0.76       119\n",
      "  HS_Physical       0.69      0.32      0.43        57\n",
      "    HS_Gender       0.68      0.38      0.49        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.61      0.64      0.63       359\n",
      "    HS_Strong       0.83      0.75      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 239.29258155822754 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3884, Accuracy: 0.9034, F1 Micro: 0.695, F1 Macro: 0.4972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2817, Accuracy: 0.9166, F1 Micro: 0.7616, F1 Macro: 0.5872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2206, Accuracy: 0.9236, F1 Micro: 0.7649, F1 Macro: 0.6194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1788, Accuracy: 0.9263, F1 Micro: 0.7832, F1 Macro: 0.648\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9263, F1 Micro: 0.7769, F1 Macro: 0.6653\n",
      "Epoch 6/10, Train Loss: 0.1119, Accuracy: 0.9246, F1 Micro: 0.778, F1 Macro: 0.6901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9277, F1 Micro: 0.7842, F1 Macro: 0.6912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0754, Accuracy: 0.9285, F1 Micro: 0.7857, F1 Macro: 0.7089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9295, F1 Micro: 0.7916, F1 Macro: 0.7179\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.925, F1 Micro: 0.783, F1 Macro: 0.7226\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9295, F1 Micro: 0.7916, F1 Macro: 0.7179\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.70      0.67      0.69       408\n",
      "  HS_Religion       0.74      0.70      0.72       168\n",
      "      HS_Race       0.77      0.80      0.78       119\n",
      "  HS_Physical       0.71      0.26      0.38        57\n",
      "    HS_Gender       0.70      0.42      0.52        55\n",
      "     HS_Other       0.83      0.80      0.82       771\n",
      "      HS_Weak       0.75      0.70      0.72       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.81      0.88      0.84        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 242.85599970817566 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9193, F1 Micro: 0.7602, F1 Macro: 0.6477\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 39.13982582092285 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3814, Accuracy: 0.9039, F1 Micro: 0.7013, F1 Macro: 0.4704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2678, Accuracy: 0.9193, F1 Micro: 0.7623, F1 Macro: 0.6081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9246, F1 Micro: 0.7754, F1 Macro: 0.6395\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9273, F1 Micro: 0.7723, F1 Macro: 0.6583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1347, Accuracy: 0.927, F1 Micro: 0.7851, F1 Macro: 0.6917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9271, F1 Micro: 0.7881, F1 Macro: 0.7154\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9249, F1 Micro: 0.785, F1 Macro: 0.7146\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9273, F1 Micro: 0.7833, F1 Macro: 0.7208\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9267, F1 Micro: 0.7733, F1 Macro: 0.7161\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.9271, F1 Micro: 0.7832, F1 Macro: 0.7224\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9271, F1 Micro: 0.7881, F1 Macro: 0.7154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.72      0.63      0.67       408\n",
      "  HS_Religion       0.70      0.73      0.72       168\n",
      "      HS_Race       0.74      0.78      0.76       119\n",
      "  HS_Physical       0.62      0.32      0.42        57\n",
      "    HS_Gender       0.76      0.47      0.58        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.71      0.73      0.72       681\n",
      "  HS_Moderate       0.67      0.52      0.59       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.75      0.69      0.72      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 246.14632892608643 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3751, Accuracy: 0.9032, F1 Micro: 0.6962, F1 Macro: 0.4745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2651, Accuracy: 0.9198, F1 Micro: 0.7643, F1 Macro: 0.6051\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2087, Accuracy: 0.9261, F1 Micro: 0.7772, F1 Macro: 0.6314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.9291, F1 Micro: 0.7826, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9278, F1 Micro: 0.7878, F1 Macro: 0.6817\n",
      "Epoch 6/10, Train Loss: 0.1085, Accuracy: 0.9179, F1 Micro: 0.7756, F1 Macro: 0.7059\n",
      "Epoch 7/10, Train Loss: 0.0919, Accuracy: 0.9283, F1 Micro: 0.7809, F1 Macro: 0.7028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.929, F1 Micro: 0.7915, F1 Macro: 0.7221\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9271, F1 Micro: 0.7826, F1 Macro: 0.7241\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.927, F1 Micro: 0.7863, F1 Macro: 0.7181\n",
      "Model 2 - Iteration 8165: Accuracy: 0.929, F1 Micro: 0.7915, F1 Macro: 0.7221\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.77      0.61      0.68       168\n",
      "      HS_Race       0.78      0.77      0.78       119\n",
      "  HS_Physical       0.69      0.39      0.49        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.75      0.72      0.73       681\n",
      "  HS_Moderate       0.65      0.57      0.61       359\n",
      "    HS_Strong       0.80      0.86      0.83        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 247.30037379264832 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3854, Accuracy: 0.9019, F1 Micro: 0.7006, F1 Macro: 0.4819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2679, Accuracy: 0.9204, F1 Micro: 0.7615, F1 Macro: 0.6042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2114, Accuracy: 0.9245, F1 Micro: 0.7739, F1 Macro: 0.6221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1659, Accuracy: 0.9287, F1 Micro: 0.7838, F1 Macro: 0.6574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1342, Accuracy: 0.9278, F1 Micro: 0.7864, F1 Macro: 0.6777\n",
      "Epoch 6/10, Train Loss: 0.1115, Accuracy: 0.9249, F1 Micro: 0.7815, F1 Macro: 0.6894\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9268, F1 Micro: 0.7802, F1 Macro: 0.6888\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.9271, F1 Micro: 0.7826, F1 Macro: 0.7101\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.9264, F1 Micro: 0.7818, F1 Macro: 0.7181\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9237, F1 Micro: 0.7808, F1 Macro: 0.7022\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9278, F1 Micro: 0.7864, F1 Macro: 0.6777\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.74      0.65      0.69       408\n",
      "  HS_Religion       0.76      0.62      0.68       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.67      0.07      0.13        57\n",
      "    HS_Gender       0.62      0.33      0.43        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.70      0.54      0.61       359\n",
      "    HS_Strong       0.79      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.65      0.68      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 245.4194278717041 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9199, F1 Micro: 0.7621, F1 Macro: 0.6515\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 35.40028405189514 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3766, Accuracy: 0.9032, F1 Micro: 0.7142, F1 Macro: 0.5185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2602, Accuracy: 0.9189, F1 Micro: 0.758, F1 Macro: 0.5955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2077, Accuracy: 0.9235, F1 Micro: 0.7761, F1 Macro: 0.6375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9287, F1 Micro: 0.7838, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9277, F1 Micro: 0.7893, F1 Macro: 0.7016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.104, Accuracy: 0.9294, F1 Micro: 0.7913, F1 Macro: 0.7144\n",
      "Epoch 7/10, Train Loss: 0.0862, Accuracy: 0.9258, F1 Micro: 0.7855, F1 Macro: 0.7093\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9283, F1 Micro: 0.7863, F1 Macro: 0.7204\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9294, F1 Micro: 0.7842, F1 Macro: 0.717\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9286, F1 Micro: 0.7869, F1 Macro: 0.7188Model 1 - Iteration 8402: Accuracy: 0.9294, F1 Micro: 0.7913, F1 Macro: 0.7144\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.76      0.77      0.76       729\n",
      "     HS_Group       0.77      0.62      0.69       408\n",
      "  HS_Religion       0.77      0.64      0.70       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.65      0.36      0.47        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.72      0.53      0.61       359\n",
      "    HS_Strong       0.81      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.68      0.71      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 253.83724689483643 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3721, Accuracy: 0.9047, F1 Micro: 0.7098, F1 Macro: 0.5259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2599, Accuracy: 0.9176, F1 Micro: 0.7613, F1 Macro: 0.6015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2084, Accuracy: 0.9228, F1 Micro: 0.7753, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1649, Accuracy: 0.9279, F1 Micro: 0.7817, F1 Macro: 0.6564\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9262, F1 Micro: 0.7758, F1 Macro: 0.667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9245, F1 Micro: 0.7833, F1 Macro: 0.7096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.9283, F1 Micro: 0.7873, F1 Macro: 0.7118\n",
      "Epoch 8/10, Train Loss: 0.0732, Accuracy: 0.9273, F1 Micro: 0.785, F1 Macro: 0.7197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9292, F1 Micro: 0.7897, F1 Macro: 0.7236\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.9265, F1 Micro: 0.7812, F1 Macro: 0.7157\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9292, F1 Micro: 0.7897, F1 Macro: 0.7236\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.78      0.71      0.74       729\n",
      "     HS_Group       0.70      0.68      0.69       408\n",
      "  HS_Religion       0.72      0.71      0.71       168\n",
      "      HS_Race       0.71      0.82      0.76       119\n",
      "  HS_Physical       0.73      0.39      0.51        57\n",
      "    HS_Gender       0.75      0.44      0.55        55\n",
      "     HS_Other       0.85      0.78      0.81       771\n",
      "      HS_Weak       0.75      0.69      0.72       681\n",
      "  HS_Moderate       0.65      0.61      0.63       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.70      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 255.69325852394104 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.9026, F1 Micro: 0.7048, F1 Macro: 0.5192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2622, Accuracy: 0.9187, F1 Micro: 0.7634, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2089, Accuracy: 0.924, F1 Micro: 0.7784, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1656, Accuracy: 0.9291, F1 Micro: 0.7843, F1 Macro: 0.6559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9273, F1 Micro: 0.7866, F1 Macro: 0.6807\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.926, F1 Micro: 0.7859, F1 Macro: 0.6908\n",
      "Epoch 7/10, Train Loss: 0.0889, Accuracy: 0.9265, F1 Micro: 0.7841, F1 Macro: 0.6933\n",
      "Epoch 8/10, Train Loss: 0.0732, Accuracy: 0.926, F1 Micro: 0.782, F1 Macro: 0.7106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9286, F1 Micro: 0.791, F1 Macro: 0.7256\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9277, F1 Micro: 0.7892, F1 Macro: 0.7149\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9286, F1 Micro: 0.791, F1 Macro: 0.7256\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.71      0.69      0.70       408\n",
      "  HS_Religion       0.71      0.68      0.69       168\n",
      "      HS_Race       0.72      0.85      0.78       119\n",
      "  HS_Physical       0.65      0.39      0.48        57\n",
      "    HS_Gender       0.65      0.47      0.55        55\n",
      "     HS_Other       0.83      0.78      0.80       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.66      0.62      0.64       359\n",
      "    HS_Strong       0.82      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 254.3447892665863 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9205, F1 Micro: 0.7639, F1 Macro: 0.6559\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 31.77815341949463 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3728, Accuracy: 0.9052, F1 Micro: 0.714, F1 Macro: 0.4958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2589, Accuracy: 0.9197, F1 Micro: 0.7575, F1 Macro: 0.5742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2048, Accuracy: 0.9262, F1 Micro: 0.7741, F1 Macro: 0.643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.926, F1 Micro: 0.7745, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.927, F1 Micro: 0.7771, F1 Macro: 0.6861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1063, Accuracy: 0.9293, F1 Micro: 0.7911, F1 Macro: 0.714\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.9289, F1 Micro: 0.786, F1 Macro: 0.7166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9259, F1 Micro: 0.7912, F1 Macro: 0.7263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.929, F1 Micro: 0.794, F1 Macro: 0.73\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9265, F1 Micro: 0.7856, F1 Macro: 0.7165\n",
      "Model 1 - Iteration 8616: Accuracy: 0.929, F1 Micro: 0.794, F1 Macro: 0.73\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.72      0.62      0.67       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.74      0.81      0.77       119\n",
      "  HS_Physical       0.68      0.46      0.55        57\n",
      "    HS_Gender       0.65      0.51      0.57        55\n",
      "     HS_Other       0.83      0.80      0.82       771\n",
      "      HS_Weak       0.73      0.76      0.74       681\n",
      "  HS_Moderate       0.66      0.54      0.59       359\n",
      "    HS_Strong       0.83      0.81      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 262.01102232933044 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3687, Accuracy: 0.9066, F1 Micro: 0.7093, F1 Macro: 0.5006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2582, Accuracy: 0.9211, F1 Micro: 0.7565, F1 Macro: 0.5859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2061, Accuracy: 0.9272, F1 Micro: 0.7773, F1 Macro: 0.6333\n",
      "Epoch 4/10, Train Loss: 0.1638, Accuracy: 0.9257, F1 Micro: 0.7766, F1 Macro: 0.6394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9287, F1 Micro: 0.7858, F1 Macro: 0.6882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9304, F1 Micro: 0.7864, F1 Macro: 0.6989\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9286, F1 Micro: 0.7828, F1 Macro: 0.7113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9279, F1 Micro: 0.7896, F1 Macro: 0.7236\n",
      "Epoch 9/10, Train Loss: 0.0603, Accuracy: 0.9291, F1 Micro: 0.7867, F1 Macro: 0.723\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9271, F1 Micro: 0.7856, F1 Macro: 0.7194\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9279, F1 Micro: 0.7896, F1 Macro: 0.7236\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.93      0.89      0.91      1008\n",
      "HS_Individual       0.74      0.75      0.75       729\n",
      "     HS_Group       0.70      0.66      0.68       408\n",
      "  HS_Religion       0.69      0.64      0.66       168\n",
      "      HS_Race       0.66      0.85      0.74       119\n",
      "  HS_Physical       0.65      0.42      0.51        57\n",
      "    HS_Gender       0.74      0.51      0.60        55\n",
      "     HS_Other       0.84      0.80      0.82       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.65      0.58      0.61       359\n",
      "    HS_Strong       0.79      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 258.0244438648224 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3774, Accuracy: 0.9043, F1 Micro: 0.7044, F1 Macro: 0.4929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2604, Accuracy: 0.9198, F1 Micro: 0.7593, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2067, Accuracy: 0.9251, F1 Micro: 0.7669, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9257, F1 Micro: 0.7772, F1 Macro: 0.6343\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9251, F1 Micro: 0.7604, F1 Macro: 0.6596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9261, F1 Micro: 0.7891, F1 Macro: 0.7028\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9258, F1 Micro: 0.7836, F1 Macro: 0.6979\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9247, F1 Micro: 0.7817, F1 Macro: 0.7043\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.926, F1 Micro: 0.7837, F1 Macro: 0.7075\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9268, F1 Micro: 0.7881, F1 Macro: 0.7169\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9261, F1 Micro: 0.7891, F1 Macro: 0.7028\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.71      0.79      0.75       729\n",
      "     HS_Group       0.73      0.63      0.68       408\n",
      "  HS_Religion       0.79      0.64      0.71       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.72      0.23      0.35        57\n",
      "    HS_Gender       0.67      0.36      0.47        55\n",
      "     HS_Other       0.77      0.85      0.81       771\n",
      "      HS_Weak       0.69      0.78      0.73       681\n",
      "  HS_Moderate       0.68      0.56      0.61       359\n",
      "    HS_Strong       0.81      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.76      0.68      0.70      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 255.9128086566925 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9209, F1 Micro: 0.7655, F1 Macro: 0.6596\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.54885458946228 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.369, Accuracy: 0.9067, F1 Micro: 0.7039, F1 Macro: 0.4532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2537, Accuracy: 0.9187, F1 Micro: 0.7374, F1 Macro: 0.5569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1983, Accuracy: 0.9235, F1 Micro: 0.7623, F1 Macro: 0.6169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1623, Accuracy: 0.9253, F1 Micro: 0.7851, F1 Macro: 0.6754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9282, F1 Micro: 0.7912, F1 Macro: 0.69\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.9277, F1 Micro: 0.7853, F1 Macro: 0.7148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.9294, F1 Micro: 0.793, F1 Macro: 0.7191\n",
      "Epoch 8/10, Train Loss: 0.0685, Accuracy: 0.9274, F1 Micro: 0.7903, F1 Macro: 0.7232\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.9274, F1 Micro: 0.7869, F1 Macro: 0.7222\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.9253, F1 Micro: 0.7825, F1 Macro: 0.7257\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9294, F1 Micro: 0.793, F1 Macro: 0.7191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.93      0.91      1008\n",
      "HS_Individual       0.75      0.74      0.74       729\n",
      "     HS_Group       0.74      0.67      0.70       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.68      0.37      0.48        57\n",
      "    HS_Gender       0.67      0.40      0.50        55\n",
      "     HS_Other       0.84      0.80      0.82       771\n",
      "      HS_Weak       0.72      0.71      0.72       681\n",
      "  HS_Moderate       0.69      0.58      0.63       359\n",
      "    HS_Strong       0.81      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 264.2854046821594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3643, Accuracy: 0.9059, F1 Micro: 0.7085, F1 Macro: 0.4612\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2533, Accuracy: 0.9204, F1 Micro: 0.7527, F1 Macro: 0.5899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.9236, F1 Micro: 0.767, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9262, F1 Micro: 0.7846, F1 Macro: 0.6711\n",
      "Epoch 5/10, Train Loss: 0.1266, Accuracy: 0.925, F1 Micro: 0.7757, F1 Macro: 0.6605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9267, F1 Micro: 0.7853, F1 Macro: 0.7101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0825, Accuracy: 0.9286, F1 Micro: 0.7878, F1 Macro: 0.716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0685, Accuracy: 0.9294, F1 Micro: 0.7927, F1 Macro: 0.721\n",
      "Epoch 9/10, Train Loss: 0.0581, Accuracy: 0.9266, F1 Micro: 0.7762, F1 Macro: 0.7145\n",
      "Epoch 10/10, Train Loss: 0.0513, Accuracy: 0.9278, F1 Micro: 0.7898, F1 Macro: 0.7242\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9294, F1 Micro: 0.7927, F1 Macro: 0.721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.91      0.92      0.91      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.75      0.58      0.66       408\n",
      "  HS_Religion       0.78      0.57      0.66       168\n",
      "      HS_Race       0.74      0.80      0.77       119\n",
      "  HS_Physical       0.76      0.39      0.51        57\n",
      "    HS_Gender       0.70      0.47      0.57        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.70      0.52      0.59       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 266.0502688884735 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3745, Accuracy: 0.9044, F1 Micro: 0.7028, F1 Macro: 0.4659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9189, F1 Micro: 0.7427, F1 Macro: 0.5842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2003, Accuracy: 0.9242, F1 Micro: 0.769, F1 Macro: 0.618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.9208, F1 Micro: 0.7748, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9274, F1 Micro: 0.7858, F1 Macro: 0.664\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.925, F1 Micro: 0.7777, F1 Macro: 0.6841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9257, F1 Micro: 0.7873, F1 Macro: 0.677\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9276, F1 Micro: 0.7805, F1 Macro: 0.6973\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9239, F1 Micro: 0.786, F1 Macro: 0.7185\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9248, F1 Micro: 0.782, F1 Macro: 0.7126\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9257, F1 Micro: 0.7873, F1 Macro: 0.677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.72      0.78      0.75       729\n",
      "     HS_Group       0.73      0.64      0.68       408\n",
      "  HS_Religion       0.69      0.70      0.69       168\n",
      "      HS_Race       0.75      0.72      0.74       119\n",
      "  HS_Physical       0.80      0.14      0.24        57\n",
      "    HS_Gender       0.61      0.20      0.30        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.69      0.76      0.72       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.84      0.74      0.79        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.75      0.66      0.68      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 264.6727910041809 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9213, F1 Micro: 0.7669, F1 Macro: 0.6622\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.99914860725403 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3606, Accuracy: 0.9071, F1 Micro: 0.7114, F1 Macro: 0.482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.9189, F1 Micro: 0.7323, F1 Macro: 0.5593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1939, Accuracy: 0.9252, F1 Micro: 0.7678, F1 Macro: 0.6401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1602, Accuracy: 0.9285, F1 Micro: 0.7928, F1 Macro: 0.6913\n",
      "Epoch 5/10, Train Loss: 0.1243, Accuracy: 0.9286, F1 Micro: 0.7855, F1 Macro: 0.696\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9276, F1 Micro: 0.7869, F1 Macro: 0.7165\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9285, F1 Micro: 0.7765, F1 Macro: 0.6995\n",
      "Epoch 8/10, Train Loss: 0.0694, Accuracy: 0.9279, F1 Micro: 0.7862, F1 Macro: 0.7158\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.926, F1 Micro: 0.7837, F1 Macro: 0.7231\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9267, F1 Micro: 0.7861, F1 Macro: 0.7246\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9285, F1 Micro: 0.7928, F1 Macro: 0.6913\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.87      0.93      0.90      1008\n",
      "HS_Individual       0.75      0.76      0.76       729\n",
      "     HS_Group       0.73      0.66      0.70       408\n",
      "  HS_Religion       0.79      0.67      0.72       168\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.79      0.19      0.31        57\n",
      "    HS_Gender       0.80      0.22      0.34        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.67      0.57      0.62       359\n",
      "    HS_Strong       0.78      0.76      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.78      0.66      0.69      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 267.4371540546417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3569, Accuracy: 0.9073, F1 Micro: 0.713, F1 Macro: 0.5116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2468, Accuracy: 0.9206, F1 Micro: 0.7482, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1936, Accuracy: 0.9235, F1 Micro: 0.7607, F1 Macro: 0.613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1612, Accuracy: 0.9269, F1 Micro: 0.7913, F1 Macro: 0.673\n",
      "Epoch 5/10, Train Loss: 0.129, Accuracy: 0.9294, F1 Micro: 0.787, F1 Macro: 0.6825\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9272, F1 Micro: 0.7804, F1 Macro: 0.7031\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9298, F1 Micro: 0.7899, F1 Macro: 0.7126\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9282, F1 Micro: 0.7879, F1 Macro: 0.7162\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9283, F1 Micro: 0.7899, F1 Macro: 0.732\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9264, F1 Micro: 0.7841, F1 Macro: 0.7202\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9269, F1 Micro: 0.7913, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1137\n",
      "      Abusive       0.86      0.93      0.89      1008\n",
      "HS_Individual       0.74      0.78      0.76       729\n",
      "     HS_Group       0.72      0.68      0.70       408\n",
      "  HS_Religion       0.76      0.65      0.70       168\n",
      "      HS_Race       0.74      0.80      0.77       119\n",
      "  HS_Physical       0.47      0.14      0.22        57\n",
      "    HS_Gender       0.62      0.15      0.24        55\n",
      "     HS_Other       0.81      0.84      0.82       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.74      0.75      0.75        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.72      0.66      0.67      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 265.49390172958374 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3633, Accuracy: 0.9047, F1 Micro: 0.7086, F1 Macro: 0.4745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2491, Accuracy: 0.9186, F1 Micro: 0.7342, F1 Macro: 0.5647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1937, Accuracy: 0.9238, F1 Micro: 0.7619, F1 Macro: 0.6266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1608, Accuracy: 0.9265, F1 Micro: 0.7866, F1 Macro: 0.6758\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9256, F1 Micro: 0.777, F1 Macro: 0.6683\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9252, F1 Micro: 0.7784, F1 Macro: 0.6818\n",
      "Epoch 7/10, Train Loss: 0.0817, Accuracy: 0.9263, F1 Micro: 0.7805, F1 Macro: 0.6956\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9241, F1 Micro: 0.7819, F1 Macro: 0.7017\n",
      "Epoch 9/10, Train Loss: 0.0559, Accuracy: 0.9256, F1 Micro: 0.7808, F1 Macro: 0.7101\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9277, F1 Micro: 0.7855, F1 Macro: 0.722\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9265, F1 Micro: 0.7866, F1 Macro: 0.6758\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.88      0.91      0.89      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.72      0.67      0.69       408\n",
      "  HS_Religion       0.79      0.64      0.71       168\n",
      "      HS_Race       0.78      0.72      0.75       119\n",
      "  HS_Physical       0.36      0.09      0.14        57\n",
      "    HS_Gender       0.60      0.27      0.37        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.66      0.57      0.61       359\n",
      "    HS_Strong       0.80      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.73      0.65      0.68      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 265.56052470207214 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9216, F1 Micro: 0.7681, F1 Macro: 0.6631\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.923979997634888 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3577, Accuracy: 0.8998, F1 Micro: 0.6435, F1 Macro: 0.4305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2452, Accuracy: 0.9216, F1 Micro: 0.7514, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1942, Accuracy: 0.9271, F1 Micro: 0.7776, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1546, Accuracy: 0.9288, F1 Micro: 0.7887, F1 Macro: 0.6907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9265, F1 Micro: 0.7913, F1 Macro: 0.7123\n",
      "Epoch 6/10, Train Loss: 0.1021, Accuracy: 0.9293, F1 Micro: 0.7901, F1 Macro: 0.7109\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9255, F1 Micro: 0.7868, F1 Macro: 0.7215\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9276, F1 Micro: 0.7899, F1 Macro: 0.7256\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9259, F1 Micro: 0.7789, F1 Macro: 0.7123\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9292, F1 Micro: 0.783, F1 Macro: 0.7136\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9265, F1 Micro: 0.7913, F1 Macro: 0.7123\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.92      0.91      1008\n",
      "HS_Individual       0.68      0.83      0.75       729\n",
      "     HS_Group       0.80      0.59      0.68       408\n",
      "  HS_Religion       0.77      0.60      0.68       168\n",
      "      HS_Race       0.84      0.73      0.78       119\n",
      "  HS_Physical       0.74      0.30      0.42        57\n",
      "    HS_Gender       0.74      0.42      0.53        55\n",
      "     HS_Other       0.79      0.85      0.82       771\n",
      "      HS_Weak       0.66      0.82      0.73       681\n",
      "  HS_Moderate       0.74      0.52      0.61       359\n",
      "    HS_Strong       0.82      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.77      0.69      0.71      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 275.226615190506 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3523, Accuracy: 0.8993, F1 Micro: 0.6334, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2444, Accuracy: 0.9217, F1 Micro: 0.7524, F1 Macro: 0.5998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1944, Accuracy: 0.9271, F1 Micro: 0.7742, F1 Macro: 0.6285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9271, F1 Micro: 0.7861, F1 Macro: 0.6662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.928, F1 Micro: 0.7905, F1 Macro: 0.6907\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.9273, F1 Micro: 0.7842, F1 Macro: 0.6917\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9258, F1 Micro: 0.784, F1 Macro: 0.7166\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9286, F1 Micro: 0.787, F1 Macro: 0.7253\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9279, F1 Micro: 0.788, F1 Macro: 0.7207\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9261, F1 Micro: 0.7873, F1 Macro: 0.7235\n",
      "Model 2 - Iteration 9216: Accuracy: 0.928, F1 Micro: 0.7905, F1 Macro: 0.6907\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.72      0.80      0.76       729\n",
      "     HS_Group       0.78      0.62      0.70       408\n",
      "  HS_Religion       0.80      0.64      0.71       168\n",
      "      HS_Race       0.75      0.75      0.75       119\n",
      "  HS_Physical       0.68      0.23      0.34        57\n",
      "    HS_Gender       0.79      0.20      0.32        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.68      0.78      0.72       681\n",
      "  HS_Moderate       0.73      0.57      0.64       359\n",
      "    HS_Strong       0.84      0.73      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.78      0.66      0.69      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 272.2075295448303 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3616, Accuracy: 0.8972, F1 Micro: 0.6199, F1 Macro: 0.4052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2453, Accuracy: 0.9225, F1 Micro: 0.7576, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9263, F1 Micro: 0.7757, F1 Macro: 0.6269\n",
      "Epoch 4/10, Train Loss: 0.1529, Accuracy: 0.9262, F1 Micro: 0.7689, F1 Macro: 0.639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.9241, F1 Micro: 0.7822, F1 Macro: 0.6692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.9271, F1 Micro: 0.7847, F1 Macro: 0.7014\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9199, F1 Micro: 0.775, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9255, F1 Micro: 0.7857, F1 Macro: 0.7111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.928, F1 Micro: 0.7866, F1 Macro: 0.7102\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9228, F1 Micro: 0.7839, F1 Macro: 0.7091\n",
      "Model 3 - Iteration 9216: Accuracy: 0.928, F1 Micro: 0.7866, F1 Macro: 0.7102\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.76      0.63      0.69       408\n",
      "  HS_Religion       0.69      0.68      0.68       168\n",
      "      HS_Race       0.74      0.80      0.77       119\n",
      "  HS_Physical       0.82      0.32      0.46        57\n",
      "    HS_Gender       0.65      0.36      0.47        55\n",
      "     HS_Other       0.83      0.79      0.81       771\n",
      "      HS_Weak       0.72      0.71      0.72       681\n",
      "  HS_Moderate       0.69      0.53      0.60       359\n",
      "    HS_Strong       0.80      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.68      0.71      5589\n",
      " weighted avg       0.81      0.77      0.78      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 275.55464577674866 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9219, F1 Micro: 0.7692, F1 Macro: 0.6652\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 19.174426794052124 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3556, Accuracy: 0.9057, F1 Micro: 0.7016, F1 Macro: 0.4382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2413, Accuracy: 0.9195, F1 Micro: 0.7491, F1 Macro: 0.569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1928, Accuracy: 0.9194, F1 Micro: 0.7773, F1 Macro: 0.6348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1527, Accuracy: 0.9264, F1 Micro: 0.7805, F1 Macro: 0.6696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.926, F1 Micro: 0.7842, F1 Macro: 0.6995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9276, F1 Micro: 0.7911, F1 Macro: 0.7174\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.925, F1 Micro: 0.7865, F1 Macro: 0.7164\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.93, F1 Micro: 0.7905, F1 Macro: 0.7198\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9254, F1 Micro: 0.78, F1 Macro: 0.712\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9297, F1 Micro: 0.7888, F1 Macro: 0.7221\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9276, F1 Micro: 0.7911, F1 Macro: 0.7174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.75       729\n",
      "     HS_Group       0.71      0.68      0.70       408\n",
      "  HS_Religion       0.77      0.64      0.70       168\n",
      "      HS_Race       0.83      0.68      0.75       119\n",
      "  HS_Physical       0.71      0.35      0.47        57\n",
      "    HS_Gender       0.61      0.45      0.52        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.73       681\n",
      "  HS_Moderate       0.66      0.59      0.62       359\n",
      "    HS_Strong       0.81      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 275.76671028137207 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3508, Accuracy: 0.9072, F1 Micro: 0.7055, F1 Macro: 0.4559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9206, F1 Micro: 0.7492, F1 Macro: 0.5856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9192, F1 Micro: 0.7723, F1 Macro: 0.6208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1536, Accuracy: 0.9261, F1 Micro: 0.7853, F1 Macro: 0.6841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9249, F1 Micro: 0.7863, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.929, F1 Micro: 0.7899, F1 Macro: 0.7055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.928, F1 Micro: 0.7906, F1 Macro: 0.7197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9284, F1 Micro: 0.7906, F1 Macro: 0.7313\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9253, F1 Micro: 0.7828, F1 Macro: 0.7121\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9262, F1 Micro: 0.7865, F1 Macro: 0.7215\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9284, F1 Micro: 0.7906, F1 Macro: 0.7313\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.72      0.77      0.74       729\n",
      "     HS_Group       0.76      0.62      0.68       408\n",
      "  HS_Religion       0.75      0.65      0.70       168\n",
      "      HS_Race       0.77      0.78      0.78       119\n",
      "  HS_Physical       0.62      0.46      0.53        57\n",
      "    HS_Gender       0.71      0.53      0.60        55\n",
      "     HS_Other       0.84      0.80      0.82       771\n",
      "      HS_Weak       0.70      0.76      0.73       681\n",
      "  HS_Moderate       0.71      0.56      0.63       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.42      5589\n",
      "\n",
      "Training completed in 277.41456866264343 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3592, Accuracy: 0.9067, F1 Micro: 0.7039, F1 Macro: 0.4642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2418, Accuracy: 0.9191, F1 Micro: 0.7466, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9227, F1 Micro: 0.7777, F1 Macro: 0.6271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1506, Accuracy: 0.9246, F1 Micro: 0.7812, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.9233, F1 Micro: 0.7831, F1 Macro: 0.6763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9266, F1 Micro: 0.7857, F1 Macro: 0.7035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9262, F1 Micro: 0.7918, F1 Macro: 0.7108\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9287, F1 Micro: 0.7899, F1 Macro: 0.7228\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9254, F1 Micro: 0.7793, F1 Macro: 0.704\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9239, F1 Micro: 0.7852, F1 Macro: 0.7178\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9262, F1 Micro: 0.7918, F1 Macro: 0.7108\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.91      0.87      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.71      0.80      0.75       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.74      0.66      0.70       168\n",
      "      HS_Race       0.70      0.84      0.76       119\n",
      "  HS_Physical       0.74      0.25      0.37        57\n",
      "    HS_Gender       0.63      0.44      0.52        55\n",
      "     HS_Other       0.78      0.84      0.81       771\n",
      "      HS_Weak       0.70      0.77      0.73       681\n",
      "  HS_Moderate       0.68      0.56      0.62       359\n",
      "    HS_Strong       0.76      0.87      0.81        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.74      0.71      0.71      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.46      0.46      0.44      5589\n",
      "\n",
      "Training completed in 275.97999930381775 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9222, F1 Micro: 0.7703, F1 Macro: 0.6678\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 18.741345405578613 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3538, Accuracy: 0.9053, F1 Micro: 0.6836, F1 Macro: 0.4887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2383, Accuracy: 0.9185, F1 Micro: 0.7618, F1 Macro: 0.5785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.9285, F1 Micro: 0.7861, F1 Macro: 0.6728\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9243, F1 Micro: 0.782, F1 Macro: 0.6796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9267, F1 Micro: 0.79, F1 Macro: 0.6893\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9282, F1 Micro: 0.7876, F1 Macro: 0.7113\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9246, F1 Micro: 0.7874, F1 Macro: 0.7163\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9286, F1 Micro: 0.7855, F1 Macro: 0.7216\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9284, F1 Micro: 0.7844, F1 Macro: 0.7182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9263, F1 Micro: 0.7942, F1 Macro: 0.7278\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9263, F1 Micro: 0.7942, F1 Macro: 0.7278\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1137\n",
      "      Abusive       0.86      0.94      0.90      1008\n",
      "HS_Individual       0.71      0.80      0.76       729\n",
      "     HS_Group       0.71      0.68      0.69       408\n",
      "  HS_Religion       0.69      0.71      0.70       168\n",
      "      HS_Race       0.71      0.82      0.76       119\n",
      "  HS_Physical       0.56      0.44      0.49        57\n",
      "    HS_Gender       0.67      0.51      0.58        55\n",
      "     HS_Other       0.79      0.85      0.82       771\n",
      "      HS_Weak       0.70      0.78      0.74       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.77      0.86      0.81        97\n",
      "\n",
      "    micro avg       0.77      0.82      0.79      5589\n",
      "    macro avg       0.72      0.74      0.73      5589\n",
      " weighted avg       0.77      0.82      0.79      5589\n",
      "  samples avg       0.46      0.47      0.45      5589\n",
      "\n",
      "Training completed in 278.4133312702179 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3481, Accuracy: 0.9049, F1 Micro: 0.6803, F1 Macro: 0.4889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2376, Accuracy: 0.9169, F1 Micro: 0.7457, F1 Macro: 0.5491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1879, Accuracy: 0.9255, F1 Micro: 0.777, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.926, F1 Micro: 0.7865, F1 Macro: 0.6709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9256, F1 Micro: 0.7871, F1 Macro: 0.6798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9273, F1 Micro: 0.7899, F1 Macro: 0.7177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9278, F1 Micro: 0.7901, F1 Macro: 0.7143\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9278, F1 Micro: 0.7852, F1 Macro: 0.7202\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9261, F1 Micro: 0.7791, F1 Macro: 0.7103\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9258, F1 Micro: 0.7859, F1 Macro: 0.7226\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9278, F1 Micro: 0.7901, F1 Macro: 0.7143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.70      0.69      0.70       408\n",
      "  HS_Religion       0.72      0.63      0.67       168\n",
      "      HS_Race       0.77      0.76      0.77       119\n",
      "  HS_Physical       0.68      0.26      0.38        57\n",
      "    HS_Gender       0.67      0.51      0.58        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.63      0.63      0.63       359\n",
      "    HS_Strong       0.84      0.75      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.71      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 281.9783079624176 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3598, Accuracy: 0.9049, F1 Micro: 0.6829, F1 Macro: 0.4956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2404, Accuracy: 0.9185, F1 Micro: 0.7595, F1 Macro: 0.5618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1865, Accuracy: 0.9269, F1 Micro: 0.7781, F1 Macro: 0.6403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1494, Accuracy: 0.9266, F1 Micro: 0.7853, F1 Macro: 0.6594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9262, F1 Micro: 0.7889, F1 Macro: 0.6777\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9271, F1 Micro: 0.7817, F1 Macro: 0.6855\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9262, F1 Micro: 0.7866, F1 Macro: 0.7056\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9272, F1 Micro: 0.7864, F1 Macro: 0.7049\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.9271, F1 Micro: 0.7852, F1 Macro: 0.7132\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9246, F1 Micro: 0.7845, F1 Macro: 0.7169\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9262, F1 Micro: 0.7889, F1 Macro: 0.6777\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.87      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.71      0.80      0.75       729\n",
      "     HS_Group       0.74      0.63      0.68       408\n",
      "  HS_Religion       0.80      0.63      0.71       168\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.62      0.09      0.15        57\n",
      "    HS_Gender       0.88      0.25      0.39        55\n",
      "     HS_Other       0.77      0.84      0.80       771\n",
      "      HS_Weak       0.70      0.78      0.74       681\n",
      "  HS_Moderate       0.69      0.55      0.62       359\n",
      "    HS_Strong       0.79      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.76      0.66      0.68      5589\n",
      " weighted avg       0.78      0.80      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 278.36790108680725 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9224, F1 Micro: 0.7712, F1 Macro: 0.6695\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.150519132614136 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3467, Accuracy: 0.8995, F1 Micro: 0.6341, F1 Macro: 0.4041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2356, Accuracy: 0.9214, F1 Micro: 0.7651, F1 Macro: 0.6015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1894, Accuracy: 0.9231, F1 Micro: 0.7782, F1 Macro: 0.6307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.9269, F1 Micro: 0.7861, F1 Macro: 0.6643\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.928, F1 Micro: 0.7828, F1 Macro: 0.6992\n",
      "Epoch 6/10, Train Loss: 0.0942, Accuracy: 0.926, F1 Micro: 0.7847, F1 Macro: 0.7118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9296, F1 Micro: 0.7922, F1 Macro: 0.706\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9287, F1 Micro: 0.7901, F1 Macro: 0.7215\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9271, F1 Micro: 0.7891, F1 Macro: 0.7304\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9275, F1 Micro: 0.7884, F1 Macro: 0.7316\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9296, F1 Micro: 0.7922, F1 Macro: 0.706\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.74      0.78      0.76       729\n",
      "     HS_Group       0.78      0.61      0.69       408\n",
      "  HS_Religion       0.79      0.64      0.70       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.79      0.26      0.39        57\n",
      "    HS_Gender       0.72      0.33      0.45        55\n",
      "     HS_Other       0.81      0.82      0.82       771\n",
      "      HS_Weak       0.70      0.75      0.73       681\n",
      "  HS_Moderate       0.73      0.53      0.62       359\n",
      "    HS_Strong       0.82      0.75      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.79      0.67      0.71      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 283.95999217033386 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3437, Accuracy: 0.9003, F1 Micro: 0.6369, F1 Macro: 0.4077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2346, Accuracy: 0.9226, F1 Micro: 0.7658, F1 Macro: 0.6089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.9255, F1 Micro: 0.7785, F1 Macro: 0.628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1534, Accuracy: 0.9263, F1 Micro: 0.7827, F1 Macro: 0.6554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9274, F1 Micro: 0.7892, F1 Macro: 0.6771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9276, F1 Micro: 0.7895, F1 Macro: 0.7135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9284, F1 Micro: 0.7931, F1 Macro: 0.7101\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9296, F1 Micro: 0.7854, F1 Macro: 0.7212\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.928, F1 Micro: 0.7874, F1 Macro: 0.7181\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9244, F1 Micro: 0.7816, F1 Macro: 0.7139\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9284, F1 Micro: 0.7931, F1 Macro: 0.7101\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.71      0.80      0.75       729\n",
      "     HS_Group       0.74      0.61      0.67       408\n",
      "  HS_Religion       0.73      0.67      0.70       168\n",
      "      HS_Race       0.75      0.78      0.77       119\n",
      "  HS_Physical       0.83      0.26      0.40        57\n",
      "    HS_Gender       0.71      0.40      0.51        55\n",
      "     HS_Other       0.80      0.83      0.82       771\n",
      "      HS_Weak       0.69      0.78      0.73       681\n",
      "  HS_Moderate       0.70      0.54      0.61       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.77      0.69      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 286.958616733551 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3519, Accuracy: 0.8994, F1 Micro: 0.6351, F1 Macro: 0.4277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2365, Accuracy: 0.9219, F1 Micro: 0.7605, F1 Macro: 0.5971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1918, Accuracy: 0.9258, F1 Micro: 0.7726, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1544, Accuracy: 0.9267, F1 Micro: 0.7861, F1 Macro: 0.6487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9264, F1 Micro: 0.788, F1 Macro: 0.6735\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9278, F1 Micro: 0.7805, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9294, F1 Micro: 0.7895, F1 Macro: 0.6863\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.928, F1 Micro: 0.7884, F1 Macro: 0.7095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9285, F1 Micro: 0.7931, F1 Macro: 0.7118\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9297, F1 Micro: 0.7894, F1 Macro: 0.7173\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9285, F1 Micro: 0.7931, F1 Macro: 0.7118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.74      0.63      0.69       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.72      0.82      0.76       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.64      0.38      0.48        55\n",
      "     HS_Other       0.81      0.80      0.81       771\n",
      "      HS_Weak       0.71      0.77      0.74       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.82      0.74      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.75      0.69      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 286.7473089694977 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9227, F1 Micro: 0.7721, F1 Macro: 0.6713\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.576757192611694 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3444, Accuracy: 0.9054, F1 Micro: 0.6847, F1 Macro: 0.4566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.229, Accuracy: 0.9213, F1 Micro: 0.7612, F1 Macro: 0.5938\n",
      "Epoch 3/10, Train Loss: 0.1794, Accuracy: 0.9237, F1 Micro: 0.7593, F1 Macro: 0.6099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1421, Accuracy: 0.9211, F1 Micro: 0.7777, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1121, Accuracy: 0.9253, F1 Micro: 0.7883, F1 Macro: 0.712\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.9255, F1 Micro: 0.7822, F1 Macro: 0.6995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9297, F1 Micro: 0.7898, F1 Macro: 0.7206\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.9262, F1 Micro: 0.7843, F1 Macro: 0.723\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.9227, F1 Micro: 0.7838, F1 Macro: 0.7233\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9281, F1 Micro: 0.7835, F1 Macro: 0.7172\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9297, F1 Micro: 0.7898, F1 Macro: 0.7206\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.77      0.62      0.68       408\n",
      "  HS_Religion       0.76      0.69      0.72       168\n",
      "      HS_Race       0.79      0.71      0.75       119\n",
      "  HS_Physical       0.72      0.40      0.52        57\n",
      "    HS_Gender       0.69      0.40      0.51        55\n",
      "     HS_Other       0.85      0.77      0.81       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.71      0.54      0.61       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.82      0.76      0.79      5589\n",
      "    macro avg       0.78      0.68      0.72      5589\n",
      " weighted avg       0.82      0.76      0.79      5589\n",
      "  samples avg       0.46      0.44      0.43      5589\n",
      "\n",
      "Training completed in 288.7214980125427 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.341, Accuracy: 0.908, F1 Micro: 0.7031, F1 Macro: 0.4699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2302, Accuracy: 0.9213, F1 Micro: 0.7631, F1 Macro: 0.5992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1804, Accuracy: 0.9259, F1 Micro: 0.7729, F1 Macro: 0.6246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.9272, F1 Micro: 0.7846, F1 Macro: 0.6704\n",
      "Epoch 5/10, Train Loss: 0.1115, Accuracy: 0.9274, F1 Micro: 0.7842, F1 Macro: 0.7028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.925, F1 Micro: 0.7877, F1 Macro: 0.7097\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.9279, F1 Micro: 0.786, F1 Macro: 0.7113\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.7157\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9274, F1 Micro: 0.7865, F1 Macro: 0.716\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9266, F1 Micro: 0.7833, F1 Macro: 0.7188\n",
      "Model 2 - Iteration 9818: Accuracy: 0.925, F1 Micro: 0.7877, F1 Macro: 0.7097\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.89      0.86      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.70      0.80      0.75       729\n",
      "     HS_Group       0.75      0.62      0.68       408\n",
      "  HS_Religion       0.71      0.68      0.70       168\n",
      "      HS_Race       0.75      0.80      0.77       119\n",
      "  HS_Physical       0.61      0.33      0.43        57\n",
      "    HS_Gender       0.66      0.45      0.54        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.66      0.79      0.72       681\n",
      "  HS_Moderate       0.66      0.55      0.60       359\n",
      "    HS_Strong       0.82      0.69      0.75        97\n",
      "\n",
      "    micro avg       0.77      0.80      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.77      0.80      0.78      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 288.4608585834503 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3487, Accuracy: 0.9059, F1 Micro: 0.7025, F1 Macro: 0.4876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2292, Accuracy: 0.9222, F1 Micro: 0.7612, F1 Macro: 0.591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1786, Accuracy: 0.9247, F1 Micro: 0.7761, F1 Macro: 0.6234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1428, Accuracy: 0.923, F1 Micro: 0.7808, F1 Macro: 0.646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1135, Accuracy: 0.9284, F1 Micro: 0.7882, F1 Macro: 0.6809\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.925, F1 Micro: 0.7836, F1 Macro: 0.6918\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.9258, F1 Micro: 0.7832, F1 Macro: 0.7067\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9255, F1 Micro: 0.7842, F1 Macro: 0.7114\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.9262, F1 Micro: 0.7804, F1 Macro: 0.7046\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9241, F1 Micro: 0.7812, F1 Macro: 0.7118\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9284, F1 Micro: 0.7882, F1 Macro: 0.6809\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.76      0.62      0.68       408\n",
      "  HS_Religion       0.76      0.63      0.69       168\n",
      "      HS_Race       0.77      0.71      0.74       119\n",
      "  HS_Physical       0.75      0.11      0.18        57\n",
      "    HS_Gender       0.70      0.29      0.41        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.70      0.54      0.61       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.78      0.65      0.68      5589\n",
      " weighted avg       0.81      0.77      0.78      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 288.37847208976746 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9229, F1 Micro: 0.7728, F1 Macro: 0.6726\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.555787086486816 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3344, Accuracy: 0.8996, F1 Micro: 0.7032, F1 Macro: 0.4057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9194, F1 Micro: 0.7473, F1 Macro: 0.5492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.177, Accuracy: 0.9252, F1 Micro: 0.7751, F1 Macro: 0.6315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1378, Accuracy: 0.9272, F1 Micro: 0.7866, F1 Macro: 0.6734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9278, F1 Micro: 0.7914, F1 Macro: 0.688\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9285, F1 Micro: 0.7877, F1 Macro: 0.7112\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.9276, F1 Micro: 0.7872, F1 Macro: 0.7096\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9281, F1 Micro: 0.7873, F1 Macro: 0.7241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9278, F1 Micro: 0.7916, F1 Macro: 0.7247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.9293, F1 Micro: 0.7943, F1 Macro: 0.7273\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9293, F1 Micro: 0.7943, F1 Macro: 0.7273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.92      0.91      0.91      1008\n",
      "HS_Individual       0.74      0.79      0.76       729\n",
      "     HS_Group       0.74      0.61      0.66       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.74      0.75      0.74       119\n",
      "  HS_Physical       0.69      0.42      0.52        57\n",
      "    HS_Gender       0.68      0.51      0.58        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.72      0.77      0.74       681\n",
      "  HS_Moderate       0.67      0.54      0.60       359\n",
      "    HS_Strong       0.82      0.81      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 296.5815625190735 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3302, Accuracy: 0.903, F1 Micro: 0.7134, F1 Macro: 0.4718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2245, Accuracy: 0.9202, F1 Micro: 0.7482, F1 Macro: 0.5585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1778, Accuracy: 0.9251, F1 Micro: 0.7779, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1383, Accuracy: 0.9286, F1 Micro: 0.7864, F1 Macro: 0.6815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9253, F1 Micro: 0.79, F1 Macro: 0.7028\n",
      "Epoch 6/10, Train Loss: 0.088, Accuracy: 0.9264, F1 Micro: 0.786, F1 Macro: 0.7122\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9251, F1 Micro: 0.7885, F1 Macro: 0.7187\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9285, F1 Micro: 0.7881, F1 Macro: 0.7223\n",
      "Epoch 9/10, Train Loss: 0.0495, Accuracy: 0.9253, F1 Micro: 0.7889, F1 Macro: 0.7263\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9271, F1 Micro: 0.7744, F1 Macro: 0.6988\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9253, F1 Micro: 0.79, F1 Macro: 0.7028\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1137\n",
      "      Abusive       0.87      0.94      0.90      1008\n",
      "HS_Individual       0.73      0.76      0.75       729\n",
      "     HS_Group       0.66      0.72      0.69       408\n",
      "  HS_Religion       0.68      0.69      0.69       168\n",
      "      HS_Race       0.74      0.79      0.76       119\n",
      "  HS_Physical       0.65      0.23      0.34        57\n",
      "    HS_Gender       0.73      0.35      0.47        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.62      0.66      0.64       359\n",
      "    HS_Strong       0.80      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.73      0.70      0.70      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 293.1284832954407 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3359, Accuracy: 0.9022, F1 Micro: 0.7113, F1 Macro: 0.4425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2243, Accuracy: 0.92, F1 Micro: 0.7479, F1 Macro: 0.5673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1776, Accuracy: 0.9235, F1 Micro: 0.7806, F1 Macro: 0.6385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1379, Accuracy: 0.9278, F1 Micro: 0.7867, F1 Macro: 0.6715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1142, Accuracy: 0.9303, F1 Micro: 0.7936, F1 Macro: 0.6797\n",
      "Epoch 6/10, Train Loss: 0.0882, Accuracy: 0.9289, F1 Micro: 0.7846, F1 Macro: 0.6944\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9277, F1 Micro: 0.7872, F1 Macro: 0.6961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9291, F1 Micro: 0.7963, F1 Macro: 0.7223\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9242, F1 Micro: 0.7823, F1 Macro: 0.7101\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9292, F1 Micro: 0.7906, F1 Macro: 0.7149\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9291, F1 Micro: 0.7963, F1 Macro: 0.7223\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.78      0.76       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.70      0.65      0.67       168\n",
      "      HS_Race       0.70      0.79      0.74       119\n",
      "  HS_Physical       0.65      0.35      0.45        57\n",
      "    HS_Gender       0.68      0.47      0.56        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.72      0.75      0.74       681\n",
      "  HS_Moderate       0.67      0.62      0.64       359\n",
      "    HS_Strong       0.79      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.80      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 293.6452639102936 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9231, F1 Micro: 0.7737, F1 Macro: 0.6744\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.77024507522583 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3321, Accuracy: 0.9006, F1 Micro: 0.6468, F1 Macro: 0.3875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2262, Accuracy: 0.9185, F1 Micro: 0.7558, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1752, Accuracy: 0.924, F1 Micro: 0.7773, F1 Macro: 0.6383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1415, Accuracy: 0.9267, F1 Micro: 0.789, F1 Macro: 0.69\n",
      "Epoch 5/10, Train Loss: 0.1135, Accuracy: 0.9263, F1 Micro: 0.7819, F1 Macro: 0.7024\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9262, F1 Micro: 0.788, F1 Macro: 0.7106\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.9271, F1 Micro: 0.7859, F1 Macro: 0.7002\n",
      "Epoch 8/10, Train Loss: 0.0582, Accuracy: 0.9261, F1 Micro: 0.7832, F1 Macro: 0.7161\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9176, F1 Micro: 0.7721, F1 Macro: 0.7075\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9256, F1 Micro: 0.7734, F1 Macro: 0.7068\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9267, F1 Micro: 0.789, F1 Macro: 0.69\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.90      0.87      1137\n",
      "      Abusive       0.84      0.94      0.89      1008\n",
      "HS_Individual       0.74      0.78      0.76       729\n",
      "     HS_Group       0.73      0.62      0.67       408\n",
      "  HS_Religion       0.76      0.67      0.71       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.83      0.18      0.29        57\n",
      "    HS_Gender       0.71      0.27      0.39        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.52      0.59       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.77      0.67      0.69      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 295.20742297172546 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3279, Accuracy: 0.9056, F1 Micro: 0.6857, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2243, Accuracy: 0.9202, F1 Micro: 0.7573, F1 Macro: 0.5941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1727, Accuracy: 0.9228, F1 Micro: 0.7782, F1 Macro: 0.6348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1427, Accuracy: 0.9254, F1 Micro: 0.7858, F1 Macro: 0.6727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1128, Accuracy: 0.9276, F1 Micro: 0.7886, F1 Macro: 0.6959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9279, F1 Micro: 0.7898, F1 Macro: 0.697\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9273, F1 Micro: 0.7896, F1 Macro: 0.7055\n",
      "Epoch 8/10, Train Loss: 0.0567, Accuracy: 0.9267, F1 Micro: 0.7792, F1 Macro: 0.7156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.9269, F1 Micro: 0.7921, F1 Macro: 0.7307\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9237, F1 Micro: 0.7866, F1 Macro: 0.7266\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9269, F1 Micro: 0.7921, F1 Macro: 0.7307\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.89      0.87      1137\n",
      "      Abusive       0.87      0.95      0.91      1008\n",
      "HS_Individual       0.69      0.80      0.74       729\n",
      "     HS_Group       0.76      0.60      0.67       408\n",
      "  HS_Religion       0.71      0.71      0.71       168\n",
      "      HS_Race       0.78      0.76      0.77       119\n",
      "  HS_Physical       0.60      0.47      0.53        57\n",
      "    HS_Gender       0.71      0.55      0.62        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.67      0.78      0.72       681\n",
      "  HS_Moderate       0.71      0.53      0.61       359\n",
      "    HS_Strong       0.84      0.78      0.81        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.75      0.72      0.73      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 301.4022250175476 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3359, Accuracy: 0.9022, F1 Micro: 0.6558, F1 Macro: 0.4115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2256, Accuracy: 0.919, F1 Micro: 0.7608, F1 Macro: 0.5995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1753, Accuracy: 0.9253, F1 Micro: 0.78, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1423, Accuracy: 0.9258, F1 Micro: 0.7855, F1 Macro: 0.6702\n",
      "Epoch 5/10, Train Loss: 0.1128, Accuracy: 0.9269, F1 Micro: 0.7814, F1 Macro: 0.6789\n",
      "Epoch 6/10, Train Loss: 0.089, Accuracy: 0.9259, F1 Micro: 0.7854, F1 Macro: 0.6815\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9216, F1 Micro: 0.7767, F1 Macro: 0.6911\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9256, F1 Micro: 0.7728, F1 Macro: 0.6996\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9195, F1 Micro: 0.7789, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.042, Accuracy: 0.9264, F1 Micro: 0.787, F1 Macro: 0.7258\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9264, F1 Micro: 0.787, F1 Macro: 0.7258\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.87      0.94      0.91      1008\n",
      "HS_Individual       0.73      0.75      0.74       729\n",
      "     HS_Group       0.74      0.66      0.70       408\n",
      "  HS_Religion       0.70      0.72      0.71       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.66      0.44      0.53        57\n",
      "    HS_Gender       0.56      0.55      0.55        55\n",
      "     HS_Other       0.82      0.74      0.78       771\n",
      "      HS_Weak       0.71      0.73      0.72       681\n",
      "  HS_Moderate       0.68      0.59      0.63       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 298.2214357852936 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9232, F1 Micro: 0.7743, F1 Macro: 0.676\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.164195775985718 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3267, Accuracy: 0.9053, F1 Micro: 0.7038, F1 Macro: 0.5015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2181, Accuracy: 0.9171, F1 Micro: 0.7282, F1 Macro: 0.5292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1758, Accuracy: 0.9255, F1 Micro: 0.7764, F1 Macro: 0.629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1385, Accuracy: 0.9291, F1 Micro: 0.7877, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1067, Accuracy: 0.9286, F1 Micro: 0.7905, F1 Macro: 0.7045\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9289, F1 Micro: 0.787, F1 Macro: 0.7052\n",
      "Epoch 7/10, Train Loss: 0.0685, Accuracy: 0.924, F1 Micro: 0.7857, F1 Macro: 0.7107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9311, F1 Micro: 0.792, F1 Macro: 0.723\n",
      "Epoch 9/10, Train Loss: 0.048, Accuracy: 0.9288, F1 Micro: 0.7891, F1 Macro: 0.719\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.9271, F1 Micro: 0.79, F1 Macro: 0.7208\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9311, F1 Micro: 0.792, F1 Macro: 0.723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.84      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.78      0.73      0.75       729\n",
      "     HS_Group       0.77      0.62      0.68       408\n",
      "  HS_Religion       0.80      0.58      0.68       168\n",
      "      HS_Race       0.76      0.73      0.74       119\n",
      "  HS_Physical       0.65      0.46      0.54        57\n",
      "    HS_Gender       0.73      0.44      0.55        55\n",
      "     HS_Other       0.85      0.77      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.72       681\n",
      "  HS_Moderate       0.72      0.55      0.63       359\n",
      "    HS_Strong       0.82      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.83      0.76      0.79      5589\n",
      "    macro avg       0.79      0.68      0.72      5589\n",
      " weighted avg       0.83      0.76      0.79      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 305.60309624671936 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3236, Accuracy: 0.9067, F1 Micro: 0.6985, F1 Macro: 0.5059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2176, Accuracy: 0.9174, F1 Micro: 0.731, F1 Macro: 0.5292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1753, Accuracy: 0.9268, F1 Micro: 0.7763, F1 Macro: 0.6361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1382, Accuracy: 0.9268, F1 Micro: 0.7887, F1 Macro: 0.6687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1101, Accuracy: 0.9285, F1 Micro: 0.7931, F1 Macro: 0.7079\n",
      "Epoch 6/10, Train Loss: 0.0876, Accuracy: 0.928, F1 Micro: 0.7864, F1 Macro: 0.6999\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.9255, F1 Micro: 0.7915, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.93, F1 Micro: 0.7941, F1 Macro: 0.7273\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9271, F1 Micro: 0.7846, F1 Macro: 0.7238\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.928, F1 Micro: 0.792, F1 Macro: 0.7282\n",
      "Model 2 - Iteration 10418: Accuracy: 0.93, F1 Micro: 0.7941, F1 Macro: 0.7273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.78      0.75      0.76       729\n",
      "     HS_Group       0.72      0.67      0.70       408\n",
      "  HS_Religion       0.74      0.64      0.68       168\n",
      "      HS_Race       0.75      0.80      0.77       119\n",
      "  HS_Physical       0.62      0.40      0.49        57\n",
      "    HS_Gender       0.69      0.49      0.57        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.65      0.62      0.64       359\n",
      "    HS_Strong       0.81      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.73      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 306.3125946521759 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3298, Accuracy: 0.9068, F1 Micro: 0.7034, F1 Macro: 0.5048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2194, Accuracy: 0.9152, F1 Micro: 0.7238, F1 Macro: 0.5298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1729, Accuracy: 0.9246, F1 Micro: 0.7758, F1 Macro: 0.627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1391, Accuracy: 0.929, F1 Micro: 0.7916, F1 Macro: 0.6588\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.9278, F1 Micro: 0.7841, F1 Macro: 0.6822\n",
      "Epoch 6/10, Train Loss: 0.0916, Accuracy: 0.9274, F1 Micro: 0.7788, F1 Macro: 0.6765\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9258, F1 Micro: 0.7864, F1 Macro: 0.7014\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9286, F1 Micro: 0.7863, F1 Macro: 0.7079\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.925, F1 Micro: 0.7821, F1 Macro: 0.7144\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9226, F1 Micro: 0.7832, F1 Macro: 0.7141\n",
      "Model 3 - Iteration 10418: Accuracy: 0.929, F1 Micro: 0.7916, F1 Macro: 0.6588\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.88      0.87      1137\n",
      "      Abusive       0.88      0.91      0.90      1008\n",
      "HS_Individual       0.78      0.72      0.75       729\n",
      "     HS_Group       0.71      0.73      0.72       408\n",
      "  HS_Religion       0.75      0.67      0.70       168\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.75      0.05      0.10        57\n",
      "    HS_Gender       0.83      0.09      0.16        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.76      0.70      0.72       681\n",
      "  HS_Moderate       0.66      0.65      0.66       359\n",
      "    HS_Strong       0.82      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.78      0.64      0.66      5589\n",
      " weighted avg       0.80      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 301.6679403781891 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9235, F1 Micro: 0.7749, F1 Macro: 0.677\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.441087484359741 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3271, Accuracy: 0.9037, F1 Micro: 0.6875, F1 Macro: 0.4307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2112, Accuracy: 0.9214, F1 Micro: 0.7545, F1 Macro: 0.5778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1708, Accuracy: 0.9255, F1 Micro: 0.78, F1 Macro: 0.6479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9247, F1 Micro: 0.7852, F1 Macro: 0.6847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1098, Accuracy: 0.9269, F1 Micro: 0.7881, F1 Macro: 0.7113\n",
      "Epoch 6/10, Train Loss: 0.0916, Accuracy: 0.9289, F1 Micro: 0.7847, F1 Macro: 0.7011\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9249, F1 Micro: 0.7857, F1 Macro: 0.7182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9244, F1 Micro: 0.7884, F1 Macro: 0.7234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.929, F1 Micro: 0.7927, F1 Macro: 0.7255\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9253, F1 Micro: 0.7841, F1 Macro: 0.7168\n",
      "Model 1 - Iteration 10535: Accuracy: 0.929, F1 Micro: 0.7927, F1 Macro: 0.7255\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.72      0.68      0.70       408\n",
      "  HS_Religion       0.73      0.70      0.71       168\n",
      "      HS_Race       0.79      0.75      0.77       119\n",
      "  HS_Physical       0.66      0.40      0.50        57\n",
      "    HS_Gender       0.62      0.45      0.53        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.67      0.60      0.63       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.73      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 309.28260803222656 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3239, Accuracy: 0.903, F1 Micro: 0.6896, F1 Macro: 0.4499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2146, Accuracy: 0.9208, F1 Micro: 0.7539, F1 Macro: 0.5882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9279, F1 Micro: 0.7786, F1 Macro: 0.6386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1377, Accuracy: 0.9272, F1 Micro: 0.7892, F1 Macro: 0.6878\n",
      "Epoch 5/10, Train Loss: 0.1111, Accuracy: 0.9243, F1 Micro: 0.787, F1 Macro: 0.6985\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9266, F1 Micro: 0.7858, F1 Macro: 0.6853\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9259, F1 Micro: 0.785, F1 Macro: 0.7175\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9256, F1 Micro: 0.7869, F1 Macro: 0.7232\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.9266, F1 Micro: 0.7861, F1 Macro: 0.7259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0409, Accuracy: 0.9262, F1 Micro: 0.7918, F1 Macro: 0.7282\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9262, F1 Micro: 0.7918, F1 Macro: 0.7282\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.90      0.87      1137\n",
      "      Abusive       0.85      0.95      0.90      1008\n",
      "HS_Individual       0.71      0.80      0.75       729\n",
      "     HS_Group       0.72      0.62      0.66       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.61      0.49      0.54        57\n",
      "    HS_Gender       0.61      0.64      0.62        55\n",
      "     HS_Other       0.81      0.83      0.82       771\n",
      "      HS_Weak       0.68      0.78      0.73       681\n",
      "  HS_Moderate       0.67      0.53      0.60       359\n",
      "    HS_Strong       0.78      0.74      0.76        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.73      0.73      0.73      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 305.92587971687317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3327, Accuracy: 0.9034, F1 Micro: 0.6943, F1 Macro: 0.4514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2162, Accuracy: 0.9204, F1 Micro: 0.7547, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.172, Accuracy: 0.9268, F1 Micro: 0.775, F1 Macro: 0.6291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.9249, F1 Micro: 0.7835, F1 Macro: 0.6641\n",
      "Epoch 5/10, Train Loss: 0.1118, Accuracy: 0.9245, F1 Micro: 0.7795, F1 Macro: 0.6769\n",
      "Epoch 6/10, Train Loss: 0.0887, Accuracy: 0.9266, F1 Micro: 0.778, F1 Macro: 0.6845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9258, F1 Micro: 0.7844, F1 Macro: 0.711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9241, F1 Micro: 0.7854, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.9244, F1 Micro: 0.7879, F1 Macro: 0.7204\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.924, F1 Micro: 0.7809, F1 Macro: 0.7105\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9244, F1 Micro: 0.7879, F1 Macro: 0.7204\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.69      0.68      0.68       408\n",
      "  HS_Religion       0.69      0.70      0.69       168\n",
      "      HS_Race       0.73      0.80      0.76       119\n",
      "  HS_Physical       0.63      0.33      0.44        57\n",
      "    HS_Gender       0.72      0.53      0.61        55\n",
      "     HS_Other       0.77      0.84      0.80       771\n",
      "      HS_Weak       0.69      0.76      0.72       681\n",
      "  HS_Moderate       0.64      0.60      0.62       359\n",
      "    HS_Strong       0.76      0.85      0.80        97\n",
      "\n",
      "    micro avg       0.77      0.81      0.79      5589\n",
      "    macro avg       0.73      0.72      0.72      5589\n",
      " weighted avg       0.77      0.81      0.79      5589\n",
      "  samples avg       0.45      0.46      0.44      5589\n",
      "\n",
      "Training completed in 309.82744121551514 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9236, F1 Micro: 0.7755, F1 Macro: 0.6787\n",
      "Total sampling time: 1337.68 seconds\n",
      "Total runtime: 20716.7689371109 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9c918",
   "metadata": {
    "papermill": {
     "duration": 0.182339,
     "end_time": "2025-01-30T11:43:28.802606",
     "exception": false,
     "start_time": "2025-01-30T11:43:28.620267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ab0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5534, Accuracy: 0.828, F1 Micro: 0.0291, F1 Macro: 0.0124\n",
      "Epoch 2/10, Train Loss: 0.4183, Accuracy: 0.8281, F1 Micro: 0.0181, F1 Macro: 0.008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3779, Accuracy: 0.8364, F1 Micro: 0.1192, F1 Macro: 0.0469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3644, Accuracy: 0.8572, F1 Micro: 0.3765, F1 Macro: 0.1175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3428, Accuracy: 0.8691, F1 Micro: 0.4752, F1 Macro: 0.1966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2967, Accuracy: 0.8805, F1 Micro: 0.5933, F1 Macro: 0.3073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2617, Accuracy: 0.8838, F1 Micro: 0.598, F1 Macro: 0.3314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2459, Accuracy: 0.8864, F1 Micro: 0.6033, F1 Macro: 0.3476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2198, Accuracy: 0.8897, F1 Micro: 0.636, F1 Macro: 0.3855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1859, Accuracy: 0.89, F1 Micro: 0.6451, F1 Macro: 0.3847\n",
      "Model 1 - Iteration 658: Accuracy: 0.89, F1 Micro: 0.6451, F1 Macro: 0.3847\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.75      0.79      1137\n",
      "      Abusive       0.82      0.77      0.79      1008\n",
      "HS_Individual       0.67      0.60      0.63       729\n",
      "     HS_Group       0.66      0.30      0.42       408\n",
      "  HS_Religion       0.64      0.20      0.31       168\n",
      "      HS_Race       1.00      0.03      0.07       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.67      0.68       771\n",
      "      HS_Weak       0.61      0.58      0.60       681\n",
      "  HS_Moderate       0.60      0.23      0.34       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.73      0.58      0.65      5589\n",
      "    macro avg       0.54      0.35      0.38      5589\n",
      " weighted avg       0.70      0.58      0.62      5589\n",
      "  samples avg       0.38      0.33      0.33      5589\n",
      "\n",
      "Training completed in 60.55369186401367 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5477, Accuracy: 0.828, F1 Micro: 0.0267, F1 Macro: 0.0114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4185, Accuracy: 0.8287, F1 Micro: 0.0302, F1 Macro: 0.013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3775, Accuracy: 0.8363, F1 Micro: 0.137, F1 Macro: 0.0485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3663, Accuracy: 0.8517, F1 Micro: 0.3192, F1 Macro: 0.1086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3479, Accuracy: 0.8694, F1 Micro: 0.4829, F1 Macro: 0.2143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3027, Accuracy: 0.8784, F1 Micro: 0.5658, F1 Macro: 0.2869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2658, Accuracy: 0.8839, F1 Micro: 0.6145, F1 Macro: 0.3334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2469, Accuracy: 0.8866, F1 Micro: 0.6217, F1 Macro: 0.3598\n",
      "Epoch 9/10, Train Loss: 0.2197, Accuracy: 0.8874, F1 Micro: 0.6163, F1 Macro: 0.3726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1901, Accuracy: 0.8892, F1 Micro: 0.6459, F1 Macro: 0.3995\n",
      "Model 2 - Iteration 658: Accuracy: 0.8892, F1 Micro: 0.6459, F1 Macro: 0.3995\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.77      0.79      1137\n",
      "      Abusive       0.85      0.74      0.79      1008\n",
      "HS_Individual       0.67      0.59      0.63       729\n",
      "     HS_Group       0.58      0.39      0.47       408\n",
      "  HS_Religion       0.57      0.26      0.36       168\n",
      "      HS_Race       1.00      0.05      0.10       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.70      0.65      0.67       771\n",
      "      HS_Weak       0.64      0.55      0.59       681\n",
      "  HS_Moderate       0.52      0.33      0.40       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.72      0.58      0.65      5589\n",
      "    macro avg       0.53      0.36      0.40      5589\n",
      " weighted avg       0.69      0.58      0.62      5589\n",
      "  samples avg       0.38      0.33      0.33      5589\n",
      "\n",
      "Training completed in 59.91262722015381 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5358, Accuracy: 0.8314, F1 Micro: 0.0672, F1 Macro: 0.0272\n",
      "Epoch 2/10, Train Loss: 0.4175, Accuracy: 0.8304, F1 Micro: 0.0467, F1 Macro: 0.0193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3757, Accuracy: 0.8411, F1 Micro: 0.1786, F1 Macro: 0.0662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3605, Accuracy: 0.8554, F1 Micro: 0.3705, F1 Macro: 0.1174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.337, Accuracy: 0.8696, F1 Micro: 0.4624, F1 Macro: 0.2066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2963, Accuracy: 0.881, F1 Micro: 0.5847, F1 Macro: 0.3321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2561, Accuracy: 0.8853, F1 Micro: 0.6121, F1 Macro: 0.372\n",
      "Epoch 8/10, Train Loss: 0.2441, Accuracy: 0.8855, F1 Micro: 0.6013, F1 Macro: 0.3664\n",
      "Epoch 9/10, Train Loss: 0.2112, Accuracy: 0.8856, F1 Micro: 0.6001, F1 Macro: 0.3704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.184, Accuracy: 0.8886, F1 Micro: 0.6498, F1 Macro: 0.3948\n",
      "Model 3 - Iteration 658: Accuracy: 0.8886, F1 Micro: 0.6498, F1 Macro: 0.3948\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.77      0.78      1137\n",
      "      Abusive       0.82      0.76      0.79      1008\n",
      "HS_Individual       0.67      0.61      0.64       729\n",
      "     HS_Group       0.62      0.41      0.49       408\n",
      "  HS_Religion       0.57      0.23      0.33       168\n",
      "      HS_Race       1.00      0.01      0.02       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       0.00      0.00      0.00        55\n",
      "     HS_Other       0.68      0.66      0.67       771\n",
      "      HS_Weak       0.61      0.61      0.61       681\n",
      "  HS_Moderate       0.56      0.32      0.41       359\n",
      "    HS_Strong       0.00      0.00      0.00        97\n",
      "\n",
      "    micro avg       0.71      0.60      0.65      5589\n",
      "    macro avg       0.53      0.37      0.39      5589\n",
      " weighted avg       0.69      0.60      0.62      5589\n",
      "  samples avg       0.38      0.34      0.33      5589\n",
      "\n",
      "Training completed in 57.05467915534973 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8893, F1 Micro: 0.6469, F1 Macro: 0.393\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 988\n",
      "Acquired samples: 988\n",
      "Sampling duration: 136.3782947063446 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5001, Accuracy: 0.8414, F1 Micro: 0.3997, F1 Macro: 0.114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4169, Accuracy: 0.8691, F1 Micro: 0.5635, F1 Macro: 0.2571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3559, Accuracy: 0.8911, F1 Micro: 0.6332, F1 Macro: 0.3659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.307, Accuracy: 0.8952, F1 Micro: 0.6858, F1 Macro: 0.452\n",
      "Epoch 5/10, Train Loss: 0.26, Accuracy: 0.9034, F1 Micro: 0.6794, F1 Macro: 0.4735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2221, Accuracy: 0.9042, F1 Micro: 0.704, F1 Macro: 0.4953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1945, Accuracy: 0.9057, F1 Micro: 0.7125, F1 Macro: 0.5169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1602, Accuracy: 0.9056, F1 Micro: 0.7157, F1 Macro: 0.5251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1288, Accuracy: 0.9083, F1 Micro: 0.7174, F1 Macro: 0.5355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1124, Accuracy: 0.9049, F1 Micro: 0.7258, F1 Macro: 0.5509\n",
      "Model 1 - Iteration 1646: Accuracy: 0.9049, F1 Micro: 0.7258, F1 Macro: 0.5509\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1137\n",
      "      Abusive       0.83      0.87      0.85      1008\n",
      "HS_Individual       0.63      0.78      0.70       729\n",
      "     HS_Group       0.73      0.47      0.57       408\n",
      "  HS_Religion       0.71      0.47      0.56       168\n",
      "      HS_Race       0.74      0.60      0.66       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       1.00      0.02      0.04        55\n",
      "     HS_Other       0.72      0.80      0.76       771\n",
      "      HS_Weak       0.60      0.76      0.67       681\n",
      "  HS_Moderate       0.64      0.37      0.47       359\n",
      "    HS_Strong       0.85      0.36      0.51        97\n",
      "\n",
      "    micro avg       0.72      0.73      0.73      5589\n",
      "    macro avg       0.69      0.53      0.55      5589\n",
      " weighted avg       0.72      0.73      0.71      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 81.25996923446655 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5012, Accuracy: 0.8303, F1 Micro: 0.3884, F1 Macro: 0.1078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4189, Accuracy: 0.8687, F1 Micro: 0.5492, F1 Macro: 0.2459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3566, Accuracy: 0.8883, F1 Micro: 0.6563, F1 Macro: 0.3825\n",
      "Epoch 4/10, Train Loss: 0.3098, Accuracy: 0.8948, F1 Micro: 0.653, F1 Macro: 0.3949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2631, Accuracy: 0.9003, F1 Micro: 0.674, F1 Macro: 0.4667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2273, Accuracy: 0.902, F1 Micro: 0.7035, F1 Macro: 0.5041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1965, Accuracy: 0.9047, F1 Micro: 0.707, F1 Macro: 0.5328\n",
      "Epoch 8/10, Train Loss: 0.1558, Accuracy: 0.9043, F1 Micro: 0.7056, F1 Macro: 0.5226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1316, Accuracy: 0.9042, F1 Micro: 0.712, F1 Macro: 0.5237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1171, Accuracy: 0.903, F1 Micro: 0.721, F1 Macro: 0.5454\n",
      "Model 2 - Iteration 1646: Accuracy: 0.903, F1 Micro: 0.721, F1 Macro: 0.5454\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1137\n",
      "      Abusive       0.84      0.84      0.84      1008\n",
      "HS_Individual       0.64      0.78      0.70       729\n",
      "     HS_Group       0.69      0.49      0.57       408\n",
      "  HS_Religion       0.66      0.54      0.59       168\n",
      "      HS_Race       0.73      0.63      0.68       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       1.00      0.02      0.04        55\n",
      "     HS_Other       0.72      0.79      0.75       771\n",
      "      HS_Weak       0.59      0.75      0.66       681\n",
      "  HS_Moderate       0.59      0.37      0.45       359\n",
      "    HS_Strong       0.85      0.29      0.43        97\n",
      "\n",
      "    micro avg       0.72      0.72      0.72      5589\n",
      "    macro avg       0.68      0.53      0.55      5589\n",
      " weighted avg       0.72      0.72      0.71      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 79.55799341201782 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4951, Accuracy: 0.8406, F1 Micro: 0.4107, F1 Macro: 0.117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4155, Accuracy: 0.8717, F1 Micro: 0.5526, F1 Macro: 0.2487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3488, Accuracy: 0.8876, F1 Micro: 0.6583, F1 Macro: 0.3967\n",
      "Epoch 4/10, Train Loss: 0.306, Accuracy: 0.8943, F1 Micro: 0.6507, F1 Macro: 0.3956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2587, Accuracy: 0.9009, F1 Micro: 0.67, F1 Macro: 0.4494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2214, Accuracy: 0.9, F1 Micro: 0.6995, F1 Macro: 0.4777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1989, Accuracy: 0.9058, F1 Micro: 0.7045, F1 Macro: 0.5144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1591, Accuracy: 0.9025, F1 Micro: 0.7121, F1 Macro: 0.5229\n",
      "Epoch 9/10, Train Loss: 0.1318, Accuracy: 0.9044, F1 Micro: 0.6973, F1 Macro: 0.5227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.117, Accuracy: 0.9033, F1 Micro: 0.721, F1 Macro: 0.5444\n",
      "Model 3 - Iteration 1646: Accuracy: 0.9033, F1 Micro: 0.721, F1 Macro: 0.5444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1137\n",
      "      Abusive       0.83      0.85      0.84      1008\n",
      "HS_Individual       0.66      0.72      0.69       729\n",
      "     HS_Group       0.65      0.56      0.60       408\n",
      "  HS_Religion       0.68      0.50      0.58       168\n",
      "      HS_Race       0.69      0.66      0.68       119\n",
      "  HS_Physical       0.00      0.00      0.00        57\n",
      "    HS_Gender       1.00      0.02      0.04        55\n",
      "     HS_Other       0.72      0.79      0.75       771\n",
      "      HS_Weak       0.62      0.71      0.66       681\n",
      "  HS_Moderate       0.57      0.47      0.51       359\n",
      "    HS_Strong       0.92      0.23      0.36        97\n",
      "\n",
      "    micro avg       0.72      0.72      0.72      5589\n",
      "    macro avg       0.68      0.53      0.54      5589\n",
      " weighted avg       0.72      0.72      0.71      5589\n",
      "  samples avg       0.42      0.41      0.39      5589\n",
      "\n",
      "Training completed in 79.92710041999817 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8965, F1 Micro: 0.6848, F1 Macro: 0.47\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 889\n",
      "Acquired samples: 889\n",
      "Sampling duration: 122.50069260597229 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4744, Accuracy: 0.84, F1 Micro: 0.4097, F1 Macro: 0.1137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.388, Accuracy: 0.8798, F1 Micro: 0.5314, F1 Macro: 0.2662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.328, Accuracy: 0.8997, F1 Micro: 0.7021, F1 Macro: 0.4993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2749, Accuracy: 0.9093, F1 Micro: 0.7199, F1 Macro: 0.5325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2276, Accuracy: 0.9103, F1 Micro: 0.7213, F1 Macro: 0.5499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1825, Accuracy: 0.9125, F1 Micro: 0.7271, F1 Macro: 0.5562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1471, Accuracy: 0.9112, F1 Micro: 0.7374, F1 Macro: 0.594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1257, Accuracy: 0.9117, F1 Micro: 0.7394, F1 Macro: 0.6035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1089, Accuracy: 0.9125, F1 Micro: 0.746, F1 Macro: 0.6144\n",
      "Epoch 10/10, Train Loss: 0.0881, Accuracy: 0.9145, F1 Micro: 0.7425, F1 Macro: 0.6097\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9125, F1 Micro: 0.746, F1 Macro: 0.6144\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.84      0.83      1137\n",
      "      Abusive       0.86      0.89      0.87      1008\n",
      "HS_Individual       0.66      0.79      0.72       729\n",
      "     HS_Group       0.76      0.49      0.60       408\n",
      "  HS_Religion       0.69      0.64      0.66       168\n",
      "      HS_Race       0.71      0.71      0.71       119\n",
      "  HS_Physical       0.29      0.04      0.06        57\n",
      "    HS_Gender       0.60      0.11      0.18        55\n",
      "     HS_Other       0.77      0.74      0.76       771\n",
      "      HS_Weak       0.62      0.76      0.68       681\n",
      "  HS_Moderate       0.72      0.41      0.52       359\n",
      "    HS_Strong       0.82      0.72      0.77        97\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5589\n",
      "    macro avg       0.69      0.60      0.61      5589\n",
      " weighted avg       0.75      0.74      0.74      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 101.47688102722168 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4754, Accuracy: 0.8322, F1 Micro: 0.4226, F1 Macro: 0.1327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3874, Accuracy: 0.8826, F1 Micro: 0.5581, F1 Macro: 0.3095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3318, Accuracy: 0.8978, F1 Micro: 0.6932, F1 Macro: 0.4569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2757, Accuracy: 0.9079, F1 Micro: 0.7135, F1 Macro: 0.5128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2282, Accuracy: 0.9092, F1 Micro: 0.7284, F1 Macro: 0.554\n",
      "Epoch 6/10, Train Loss: 0.1847, Accuracy: 0.9092, F1 Micro: 0.713, F1 Macro: 0.5272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1488, Accuracy: 0.9117, F1 Micro: 0.736, F1 Macro: 0.5716\n",
      "Epoch 8/10, Train Loss: 0.1291, Accuracy: 0.914, F1 Micro: 0.7329, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1065, Accuracy: 0.9118, F1 Micro: 0.7455, F1 Macro: 0.6172\n",
      "Epoch 10/10, Train Loss: 0.0903, Accuracy: 0.914, F1 Micro: 0.737, F1 Macro: 0.6068\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9118, F1 Micro: 0.7455, F1 Macro: 0.6172\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1137\n",
      "      Abusive       0.82      0.90      0.86      1008\n",
      "HS_Individual       0.69      0.75      0.72       729\n",
      "     HS_Group       0.70      0.59      0.64       408\n",
      "  HS_Religion       0.63      0.69      0.66       168\n",
      "      HS_Race       0.71      0.71      0.71       119\n",
      "  HS_Physical       0.23      0.05      0.09        57\n",
      "    HS_Gender       0.55      0.11      0.18        55\n",
      "     HS_Other       0.80      0.73      0.76       771\n",
      "      HS_Weak       0.65      0.72      0.68       681\n",
      "  HS_Moderate       0.62      0.51      0.56       359\n",
      "    HS_Strong       0.79      0.65      0.71        97\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5589\n",
      "    macro avg       0.67      0.61      0.62      5589\n",
      " weighted avg       0.74      0.75      0.74      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 98.31337475776672 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4712, Accuracy: 0.8411, F1 Micro: 0.4401, F1 Macro: 0.1464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3819, Accuracy: 0.8827, F1 Micro: 0.5487, F1 Macro: 0.2943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3248, Accuracy: 0.8992, F1 Micro: 0.6904, F1 Macro: 0.4554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2773, Accuracy: 0.9064, F1 Micro: 0.7238, F1 Macro: 0.5216\n",
      "Epoch 5/10, Train Loss: 0.23, Accuracy: 0.9099, F1 Micro: 0.7231, F1 Macro: 0.5352\n",
      "Epoch 6/10, Train Loss: 0.1813, Accuracy: 0.9118, F1 Micro: 0.7136, F1 Macro: 0.5272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1509, Accuracy: 0.911, F1 Micro: 0.7366, F1 Macro: 0.5688\n",
      "Epoch 8/10, Train Loss: 0.1268, Accuracy: 0.9121, F1 Micro: 0.7265, F1 Macro: 0.5871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.107, Accuracy: 0.912, F1 Micro: 0.7405, F1 Macro: 0.6073\n",
      "Epoch 10/10, Train Loss: 0.0902, Accuracy: 0.9157, F1 Micro: 0.737, F1 Macro: 0.6066\n",
      "Model 3 - Iteration 2535: Accuracy: 0.912, F1 Micro: 0.7405, F1 Macro: 0.6073\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.82      0.83      1137\n",
      "      Abusive       0.84      0.90      0.87      1008\n",
      "HS_Individual       0.70      0.71      0.71       729\n",
      "     HS_Group       0.70      0.58      0.64       408\n",
      "  HS_Religion       0.66      0.68      0.67       168\n",
      "      HS_Race       0.70      0.70      0.70       119\n",
      "  HS_Physical       0.30      0.05      0.09        57\n",
      "    HS_Gender       0.60      0.11      0.18        55\n",
      "     HS_Other       0.79      0.72      0.75       771\n",
      "      HS_Weak       0.65      0.68      0.67       681\n",
      "  HS_Moderate       0.64      0.52      0.57       359\n",
      "    HS_Strong       0.80      0.51      0.62        97\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5589\n",
      "    macro avg       0.68      0.58      0.61      5589\n",
      " weighted avg       0.75      0.72      0.73      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 96.19956684112549 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.9017, F1 Micro: 0.7045, F1 Macro: 0.5176\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 800\n",
      "Acquired samples: 800\n",
      "Sampling duration: 110.77386593818665 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4601, Accuracy: 0.8588, F1 Micro: 0.5248, F1 Macro: 0.2197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3631, Accuracy: 0.9007, F1 Micro: 0.676, F1 Macro: 0.4652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2958, Accuracy: 0.9051, F1 Micro: 0.73, F1 Macro: 0.5439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2448, Accuracy: 0.916, F1 Micro: 0.7405, F1 Macro: 0.5765\n",
      "Epoch 5/10, Train Loss: 0.1939, Accuracy: 0.916, F1 Micro: 0.737, F1 Macro: 0.5744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1616, Accuracy: 0.9177, F1 Micro: 0.742, F1 Macro: 0.5875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1363, Accuracy: 0.9178, F1 Micro: 0.7447, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1022, Accuracy: 0.9154, F1 Micro: 0.7572, F1 Macro: 0.6245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0923, Accuracy: 0.9201, F1 Micro: 0.7628, F1 Macro: 0.645\n",
      "Epoch 10/10, Train Loss: 0.0805, Accuracy: 0.9193, F1 Micro: 0.7568, F1 Macro: 0.6469\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9201, F1 Micro: 0.7628, F1 Macro: 0.645\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1137\n",
      "      Abusive       0.88      0.88      0.88      1008\n",
      "HS_Individual       0.74      0.73      0.73       729\n",
      "     HS_Group       0.71      0.64      0.67       408\n",
      "  HS_Religion       0.70      0.68      0.69       168\n",
      "      HS_Race       0.70      0.76      0.73       119\n",
      "  HS_Physical       0.75      0.11      0.18        57\n",
      "    HS_Gender       0.50      0.11      0.18        55\n",
      "     HS_Other       0.81      0.73      0.77       771\n",
      "      HS_Weak       0.71      0.69      0.70       681\n",
      "  HS_Moderate       0.63      0.56      0.59       359\n",
      "    HS_Strong       0.80      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.79      0.74      0.76      5589\n",
      "    macro avg       0.73      0.62      0.64      5589\n",
      " weighted avg       0.78      0.74      0.76      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 117.03753805160522 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4572, Accuracy: 0.8532, F1 Micro: 0.5345, F1 Macro: 0.2355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3657, Accuracy: 0.9, F1 Micro: 0.6768, F1 Macro: 0.4588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2974, Accuracy: 0.9053, F1 Micro: 0.7314, F1 Macro: 0.5447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2458, Accuracy: 0.9147, F1 Micro: 0.7381, F1 Macro: 0.5639\n",
      "Epoch 5/10, Train Loss: 0.1908, Accuracy: 0.9153, F1 Micro: 0.7304, F1 Macro: 0.5538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1562, Accuracy: 0.9177, F1 Micro: 0.7433, F1 Macro: 0.5892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1283, Accuracy: 0.9167, F1 Micro: 0.7474, F1 Macro: 0.6018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0998, Accuracy: 0.9166, F1 Micro: 0.7482, F1 Macro: 0.6183\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9164, F1 Micro: 0.7475, F1 Macro: 0.6345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0785, Accuracy: 0.9168, F1 Micro: 0.7487, F1 Macro: 0.6258\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9168, F1 Micro: 0.7487, F1 Macro: 0.6258\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1137\n",
      "      Abusive       0.88      0.87      0.87      1008\n",
      "HS_Individual       0.72      0.69      0.70       729\n",
      "     HS_Group       0.70      0.60      0.65       408\n",
      "  HS_Religion       0.73      0.57      0.64       168\n",
      "      HS_Race       0.73      0.76      0.74       119\n",
      "  HS_Physical       0.57      0.07      0.12        57\n",
      "    HS_Gender       0.83      0.09      0.16        55\n",
      "     HS_Other       0.79      0.74      0.77       771\n",
      "      HS_Weak       0.71      0.63      0.67       681\n",
      "  HS_Moderate       0.66      0.49      0.56       359\n",
      "    HS_Strong       0.75      0.81      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5589\n",
      "    macro avg       0.74      0.59      0.63      5589\n",
      " weighted avg       0.78      0.72      0.74      5589\n",
      "  samples avg       0.44      0.41      0.41      5589\n",
      "\n",
      "Training completed in 116.86362218856812 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4563, Accuracy: 0.8531, F1 Micro: 0.5524, F1 Macro: 0.2501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3621, Accuracy: 0.899, F1 Micro: 0.6704, F1 Macro: 0.447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2979, Accuracy: 0.9067, F1 Micro: 0.7264, F1 Macro: 0.5301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2497, Accuracy: 0.9141, F1 Micro: 0.7367, F1 Macro: 0.5527\n",
      "Epoch 5/10, Train Loss: 0.1976, Accuracy: 0.9133, F1 Micro: 0.7222, F1 Macro: 0.5191\n",
      "Epoch 6/10, Train Loss: 0.164, Accuracy: 0.9158, F1 Micro: 0.7311, F1 Macro: 0.5604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1352, Accuracy: 0.9165, F1 Micro: 0.7401, F1 Macro: 0.5881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1044, Accuracy: 0.9148, F1 Micro: 0.7476, F1 Macro: 0.6282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0905, Accuracy: 0.9181, F1 Micro: 0.7529, F1 Macro: 0.6339\n",
      "Epoch 10/10, Train Loss: 0.0824, Accuracy: 0.9202, F1 Micro: 0.7529, F1 Macro: 0.6169\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9181, F1 Micro: 0.7529, F1 Macro: 0.6339\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.81      0.83      1137\n",
      "      Abusive       0.88      0.86      0.87      1008\n",
      "HS_Individual       0.72      0.72      0.72       729\n",
      "     HS_Group       0.73      0.57      0.64       408\n",
      "  HS_Religion       0.70      0.62      0.66       168\n",
      "      HS_Race       0.74      0.69      0.71       119\n",
      "  HS_Physical       0.54      0.12      0.20        57\n",
      "    HS_Gender       0.67      0.15      0.24        55\n",
      "     HS_Other       0.82      0.74      0.78       771\n",
      "      HS_Weak       0.69      0.70      0.69       681\n",
      "  HS_Moderate       0.69      0.48      0.57       359\n",
      "    HS_Strong       0.77      0.64      0.70        97\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5589\n",
      "    macro avg       0.73      0.59      0.63      5589\n",
      " weighted avg       0.78      0.72      0.75      5589\n",
      "  samples avg       0.44      0.41      0.41      5589\n",
      "\n",
      "Training completed in 115.91583514213562 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.9059, F1 Micro: 0.7171, F1 Macro: 0.5469\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 720\n",
      "Acquired samples: 720\n",
      "Sampling duration: 99.59265732765198 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4467, Accuracy: 0.875, F1 Micro: 0.5228, F1 Macro: 0.234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3394, Accuracy: 0.8987, F1 Micro: 0.7061, F1 Macro: 0.5012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2827, Accuracy: 0.9106, F1 Micro: 0.7152, F1 Macro: 0.5662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2378, Accuracy: 0.9164, F1 Micro: 0.7436, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1916, Accuracy: 0.9178, F1 Micro: 0.7641, F1 Macro: 0.6128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.15, Accuracy: 0.9198, F1 Micro: 0.7653, F1 Macro: 0.6277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1214, Accuracy: 0.9178, F1 Micro: 0.7656, F1 Macro: 0.6329\n",
      "Epoch 8/10, Train Loss: 0.1072, Accuracy: 0.9143, F1 Micro: 0.7601, F1 Macro: 0.6364\n",
      "Epoch 9/10, Train Loss: 0.0883, Accuracy: 0.9187, F1 Micro: 0.7654, F1 Macro: 0.6485\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.9216, F1 Micro: 0.7608, F1 Macro: 0.6405\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9178, F1 Micro: 0.7656, F1 Macro: 0.6329\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1137\n",
      "      Abusive       0.85      0.91      0.88      1008\n",
      "HS_Individual       0.72      0.72      0.72       729\n",
      "     HS_Group       0.67      0.68      0.68       408\n",
      "  HS_Religion       0.74      0.62      0.68       168\n",
      "      HS_Race       0.74      0.73      0.73       119\n",
      "  HS_Physical       1.00      0.02      0.03        57\n",
      "    HS_Gender       0.62      0.09      0.16        55\n",
      "     HS_Other       0.74      0.83      0.78       771\n",
      "      HS_Weak       0.69      0.71      0.70       681\n",
      "  HS_Moderate       0.62      0.60      0.61       359\n",
      "    HS_Strong       0.81      0.75      0.78        97\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5589\n",
      "    macro avg       0.75      0.63      0.63      5589\n",
      " weighted avg       0.76      0.77      0.76      5589\n",
      "  samples avg       0.43      0.44      0.42      5589\n",
      "\n",
      "Training completed in 131.1662802696228 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4487, Accuracy: 0.8697, F1 Micro: 0.5302, F1 Macro: 0.2516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3429, Accuracy: 0.9008, F1 Micro: 0.7045, F1 Macro: 0.4845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2893, Accuracy: 0.9091, F1 Micro: 0.733, F1 Macro: 0.5792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2365, Accuracy: 0.9156, F1 Micro: 0.734, F1 Macro: 0.5675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1874, Accuracy: 0.9198, F1 Micro: 0.7582, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1535, Accuracy: 0.9188, F1 Micro: 0.7639, F1 Macro: 0.6361\n",
      "Epoch 7/10, Train Loss: 0.1223, Accuracy: 0.9209, F1 Micro: 0.7635, F1 Macro: 0.627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1027, Accuracy: 0.9187, F1 Micro: 0.7665, F1 Macro: 0.6367\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9205, F1 Micro: 0.7659, F1 Macro: 0.6458\n",
      "Epoch 10/10, Train Loss: 0.0728, Accuracy: 0.9209, F1 Micro: 0.7628, F1 Macro: 0.6517\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9187, F1 Micro: 0.7665, F1 Macro: 0.6367\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1137\n",
      "      Abusive       0.84      0.92      0.88      1008\n",
      "HS_Individual       0.69      0.77      0.73       729\n",
      "     HS_Group       0.72      0.62      0.67       408\n",
      "  HS_Religion       0.73      0.62      0.67       168\n",
      "      HS_Race       0.80      0.60      0.68       119\n",
      "  HS_Physical       1.00      0.05      0.10        57\n",
      "    HS_Gender       0.78      0.13      0.22        55\n",
      "     HS_Other       0.77      0.80      0.79       771\n",
      "      HS_Weak       0.67      0.75      0.71       681\n",
      "  HS_Moderate       0.65      0.55      0.59       359\n",
      "    HS_Strong       0.81      0.71      0.76        97\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5589\n",
      "    macro avg       0.77      0.62      0.64      5589\n",
      " weighted avg       0.76      0.77      0.76      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 131.11515092849731 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4447, Accuracy: 0.8727, F1 Micro: 0.5232, F1 Macro: 0.2391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3419, Accuracy: 0.9017, F1 Micro: 0.7047, F1 Macro: 0.482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2884, Accuracy: 0.9067, F1 Micro: 0.7087, F1 Macro: 0.5483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2402, Accuracy: 0.9156, F1 Micro: 0.742, F1 Macro: 0.559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1917, Accuracy: 0.9171, F1 Micro: 0.7571, F1 Macro: 0.5946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1494, Accuracy: 0.92, F1 Micro: 0.7594, F1 Macro: 0.65\n",
      "Epoch 7/10, Train Loss: 0.1258, Accuracy: 0.9191, F1 Micro: 0.754, F1 Macro: 0.6306\n",
      "Epoch 8/10, Train Loss: 0.1018, Accuracy: 0.9198, F1 Micro: 0.7538, F1 Macro: 0.6245\n",
      "Epoch 9/10, Train Loss: 0.0847, Accuracy: 0.9167, F1 Micro: 0.7574, F1 Macro: 0.6461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9204, F1 Micro: 0.7603, F1 Macro: 0.6599\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9204, F1 Micro: 0.7603, F1 Macro: 0.6599\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.84      1137\n",
      "      Abusive       0.89      0.86      0.87      1008\n",
      "HS_Individual       0.73      0.71      0.72       729\n",
      "     HS_Group       0.73      0.59      0.65       408\n",
      "  HS_Religion       0.74      0.63      0.68       168\n",
      "      HS_Race       0.78      0.71      0.75       119\n",
      "  HS_Physical       0.64      0.16      0.25        57\n",
      "    HS_Gender       0.85      0.20      0.32        55\n",
      "     HS_Other       0.81      0.76      0.78       771\n",
      "      HS_Weak       0.69      0.69      0.69       681\n",
      "  HS_Moderate       0.66      0.50      0.57       359\n",
      "    HS_Strong       0.81      0.75      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.73      0.76      5589\n",
      "    macro avg       0.77      0.62      0.66      5589\n",
      " weighted avg       0.79      0.73      0.76      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 130.80222082138062 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9085, F1 Micro: 0.7265, F1 Macro: 0.5662\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 648\n",
      "Acquired samples: 648\n",
      "Sampling duration: 89.29351139068604 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4401, Accuracy: 0.8871, F1 Micro: 0.6337, F1 Macro: 0.3246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3333, Accuracy: 0.9078, F1 Micro: 0.7169, F1 Macro: 0.539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2711, Accuracy: 0.9177, F1 Micro: 0.748, F1 Macro: 0.5849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2182, Accuracy: 0.9173, F1 Micro: 0.7591, F1 Macro: 0.6165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1804, Accuracy: 0.9209, F1 Micro: 0.7608, F1 Macro: 0.634\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9178, F1 Micro: 0.759, F1 Macro: 0.6311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9194, F1 Micro: 0.7637, F1 Macro: 0.6483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0961, Accuracy: 0.9173, F1 Micro: 0.7645, F1 Macro: 0.6502\n",
      "Epoch 9/10, Train Loss: 0.0789, Accuracy: 0.9216, F1 Micro: 0.761, F1 Macro: 0.6707\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9202, F1 Micro: 0.7617, F1 Macro: 0.6768\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9173, F1 Micro: 0.7645, F1 Macro: 0.6502\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1137\n",
      "      Abusive       0.88      0.88      0.88      1008\n",
      "HS_Individual       0.66      0.83      0.73       729\n",
      "     HS_Group       0.79      0.50      0.61       408\n",
      "  HS_Religion       0.77      0.58      0.66       168\n",
      "      HS_Race       0.74      0.70      0.72       119\n",
      "  HS_Physical       0.64      0.16      0.25        57\n",
      "    HS_Gender       0.89      0.15      0.25        55\n",
      "     HS_Other       0.75      0.84      0.79       771\n",
      "      HS_Weak       0.63      0.82      0.71       681\n",
      "  HS_Moderate       0.70      0.44      0.54       359\n",
      "    HS_Strong       0.82      0.77      0.80        97\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5589\n",
      "    macro avg       0.76      0.63      0.65      5589\n",
      " weighted avg       0.76      0.77      0.76      5589\n",
      "  samples avg       0.43      0.44      0.42      5589\n",
      "\n",
      "Training completed in 145.68007898330688 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4427, Accuracy: 0.881, F1 Micro: 0.6096, F1 Macro: 0.3002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3363, Accuracy: 0.9068, F1 Micro: 0.7076, F1 Macro: 0.5047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2737, Accuracy: 0.9164, F1 Micro: 0.7423, F1 Macro: 0.5849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2231, Accuracy: 0.9127, F1 Micro: 0.7549, F1 Macro: 0.6143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1792, Accuracy: 0.9203, F1 Micro: 0.7605, F1 Macro: 0.6144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1405, Accuracy: 0.9226, F1 Micro: 0.767, F1 Macro: 0.6301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1157, Accuracy: 0.9212, F1 Micro: 0.7703, F1 Macro: 0.6458\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.9209, F1 Micro: 0.7578, F1 Macro: 0.6406\n",
      "Epoch 9/10, Train Loss: 0.0793, Accuracy: 0.922, F1 Micro: 0.7675, F1 Macro: 0.6619\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9217, F1 Micro: 0.7675, F1 Macro: 0.6647\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9212, F1 Micro: 0.7703, F1 Macro: 0.6458\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.87      0.90      0.88      1008\n",
      "HS_Individual       0.72      0.76      0.74       729\n",
      "     HS_Group       0.72      0.62      0.67       408\n",
      "  HS_Religion       0.75      0.58      0.66       168\n",
      "      HS_Race       0.80      0.65      0.72       119\n",
      "  HS_Physical       0.44      0.07      0.12        57\n",
      "    HS_Gender       0.78      0.13      0.22        55\n",
      "     HS_Other       0.78      0.81      0.79       771\n",
      "      HS_Weak       0.69      0.73      0.71       681\n",
      "  HS_Moderate       0.66      0.55      0.60       359\n",
      "    HS_Strong       0.81      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.74      0.62      0.65      5589\n",
      " weighted avg       0.77      0.76      0.76      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 145.59767270088196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4378, Accuracy: 0.8834, F1 Micro: 0.6206, F1 Macro: 0.3142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3326, Accuracy: 0.9048, F1 Micro: 0.7173, F1 Macro: 0.5167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.274, Accuracy: 0.916, F1 Micro: 0.7358, F1 Macro: 0.5614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2181, Accuracy: 0.913, F1 Micro: 0.755, F1 Macro: 0.6002\n",
      "Epoch 5/10, Train Loss: 0.1819, Accuracy: 0.9185, F1 Micro: 0.7482, F1 Macro: 0.6108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1462, Accuracy: 0.9163, F1 Micro: 0.7623, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1175, Accuracy: 0.9212, F1 Micro: 0.77, F1 Macro: 0.6631\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9178, F1 Micro: 0.7579, F1 Macro: 0.6401\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9225, F1 Micro: 0.7633, F1 Macro: 0.6682\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9207, F1 Micro: 0.7634, F1 Macro: 0.6693\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9212, F1 Micro: 0.77, F1 Macro: 0.6631\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1137\n",
      "      Abusive       0.85      0.91      0.88      1008\n",
      "HS_Individual       0.72      0.76      0.74       729\n",
      "     HS_Group       0.75      0.60      0.67       408\n",
      "  HS_Religion       0.74      0.60      0.66       168\n",
      "      HS_Race       0.80      0.63      0.70       119\n",
      "  HS_Physical       0.62      0.18      0.27        57\n",
      "    HS_Gender       0.85      0.20      0.32        55\n",
      "     HS_Other       0.79      0.80      0.79       771\n",
      "      HS_Weak       0.67      0.74      0.70       681\n",
      "  HS_Moderate       0.70      0.52      0.59       359\n",
      "    HS_Strong       0.82      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5589\n",
      "    macro avg       0.76      0.63      0.66      5589\n",
      " weighted avg       0.78      0.76      0.76      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 144.08345580101013 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9104, F1 Micro: 0.7335, F1 Macro: 0.5807\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 81.34097695350647 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4346, Accuracy: 0.8901, F1 Micro: 0.6521, F1 Macro: 0.3921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3212, Accuracy: 0.9103, F1 Micro: 0.7122, F1 Macro: 0.5176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2594, Accuracy: 0.918, F1 Micro: 0.7455, F1 Macro: 0.6036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9199, F1 Micro: 0.7695, F1 Macro: 0.635\n",
      "Epoch 5/10, Train Loss: 0.1678, Accuracy: 0.9212, F1 Micro: 0.7583, F1 Macro: 0.6492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1394, Accuracy: 0.9222, F1 Micro: 0.7703, F1 Macro: 0.6538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1116, Accuracy: 0.9241, F1 Micro: 0.7764, F1 Macro: 0.6751\n",
      "Epoch 8/10, Train Loss: 0.0932, Accuracy: 0.9236, F1 Micro: 0.7744, F1 Macro: 0.6936\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9254, F1 Micro: 0.7729, F1 Macro: 0.6975\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9232, F1 Micro: 0.7736, F1 Macro: 0.6917\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9241, F1 Micro: 0.7764, F1 Macro: 0.6751\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.86      0.88      1008\n",
      "HS_Individual       0.70      0.79      0.74       729\n",
      "     HS_Group       0.77      0.55      0.65       408\n",
      "  HS_Religion       0.77      0.68      0.72       168\n",
      "      HS_Race       0.75      0.75      0.75       119\n",
      "  HS_Physical       0.65      0.23      0.34        57\n",
      "    HS_Gender       0.90      0.16      0.28        55\n",
      "     HS_Other       0.81      0.80      0.80       771\n",
      "      HS_Weak       0.68      0.77      0.72       681\n",
      "  HS_Moderate       0.73      0.47      0.57       359\n",
      "    HS_Strong       0.78      0.80      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5589\n",
      "    macro avg       0.78      0.64      0.68      5589\n",
      " weighted avg       0.79      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 156.4123570919037 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4351, Accuracy: 0.8856, F1 Micro: 0.6458, F1 Macro: 0.349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3267, Accuracy: 0.9102, F1 Micro: 0.7101, F1 Macro: 0.5511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2627, Accuracy: 0.9181, F1 Micro: 0.7525, F1 Macro: 0.6013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2097, Accuracy: 0.9161, F1 Micro: 0.7645, F1 Macro: 0.6157\n",
      "Epoch 5/10, Train Loss: 0.1685, Accuracy: 0.92, F1 Micro: 0.7642, F1 Macro: 0.6493\n",
      "Epoch 6/10, Train Loss: 0.1363, Accuracy: 0.919, F1 Micro: 0.7634, F1 Macro: 0.6422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9227, F1 Micro: 0.7745, F1 Macro: 0.6737\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9184, F1 Micro: 0.769, F1 Macro: 0.6957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9234, F1 Micro: 0.7753, F1 Macro: 0.6924\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9243, F1 Micro: 0.7683, F1 Macro: 0.6885\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9234, F1 Micro: 0.7753, F1 Macro: 0.6924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1137\n",
      "      Abusive       0.89      0.88      0.88      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.70      0.64      0.67       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.72      0.74      0.73       119\n",
      "  HS_Physical       0.72      0.23      0.35        57\n",
      "    HS_Gender       0.70      0.35      0.46        55\n",
      "     HS_Other       0.81      0.78      0.80       771\n",
      "      HS_Weak       0.71      0.72      0.72       681\n",
      "  HS_Moderate       0.65      0.54      0.59       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5589\n",
      "    macro avg       0.75      0.67      0.69      5589\n",
      " weighted avg       0.79      0.76      0.77      5589\n",
      "  samples avg       0.44      0.43      0.42      5589\n",
      "\n",
      "Training completed in 156.2214241027832 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4313, Accuracy: 0.8868, F1 Micro: 0.6639, F1 Macro: 0.4191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3234, Accuracy: 0.9098, F1 Micro: 0.7015, F1 Macro: 0.5162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2615, Accuracy: 0.9161, F1 Micro: 0.7379, F1 Macro: 0.5703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2125, Accuracy: 0.9166, F1 Micro: 0.756, F1 Macro: 0.6046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1697, Accuracy: 0.9203, F1 Micro: 0.7598, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1353, Accuracy: 0.9174, F1 Micro: 0.7604, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9217, F1 Micro: 0.7692, F1 Macro: 0.6733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0928, Accuracy: 0.9226, F1 Micro: 0.7704, F1 Macro: 0.6864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9211, F1 Micro: 0.7709, F1 Macro: 0.6993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9247, F1 Micro: 0.7722, F1 Macro: 0.6833\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9247, F1 Micro: 0.7722, F1 Macro: 0.6833\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1137\n",
      "      Abusive       0.91      0.85      0.88      1008\n",
      "HS_Individual       0.73      0.74      0.74       729\n",
      "     HS_Group       0.76      0.57      0.65       408\n",
      "  HS_Religion       0.75      0.65      0.70       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.71      0.26      0.38        57\n",
      "    HS_Gender       0.65      0.24      0.35        55\n",
      "     HS_Other       0.83      0.75      0.79       771\n",
      "      HS_Weak       0.71      0.72      0.71       681\n",
      "  HS_Moderate       0.72      0.49      0.58       359\n",
      "    HS_Strong       0.78      0.78      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5589\n",
      "    macro avg       0.77      0.64      0.68      5589\n",
      " weighted avg       0.81      0.74      0.77      5589\n",
      "  samples avg       0.42      0.41      0.40      5589\n",
      "\n",
      "Training completed in 163.28667950630188 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9123, F1 Micro: 0.7393, F1 Macro: 0.5954\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 525\n",
      "Acquired samples: 525\n",
      "Sampling duration: 73.54701614379883 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4283, Accuracy: 0.8938, F1 Micro: 0.648, F1 Macro: 0.4074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3115, Accuracy: 0.9131, F1 Micro: 0.7174, F1 Macro: 0.5489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2457, Accuracy: 0.9198, F1 Micro: 0.7624, F1 Macro: 0.6123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.9223, F1 Micro: 0.7706, F1 Macro: 0.6326\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.9251, F1 Micro: 0.7667, F1 Macro: 0.6551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1323, Accuracy: 0.9238, F1 Micro: 0.7752, F1 Macro: 0.6742\n",
      "Epoch 7/10, Train Loss: 0.1076, Accuracy: 0.9216, F1 Micro: 0.7709, F1 Macro: 0.6896\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9221, F1 Micro: 0.7734, F1 Macro: 0.7031\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9252, F1 Micro: 0.7696, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9263, F1 Micro: 0.7761, F1 Macro: 0.7135\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9263, F1 Micro: 0.7761, F1 Macro: 0.7135\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.81      0.85      1137\n",
      "      Abusive       0.90      0.88      0.89      1008\n",
      "HS_Individual       0.80      0.66      0.72       729\n",
      "     HS_Group       0.69      0.68      0.68       408\n",
      "  HS_Religion       0.75      0.68      0.71       168\n",
      "      HS_Race       0.71      0.80      0.75       119\n",
      "  HS_Physical       0.76      0.33      0.46        57\n",
      "    HS_Gender       0.66      0.45      0.54        55\n",
      "     HS_Other       0.87      0.73      0.79       771\n",
      "      HS_Weak       0.78      0.63      0.70       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.81      0.85      0.83        97\n",
      "\n",
      "    micro avg       0.82      0.74      0.78      5589\n",
      "    macro avg       0.77      0.68      0.71      5589\n",
      " weighted avg       0.82      0.74      0.77      5589\n",
      "  samples avg       0.44      0.42      0.41      5589\n",
      "\n",
      "Training completed in 168.15073990821838 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4298, Accuracy: 0.8885, F1 Micro: 0.6105, F1 Macro: 0.3644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3178, Accuracy: 0.9117, F1 Micro: 0.7087, F1 Macro: 0.5443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2506, Accuracy: 0.9174, F1 Micro: 0.7607, F1 Macro: 0.6158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.205, Accuracy: 0.9218, F1 Micro: 0.7652, F1 Macro: 0.6192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1632, Accuracy: 0.9214, F1 Micro: 0.767, F1 Macro: 0.6502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1339, Accuracy: 0.9258, F1 Micro: 0.7764, F1 Macro: 0.6697\n",
      "Epoch 7/10, Train Loss: 0.104, Accuracy: 0.9234, F1 Micro: 0.7678, F1 Macro: 0.6769\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.9227, F1 Micro: 0.7663, F1 Macro: 0.6953\n",
      "Epoch 9/10, Train Loss: 0.0769, Accuracy: 0.9223, F1 Micro: 0.7726, F1 Macro: 0.7028\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9227, F1 Micro: 0.7728, F1 Macro: 0.7086\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9258, F1 Micro: 0.7764, F1 Macro: 0.6697\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.82      0.85      1137\n",
      "      Abusive       0.88      0.89      0.89      1008\n",
      "HS_Individual       0.78      0.71      0.74       729\n",
      "     HS_Group       0.72      0.65      0.68       408\n",
      "  HS_Religion       0.78      0.65      0.71       168\n",
      "      HS_Race       0.80      0.66      0.72       119\n",
      "  HS_Physical       0.58      0.19      0.29        57\n",
      "    HS_Gender       0.73      0.15      0.24        55\n",
      "     HS_Other       0.82      0.77      0.79       771\n",
      "      HS_Weak       0.75      0.68      0.71       681\n",
      "  HS_Moderate       0.67      0.58      0.62       359\n",
      "    HS_Strong       0.79      0.78      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.74      0.78      5589\n",
      "    macro avg       0.76      0.63      0.67      5589\n",
      " weighted avg       0.81      0.74      0.77      5589\n",
      "  samples avg       0.44      0.42      0.42      5589\n",
      "\n",
      "Training completed in 167.9879150390625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4255, Accuracy: 0.8916, F1 Micro: 0.625, F1 Macro: 0.3889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3169, Accuracy: 0.9136, F1 Micro: 0.7217, F1 Macro: 0.5402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2499, Accuracy: 0.9175, F1 Micro: 0.761, F1 Macro: 0.6058\n",
      "Epoch 4/10, Train Loss: 0.2039, Accuracy: 0.9219, F1 Micro: 0.7605, F1 Macro: 0.6255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1661, Accuracy: 0.9224, F1 Micro: 0.7657, F1 Macro: 0.6591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9227, F1 Micro: 0.7735, F1 Macro: 0.6718\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.9229, F1 Micro: 0.7694, F1 Macro: 0.6875\n",
      "Epoch 8/10, Train Loss: 0.0877, Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9215, F1 Micro: 0.7737, F1 Macro: 0.7019\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.92, F1 Micro: 0.7701, F1 Macro: 0.7071\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9215, F1 Micro: 0.7737, F1 Macro: 0.7019\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1137\n",
      "      Abusive       0.88      0.89      0.89      1008\n",
      "HS_Individual       0.74      0.71      0.73       729\n",
      "     HS_Group       0.66      0.71      0.68       408\n",
      "  HS_Religion       0.74      0.62      0.68       168\n",
      "      HS_Race       0.78      0.75      0.76       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.62      0.42      0.50        55\n",
      "     HS_Other       0.77      0.82      0.80       771\n",
      "      HS_Weak       0.71      0.68      0.70       681\n",
      "  HS_Moderate       0.61      0.64      0.62       359\n",
      "    HS_Strong       0.81      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5589\n",
      "    macro avg       0.74      0.68      0.70      5589\n",
      " weighted avg       0.77      0.77      0.77      5589\n",
      "  samples avg       0.44      0.44      0.42      5589\n",
      "\n",
      "Training completed in 168.55449390411377 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9139, F1 Micro: 0.7439, F1 Macro: 0.6078\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 473\n",
      "Acquired samples: 473\n",
      "Sampling duration: 65.5897068977356 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.419, Accuracy: 0.8973, F1 Micro: 0.6679, F1 Macro: 0.4388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3068, Accuracy: 0.9139, F1 Micro: 0.7502, F1 Macro: 0.5999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.247, Accuracy: 0.9234, F1 Micro: 0.7658, F1 Macro: 0.6119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.9227, F1 Micro: 0.7742, F1 Macro: 0.6403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.9243, F1 Micro: 0.7784, F1 Macro: 0.678\n",
      "Epoch 6/10, Train Loss: 0.1187, Accuracy: 0.9242, F1 Micro: 0.776, F1 Macro: 0.6768\n",
      "Epoch 7/10, Train Loss: 0.1039, Accuracy: 0.9259, F1 Micro: 0.7756, F1 Macro: 0.7007\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9236, F1 Micro: 0.7742, F1 Macro: 0.7041\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9248, F1 Micro: 0.774, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9232, F1 Micro: 0.779, F1 Macro: 0.7189\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9232, F1 Micro: 0.779, F1 Macro: 0.7189\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.72      0.75      0.74       729\n",
      "     HS_Group       0.69      0.62      0.65       408\n",
      "  HS_Religion       0.70      0.76      0.73       168\n",
      "      HS_Race       0.70      0.78      0.74       119\n",
      "  HS_Physical       0.60      0.42      0.49        57\n",
      "    HS_Gender       0.70      0.55      0.61        55\n",
      "     HS_Other       0.81      0.78      0.80       771\n",
      "      HS_Weak       0.70      0.72      0.71       681\n",
      "  HS_Moderate       0.65      0.54      0.59       359\n",
      "    HS_Strong       0.79      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.73      0.71      0.72      5589\n",
      " weighted avg       0.77      0.78      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 179.1755886077881 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4186, Accuracy: 0.8927, F1 Micro: 0.6335, F1 Macro: 0.3719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3112, Accuracy: 0.9083, F1 Micro: 0.745, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2488, Accuracy: 0.9222, F1 Micro: 0.7594, F1 Macro: 0.6045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1979, Accuracy: 0.9233, F1 Micro: 0.7751, F1 Macro: 0.6364\n",
      "Epoch 5/10, Train Loss: 0.159, Accuracy: 0.9209, F1 Micro: 0.7725, F1 Macro: 0.6741\n",
      "Epoch 6/10, Train Loss: 0.1221, Accuracy: 0.9185, F1 Micro: 0.7715, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1004, Accuracy: 0.9248, F1 Micro: 0.7779, F1 Macro: 0.6829\n",
      "Epoch 8/10, Train Loss: 0.0864, Accuracy: 0.9227, F1 Micro: 0.777, F1 Macro: 0.7033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9244, F1 Micro: 0.7801, F1 Macro: 0.7177\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9214, F1 Micro: 0.7779, F1 Macro: 0.7226\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9244, F1 Micro: 0.7801, F1 Macro: 0.7177\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1137\n",
      "      Abusive       0.88      0.91      0.90      1008\n",
      "HS_Individual       0.79      0.69      0.73       729\n",
      "     HS_Group       0.63      0.74      0.68       408\n",
      "  HS_Religion       0.74      0.67      0.70       168\n",
      "      HS_Race       0.75      0.78      0.77       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.69      0.44      0.53        55\n",
      "     HS_Other       0.81      0.80      0.80       771\n",
      "      HS_Weak       0.76      0.66      0.71       681\n",
      "  HS_Moderate       0.58      0.67      0.62       359\n",
      "    HS_Strong       0.82      0.85      0.83        97\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.79      0.77      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 179.3448362350464 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4162, Accuracy: 0.8945, F1 Micro: 0.6406, F1 Macro: 0.3988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3098, Accuracy: 0.9118, F1 Micro: 0.745, F1 Macro: 0.5792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.247, Accuracy: 0.922, F1 Micro: 0.7605, F1 Macro: 0.5954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1968, Accuracy: 0.9176, F1 Micro: 0.7634, F1 Macro: 0.6267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1576, Accuracy: 0.9218, F1 Micro: 0.7758, F1 Macro: 0.687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1217, Accuracy: 0.9228, F1 Micro: 0.777, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.927, F1 Micro: 0.7858, F1 Macro: 0.693\n",
      "Epoch 8/10, Train Loss: 0.0866, Accuracy: 0.9221, F1 Micro: 0.7745, F1 Macro: 0.6949\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9237, F1 Micro: 0.7769, F1 Macro: 0.699\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9189, F1 Micro: 0.7736, F1 Macro: 0.7086\n",
      "Model 3 - Iteration 6285: Accuracy: 0.927, F1 Micro: 0.7858, F1 Macro: 0.693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.87      0.92      0.89      1008\n",
      "HS_Individual       0.76      0.76      0.76       729\n",
      "     HS_Group       0.74      0.62      0.68       408\n",
      "  HS_Religion       0.80      0.65      0.72       168\n",
      "      HS_Race       0.76      0.73      0.74       119\n",
      "  HS_Physical       0.77      0.30      0.43        57\n",
      "    HS_Gender       0.80      0.22      0.34        55\n",
      "     HS_Other       0.81      0.79      0.80       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.56      0.61       359\n",
      "    HS_Strong       0.79      0.71      0.75        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.78      0.66      0.69      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 180.9697391986847 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9151, F1 Micro: 0.7481, F1 Macro: 0.6192\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Samples above threshold: 425\n",
      "Acquired samples: 299\n",
      "Sampling duration: 59.613144397735596 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4161, Accuracy: 0.8964, F1 Micro: 0.6444, F1 Macro: 0.4266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3, Accuracy: 0.9163, F1 Micro: 0.7416, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2483, Accuracy: 0.9203, F1 Micro: 0.7604, F1 Macro: 0.6065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2084, Accuracy: 0.9243, F1 Micro: 0.767, F1 Macro: 0.6388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1624, Accuracy: 0.9253, F1 Micro: 0.7783, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1262, Accuracy: 0.9258, F1 Micro: 0.7792, F1 Macro: 0.6788\n",
      "Epoch 7/10, Train Loss: 0.108, Accuracy: 0.9259, F1 Micro: 0.7784, F1 Macro: 0.688\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9231, F1 Micro: 0.7721, F1 Macro: 0.7097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9239, F1 Micro: 0.7798, F1 Macro: 0.7161\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9267, F1 Micro: 0.7715, F1 Macro: 0.6964\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9239, F1 Micro: 0.7798, F1 Macro: 0.7161\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.72      0.75      0.73       729\n",
      "     HS_Group       0.70      0.62      0.66       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.74      0.76      0.75       119\n",
      "  HS_Physical       0.61      0.39      0.47        57\n",
      "    HS_Gender       0.70      0.51      0.59        55\n",
      "     HS_Other       0.81      0.80      0.81       771\n",
      "      HS_Weak       0.70      0.72      0.71       681\n",
      "  HS_Moderate       0.66      0.57      0.61       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5589\n",
      "    macro avg       0.74      0.70      0.72      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 186.2916808128357 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4181, Accuracy: 0.8911, F1 Micro: 0.6172, F1 Macro: 0.3585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3047, Accuracy: 0.9141, F1 Micro: 0.7265, F1 Macro: 0.5727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2483, Accuracy: 0.9179, F1 Micro: 0.7429, F1 Macro: 0.5797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2087, Accuracy: 0.9222, F1 Micro: 0.7695, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1602, Accuracy: 0.9244, F1 Micro: 0.7774, F1 Macro: 0.6505\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9257, F1 Micro: 0.7735, F1 Macro: 0.6701\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9229, F1 Micro: 0.773, F1 Macro: 0.6628\n",
      "Epoch 8/10, Train Loss: 0.0902, Accuracy: 0.9261, F1 Micro: 0.7752, F1 Macro: 0.7041\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9239, F1 Micro: 0.7735, F1 Macro: 0.7039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9262, F1 Micro: 0.7826, F1 Macro: 0.7096\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9262, F1 Micro: 0.7826, F1 Macro: 0.7096\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1137\n",
      "      Abusive       0.87      0.92      0.90      1008\n",
      "HS_Individual       0.75      0.72      0.74       729\n",
      "     HS_Group       0.73      0.65      0.69       408\n",
      "  HS_Religion       0.77      0.65      0.71       168\n",
      "      HS_Race       0.77      0.76      0.76       119\n",
      "  HS_Physical       0.64      0.37      0.47        57\n",
      "    HS_Gender       0.68      0.35      0.46        55\n",
      "     HS_Other       0.81      0.78      0.79       771\n",
      "      HS_Weak       0.73      0.70      0.72       681\n",
      "  HS_Moderate       0.68      0.58      0.62       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 185.45961380004883 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4164, Accuracy: 0.8932, F1 Micro: 0.6206, F1 Macro: 0.3884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3047, Accuracy: 0.915, F1 Micro: 0.7366, F1 Macro: 0.5678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2501, Accuracy: 0.9182, F1 Micro: 0.7446, F1 Macro: 0.5774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2055, Accuracy: 0.9238, F1 Micro: 0.7603, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.161, Accuracy: 0.9235, F1 Micro: 0.7796, F1 Macro: 0.6741\n",
      "Epoch 6/10, Train Loss: 0.1251, Accuracy: 0.9253, F1 Micro: 0.7763, F1 Macro: 0.6817\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9237, F1 Micro: 0.7787, F1 Macro: 0.6852\n",
      "Epoch 8/10, Train Loss: 0.0831, Accuracy: 0.9226, F1 Micro: 0.7714, F1 Macro: 0.7023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9275, F1 Micro: 0.7866, F1 Macro: 0.719\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.927, F1 Micro: 0.7782, F1 Macro: 0.6931\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9275, F1 Micro: 0.7866, F1 Macro: 0.719\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.87      0.93      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.73      0.60      0.66       408\n",
      "  HS_Religion       0.75      0.63      0.69       168\n",
      "      HS_Race       0.75      0.75      0.75       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.73      0.49      0.59        55\n",
      "     HS_Other       0.83      0.78      0.80       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.69      0.52      0.60       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 185.29635286331177 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9162, F1 Micro: 0.7515, F1 Macro: 0.6287\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 396\n",
      "Acquired samples: 396\n",
      "Sampling duration: 56.341267347335815 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4056, Accuracy: 0.9002, F1 Micro: 0.6651, F1 Macro: 0.4479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2939, Accuracy: 0.9179, F1 Micro: 0.7567, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2354, Accuracy: 0.9229, F1 Micro: 0.7585, F1 Macro: 0.5997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1922, Accuracy: 0.9254, F1 Micro: 0.7798, F1 Macro: 0.6442\n",
      "Epoch 5/10, Train Loss: 0.1528, Accuracy: 0.9239, F1 Micro: 0.7695, F1 Macro: 0.6749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9272, F1 Micro: 0.7806, F1 Macro: 0.6823\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.924, F1 Micro: 0.7734, F1 Macro: 0.6706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0801, Accuracy: 0.9269, F1 Micro: 0.7841, F1 Macro: 0.7156\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9247, F1 Micro: 0.7829, F1 Macro: 0.7164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9253, F1 Micro: 0.7866, F1 Macro: 0.7253\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9253, F1 Micro: 0.7866, F1 Macro: 0.7253\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.71      0.78      0.74       729\n",
      "     HS_Group       0.71      0.63      0.67       408\n",
      "  HS_Religion       0.78      0.64      0.70       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.67      0.42      0.52        57\n",
      "    HS_Gender       0.71      0.53      0.60        55\n",
      "     HS_Other       0.78      0.85      0.81       771\n",
      "      HS_Weak       0.69      0.75      0.72       681\n",
      "  HS_Moderate       0.67      0.56      0.61       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.75      0.71      0.73      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 196.7490632534027 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4087, Accuracy: 0.8962, F1 Micro: 0.6539, F1 Macro: 0.4243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2978, Accuracy: 0.9136, F1 Micro: 0.7507, F1 Macro: 0.598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2379, Accuracy: 0.9222, F1 Micro: 0.7573, F1 Macro: 0.6128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.191, Accuracy: 0.9208, F1 Micro: 0.7734, F1 Macro: 0.6301\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9234, F1 Micro: 0.7727, F1 Macro: 0.6625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1253, Accuracy: 0.9269, F1 Micro: 0.784, F1 Macro: 0.6901\n",
      "Epoch 7/10, Train Loss: 0.0985, Accuracy: 0.9244, F1 Micro: 0.776, F1 Macro: 0.6841\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9253, F1 Micro: 0.7828, F1 Macro: 0.7038\n",
      "Epoch 9/10, Train Loss: 0.0684, Accuracy: 0.9238, F1 Micro: 0.7718, F1 Macro: 0.705\n",
      "Epoch 10/10, Train Loss: 0.061, Accuracy: 0.9277, F1 Micro: 0.7816, F1 Macro: 0.7127\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9269, F1 Micro: 0.784, F1 Macro: 0.6901\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.88      0.90      0.89      1008\n",
      "HS_Individual       0.77      0.73      0.75       729\n",
      "     HS_Group       0.69      0.66      0.68       408\n",
      "  HS_Religion       0.76      0.64      0.70       168\n",
      "      HS_Race       0.77      0.76      0.76       119\n",
      "  HS_Physical       0.62      0.18      0.27        57\n",
      "    HS_Gender       0.83      0.27      0.41        55\n",
      "     HS_Other       0.82      0.79      0.81       771\n",
      "      HS_Weak       0.75      0.70      0.73       681\n",
      "  HS_Moderate       0.65      0.59      0.62       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5589\n",
      "    macro avg       0.77      0.66      0.69      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 193.62860465049744 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4043, Accuracy: 0.8971, F1 Micro: 0.6555, F1 Macro: 0.4456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2956, Accuracy: 0.9131, F1 Micro: 0.7525, F1 Macro: 0.589\n",
      "Epoch 3/10, Train Loss: 0.2379, Accuracy: 0.9218, F1 Micro: 0.75, F1 Macro: 0.5864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1945, Accuracy: 0.9225, F1 Micro: 0.7796, F1 Macro: 0.6473\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9239, F1 Micro: 0.7684, F1 Macro: 0.6686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.928, F1 Micro: 0.7809, F1 Macro: 0.6754\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9271, F1 Micro: 0.7777, F1 Macro: 0.6747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0829, Accuracy: 0.9251, F1 Micro: 0.7854, F1 Macro: 0.7171\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9246, F1 Micro: 0.7827, F1 Macro: 0.7175\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.9256, F1 Micro: 0.7818, F1 Macro: 0.7145\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9251, F1 Micro: 0.7854, F1 Macro: 0.7171\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.86      0.93      0.89      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.70      0.63      0.66       408\n",
      "  HS_Religion       0.72      0.67      0.69       168\n",
      "      HS_Race       0.73      0.81      0.77       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.74      0.45      0.56        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.70      0.75      0.73       681\n",
      "  HS_Moderate       0.66      0.55      0.60       359\n",
      "    HS_Strong       0.78      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.70      0.72      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 193.42841482162476 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.917, F1 Micro: 0.7546, F1 Macro: 0.6362\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 356\n",
      "Acquired samples: 356\n",
      "Sampling duration: 51.885664224624634 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.8969, F1 Micro: 0.6347, F1 Macro: 0.3999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2882, Accuracy: 0.916, F1 Micro: 0.7457, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2293, Accuracy: 0.9241, F1 Micro: 0.7627, F1 Macro: 0.6108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.924, F1 Micro: 0.7734, F1 Macro: 0.6597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9244, F1 Micro: 0.7805, F1 Macro: 0.6701\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9248, F1 Micro: 0.7753, F1 Macro: 0.6872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9273, F1 Micro: 0.7814, F1 Macro: 0.7076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9254, F1 Micro: 0.786, F1 Macro: 0.7117\n",
      "Epoch 9/10, Train Loss: 0.0709, Accuracy: 0.9238, F1 Micro: 0.7769, F1 Macro: 0.7076\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.9206, F1 Micro: 0.7784, F1 Macro: 0.7159\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9254, F1 Micro: 0.786, F1 Macro: 0.7117\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.70      0.80      0.75       729\n",
      "     HS_Group       0.75      0.58      0.65       408\n",
      "  HS_Religion       0.78      0.66      0.71       168\n",
      "      HS_Race       0.74      0.72      0.73       119\n",
      "  HS_Physical       0.71      0.39      0.50        57\n",
      "    HS_Gender       0.73      0.40      0.52        55\n",
      "     HS_Other       0.79      0.84      0.82       771\n",
      "      HS_Weak       0.67      0.78      0.73       681\n",
      "  HS_Moderate       0.71      0.50      0.58       359\n",
      "    HS_Strong       0.80      0.79      0.80        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.76      0.69      0.71      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 206.60897183418274 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4034, Accuracy: 0.894, F1 Micro: 0.6223, F1 Macro: 0.3769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2903, Accuracy: 0.9181, F1 Micro: 0.7493, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2302, Accuracy: 0.9231, F1 Micro: 0.7622, F1 Macro: 0.613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1845, Accuracy: 0.9237, F1 Micro: 0.7783, F1 Macro: 0.6691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1539, Accuracy: 0.9253, F1 Micro: 0.7816, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1204, Accuracy: 0.9235, F1 Micro: 0.7827, F1 Macro: 0.6917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0989, Accuracy: 0.9293, F1 Micro: 0.7905, F1 Macro: 0.7095\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9206, F1 Micro: 0.7736, F1 Macro: 0.6998\n",
      "Epoch 9/10, Train Loss: 0.0691, Accuracy: 0.924, F1 Micro: 0.7806, F1 Macro: 0.7086\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9253, F1 Micro: 0.7867, F1 Macro: 0.7247\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9293, F1 Micro: 0.7905, F1 Macro: 0.7095\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.88      0.92      0.90      1008\n",
      "HS_Individual       0.79      0.72      0.75       729\n",
      "     HS_Group       0.71      0.69      0.70       408\n",
      "  HS_Religion       0.78      0.64      0.71       168\n",
      "      HS_Race       0.71      0.76      0.73       119\n",
      "  HS_Physical       0.73      0.28      0.41        57\n",
      "    HS_Gender       0.70      0.35      0.46        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.76      0.69      0.72       681\n",
      "  HS_Moderate       0.68      0.62      0.64       359\n",
      "    HS_Strong       0.81      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.68      0.71      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 207.66716527938843 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4014, Accuracy: 0.8979, F1 Micro: 0.6446, F1 Macro: 0.4111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2886, Accuracy: 0.9157, F1 Micro: 0.7406, F1 Macro: 0.5544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.229, Accuracy: 0.9228, F1 Micro: 0.762, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9219, F1 Micro: 0.7755, F1 Macro: 0.6595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1529, Accuracy: 0.9259, F1 Micro: 0.7849, F1 Macro: 0.6848\n",
      "Epoch 6/10, Train Loss: 0.1182, Accuracy: 0.9263, F1 Micro: 0.7804, F1 Macro: 0.696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.9272, F1 Micro: 0.785, F1 Macro: 0.6982\n",
      "Epoch 8/10, Train Loss: 0.0802, Accuracy: 0.9215, F1 Micro: 0.7768, F1 Macro: 0.6972\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.9253, F1 Micro: 0.773, F1 Macro: 0.7053\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9258, F1 Micro: 0.7844, F1 Macro: 0.709\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9272, F1 Micro: 0.785, F1 Macro: 0.6982\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1137\n",
      "      Abusive       0.88      0.91      0.89      1008\n",
      "HS_Individual       0.78      0.71      0.74       729\n",
      "     HS_Group       0.68      0.68      0.68       408\n",
      "  HS_Religion       0.73      0.65      0.69       168\n",
      "      HS_Race       0.77      0.75      0.76       119\n",
      "  HS_Physical       0.80      0.28      0.42        57\n",
      "    HS_Gender       0.65      0.27      0.38        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.64      0.60      0.62       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5589\n",
      "    macro avg       0.76      0.67      0.70      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 203.34527778625488 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9179, F1 Micro: 0.7573, F1 Macro: 0.6421\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 320\n",
      "Acquired samples: 320\n",
      "Sampling duration: 45.661991119384766 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3947, Accuracy: 0.9014, F1 Micro: 0.6952, F1 Macro: 0.4411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2812, Accuracy: 0.9133, F1 Micro: 0.7567, F1 Macro: 0.5936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.9229, F1 Micro: 0.7595, F1 Macro: 0.6156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1814, Accuracy: 0.9271, F1 Micro: 0.7745, F1 Macro: 0.6306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1477, Accuracy: 0.9269, F1 Micro: 0.7861, F1 Macro: 0.7045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9276, F1 Micro: 0.7874, F1 Macro: 0.7088\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.9275, F1 Micro: 0.7828, F1 Macro: 0.6951\n",
      "Epoch 8/10, Train Loss: 0.081, Accuracy: 0.9268, F1 Micro: 0.782, F1 Macro: 0.7137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9276, F1 Micro: 0.7884, F1 Macro: 0.7215\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9271, F1 Micro: 0.788, F1 Macro: 0.7234\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9276, F1 Micro: 0.7884, F1 Macro: 0.7215\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.91      0.89      0.90      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.74      0.60      0.66       408\n",
      "  HS_Religion       0.72      0.69      0.71       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.68      0.40      0.51        57\n",
      "    HS_Gender       0.70      0.47      0.57        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.71      0.74      0.73       681\n",
      "  HS_Moderate       0.69      0.51      0.59       359\n",
      "    HS_Strong       0.78      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 211.97049808502197 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3959, Accuracy: 0.9005, F1 Micro: 0.6764, F1 Macro: 0.413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2829, Accuracy: 0.9158, F1 Micro: 0.7598, F1 Macro: 0.5963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2303, Accuracy: 0.9243, F1 Micro: 0.7752, F1 Macro: 0.6319\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9256, F1 Micro: 0.7739, F1 Macro: 0.6381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9224, F1 Micro: 0.7776, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.9244, F1 Micro: 0.7837, F1 Macro: 0.6972\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9268, F1 Micro: 0.777, F1 Macro: 0.6847\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9266, F1 Micro: 0.7797, F1 Macro: 0.7127\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9265, F1 Micro: 0.7806, F1 Macro: 0.7137\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.9252, F1 Micro: 0.7816, F1 Macro: 0.7167\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9244, F1 Micro: 0.7837, F1 Macro: 0.6972\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.73      0.73      0.73       729\n",
      "     HS_Group       0.66      0.72      0.69       408\n",
      "  HS_Religion       0.70      0.70      0.70       168\n",
      "      HS_Race       0.72      0.78      0.75       119\n",
      "  HS_Physical       0.79      0.19      0.31        57\n",
      "    HS_Gender       0.78      0.33      0.46        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.71      0.72       681\n",
      "  HS_Moderate       0.61      0.65      0.63       359\n",
      "    HS_Strong       0.84      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5589\n",
      "    macro avg       0.76      0.69      0.70      5589\n",
      " weighted avg       0.78      0.79      0.78      5589\n",
      "  samples avg       0.44      0.45      0.43      5589\n",
      "\n",
      "Training completed in 207.83544826507568 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.393, Accuracy: 0.9011, F1 Micro: 0.6924, F1 Macro: 0.4192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2832, Accuracy: 0.9139, F1 Micro: 0.7588, F1 Macro: 0.5851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2271, Accuracy: 0.9232, F1 Micro: 0.7695, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.9265, F1 Micro: 0.7817, F1 Macro: 0.6488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1441, Accuracy: 0.926, F1 Micro: 0.7833, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1163, Accuracy: 0.9261, F1 Micro: 0.7854, F1 Macro: 0.7075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.9286, F1 Micro: 0.789, F1 Macro: 0.7027\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9258, F1 Micro: 0.7774, F1 Macro: 0.7032\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9265, F1 Micro: 0.7879, F1 Macro: 0.7094\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9279, F1 Micro: 0.7889, F1 Macro: 0.7156\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9286, F1 Micro: 0.789, F1 Macro: 0.7027\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.87      1137\n",
      "      Abusive       0.90      0.89      0.90      1008\n",
      "HS_Individual       0.79      0.71      0.74       729\n",
      "     HS_Group       0.67      0.71      0.69       408\n",
      "  HS_Religion       0.72      0.71      0.71       168\n",
      "      HS_Race       0.79      0.74      0.76       119\n",
      "  HS_Physical       0.87      0.23      0.36        57\n",
      "    HS_Gender       0.84      0.29      0.43        55\n",
      "     HS_Other       0.82      0.81      0.82       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.64      0.64      0.64       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.79      0.67      0.70      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 211.6745777130127 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9186, F1 Micro: 0.7596, F1 Macro: 0.6471\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Samples above threshold: 288\n",
      "Acquired samples: 245\n",
      "Sampling duration: 41.56919026374817 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3909, Accuracy: 0.901, F1 Micro: 0.6956, F1 Macro: 0.4888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2791, Accuracy: 0.9152, F1 Micro: 0.7609, F1 Macro: 0.6035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9202, F1 Micro: 0.7733, F1 Macro: 0.6364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9253, F1 Micro: 0.7812, F1 Macro: 0.6547\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9274, F1 Micro: 0.7808, F1 Macro: 0.6781\n",
      "Epoch 6/10, Train Loss: 0.1138, Accuracy: 0.9267, F1 Micro: 0.7804, F1 Macro: 0.6951\n",
      "Epoch 7/10, Train Loss: 0.0934, Accuracy: 0.9211, F1 Micro: 0.7798, F1 Macro: 0.7008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9251, F1 Micro: 0.7824, F1 Macro: 0.7151\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9249, F1 Micro: 0.7808, F1 Macro: 0.7186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9257, F1 Micro: 0.7852, F1 Macro: 0.7186\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9257, F1 Micro: 0.7852, F1 Macro: 0.7186\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.68      0.67      0.67       408\n",
      "  HS_Religion       0.71      0.67      0.69       168\n",
      "      HS_Race       0.70      0.81      0.75       119\n",
      "  HS_Physical       0.71      0.39      0.50        57\n",
      "    HS_Gender       0.66      0.45      0.54        55\n",
      "     HS_Other       0.79      0.81      0.80       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.63      0.60      0.61       359\n",
      "    HS_Strong       0.81      0.85      0.83        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.74      0.71      0.72      5589\n",
      " weighted avg       0.78      0.78      0.78      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 214.6275496482849 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3917, Accuracy: 0.8995, F1 Micro: 0.6773, F1 Macro: 0.4677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2811, Accuracy: 0.9155, F1 Micro: 0.7586, F1 Macro: 0.5994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2186, Accuracy: 0.9205, F1 Micro: 0.7717, F1 Macro: 0.6241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9247, F1 Micro: 0.7766, F1 Macro: 0.6527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9276, F1 Micro: 0.7805, F1 Macro: 0.6795\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.9266, F1 Micro: 0.7773, F1 Macro: 0.6791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9264, F1 Micro: 0.7847, F1 Macro: 0.7159\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.9266, F1 Micro: 0.7811, F1 Macro: 0.716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.927, F1 Micro: 0.7869, F1 Macro: 0.7182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9284, F1 Micro: 0.7872, F1 Macro: 0.721\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9284, F1 Micro: 0.7872, F1 Macro: 0.721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.74      0.63      0.68       408\n",
      "  HS_Religion       0.77      0.64      0.70       168\n",
      "      HS_Race       0.76      0.77      0.77       119\n",
      "  HS_Physical       0.62      0.42      0.50        57\n",
      "    HS_Gender       0.74      0.42      0.53        55\n",
      "     HS_Other       0.82      0.79      0.81       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.71      0.54      0.61       359\n",
      "    HS_Strong       0.82      0.84      0.83        97\n",
      "\n",
      "    micro avg       0.81      0.76      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.81      0.76      0.78      5589\n",
      "  samples avg       0.45      0.43      0.43      5589\n",
      "\n",
      "Training completed in 218.8571445941925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3888, Accuracy: 0.8976, F1 Micro: 0.698, F1 Macro: 0.5021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2802, Accuracy: 0.9147, F1 Micro: 0.7524, F1 Macro: 0.5783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.9205, F1 Micro: 0.7754, F1 Macro: 0.6201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9247, F1 Micro: 0.7788, F1 Macro: 0.6581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9267, F1 Micro: 0.7827, F1 Macro: 0.6871\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9251, F1 Micro: 0.7747, F1 Macro: 0.6809\n",
      "Epoch 7/10, Train Loss: 0.092, Accuracy: 0.9278, F1 Micro: 0.7821, F1 Macro: 0.702\n",
      "Epoch 8/10, Train Loss: 0.0754, Accuracy: 0.9275, F1 Micro: 0.7821, F1 Macro: 0.7057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9271, F1 Micro: 0.7879, F1 Macro: 0.7216\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9268, F1 Micro: 0.7838, F1 Macro: 0.7142\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9271, F1 Micro: 0.7879, F1 Macro: 0.7216\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.69      0.68      0.68       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.75      0.77      0.76       119\n",
      "  HS_Physical       0.74      0.44      0.55        57\n",
      "    HS_Gender       0.68      0.42      0.52        55\n",
      "     HS_Other       0.81      0.82      0.82       771\n",
      "      HS_Weak       0.72      0.71      0.71       681\n",
      "  HS_Moderate       0.64      0.60      0.62       359\n",
      "    HS_Strong       0.78      0.82      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 215.95371985435486 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9192, F1 Micro: 0.7616, F1 Macro: 0.6523\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 264\n",
      "Acquired samples: 264\n",
      "Sampling duration: 38.289329528808594 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.9026, F1 Micro: 0.7059, F1 Macro: 0.4918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.9203, F1 Micro: 0.7599, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2149, Accuracy: 0.9259, F1 Micro: 0.7761, F1 Macro: 0.6233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9269, F1 Micro: 0.777, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1403, Accuracy: 0.9289, F1 Micro: 0.7829, F1 Macro: 0.6869\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9254, F1 Micro: 0.7788, F1 Macro: 0.6924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9247, F1 Micro: 0.7849, F1 Macro: 0.7062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0733, Accuracy: 0.9266, F1 Micro: 0.7853, F1 Macro: 0.7198\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.925, F1 Micro: 0.7804, F1 Macro: 0.7108\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9272, F1 Micro: 0.7808, F1 Macro: 0.7089\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9266, F1 Micro: 0.7853, F1 Macro: 0.7198\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1137\n",
      "      Abusive       0.88      0.93      0.90      1008\n",
      "HS_Individual       0.77      0.71      0.74       729\n",
      "     HS_Group       0.67      0.70      0.69       408\n",
      "  HS_Religion       0.77      0.64      0.70       168\n",
      "      HS_Race       0.72      0.76      0.74       119\n",
      "  HS_Physical       0.81      0.37      0.51        57\n",
      "    HS_Gender       0.75      0.44      0.55        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.75      0.67      0.70       681\n",
      "  HS_Moderate       0.62      0.62      0.62       359\n",
      "    HS_Strong       0.82      0.82      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 223.20575284957886 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3838, Accuracy: 0.9026, F1 Micro: 0.7004, F1 Macro: 0.4787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2715, Accuracy: 0.9176, F1 Micro: 0.7392, F1 Macro: 0.5746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.9258, F1 Micro: 0.7763, F1 Macro: 0.6255\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9263, F1 Micro: 0.7703, F1 Macro: 0.6382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9278, F1 Micro: 0.7791, F1 Macro: 0.6495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1073, Accuracy: 0.9258, F1 Micro: 0.783, F1 Macro: 0.6877\n",
      "Epoch 7/10, Train Loss: 0.0896, Accuracy: 0.9218, F1 Micro: 0.7796, F1 Macro: 0.7141\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.926, F1 Micro: 0.7788, F1 Macro: 0.7153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9264, F1 Micro: 0.7841, F1 Macro: 0.7232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.9262, F1 Micro: 0.7879, F1 Macro: 0.7261\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9262, F1 Micro: 0.7879, F1 Macro: 0.7261\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.73      0.76      0.75       729\n",
      "     HS_Group       0.69      0.66      0.68       408\n",
      "  HS_Religion       0.74      0.68      0.71       168\n",
      "      HS_Race       0.77      0.78      0.78       119\n",
      "  HS_Physical       0.61      0.44      0.51        57\n",
      "    HS_Gender       0.67      0.47      0.55        55\n",
      "     HS_Other       0.80      0.80      0.80       771\n",
      "      HS_Weak       0.71      0.74      0.73       681\n",
      "  HS_Moderate       0.64      0.59      0.61       359\n",
      "    HS_Strong       0.81      0.87      0.84        97\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5589\n",
      "    macro avg       0.74      0.72      0.73      5589\n",
      " weighted avg       0.78      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 223.04835987091064 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3799, Accuracy: 0.9019, F1 Micro: 0.7063, F1 Macro: 0.4841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2688, Accuracy: 0.9172, F1 Micro: 0.7381, F1 Macro: 0.5586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2166, Accuracy: 0.9232, F1 Micro: 0.7768, F1 Macro: 0.6262\n",
      "Epoch 4/10, Train Loss: 0.1736, Accuracy: 0.9253, F1 Micro: 0.7723, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9267, F1 Micro: 0.7794, F1 Macro: 0.6752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1123, Accuracy: 0.9257, F1 Micro: 0.7846, F1 Macro: 0.6918\n",
      "Epoch 7/10, Train Loss: 0.0921, Accuracy: 0.9259, F1 Micro: 0.7832, F1 Macro: 0.7073\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9244, F1 Micro: 0.7757, F1 Macro: 0.7059\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9267, F1 Micro: 0.7809, F1 Macro: 0.7076\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9249, F1 Micro: 0.784, F1 Macro: 0.7187\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9257, F1 Micro: 0.7846, F1 Macro: 0.6918\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.87      0.92      0.89      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.74      0.62      0.67       408\n",
      "  HS_Religion       0.81      0.62      0.70       168\n",
      "      HS_Race       0.75      0.72      0.74       119\n",
      "  HS_Physical       0.81      0.23      0.36        57\n",
      "    HS_Gender       0.63      0.31      0.41        55\n",
      "     HS_Other       0.79      0.81      0.80       771\n",
      "      HS_Weak       0.70      0.76      0.73       681\n",
      "  HS_Moderate       0.70      0.53      0.60       359\n",
      "    HS_Strong       0.79      0.77      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5589\n",
      "    macro avg       0.76      0.66      0.69      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 219.89462733268738 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9197, F1 Micro: 0.7632, F1 Macro: 0.6563\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 237\n",
      "Acquired samples: 237\n",
      "Sampling duration: 34.59593439102173 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3738, Accuracy: 0.9045, F1 Micro: 0.7103, F1 Macro: 0.4758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2631, Accuracy: 0.9212, F1 Micro: 0.7639, F1 Macro: 0.6013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.208, Accuracy: 0.927, F1 Micro: 0.7839, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.9269, F1 Micro: 0.7854, F1 Macro: 0.6581\n",
      "Epoch 5/10, Train Loss: 0.1336, Accuracy: 0.9246, F1 Micro: 0.7807, F1 Macro: 0.6732\n",
      "Epoch 6/10, Train Loss: 0.1065, Accuracy: 0.9245, F1 Micro: 0.7814, F1 Macro: 0.6951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9273, F1 Micro: 0.7863, F1 Macro: 0.7117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9302, F1 Micro: 0.7889, F1 Macro: 0.7094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9287, F1 Micro: 0.792, F1 Macro: 0.7181\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.9294, F1 Micro: 0.789, F1 Macro: 0.7252\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9287, F1 Micro: 0.792, F1 Macro: 0.7181\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.92      0.91      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.76      0.60      0.67       408\n",
      "  HS_Religion       0.76      0.65      0.70       168\n",
      "      HS_Race       0.71      0.80      0.75       119\n",
      "  HS_Physical       0.69      0.44      0.54        57\n",
      "    HS_Gender       0.65      0.40      0.49        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.70      0.77      0.73       681\n",
      "  HS_Moderate       0.71      0.53      0.61       359\n",
      "    HS_Strong       0.81      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 228.65235304832458 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3768, Accuracy: 0.904, F1 Micro: 0.7014, F1 Macro: 0.4665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2667, Accuracy: 0.9202, F1 Micro: 0.7625, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2107, Accuracy: 0.9261, F1 Micro: 0.7789, F1 Macro: 0.6315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1705, Accuracy: 0.9269, F1 Micro: 0.7828, F1 Macro: 0.655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9278, F1 Micro: 0.7828, F1 Macro: 0.6797\n",
      "Epoch 6/10, Train Loss: 0.107, Accuracy: 0.922, F1 Micro: 0.7742, F1 Macro: 0.6831\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9234, F1 Micro: 0.7809, F1 Macro: 0.6942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0726, Accuracy: 0.9279, F1 Micro: 0.7859, F1 Macro: 0.7088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0603, Accuracy: 0.9281, F1 Micro: 0.7867, F1 Macro: 0.7171\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.9271, F1 Micro: 0.7839, F1 Macro: 0.7183\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9281, F1 Micro: 0.7867, F1 Macro: 0.7171\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.77      0.74      0.75       729\n",
      "     HS_Group       0.73      0.64      0.68       408\n",
      "  HS_Religion       0.76      0.64      0.69       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.75      0.37      0.49        57\n",
      "    HS_Gender       0.74      0.42      0.53        55\n",
      "     HS_Other       0.82      0.78      0.80       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.67      0.57      0.61       359\n",
      "    HS_Strong       0.80      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.77      0.68      0.72      5589\n",
      " weighted avg       0.81      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 228.1491858959198 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3727, Accuracy: 0.9015, F1 Micro: 0.691, F1 Macro: 0.4389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.264, Accuracy: 0.9194, F1 Micro: 0.7591, F1 Macro: 0.582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2091, Accuracy: 0.9253, F1 Micro: 0.7781, F1 Macro: 0.6237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1709, Accuracy: 0.9267, F1 Micro: 0.7842, F1 Macro: 0.6599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.9279, F1 Micro: 0.7858, F1 Macro: 0.6762\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.921, F1 Micro: 0.7757, F1 Macro: 0.6912\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.9259, F1 Micro: 0.7853, F1 Macro: 0.6993\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9277, F1 Micro: 0.781, F1 Macro: 0.7045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.928, F1 Micro: 0.7874, F1 Macro: 0.7084\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9241, F1 Micro: 0.7795, F1 Macro: 0.7113\n",
      "Model 3 - Iteration 8402: Accuracy: 0.928, F1 Micro: 0.7874, F1 Macro: 0.7084\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.72      0.62      0.66       408\n",
      "  HS_Religion       0.81      0.62      0.70       168\n",
      "      HS_Race       0.71      0.77      0.74       119\n",
      "  HS_Physical       0.67      0.32      0.43        57\n",
      "    HS_Gender       0.71      0.40      0.51        55\n",
      "     HS_Other       0.83      0.77      0.80       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.68      0.53      0.60       359\n",
      "    HS_Strong       0.78      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.46      0.44      0.43      5589\n",
      "\n",
      "Training completed in 226.87680578231812 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9202, F1 Micro: 0.7648, F1 Macro: 0.66\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 214\n",
      "Acquired samples: 214\n",
      "Sampling duration: 30.92144536972046 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3665, Accuracy: 0.9047, F1 Micro: 0.6988, F1 Macro: 0.4698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.264, Accuracy: 0.9195, F1 Micro: 0.7432, F1 Macro: 0.5796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9231, F1 Micro: 0.778, F1 Macro: 0.6453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1708, Accuracy: 0.9262, F1 Micro: 0.7789, F1 Macro: 0.6638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1337, Accuracy: 0.9282, F1 Micro: 0.788, F1 Macro: 0.7028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9303, F1 Micro: 0.7882, F1 Macro: 0.6974\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9262, F1 Micro: 0.7861, F1 Macro: 0.6929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0724, Accuracy: 0.9276, F1 Micro: 0.7896, F1 Macro: 0.7192\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.928, F1 Micro: 0.7871, F1 Macro: 0.7213\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9263, F1 Micro: 0.7842, F1 Macro: 0.7186\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9276, F1 Micro: 0.7896, F1 Macro: 0.7192\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.73      0.75      0.74       729\n",
      "     HS_Group       0.71      0.66      0.68       408\n",
      "  HS_Religion       0.80      0.62      0.70       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.68      0.33      0.45        57\n",
      "    HS_Gender       0.75      0.44      0.55        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.71      0.72      0.72       681\n",
      "  HS_Moderate       0.67      0.59      0.63       359\n",
      "    HS_Strong       0.84      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 232.2159788608551 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3692, Accuracy: 0.9029, F1 Micro: 0.7099, F1 Macro: 0.5009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.9197, F1 Micro: 0.7486, F1 Macro: 0.5835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.203, Accuracy: 0.9239, F1 Micro: 0.7728, F1 Macro: 0.6313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9242, F1 Micro: 0.7751, F1 Macro: 0.653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1366, Accuracy: 0.9275, F1 Micro: 0.7865, F1 Macro: 0.6901\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9247, F1 Micro: 0.7857, F1 Macro: 0.698\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9274, F1 Micro: 0.777, F1 Macro: 0.6977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9274, F1 Micro: 0.7867, F1 Macro: 0.708\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.928, F1 Micro: 0.785, F1 Macro: 0.7202\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9264, F1 Micro: 0.7838, F1 Macro: 0.7187\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9274, F1 Micro: 0.7867, F1 Macro: 0.708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.78      0.70      0.74       729\n",
      "     HS_Group       0.66      0.73      0.69       408\n",
      "  HS_Religion       0.76      0.65      0.70       168\n",
      "      HS_Race       0.75      0.76      0.76       119\n",
      "  HS_Physical       0.74      0.30      0.42        57\n",
      "    HS_Gender       0.69      0.33      0.44        55\n",
      "     HS_Other       0.81      0.82      0.82       771\n",
      "      HS_Weak       0.76      0.66      0.71       681\n",
      "  HS_Moderate       0.61      0.65      0.63       359\n",
      "    HS_Strong       0.83      0.80      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 230.9267852306366 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3675, Accuracy: 0.9017, F1 Micro: 0.7129, F1 Macro: 0.5058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2647, Accuracy: 0.919, F1 Micro: 0.7485, F1 Macro: 0.5784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.9229, F1 Micro: 0.7761, F1 Macro: 0.632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.9252, F1 Micro: 0.7771, F1 Macro: 0.657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9277, F1 Micro: 0.7804, F1 Macro: 0.6813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1036, Accuracy: 0.9286, F1 Micro: 0.7877, F1 Macro: 0.7034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.928, F1 Micro: 0.7896, F1 Macro: 0.7125\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9265, F1 Micro: 0.784, F1 Macro: 0.7079\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9252, F1 Micro: 0.7852, F1 Macro: 0.715\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9278, F1 Micro: 0.7879, F1 Macro: 0.7148\n",
      "Model 3 - Iteration 8616: Accuracy: 0.928, F1 Micro: 0.7896, F1 Macro: 0.7125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.90      0.90      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.70      0.68      0.69       408\n",
      "  HS_Religion       0.71      0.73      0.72       168\n",
      "      HS_Race       0.72      0.82      0.77       119\n",
      "  HS_Physical       0.63      0.30      0.40        57\n",
      "    HS_Gender       0.65      0.40      0.49        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.65      0.60      0.62       359\n",
      "    HS_Strong       0.79      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 234.49296975135803 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9206, F1 Micro: 0.7662, F1 Macro: 0.6631\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 192\n",
      "Acquired samples: 200\n",
      "Sampling duration: 27.39247226715088 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3682, Accuracy: 0.904, F1 Micro: 0.7115, F1 Macro: 0.4886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.9173, F1 Micro: 0.7502, F1 Macro: 0.5723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2028, Accuracy: 0.9265, F1 Micro: 0.7824, F1 Macro: 0.6307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1605, Accuracy: 0.9287, F1 Micro: 0.786, F1 Macro: 0.6723\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9273, F1 Micro: 0.7822, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9265, F1 Micro: 0.7885, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.928, F1 Micro: 0.7891, F1 Macro: 0.7183\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.9273, F1 Micro: 0.7875, F1 Macro: 0.7199\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.925, F1 Micro: 0.7845, F1 Macro: 0.7241\n",
      "Epoch 10/10, Train Loss: 0.0519, Accuracy: 0.9262, F1 Micro: 0.7847, F1 Macro: 0.7199\n",
      "Model 1 - Iteration 8816: Accuracy: 0.928, F1 Micro: 0.7891, F1 Macro: 0.7183\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.76      0.72      0.74       729\n",
      "     HS_Group       0.71      0.71      0.71       408\n",
      "  HS_Religion       0.75      0.64      0.69       168\n",
      "      HS_Race       0.71      0.81      0.75       119\n",
      "  HS_Physical       0.64      0.37      0.47        57\n",
      "    HS_Gender       0.74      0.42      0.53        55\n",
      "     HS_Other       0.82      0.79      0.81       771\n",
      "      HS_Weak       0.73      0.69      0.71       681\n",
      "  HS_Moderate       0.66      0.64      0.65       359\n",
      "    HS_Strong       0.82      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 235.73248052597046 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3708, Accuracy: 0.9029, F1 Micro: 0.6979, F1 Macro: 0.4611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.254, Accuracy: 0.9176, F1 Micro: 0.75, F1 Macro: 0.5715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2039, Accuracy: 0.9238, F1 Micro: 0.7759, F1 Macro: 0.622\n",
      "Epoch 4/10, Train Loss: 0.1625, Accuracy: 0.9256, F1 Micro: 0.7709, F1 Macro: 0.6571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.9272, F1 Micro: 0.7821, F1 Macro: 0.6655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1017, Accuracy: 0.9278, F1 Micro: 0.7904, F1 Macro: 0.7043\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9278, F1 Micro: 0.7853, F1 Macro: 0.7128\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9258, F1 Micro: 0.781, F1 Macro: 0.7156\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9234, F1 Micro: 0.7851, F1 Macro: 0.7239\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.926, F1 Micro: 0.7865, F1 Macro: 0.7248\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9278, F1 Micro: 0.7904, F1 Macro: 0.7043\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.75       729\n",
      "     HS_Group       0.72      0.65      0.68       408\n",
      "  HS_Religion       0.77      0.68      0.72       168\n",
      "      HS_Race       0.74      0.71      0.73       119\n",
      "  HS_Physical       0.68      0.26      0.38        57\n",
      "    HS_Gender       0.81      0.31      0.45        55\n",
      "     HS_Other       0.80      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.68      0.58      0.63       359\n",
      "    HS_Strong       0.81      0.80      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.77      0.68      0.70      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 234.03456234931946 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3669, Accuracy: 0.9027, F1 Micro: 0.7071, F1 Macro: 0.4572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2521, Accuracy: 0.9163, F1 Micro: 0.7409, F1 Macro: 0.5485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.203, Accuracy: 0.9247, F1 Micro: 0.773, F1 Macro: 0.618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9266, F1 Micro: 0.7749, F1 Macro: 0.6595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.926, F1 Micro: 0.7772, F1 Macro: 0.6977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9266, F1 Micro: 0.7882, F1 Macro: 0.7065\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.9284, F1 Micro: 0.7877, F1 Macro: 0.7137\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9231, F1 Micro: 0.7826, F1 Macro: 0.7136\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9275, F1 Micro: 0.783, F1 Macro: 0.7136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.9271, F1 Micro: 0.7895, F1 Macro: 0.7135\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9271, F1 Micro: 0.7895, F1 Macro: 0.7135\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.74      0.78      0.76       729\n",
      "     HS_Group       0.70      0.64      0.67       408\n",
      "  HS_Religion       0.70      0.67      0.68       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.63      0.39      0.48        57\n",
      "    HS_Gender       0.60      0.44      0.51        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.75      0.73       681\n",
      "  HS_Moderate       0.65      0.57      0.61       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.74      0.70      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.45      0.45      0.43      5589\n",
      "\n",
      "Training completed in 237.2854506969452 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.921, F1 Micro: 0.7675, F1 Macro: 0.6658\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 172\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.302922010421753 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3646, Accuracy: 0.9019, F1 Micro: 0.6939, F1 Macro: 0.4823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2438, Accuracy: 0.9169, F1 Micro: 0.7568, F1 Macro: 0.5808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.923, F1 Micro: 0.7816, F1 Macro: 0.6329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1589, Accuracy: 0.9249, F1 Micro: 0.783, F1 Macro: 0.6672\n",
      "Epoch 5/10, Train Loss: 0.1267, Accuracy: 0.9268, F1 Micro: 0.7803, F1 Macro: 0.6739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9256, F1 Micro: 0.7842, F1 Macro: 0.7075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9283, F1 Micro: 0.79, F1 Macro: 0.7145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9283, F1 Micro: 0.791, F1 Macro: 0.7199\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9255, F1 Micro: 0.7871, F1 Macro: 0.725\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9263, F1 Micro: 0.7807, F1 Macro: 0.7166\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9283, F1 Micro: 0.791, F1 Macro: 0.7199\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.92      0.92      1008\n",
      "HS_Individual       0.75      0.74      0.75       729\n",
      "     HS_Group       0.71      0.68      0.70       408\n",
      "  HS_Religion       0.75      0.61      0.67       168\n",
      "      HS_Race       0.77      0.77      0.77       119\n",
      "  HS_Physical       0.65      0.39      0.48        57\n",
      "    HS_Gender       0.67      0.44      0.53        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.72      0.72      0.72       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 241.559561252594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3674, Accuracy: 0.902, F1 Micro: 0.6972, F1 Macro: 0.4626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.919, F1 Micro: 0.7562, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1955, Accuracy: 0.9246, F1 Micro: 0.7804, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1593, Accuracy: 0.9246, F1 Micro: 0.7821, F1 Macro: 0.6469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9266, F1 Micro: 0.7856, F1 Macro: 0.663\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9245, F1 Micro: 0.7671, F1 Macro: 0.6871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.9276, F1 Micro: 0.7867, F1 Macro: 0.7107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9278, F1 Micro: 0.7888, F1 Macro: 0.7171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0605, Accuracy: 0.9258, F1 Micro: 0.7893, F1 Macro: 0.7262\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9284, F1 Micro: 0.7836, F1 Macro: 0.7198\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9258, F1 Micro: 0.7893, F1 Macro: 0.7262\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.73      0.76      0.74       729\n",
      "     HS_Group       0.68      0.71      0.70       408\n",
      "  HS_Religion       0.69      0.71      0.70       168\n",
      "      HS_Race       0.65      0.84      0.73       119\n",
      "  HS_Physical       0.65      0.42      0.51        57\n",
      "    HS_Gender       0.58      0.55      0.56        55\n",
      "     HS_Other       0.79      0.82      0.81       771\n",
      "      HS_Weak       0.71      0.74      0.73       681\n",
      "  HS_Moderate       0.64      0.65      0.65       359\n",
      "    HS_Strong       0.84      0.81      0.83        97\n",
      "\n",
      "    micro avg       0.78      0.80      0.79      5589\n",
      "    macro avg       0.73      0.73      0.73      5589\n",
      " weighted avg       0.78      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 242.85203647613525 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.363, Accuracy: 0.9022, F1 Micro: 0.6957, F1 Macro: 0.4747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2456, Accuracy: 0.9169, F1 Micro: 0.7512, F1 Macro: 0.5562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1946, Accuracy: 0.922, F1 Micro: 0.777, F1 Macro: 0.6343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9231, F1 Micro: 0.7822, F1 Macro: 0.6761\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.9276, F1 Micro: 0.7745, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0997, Accuracy: 0.9271, F1 Micro: 0.7822, F1 Macro: 0.7043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9268, F1 Micro: 0.7854, F1 Macro: 0.7075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.9277, F1 Micro: 0.7886, F1 Macro: 0.7239\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9253, F1 Micro: 0.7863, F1 Macro: 0.7177\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9265, F1 Micro: 0.7856, F1 Macro: 0.7156\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9277, F1 Micro: 0.7886, F1 Macro: 0.7239\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.76      0.72      0.74       729\n",
      "     HS_Group       0.68      0.71      0.70       408\n",
      "  HS_Religion       0.74      0.62      0.68       168\n",
      "      HS_Race       0.70      0.82      0.76       119\n",
      "  HS_Physical       0.67      0.39      0.49        57\n",
      "    HS_Gender       0.68      0.47      0.56        55\n",
      "     HS_Other       0.81      0.80      0.81       771\n",
      "      HS_Weak       0.74      0.69      0.71       681\n",
      "  HS_Moderate       0.64      0.64      0.64       359\n",
      "    HS_Strong       0.80      0.88      0.84        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.71      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 241.48495984077454 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9213, F1 Micro: 0.7686, F1 Macro: 0.6688\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 152\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.906168937683105 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.361, Accuracy: 0.9074, F1 Micro: 0.7128, F1 Macro: 0.5036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2464, Accuracy: 0.9216, F1 Micro: 0.7613, F1 Macro: 0.6027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.923, F1 Micro: 0.7781, F1 Macro: 0.6374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1594, Accuracy: 0.925, F1 Micro: 0.7862, F1 Macro: 0.6839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9279, F1 Micro: 0.7891, F1 Macro: 0.694\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.927, F1 Micro: 0.7844, F1 Macro: 0.7096\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9255, F1 Micro: 0.7839, F1 Macro: 0.7097\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9237, F1 Micro: 0.7839, F1 Macro: 0.718\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9261, F1 Micro: 0.7873, F1 Macro: 0.7184\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9281, F1 Micro: 0.7787, F1 Macro: 0.7076\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9279, F1 Micro: 0.7891, F1 Macro: 0.694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.77      0.60      0.68       408\n",
      "  HS_Religion       0.72      0.68      0.70       168\n",
      "      HS_Race       0.82      0.70      0.75       119\n",
      "  HS_Physical       0.63      0.30      0.40        57\n",
      "    HS_Gender       0.69      0.20      0.31        55\n",
      "     HS_Other       0.82      0.80      0.81       771\n",
      "      HS_Weak       0.70      0.76      0.73       681\n",
      "  HS_Moderate       0.72      0.52      0.61       359\n",
      "    HS_Strong       0.85      0.79      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.66      0.69      5589\n",
      " weighted avg       0.80      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 242.6249189376831 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3631, Accuracy: 0.905, F1 Micro: 0.701, F1 Macro: 0.4691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2482, Accuracy: 0.921, F1 Micro: 0.7619, F1 Macro: 0.6052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.19, Accuracy: 0.926, F1 Micro: 0.7758, F1 Macro: 0.6337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9266, F1 Micro: 0.7867, F1 Macro: 0.6737\n",
      "Epoch 5/10, Train Loss: 0.1218, Accuracy: 0.9257, F1 Micro: 0.7806, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9276, F1 Micro: 0.7907, F1 Macro: 0.7064\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9226, F1 Micro: 0.7776, F1 Macro: 0.7098\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9253, F1 Micro: 0.7839, F1 Macro: 0.72\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9259, F1 Micro: 0.7775, F1 Macro: 0.7128\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9254, F1 Micro: 0.7792, F1 Macro: 0.7108\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9276, F1 Micro: 0.7907, F1 Macro: 0.7064\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.90      0.87      1137\n",
      "      Abusive       0.89      0.90      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.75       729\n",
      "     HS_Group       0.71      0.65      0.68       408\n",
      "  HS_Religion       0.76      0.72      0.74       168\n",
      "      HS_Race       0.75      0.72      0.74       119\n",
      "  HS_Physical       0.72      0.23      0.35        57\n",
      "    HS_Gender       0.73      0.40      0.52        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.73      0.74      0.74       681\n",
      "  HS_Moderate       0.66      0.54      0.60       359\n",
      "    HS_Strong       0.77      0.81      0.79        97\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 242.4862060546875 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3605, Accuracy: 0.9056, F1 Micro: 0.6982, F1 Macro: 0.4657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2474, Accuracy: 0.9212, F1 Micro: 0.7584, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1935, Accuracy: 0.9246, F1 Micro: 0.7733, F1 Macro: 0.6225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1582, Accuracy: 0.9245, F1 Micro: 0.7878, F1 Macro: 0.6857\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.927, F1 Micro: 0.7866, F1 Macro: 0.6977\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.928, F1 Micro: 0.7825, F1 Macro: 0.6976\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9262, F1 Micro: 0.785, F1 Macro: 0.7097\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9263, F1 Micro: 0.7836, F1 Macro: 0.7167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9279, F1 Micro: 0.79, F1 Macro: 0.723\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9259, F1 Micro: 0.7865, F1 Macro: 0.7182\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9279, F1 Micro: 0.79, F1 Macro: 0.723\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.71      0.68      0.69       408\n",
      "  HS_Religion       0.73      0.70      0.71       168\n",
      "      HS_Race       0.76      0.82      0.79       119\n",
      "  HS_Physical       0.61      0.44      0.51        57\n",
      "    HS_Gender       0.69      0.40      0.51        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.72      0.71      0.71       681\n",
      "  HS_Moderate       0.66      0.60      0.63       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 241.8862500190735 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9217, F1 Micro: 0.7697, F1 Macro: 0.6708\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 2\n",
      "Sampling duration: 18.38277292251587 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3577, Accuracy: 0.9024, F1 Micro: 0.6759, F1 Macro: 0.5035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.9174, F1 Micro: 0.7639, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2008, Accuracy: 0.9271, F1 Micro: 0.777, F1 Macro: 0.6262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9254, F1 Micro: 0.7828, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9259, F1 Micro: 0.7836, F1 Macro: 0.659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9263, F1 Micro: 0.785, F1 Macro: 0.7008\n",
      "Epoch 7/10, Train Loss: 0.0826, Accuracy: 0.926, F1 Micro: 0.7829, F1 Macro: 0.7044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.9281, F1 Micro: 0.7879, F1 Macro: 0.7228\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9227, F1 Micro: 0.7799, F1 Macro: 0.714\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9283, F1 Micro: 0.7815, F1 Macro: 0.7074\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9281, F1 Micro: 0.7879, F1 Macro: 0.7228\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.74      0.74      0.74       729\n",
      "     HS_Group       0.73      0.62      0.67       408\n",
      "  HS_Religion       0.80      0.62      0.70       168\n",
      "      HS_Race       0.80      0.76      0.78       119\n",
      "  HS_Physical       0.59      0.40      0.48        57\n",
      "    HS_Gender       0.64      0.53      0.58        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.72      0.71      0.72       681\n",
      "  HS_Moderate       0.67      0.56      0.61       359\n",
      "    HS_Strong       0.84      0.79      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.79      5589\n",
      "  samples avg       0.44      0.44      0.42      5589\n",
      "\n",
      "Training completed in 247.81351923942566 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3581, Accuracy: 0.9038, F1 Micro: 0.6865, F1 Macro: 0.4923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2512, Accuracy: 0.9168, F1 Micro: 0.7623, F1 Macro: 0.5967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1971, Accuracy: 0.9252, F1 Micro: 0.7632, F1 Macro: 0.6157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9277, F1 Micro: 0.7897, F1 Macro: 0.6671\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9273, F1 Micro: 0.7792, F1 Macro: 0.6643\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9273, F1 Micro: 0.7856, F1 Macro: 0.7029\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9264, F1 Micro: 0.7809, F1 Macro: 0.7046\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9284, F1 Micro: 0.7883, F1 Macro: 0.7192\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9273, F1 Micro: 0.7781, F1 Macro: 0.7111\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9272, F1 Micro: 0.7821, F1 Macro: 0.7179\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9277, F1 Micro: 0.7897, F1 Macro: 0.6671\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1137\n",
      "      Abusive       0.90      0.88      0.89      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.73      0.66      0.69       408\n",
      "  HS_Religion       0.76      0.67      0.71       168\n",
      "      HS_Race       0.82      0.71      0.76       119\n",
      "  HS_Physical       0.67      0.11      0.18        57\n",
      "    HS_Gender       0.86      0.11      0.19        55\n",
      "     HS_Other       0.79      0.83      0.81       771\n",
      "      HS_Weak       0.73      0.77      0.75       681\n",
      "  HS_Moderate       0.67      0.59      0.63       359\n",
      "    HS_Strong       0.81      0.73      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.78      0.64      0.67      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 242.6866672039032 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3568, Accuracy: 0.9048, F1 Micro: 0.6906, F1 Macro: 0.5126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2484, Accuracy: 0.9172, F1 Micro: 0.7581, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1966, Accuracy: 0.9254, F1 Micro: 0.7655, F1 Macro: 0.619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.9244, F1 Micro: 0.7855, F1 Macro: 0.676\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9261, F1 Micro: 0.7727, F1 Macro: 0.6678\n",
      "Epoch 6/10, Train Loss: 0.0959, Accuracy: 0.9281, F1 Micro: 0.7813, F1 Macro: 0.6909\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9258, F1 Micro: 0.7833, F1 Macro: 0.7038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.928, F1 Micro: 0.7879, F1 Macro: 0.7199\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9243, F1 Micro: 0.78, F1 Macro: 0.7146\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9259, F1 Micro: 0.7812, F1 Macro: 0.7175\n",
      "Model 3 - Iteration 9218: Accuracy: 0.928, F1 Micro: 0.7879, F1 Macro: 0.7199\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1137\n",
      "      Abusive       0.89      0.92      0.90      1008\n",
      "HS_Individual       0.75      0.74      0.74       729\n",
      "     HS_Group       0.72      0.64      0.68       408\n",
      "  HS_Religion       0.82      0.62      0.71       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.65      0.35      0.45        57\n",
      "    HS_Gender       0.68      0.51      0.58        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.71      0.72       681\n",
      "  HS_Moderate       0.67      0.56      0.61       359\n",
      "    HS_Strong       0.80      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.69      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 244.11618328094482 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.922, F1 Micro: 0.7706, F1 Macro: 0.6723\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 132\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.260950803756714 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3531, Accuracy: 0.9049, F1 Micro: 0.7065, F1 Macro: 0.507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2435, Accuracy: 0.9186, F1 Micro: 0.7609, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9246, F1 Micro: 0.7793, F1 Macro: 0.6411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1512, Accuracy: 0.9267, F1 Micro: 0.7814, F1 Macro: 0.6826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9277, F1 Micro: 0.7899, F1 Macro: 0.6979\n",
      "Epoch 6/10, Train Loss: 0.1033, Accuracy: 0.9286, F1 Micro: 0.7893, F1 Macro: 0.706\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.9262, F1 Micro: 0.7858, F1 Macro: 0.7126\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.9249, F1 Micro: 0.7859, F1 Macro: 0.7163\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9263, F1 Micro: 0.7796, F1 Macro: 0.7169\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9263, F1 Micro: 0.7871, F1 Macro: 0.7221\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9277, F1 Micro: 0.7899, F1 Macro: 0.6979\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.74      0.66      0.70       408\n",
      "  HS_Religion       0.75      0.65      0.69       168\n",
      "      HS_Race       0.73      0.76      0.75       119\n",
      "  HS_Physical       0.62      0.18      0.27        57\n",
      "    HS_Gender       0.69      0.36      0.48        55\n",
      "     HS_Other       0.81      0.82      0.82       771\n",
      "      HS_Weak       0.71      0.74      0.72       681\n",
      "  HS_Moderate       0.68      0.61      0.64       359\n",
      "    HS_Strong       0.81      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.67      0.70      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 251.7137107849121 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3531, Accuracy: 0.9036, F1 Micro: 0.7184, F1 Macro: 0.5321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2453, Accuracy: 0.9171, F1 Micro: 0.7611, F1 Macro: 0.5863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.9218, F1 Micro: 0.7747, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1514, Accuracy: 0.9274, F1 Micro: 0.7791, F1 Macro: 0.6555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9301, F1 Micro: 0.7942, F1 Macro: 0.7028\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9285, F1 Micro: 0.7827, F1 Macro: 0.6909\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.926, F1 Micro: 0.7851, F1 Macro: 0.7099\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9267, F1 Micro: 0.7741, F1 Macro: 0.7015\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9254, F1 Micro: 0.785, F1 Macro: 0.7234\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9254, F1 Micro: 0.7873, F1 Macro: 0.7242\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9301, F1 Micro: 0.7942, F1 Macro: 0.7028\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.91      0.90      0.90      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.77      0.64      0.70       408\n",
      "  HS_Religion       0.78      0.67      0.72       168\n",
      "      HS_Race       0.72      0.73      0.73       119\n",
      "  HS_Physical       0.50      0.16      0.24        57\n",
      "    HS_Gender       0.88      0.38      0.53        55\n",
      "     HS_Other       0.82      0.81      0.81       771\n",
      "      HS_Weak       0.73      0.75      0.74       681\n",
      "  HS_Moderate       0.72      0.59      0.65       359\n",
      "    HS_Strong       0.81      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.79      5589\n",
      "    macro avg       0.77      0.67      0.70      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 248.073260307312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3511, Accuracy: 0.9043, F1 Micro: 0.7101, F1 Macro: 0.5185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9142, F1 Micro: 0.7568, F1 Macro: 0.5692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1937, Accuracy: 0.9244, F1 Micro: 0.7706, F1 Macro: 0.6269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9251, F1 Micro: 0.7795, F1 Macro: 0.6658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1257, Accuracy: 0.9261, F1 Micro: 0.7859, F1 Macro: 0.6839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1003, Accuracy: 0.9292, F1 Micro: 0.7893, F1 Macro: 0.7102\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.927, F1 Micro: 0.7836, F1 Macro: 0.7069\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9284, F1 Micro: 0.7879, F1 Macro: 0.7171\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9272, F1 Micro: 0.7885, F1 Macro: 0.7202\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9249, F1 Micro: 0.7866, F1 Macro: 0.7184\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9292, F1 Micro: 0.7893, F1 Macro: 0.7102\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.86      0.87      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.78      0.70      0.74       729\n",
      "     HS_Group       0.69      0.68      0.68       408\n",
      "  HS_Religion       0.75      0.65      0.70       168\n",
      "      HS_Race       0.77      0.71      0.74       119\n",
      "  HS_Physical       0.84      0.28      0.42        57\n",
      "    HS_Gender       0.80      0.36      0.50        55\n",
      "     HS_Other       0.84      0.79      0.81       771\n",
      "      HS_Weak       0.76      0.68      0.72       681\n",
      "  HS_Moderate       0.65      0.62      0.63       359\n",
      "    HS_Strong       0.83      0.78      0.80        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.79      0.67      0.71      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.43      0.43      5589\n",
      "\n",
      "Training completed in 249.38940739631653 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9223, F1 Micro: 0.7715, F1 Macro: 0.6738\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 112\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.110610961914062 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3435, Accuracy: 0.906, F1 Micro: 0.6975, F1 Macro: 0.4676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2324, Accuracy: 0.9169, F1 Micro: 0.7573, F1 Macro: 0.6002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1814, Accuracy: 0.9235, F1 Micro: 0.7812, F1 Macro: 0.6355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9284, F1 Micro: 0.7866, F1 Macro: 0.6808\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9223, F1 Micro: 0.7811, F1 Macro: 0.6888\n",
      "Epoch 6/10, Train Loss: 0.0959, Accuracy: 0.9255, F1 Micro: 0.7785, F1 Macro: 0.7036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9276, F1 Micro: 0.7893, F1 Macro: 0.7081\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.928, F1 Micro: 0.7882, F1 Macro: 0.7042\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9264, F1 Micro: 0.7858, F1 Macro: 0.7207\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9268, F1 Micro: 0.789, F1 Macro: 0.721\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9276, F1 Micro: 0.7893, F1 Macro: 0.7081\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.88      0.93      0.91      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.74      0.63      0.68       408\n",
      "  HS_Religion       0.73      0.65      0.69       168\n",
      "      HS_Race       0.67      0.82      0.73       119\n",
      "  HS_Physical       0.73      0.28      0.41        57\n",
      "    HS_Gender       0.73      0.40      0.52        55\n",
      "     HS_Other       0.84      0.80      0.82       771\n",
      "      HS_Weak       0.71      0.74      0.73       681\n",
      "  HS_Moderate       0.67      0.58      0.62       359\n",
      "    HS_Strong       0.82      0.76      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.68      0.71      5589\n",
      " weighted avg       0.79      0.78      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 252.64338088035583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.347, Accuracy: 0.9066, F1 Micro: 0.6952, F1 Macro: 0.4762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2344, Accuracy: 0.9198, F1 Micro: 0.7594, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1823, Accuracy: 0.9261, F1 Micro: 0.7813, F1 Macro: 0.6295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1456, Accuracy: 0.9285, F1 Micro: 0.7897, F1 Macro: 0.6651\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.9226, F1 Micro: 0.7833, F1 Macro: 0.6957\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9241, F1 Micro: 0.7812, F1 Macro: 0.699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9303, F1 Micro: 0.7901, F1 Macro: 0.715\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.922, F1 Micro: 0.7833, F1 Macro: 0.7133\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9275, F1 Micro: 0.784, F1 Macro: 0.7131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9274, F1 Micro: 0.792, F1 Macro: 0.7237\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9274, F1 Micro: 0.792, F1 Macro: 0.7237\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.87      1137\n",
      "      Abusive       0.91      0.90      0.91      1008\n",
      "HS_Individual       0.72      0.79      0.75       729\n",
      "     HS_Group       0.72      0.64      0.68       408\n",
      "  HS_Religion       0.74      0.64      0.69       168\n",
      "      HS_Race       0.72      0.85      0.78       119\n",
      "  HS_Physical       0.61      0.40      0.48        57\n",
      "    HS_Gender       0.62      0.53      0.57        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.70      0.77      0.73       681\n",
      "  HS_Moderate       0.67      0.55      0.60       359\n",
      "    HS_Strong       0.79      0.84      0.81        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5589\n",
      "    macro avg       0.74      0.72      0.72      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.45      0.45      0.44      5589\n",
      "\n",
      "Training completed in 253.98374795913696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3459, Accuracy: 0.9056, F1 Micro: 0.6904, F1 Macro: 0.4663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2345, Accuracy: 0.9188, F1 Micro: 0.752, F1 Macro: 0.5821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1834, Accuracy: 0.9229, F1 Micro: 0.7795, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9284, F1 Micro: 0.7904, F1 Macro: 0.6639\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9236, F1 Micro: 0.7851, F1 Macro: 0.6957\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.9238, F1 Micro: 0.782, F1 Macro: 0.7077\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9253, F1 Micro: 0.7861, F1 Macro: 0.7087\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9292, F1 Micro: 0.7893, F1 Macro: 0.7055\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9276, F1 Micro: 0.7886, F1 Macro: 0.713\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9276, F1 Micro: 0.7886, F1 Macro: 0.718\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9284, F1 Micro: 0.7904, F1 Macro: 0.6639\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.89      0.90      0.89      1008\n",
      "HS_Individual       0.75      0.77      0.76       729\n",
      "     HS_Group       0.74      0.65      0.69       408\n",
      "  HS_Religion       0.78      0.59      0.67       168\n",
      "      HS_Race       0.80      0.72      0.76       119\n",
      "  HS_Physical       0.70      0.12      0.21        57\n",
      "    HS_Gender       0.71      0.09      0.16        55\n",
      "     HS_Other       0.80      0.83      0.81       771\n",
      "      HS_Weak       0.72      0.75      0.73       681\n",
      "  HS_Moderate       0.70      0.58      0.63       359\n",
      "    HS_Strong       0.80      0.74      0.77        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.64      0.66      5589\n",
      " weighted avg       0.80      0.78      0.78      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 251.1559112071991 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9225, F1 Micro: 0.7724, F1 Macro: 0.6748\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 92\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.614372491836548 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3404, Accuracy: 0.9077, F1 Micro: 0.715, F1 Macro: 0.537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2299, Accuracy: 0.9208, F1 Micro: 0.7571, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.185, Accuracy: 0.9218, F1 Micro: 0.7772, F1 Macro: 0.6448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9297, F1 Micro: 0.7859, F1 Macro: 0.6542\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.9288, F1 Micro: 0.7836, F1 Macro: 0.6839\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9258, F1 Micro: 0.7847, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9282, F1 Micro: 0.7913, F1 Macro: 0.7063\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9294, F1 Micro: 0.7882, F1 Macro: 0.7141\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9276, F1 Micro: 0.7815, F1 Macro: 0.7162\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9281, F1 Micro: 0.7871, F1 Macro: 0.716\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9282, F1 Micro: 0.7913, F1 Macro: 0.7063\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.90      0.91      0.91      1008\n",
      "HS_Individual       0.73      0.79      0.76       729\n",
      "     HS_Group       0.76      0.60      0.67       408\n",
      "  HS_Religion       0.82      0.57      0.67       168\n",
      "      HS_Race       0.83      0.66      0.74       119\n",
      "  HS_Physical       0.76      0.33      0.46        57\n",
      "    HS_Gender       0.68      0.35      0.46        55\n",
      "     HS_Other       0.79      0.85      0.82       771\n",
      "      HS_Weak       0.70      0.77      0.74       681\n",
      "  HS_Moderate       0.71      0.53      0.61       359\n",
      "    HS_Strong       0.80      0.77      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.78      0.67      0.71      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 256.89253401756287 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3455, Accuracy: 0.9056, F1 Micro: 0.6857, F1 Macro: 0.4782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2298, Accuracy: 0.9212, F1 Micro: 0.7638, F1 Macro: 0.5986\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1857, Accuracy: 0.9235, F1 Micro: 0.7785, F1 Macro: 0.6338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1451, Accuracy: 0.9281, F1 Micro: 0.7848, F1 Macro: 0.6561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.9291, F1 Micro: 0.7905, F1 Macro: 0.6913\n",
      "Epoch 6/10, Train Loss: 0.0902, Accuracy: 0.9253, F1 Micro: 0.7834, F1 Macro: 0.6919\n",
      "Epoch 7/10, Train Loss: 0.0714, Accuracy: 0.9282, F1 Micro: 0.7873, F1 Macro: 0.7067\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.929, F1 Micro: 0.7881, F1 Macro: 0.7194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9299, F1 Micro: 0.7922, F1 Macro: 0.727\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9283, F1 Micro: 0.7875, F1 Macro: 0.7222\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9299, F1 Micro: 0.7922, F1 Macro: 0.727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.85      0.86      1137\n",
      "      Abusive       0.92      0.90      0.91      1008\n",
      "HS_Individual       0.77      0.72      0.74       729\n",
      "     HS_Group       0.71      0.66      0.68       408\n",
      "  HS_Religion       0.80      0.68      0.74       168\n",
      "      HS_Race       0.73      0.76      0.74       119\n",
      "  HS_Physical       0.62      0.42      0.50        57\n",
      "    HS_Gender       0.68      0.45      0.54        55\n",
      "     HS_Other       0.81      0.82      0.81       771\n",
      "      HS_Weak       0.74      0.68      0.71       681\n",
      "  HS_Moderate       0.68      0.60      0.64       359\n",
      "    HS_Strong       0.80      0.87      0.83        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.76      0.70      0.73      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 258.21734070777893 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3413, Accuracy: 0.9071, F1 Micro: 0.6987, F1 Macro: 0.508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2291, Accuracy: 0.92, F1 Micro: 0.7488, F1 Macro: 0.5689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1857, Accuracy: 0.9217, F1 Micro: 0.777, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9268, F1 Micro: 0.7862, F1 Macro: 0.6562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9254, F1 Micro: 0.7866, F1 Macro: 0.6952\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9244, F1 Micro: 0.7824, F1 Macro: 0.6893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.928, F1 Micro: 0.7893, F1 Macro: 0.7146\n",
      "Epoch 8/10, Train Loss: 0.0569, Accuracy: 0.9266, F1 Micro: 0.7822, F1 Macro: 0.7162\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.926, F1 Micro: 0.7804, F1 Macro: 0.7144\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9229, F1 Micro: 0.7804, F1 Macro: 0.7146\n",
      "Model 3 - Iteration 9818: Accuracy: 0.928, F1 Micro: 0.7893, F1 Macro: 0.7146\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.73      0.74       729\n",
      "     HS_Group       0.71      0.66      0.68       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.77      0.72      0.74       119\n",
      "  HS_Physical       0.75      0.32      0.44        57\n",
      "    HS_Gender       0.79      0.40      0.53        55\n",
      "     HS_Other       0.81      0.80      0.81       771\n",
      "      HS_Weak       0.73      0.72      0.72       681\n",
      "  HS_Moderate       0.66      0.58      0.62       359\n",
      "    HS_Strong       0.79      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.69      0.71      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 257.99391651153564 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9228, F1 Micro: 0.7731, F1 Macro: 0.6765\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 72\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.9234938621521 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3353, Accuracy: 0.9069, F1 Micro: 0.6989, F1 Macro: 0.516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2246, Accuracy: 0.9218, F1 Micro: 0.755, F1 Macro: 0.5917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1814, Accuracy: 0.9251, F1 Micro: 0.7833, F1 Macro: 0.6425\n",
      "Epoch 4/10, Train Loss: 0.1471, Accuracy: 0.9276, F1 Micro: 0.7784, F1 Macro: 0.6556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1184, Accuracy: 0.927, F1 Micro: 0.7883, F1 Macro: 0.6901\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9262, F1 Micro: 0.7805, F1 Macro: 0.6953\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.9297, F1 Micro: 0.7859, F1 Macro: 0.712\n",
      "Epoch 8/10, Train Loss: 0.0602, Accuracy: 0.927, F1 Micro: 0.7779, F1 Macro: 0.7043\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9262, F1 Micro: 0.7839, F1 Macro: 0.7096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9288, F1 Micro: 0.7889, F1 Macro: 0.7239\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9288, F1 Micro: 0.7889, F1 Macro: 0.7239\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.75      0.74      0.74       729\n",
      "     HS_Group       0.75      0.61      0.67       408\n",
      "  HS_Religion       0.78      0.64      0.70       168\n",
      "      HS_Race       0.72      0.77      0.75       119\n",
      "  HS_Physical       0.57      0.46      0.50        57\n",
      "    HS_Gender       0.62      0.58      0.60        55\n",
      "     HS_Other       0.85      0.77      0.81       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.70      0.53      0.61       359\n",
      "    HS_Strong       0.80      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.81      0.77      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.81      0.77      0.79      5589\n",
      "  samples avg       0.44      0.44      0.43      5589\n",
      "\n",
      "Training completed in 261.48824191093445 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3381, Accuracy: 0.9055, F1 Micro: 0.6903, F1 Macro: 0.4679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2274, Accuracy: 0.9212, F1 Micro: 0.7534, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1815, Accuracy: 0.9263, F1 Micro: 0.7833, F1 Macro: 0.6343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1479, Accuracy: 0.928, F1 Micro: 0.7883, F1 Macro: 0.6579\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9275, F1 Micro: 0.7849, F1 Macro: 0.6815\n",
      "Epoch 6/10, Train Loss: 0.0903, Accuracy: 0.9231, F1 Micro: 0.782, F1 Macro: 0.6838\n",
      "Epoch 7/10, Train Loss: 0.0676, Accuracy: 0.927, F1 Micro: 0.7793, F1 Macro: 0.7004\n",
      "Epoch 8/10, Train Loss: 0.0602, Accuracy: 0.9285, F1 Micro: 0.7866, F1 Macro: 0.7126\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9257, F1 Micro: 0.7835, F1 Macro: 0.7149\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9238, F1 Micro: 0.7791, F1 Macro: 0.7146\n",
      "Model 2 - Iteration 10018: Accuracy: 0.928, F1 Micro: 0.7883, F1 Macro: 0.6579\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.91      0.88      0.90      1008\n",
      "HS_Individual       0.74      0.77      0.76       729\n",
      "     HS_Group       0.76      0.63      0.69       408\n",
      "  HS_Religion       0.79      0.62      0.70       168\n",
      "      HS_Race       0.82      0.71      0.76       119\n",
      "  HS_Physical       0.50      0.05      0.10        57\n",
      "    HS_Gender       0.75      0.11      0.19        55\n",
      "     HS_Other       0.80      0.84      0.82       771\n",
      "      HS_Weak       0.70      0.75      0.72       681\n",
      "  HS_Moderate       0.72      0.55      0.62       359\n",
      "    HS_Strong       0.78      0.78      0.78        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.63      0.66      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 259.4183065891266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.334, Accuracy: 0.9078, F1 Micro: 0.701, F1 Macro: 0.4996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2255, Accuracy: 0.9204, F1 Micro: 0.7537, F1 Macro: 0.576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1833, Accuracy: 0.926, F1 Micro: 0.7817, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1472, Accuracy: 0.9282, F1 Micro: 0.7825, F1 Macro: 0.6636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1166, Accuracy: 0.9289, F1 Micro: 0.7878, F1 Macro: 0.6848\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.928, F1 Micro: 0.789, F1 Macro: 0.7016\n",
      "Epoch 7/10, Train Loss: 0.0705, Accuracy: 0.9268, F1 Micro: 0.7825, F1 Macro: 0.7106\n",
      "Epoch 8/10, Train Loss: 0.0602, Accuracy: 0.9265, F1 Micro: 0.7842, F1 Macro: 0.7099\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9253, F1 Micro: 0.7782, F1 Macro: 0.7069\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9276, F1 Micro: 0.782, F1 Macro: 0.7192\n",
      "Model 3 - Iteration 10018: Accuracy: 0.928, F1 Micro: 0.789, F1 Macro: 0.7016\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.87      0.87      1137\n",
      "      Abusive       0.92      0.89      0.90      1008\n",
      "HS_Individual       0.73      0.80      0.76       729\n",
      "     HS_Group       0.78      0.59      0.67       408\n",
      "  HS_Religion       0.71      0.69      0.70       168\n",
      "      HS_Race       0.76      0.76      0.76       119\n",
      "  HS_Physical       0.79      0.26      0.39        57\n",
      "    HS_Gender       0.66      0.35      0.45        55\n",
      "     HS_Other       0.81      0.80      0.81       771\n",
      "      HS_Weak       0.70      0.78      0.74       681\n",
      "  HS_Moderate       0.72      0.48      0.58       359\n",
      "    HS_Strong       0.79      0.79      0.79        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.77      0.67      0.70      5589\n",
      " weighted avg       0.80      0.78      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 263.34196972846985 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.923, F1 Micro: 0.7738, F1 Macro: 0.6773\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 52\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.565875768661499 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3288, Accuracy: 0.9052, F1 Micro: 0.6982, F1 Macro: 0.5217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2238, Accuracy: 0.9177, F1 Micro: 0.765, F1 Macro: 0.6005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1742, Accuracy: 0.9234, F1 Micro: 0.7763, F1 Macro: 0.6278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1413, Accuracy: 0.9254, F1 Micro: 0.7801, F1 Macro: 0.658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9277, F1 Micro: 0.7921, F1 Macro: 0.7118\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9262, F1 Micro: 0.7871, F1 Macro: 0.7017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.9301, F1 Micro: 0.7951, F1 Macro: 0.7246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9298, F1 Micro: 0.7964, F1 Macro: 0.7271\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9227, F1 Micro: 0.782, F1 Macro: 0.714\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9282, F1 Micro: 0.7901, F1 Macro: 0.7234\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9298, F1 Micro: 0.7964, F1 Macro: 0.7271\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.87      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.74      0.76      0.75       729\n",
      "     HS_Group       0.75      0.65      0.69       408\n",
      "  HS_Religion       0.78      0.64      0.71       168\n",
      "      HS_Race       0.77      0.76      0.76       119\n",
      "  HS_Physical       0.72      0.37      0.49        57\n",
      "    HS_Gender       0.75      0.44      0.55        55\n",
      "     HS_Other       0.79      0.85      0.82       771\n",
      "      HS_Weak       0.72      0.74      0.73       681\n",
      "  HS_Moderate       0.71      0.56      0.63       359\n",
      "    HS_Strong       0.79      0.86      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.80      5589\n",
      "    macro avg       0.77      0.70      0.73      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 269.1063930988312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3317, Accuracy: 0.9036, F1 Micro: 0.6776, F1 Macro: 0.4946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2247, Accuracy: 0.9185, F1 Micro: 0.7582, F1 Macro: 0.5929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1771, Accuracy: 0.9249, F1 Micro: 0.7697, F1 Macro: 0.6276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1414, Accuracy: 0.9251, F1 Micro: 0.7802, F1 Macro: 0.6606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.9271, F1 Micro: 0.7888, F1 Macro: 0.6948\n",
      "Epoch 6/10, Train Loss: 0.0892, Accuracy: 0.9272, F1 Micro: 0.786, F1 Macro: 0.7025\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.9271, F1 Micro: 0.7824, F1 Macro: 0.7029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9285, F1 Micro: 0.7928, F1 Macro: 0.717\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9271, F1 Micro: 0.7873, F1 Macro: 0.7263\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9254, F1 Micro: 0.7775, F1 Macro: 0.708\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9285, F1 Micro: 0.7928, F1 Macro: 0.717\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.75      0.75      0.75       729\n",
      "     HS_Group       0.72      0.68      0.70       408\n",
      "  HS_Religion       0.77      0.61      0.68       168\n",
      "      HS_Race       0.79      0.76      0.78       119\n",
      "  HS_Physical       0.64      0.32      0.42        57\n",
      "    HS_Gender       0.77      0.36      0.49        55\n",
      "     HS_Other       0.79      0.84      0.81       771\n",
      "      HS_Weak       0.73      0.73      0.73       681\n",
      "  HS_Moderate       0.68      0.61      0.64       359\n",
      "    HS_Strong       0.81      0.87      0.84        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.77      0.69      0.72      5589\n",
      " weighted avg       0.79      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 266.51644492149353 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3294, Accuracy: 0.9058, F1 Micro: 0.6906, F1 Macro: 0.5009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2234, Accuracy: 0.919, F1 Micro: 0.7607, F1 Macro: 0.5777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.175, Accuracy: 0.9249, F1 Micro: 0.7712, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1416, Accuracy: 0.9253, F1 Micro: 0.7805, F1 Macro: 0.6573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1131, Accuracy: 0.9284, F1 Micro: 0.7889, F1 Macro: 0.7057\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.924, F1 Micro: 0.787, F1 Macro: 0.7076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.931, F1 Micro: 0.7963, F1 Macro: 0.7079\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.928, F1 Micro: 0.7833, F1 Macro: 0.7059\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9262, F1 Micro: 0.78, F1 Macro: 0.714\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.928, F1 Micro: 0.7923, F1 Macro: 0.7234\n",
      "Model 3 - Iteration 10218: Accuracy: 0.931, F1 Micro: 0.7963, F1 Macro: 0.7079\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.86      0.87      1137\n",
      "      Abusive       0.91      0.91      0.91      1008\n",
      "HS_Individual       0.77      0.74      0.76       729\n",
      "     HS_Group       0.72      0.68      0.70       408\n",
      "  HS_Religion       0.76      0.67      0.71       168\n",
      "      HS_Race       0.77      0.74      0.75       119\n",
      "  HS_Physical       0.71      0.30      0.42        57\n",
      "    HS_Gender       0.67      0.29      0.41        55\n",
      "     HS_Other       0.83      0.81      0.82       771\n",
      "      HS_Weak       0.74      0.72      0.73       681\n",
      "  HS_Moderate       0.69      0.60      0.64       359\n",
      "    HS_Strong       0.80      0.76      0.78        97\n",
      "\n",
      "    micro avg       0.81      0.78      0.80      5589\n",
      "    macro avg       0.77      0.67      0.71      5589\n",
      " weighted avg       0.81      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 269.92161321640015 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9233, F1 Micro: 0.7746, F1 Macro: 0.6788\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 32\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.913076877593994 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3262, Accuracy: 0.9069, F1 Micro: 0.7072, F1 Macro: 0.4927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2169, Accuracy: 0.9182, F1 Micro: 0.7442, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1764, Accuracy: 0.9232, F1 Micro: 0.7636, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9269, F1 Micro: 0.779, F1 Macro: 0.6788\n",
      "Epoch 5/10, Train Loss: 0.1097, Accuracy: 0.92, F1 Micro: 0.7716, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9205, F1 Micro: 0.7793, F1 Macro: 0.6954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9241, F1 Micro: 0.7841, F1 Macro: 0.7141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9279, F1 Micro: 0.7885, F1 Macro: 0.7246\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9238, F1 Micro: 0.781, F1 Macro: 0.7132\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.9292, F1 Micro: 0.7869, F1 Macro: 0.7247\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9279, F1 Micro: 0.7885, F1 Macro: 0.7246\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.90      0.92      0.91      1008\n",
      "HS_Individual       0.73      0.76      0.74       729\n",
      "     HS_Group       0.73      0.61      0.66       408\n",
      "  HS_Religion       0.78      0.64      0.70       168\n",
      "      HS_Race       0.75      0.79      0.77       119\n",
      "  HS_Physical       0.62      0.44      0.52        57\n",
      "    HS_Gender       0.64      0.53      0.58        55\n",
      "     HS_Other       0.83      0.80      0.81       771\n",
      "      HS_Weak       0.71      0.72      0.72       681\n",
      "  HS_Moderate       0.70      0.52      0.60       359\n",
      "    HS_Strong       0.80      0.85      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.78      0.79      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 274.27326250076294 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.329, Accuracy: 0.9081, F1 Micro: 0.722, F1 Macro: 0.5253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2181, Accuracy: 0.9192, F1 Micro: 0.7594, F1 Macro: 0.5992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.9259, F1 Micro: 0.7792, F1 Macro: 0.6226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.9269, F1 Micro: 0.7796, F1 Macro: 0.6603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1083, Accuracy: 0.9243, F1 Micro: 0.7826, F1 Macro: 0.6809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.924, F1 Micro: 0.7831, F1 Macro: 0.6903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9271, F1 Micro: 0.786, F1 Macro: 0.7166\n",
      "Epoch 8/10, Train Loss: 0.0564, Accuracy: 0.9242, F1 Micro: 0.7845, F1 Macro: 0.7125\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9272, F1 Micro: 0.7859, F1 Macro: 0.7196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9277, F1 Micro: 0.7869, F1 Macro: 0.7214\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9277, F1 Micro: 0.7869, F1 Macro: 0.7214\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1137\n",
      "      Abusive       0.91      0.92      0.91      1008\n",
      "HS_Individual       0.73      0.77      0.75       729\n",
      "     HS_Group       0.76      0.59      0.66       408\n",
      "  HS_Religion       0.71      0.67      0.69       168\n",
      "      HS_Race       0.73      0.80      0.76       119\n",
      "  HS_Physical       0.70      0.40      0.51        57\n",
      "    HS_Gender       0.68      0.51      0.58        55\n",
      "     HS_Other       0.84      0.78      0.81       771\n",
      "      HS_Weak       0.70      0.75      0.72       681\n",
      "  HS_Moderate       0.70      0.50      0.59       359\n",
      "    HS_Strong       0.81      0.81      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5589\n",
      "    macro avg       0.76      0.70      0.72      5589\n",
      " weighted avg       0.80      0.77      0.78      5589\n",
      "  samples avg       0.45      0.44      0.43      5589\n",
      "\n",
      "Training completed in 276.93021059036255 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3246, Accuracy: 0.9107, F1 Micro: 0.7202, F1 Macro: 0.5292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2172, Accuracy: 0.9195, F1 Micro: 0.7498, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.9252, F1 Micro: 0.7734, F1 Macro: 0.6264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1388, Accuracy: 0.9284, F1 Micro: 0.7817, F1 Macro: 0.6788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1091, Accuracy: 0.9275, F1 Micro: 0.7883, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0866, Accuracy: 0.9279, F1 Micro: 0.7927, F1 Macro: 0.7144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9286, F1 Micro: 0.7944, F1 Macro: 0.7268\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9284, F1 Micro: 0.7907, F1 Macro: 0.7219\n",
      "Epoch 9/10, Train Loss: 0.0484, Accuracy: 0.9263, F1 Micro: 0.7837, F1 Macro: 0.7164\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9225, F1 Micro: 0.7827, F1 Macro: 0.7217\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9286, F1 Micro: 0.7944, F1 Macro: 0.7268\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.88      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.73      0.78      0.75       729\n",
      "     HS_Group       0.73      0.65      0.68       408\n",
      "  HS_Religion       0.79      0.65      0.71       168\n",
      "      HS_Race       0.81      0.73      0.77       119\n",
      "  HS_Physical       0.66      0.44      0.53        57\n",
      "    HS_Gender       0.68      0.45      0.54        55\n",
      "     HS_Other       0.79      0.84      0.81       771\n",
      "      HS_Weak       0.72      0.75      0.74       681\n",
      "  HS_Moderate       0.68      0.55      0.61       359\n",
      "    HS_Strong       0.78      0.81      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.80      0.79      5589\n",
      "    macro avg       0.76      0.71      0.73      5589\n",
      " weighted avg       0.79      0.80      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 277.1517844200134 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9234, F1 Micro: 0.7752, F1 Macro: 0.6805\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Samples above threshold: 12\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.4049999713897705 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3265, Accuracy: 0.9046, F1 Micro: 0.6759, F1 Macro: 0.4484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2144, Accuracy: 0.9219, F1 Micro: 0.7634, F1 Macro: 0.608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1706, Accuracy: 0.927, F1 Micro: 0.7739, F1 Macro: 0.6372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1356, Accuracy: 0.9247, F1 Micro: 0.7777, F1 Macro: 0.6693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9255, F1 Micro: 0.7833, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0833, Accuracy: 0.9272, F1 Micro: 0.7883, F1 Macro: 0.7015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.929, F1 Micro: 0.7932, F1 Macro: 0.709\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9258, F1 Micro: 0.7869, F1 Macro: 0.7143\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9273, F1 Micro: 0.7883, F1 Macro: 0.7191\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9259, F1 Micro: 0.7845, F1 Macro: 0.7175\n",
      "Model 1 - Iteration 10535: Accuracy: 0.929, F1 Micro: 0.7932, F1 Macro: 0.709\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.86      1137\n",
      "      Abusive       0.90      0.91      0.90      1008\n",
      "HS_Individual       0.77      0.75      0.76       729\n",
      "     HS_Group       0.70      0.68      0.69       408\n",
      "  HS_Religion       0.72      0.70      0.71       168\n",
      "      HS_Race       0.75      0.76      0.76       119\n",
      "  HS_Physical       0.81      0.30      0.44        57\n",
      "    HS_Gender       0.62      0.29      0.40        55\n",
      "     HS_Other       0.82      0.82      0.82       771\n",
      "      HS_Weak       0.73      0.72      0.73       681\n",
      "  HS_Moderate       0.65      0.61      0.63       359\n",
      "    HS_Strong       0.80      0.84      0.82        97\n",
      "\n",
      "    micro avg       0.80      0.79      0.79      5589\n",
      "    macro avg       0.76      0.69      0.71      5589\n",
      " weighted avg       0.80      0.79      0.79      5589\n",
      "  samples avg       0.46      0.45      0.44      5589\n",
      "\n",
      "Training completed in 276.73084235191345 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3289, Accuracy: 0.9053, F1 Micro: 0.6883, F1 Macro: 0.4659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2157, Accuracy: 0.9199, F1 Micro: 0.7595, F1 Macro: 0.5918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1714, Accuracy: 0.9274, F1 Micro: 0.7795, F1 Macro: 0.6393\n",
      "Epoch 4/10, Train Loss: 0.1354, Accuracy: 0.9239, F1 Micro: 0.7727, F1 Macro: 0.6611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1083, Accuracy: 0.9261, F1 Micro: 0.7827, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0818, Accuracy: 0.9293, F1 Micro: 0.7861, F1 Macro: 0.7036\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.928, F1 Micro: 0.7842, F1 Macro: 0.7055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9269, F1 Micro: 0.7866, F1 Macro: 0.7204\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9264, F1 Micro: 0.7862, F1 Macro: 0.7233\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9261, F1 Micro: 0.7829, F1 Macro: 0.7237\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9269, F1 Micro: 0.7866, F1 Macro: 0.7204\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1137\n",
      "      Abusive       0.89      0.93      0.91      1008\n",
      "HS_Individual       0.74      0.75      0.74       729\n",
      "     HS_Group       0.72      0.63      0.67       408\n",
      "  HS_Religion       0.73      0.68      0.70       168\n",
      "      HS_Race       0.71      0.81      0.76       119\n",
      "  HS_Physical       0.63      0.39      0.48        57\n",
      "    HS_Gender       0.67      0.51      0.58        55\n",
      "     HS_Other       0.82      0.77      0.79       771\n",
      "      HS_Weak       0.72      0.73      0.72       681\n",
      "  HS_Moderate       0.68      0.57      0.62       359\n",
      "    HS_Strong       0.80      0.82      0.81        97\n",
      "\n",
      "    micro avg       0.80      0.78      0.79      5589\n",
      "    macro avg       0.75      0.70      0.72      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 273.77859354019165 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3261, Accuracy: 0.9044, F1 Micro: 0.6747, F1 Macro: 0.4619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2161, Accuracy: 0.9181, F1 Micro: 0.749, F1 Macro: 0.5582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1726, Accuracy: 0.9259, F1 Micro: 0.776, F1 Macro: 0.6448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1364, Accuracy: 0.9268, F1 Micro: 0.7812, F1 Macro: 0.6718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1101, Accuracy: 0.927, F1 Micro: 0.7876, F1 Macro: 0.6951\n",
      "Epoch 6/10, Train Loss: 0.0838, Accuracy: 0.9264, F1 Micro: 0.7834, F1 Macro: 0.6859\n",
      "Epoch 7/10, Train Loss: 0.0696, Accuracy: 0.9279, F1 Micro: 0.7815, F1 Macro: 0.7042\n",
      "Epoch 8/10, Train Loss: 0.0576, Accuracy: 0.928, F1 Micro: 0.7864, F1 Macro: 0.7126\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.9244, F1 Micro: 0.7844, F1 Macro: 0.7171\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9261, F1 Micro: 0.7782, F1 Macro: 0.7073\n",
      "Model 3 - Iteration 10535: Accuracy: 0.927, F1 Micro: 0.7876, F1 Macro: 0.6951\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1137\n",
      "      Abusive       0.89      0.91      0.90      1008\n",
      "HS_Individual       0.76      0.74      0.75       729\n",
      "     HS_Group       0.69      0.69      0.69       408\n",
      "  HS_Religion       0.70      0.71      0.71       168\n",
      "      HS_Race       0.69      0.78      0.74       119\n",
      "  HS_Physical       0.72      0.23      0.35        57\n",
      "    HS_Gender       0.59      0.29      0.39        55\n",
      "     HS_Other       0.81      0.81      0.81       771\n",
      "      HS_Weak       0.74      0.71      0.72       681\n",
      "  HS_Moderate       0.68      0.60      0.63       359\n",
      "    HS_Strong       0.79      0.80      0.80        97\n",
      "\n",
      "    micro avg       0.79      0.78      0.79      5589\n",
      "    macro avg       0.74      0.68      0.70      5589\n",
      " weighted avg       0.79      0.78      0.78      5589\n",
      "  samples avg       0.46      0.44      0.44      5589\n",
      "\n",
      "Training completed in 275.2778322696686 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9236, F1 Micro: 0.7757, F1 Macro: 0.6815\n",
      "Total sampling time: 1304.67 seconds\n",
      "Total runtime: 18794.336451768875 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6530808,
     "sourceId": 10555762,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20706.425034,
   "end_time": "2025-01-30T11:43:32.543289",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-30T05:58:26.118255",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05c8db3b81bb4cdda0f389e742a426d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b06781e87504f8997d41ae09856085d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15abe137d8c34eabbbdb7fc31e6ee408": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20cdbb003ca44a92a323397935a5ba5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a5da777334e4bab9b47d9513cb161eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "359b11ec942f42858920fc6952c44c42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4063326370dc4782ab517eea5961ab2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7380a2ea7ad447b78022e44242e277f2",
        "IPY_MODEL_fcd36261874a4e83ae0f8bdb6ef8b690",
        "IPY_MODEL_7da8c3b0483c4eb9b24eeae8d7e96c5f"
       ],
       "layout": "IPY_MODEL_ba12c594b3e04024a540ee285e45c94e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "40ae3d8fc7d0495b916fe2f052d09d02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "468d5caa8be64342a9a3a3702f4587fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "52c93da4431b4d7687d8d1e0c02a9e9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_20cdbb003ca44a92a323397935a5ba5b",
       "placeholder": "​",
       "style": "IPY_MODEL_40ae3d8fc7d0495b916fe2f052d09d02",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "5870411857cc4a18bd41786aa5061b14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5b16797b6d2c4d549eebc4af7ba16a95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5c5fc85fa3394642be31eb1447fab601": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e87ab03396d4b23adeb284721f16717": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a817a13f7a494f188de14a5bf8db1738",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bd37fb4cc190449e9c27d5162bafb8b2",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "605ecda1f7004801ae36acb24fbbc5a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6e97afa03f8f4fa5bcd17e770200ab85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7380a2ea7ad447b78022e44242e277f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_05c8db3b81bb4cdda0f389e742a426d2",
       "placeholder": "​",
       "style": "IPY_MODEL_5b16797b6d2c4d549eebc4af7ba16a95",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "780e70692e6946e188c03179be057f60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "783e187d047b44b49b67cf048d63c6d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7da8c3b0483c4eb9b24eeae8d7e96c5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_98b70adbea0843778480839f2a5a050b",
       "placeholder": "​",
       "style": "IPY_MODEL_780e70692e6946e188c03179be057f60",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/1.53k [00:00&lt;00:00, 167kB/s]"
      }
     },
     "82ea559993c24ef7ae57c41c3cb7bb86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d376e68c1f74d6ca91decb59789664f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f7d6d07fde6541abb09f47ddbbeae36a",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5870411857cc4a18bd41786aa5061b14",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "8e4d0b91e2594208b4f7f18d244588ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0b06781e87504f8997d41ae09856085d",
       "placeholder": "​",
       "style": "IPY_MODEL_468d5caa8be64342a9a3a3702f4587fa",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: 100%"
      }
     },
     "938f407dde21450bbe2ac2d7f0c11a2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97d6110ef54d4ef69dfb81d8cd8a5f9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_de876459db1b4c109241f33054032d55",
       "placeholder": "​",
       "style": "IPY_MODEL_605ecda1f7004801ae36acb24fbbc5a8",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 11.5kB/s]"
      }
     },
     "98b70adbea0843778480839f2a5a050b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a0fce46e2f1544139f8d10938926d3b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a3225353ec3f4d9cbe6b1fcbd47a6903": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d52ed05550e143e28794de583f5c044d",
       "placeholder": "​",
       "style": "IPY_MODEL_359b11ec942f42858920fc6952c44c42",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "a42df274e1a4470bb1c80dd93adf460f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2a5da777334e4bab9b47d9513cb161eb",
       "placeholder": "​",
       "style": "IPY_MODEL_b65c5efd57c4436a87661ddfbd4431c7",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/229k [00:00&lt;00:00, 6.32MB/s]"
      }
     },
     "a4fcfbfae30847ab9da3ad8530fb509c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_82ea559993c24ef7ae57c41c3cb7bb86",
       "placeholder": "​",
       "style": "IPY_MODEL_6e97afa03f8f4fa5bcd17e770200ab85",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 184B/s]"
      }
     },
     "a817a13f7a494f188de14a5bf8db1738": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b65c5efd57c4436a87661ddfbd4431c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b6cc8d13c5d8479a99e1a72ee2116e75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5c5fc85fa3394642be31eb1447fab601",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_db064f8a02f84cac8401ae3eb6e0e169",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "ba12c594b3e04024a540ee285e45c94e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd37fb4cc190449e9c27d5162bafb8b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c1e4cf55624042c0911aa875ef9b7518": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d52ed05550e143e28794de583f5c044d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db064f8a02f84cac8401ae3eb6e0e169": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "de876459db1b4c109241f33054032d55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1b0966e66904aed8e1c8f46d7b5c6ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_52c93da4431b4d7687d8d1e0c02a9e9b",
        "IPY_MODEL_8d376e68c1f74d6ca91decb59789664f",
        "IPY_MODEL_a4fcfbfae30847ab9da3ad8530fb509c"
       ],
       "layout": "IPY_MODEL_a0fce46e2f1544139f8d10938926d3b7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ef7ec9af2c0a46f49769d1531ccdabc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8e4d0b91e2594208b4f7f18d244588ff",
        "IPY_MODEL_5e87ab03396d4b23adeb284721f16717",
        "IPY_MODEL_a42df274e1a4470bb1c80dd93adf460f"
       ],
       "layout": "IPY_MODEL_938f407dde21450bbe2ac2d7f0c11a2c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f7d6d07fde6541abb09f47ddbbeae36a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa8e7ce8dadb4d22a6b61dd91112bba9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a3225353ec3f4d9cbe6b1fcbd47a6903",
        "IPY_MODEL_b6cc8d13c5d8479a99e1a72ee2116e75",
        "IPY_MODEL_97d6110ef54d4ef69dfb81d8cd8a5f9d"
       ],
       "layout": "IPY_MODEL_c1e4cf55624042c0911aa875ef9b7518",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fcd36261874a4e83ae0f8bdb6ef8b690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_15abe137d8c34eabbbdb7fc31e6ee408",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_783e187d047b44b49b67cf048d63c6d9",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
