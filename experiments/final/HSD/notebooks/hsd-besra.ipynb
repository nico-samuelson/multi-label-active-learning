{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce96c29",
   "metadata": {
    "papermill": {
     "duration": 0.012201,
     "end_time": "2025-03-15T07:48:58.211899",
     "exception": false,
     "start_time": "2025-03-15T07:48:58.199698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fda6ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:48:58.235689Z",
     "iopub.status.busy": "2025-03-15T07:48:58.235356Z",
     "iopub.status.idle": "2025-03-15T07:49:23.302065Z",
     "shell.execute_reply": "2025-03-15T07:49:23.301352Z"
    },
    "papermill": {
     "duration": 25.080531,
     "end_time": "2025-03-15T07:49:23.303720",
     "exception": false,
     "start_time": "2025-03-15T07:48:58.223189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35059377",
   "metadata": {
    "papermill": {
     "duration": 0.010895,
     "end_time": "2025-03-15T07:49:23.326509",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.315614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348693c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.349627Z",
     "iopub.status.busy": "2025-03-15T07:49:23.349100Z",
     "iopub.status.idle": "2025-03-15T07:49:23.352865Z",
     "shell.execute_reply": "2025-03-15T07:49:23.352112Z"
    },
    "papermill": {
     "duration": 0.016511,
     "end_time": "2025-03-15T07:49:23.353993",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.337482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132bf72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.376853Z",
     "iopub.status.busy": "2025-03-15T07:49:23.376641Z",
     "iopub.status.idle": "2025-03-15T07:49:23.380039Z",
     "shell.execute_reply": "2025-03-15T07:49:23.379440Z"
    },
    "papermill": {
     "duration": 0.015948,
     "end_time": "2025-03-15T07:49:23.381106",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.365158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4b4948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.404412Z",
     "iopub.status.busy": "2025-03-15T07:49:23.404188Z",
     "iopub.status.idle": "2025-03-15T07:49:23.413809Z",
     "shell.execute_reply": "2025-03-15T07:49:23.413155Z"
    },
    "papermill": {
     "duration": 0.022461,
     "end_time": "2025-03-15T07:49:23.415040",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.392579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bb2eb",
   "metadata": {
    "papermill": {
     "duration": 0.011063,
     "end_time": "2025-03-15T07:49:23.437913",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.426850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d50417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.461139Z",
     "iopub.status.busy": "2025-03-15T07:49:23.460867Z",
     "iopub.status.idle": "2025-03-15T07:49:23.516446Z",
     "shell.execute_reply": "2025-03-15T07:49:23.514944Z"
    },
    "papermill": {
     "duration": 0.069126,
     "end_time": "2025-03-15T07:49:23.518198",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.449072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "data_used = manager.list()\n",
    "sampling_dur = manager.list()\n",
    "new_samples = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'hsd-besra'\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "sequence_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6d3a6",
   "metadata": {
    "papermill": {
     "duration": 0.010891,
     "end_time": "2025-03-15T07:49:23.540837",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.529946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06270050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.563997Z",
     "iopub.status.busy": "2025-03-15T07:49:23.563704Z",
     "iopub.status.idle": "2025-03-15T07:49:23.704912Z",
     "shell.execute_reply": "2025-03-15T07:49:23.703973Z"
    },
    "papermill": {
     "duration": 0.154424,
     "end_time": "2025-03-15T07:49:23.706193",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.551769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (13169, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/re_dataset.csv', encoding='latin-1')\n",
    "\n",
    "alay_dict = pd.read_csv('/kaggle/input/multi-label-hate-speech-2/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "\n",
    "print(\"Shape: \", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143499bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.731116Z",
     "iopub.status.busy": "2025-03-15T07:49:23.730855Z",
     "iopub.status.idle": "2025-03-15T07:49:23.741426Z",
     "shell.execute_reply": "2025-03-15T07:49:23.740543Z"
    },
    "papermill": {
     "duration": 0.024384,
     "end_time": "2025-03-15T07:49:23.742730",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.718346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>HS_Individual</th>\n",
       "      <th>HS_Group</th>\n",
       "      <th>HS_Religion</th>\n",
       "      <th>HS_Race</th>\n",
       "      <th>HS_Physical</th>\n",
       "      <th>HS_Gender</th>\n",
       "      <th>HS_Other</th>\n",
       "      <th>HS_Weak</th>\n",
       "      <th>HS_Moderate</th>\n",
       "      <th>HS_Strong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- disaat semua cowok berusaha melacak perhatia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT USER: USER siapa yang telat ngasih tau elu?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41. Kadang aku berfikir, kenapa aku tetap perc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER USER Kaum cebong kapir udah keliatan dong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  HS  Abusive  \\\n",
       "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
       "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
       "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
       "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
       "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
       "\n",
       "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
       "0              1         0            0        0            0          0   \n",
       "1              0         0            0        0            0          0   \n",
       "2              0         0            0        0            0          0   \n",
       "3              0         0            0        0            0          0   \n",
       "4              0         1            1        0            0          0   \n",
       "\n",
       "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
       "0         1        1            0          0  \n",
       "1         0        0            0          0  \n",
       "2         0        0            0          0  \n",
       "3         0        0            0          0  \n",
       "4         0        0            1          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6b2e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.767579Z",
     "iopub.status.busy": "2025-03-15T07:49:23.767284Z",
     "iopub.status.idle": "2025-03-15T07:49:23.780557Z",
     "shell.execute_reply": "2025-03-15T07:49:23.779712Z"
    },
    "papermill": {
     "duration": 0.026611,
     "end_time": "2025-03-15T07:49:23.781863",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.755252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HS\n",
       "0    7608\n",
       "1    5561\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.HS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74bb21d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.806896Z",
     "iopub.status.busy": "2025-03-15T07:49:23.806672Z",
     "iopub.status.idle": "2025-03-15T07:49:23.812192Z",
     "shell.execute_reply": "2025-03-15T07:49:23.811342Z"
    },
    "papermill": {
     "duration": 0.019398,
     "end_time": "2025-03-15T07:49:23.813598",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.794200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abusive\n",
       "0    8126\n",
       "1    5043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abusive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459aaa75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.837461Z",
     "iopub.status.busy": "2025-03-15T07:49:23.837204Z",
     "iopub.status.idle": "2025-03-15T07:49:23.850856Z",
     "shell.execute_reply": "2025-03-15T07:49:23.849993Z"
    },
    "papermill": {
     "duration": 0.027249,
     "end_time": "2025-03-15T07:49:23.852377",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.825128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic shape:  (7309, 13)\n",
      "Non-toxic shape:  (5860, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93f1a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.876997Z",
     "iopub.status.busy": "2025-03-15T07:49:23.876770Z",
     "iopub.status.idle": "2025-03-15T07:49:23.884924Z",
     "shell.execute_reply": "2025-03-15T07:49:23.884269Z"
    },
    "papermill": {
     "duration": 0.0216,
     "end_time": "2025-03-15T07:49:23.886140",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.864540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (15167, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anakjakartaasikasik</td>\n",
       "      <td>anak jakarta asyik asyik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pakcikdahtua</td>\n",
       "      <td>pak cik sudah tua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakcikmudalagi</td>\n",
       "      <td>pak cik muda lagi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3tapjokowi</td>\n",
       "      <td>tetap jokowi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3x</td>\n",
       "      <td>tiga kali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aamiinn</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aamin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aammiin</td>\n",
       "      <td>amin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abis</td>\n",
       "      <td>habis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abisin</td>\n",
       "      <td>habiskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acau</td>\n",
       "      <td>kacau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>achok</td>\n",
       "      <td>ahok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ad</td>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adek</td>\n",
       "      <td>adik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original               replacement\n",
       "0   anakjakartaasikasik  anak jakarta asyik asyik\n",
       "1          pakcikdahtua         pak cik sudah tua\n",
       "2        pakcikmudalagi         pak cik muda lagi\n",
       "3           t3tapjokowi              tetap jokowi\n",
       "4                    3x                 tiga kali\n",
       "5                aamiin                      amin\n",
       "6               aamiinn                      amin\n",
       "7                 aamin                      amin\n",
       "8               aammiin                      amin\n",
       "9                  abis                     habis\n",
       "10               abisin                  habiskan\n",
       "11                 acau                     kacau\n",
       "12                achok                      ahok\n",
       "13                   ad                       ada\n",
       "14                 adek                      adik"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape: \", alay_dict.shape)\n",
    "alay_dict.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945f22e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.912214Z",
     "iopub.status.busy": "2025-03-15T07:49:23.911926Z",
     "iopub.status.idle": "2025-03-15T07:49:23.924873Z",
     "shell.execute_reply": "2025-03-15T07:49:23.924165Z"
    },
    "papermill": {
     "duration": 0.027241,
     "end_time": "2025-03-15T07:49:23.926038",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.898797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_nonaplhanumeric:  Halooo duniaa \n",
      "lowercase:  halooo, duniaa!\n",
      "remove_unnecessary_char:  Hehe RT USER USER apa kabs hehe URL \n",
      "normalize_alay:  amin adik habis\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('url', ' ', text) # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub(r'\\b(?:x[a-fA-F0-9]{2}\\s*)+\\b', '', text) # Remove emoji bytecode\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa \\x8f \\xd2\\1 !!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe URL xf8 x2a x89\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b42b61c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.950989Z",
     "iopub.status.busy": "2025-03-15T07:49:23.950772Z",
     "iopub.status.idle": "2025-03-15T07:49:23.954086Z",
     "shell.execute_reply": "2025-03-15T07:49:23.953436Z"
    },
    "papermill": {
     "duration": 0.017042,
     "end_time": "2025-03-15T07:49:23.955229",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.938187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = normalize_alay(text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a17dc54f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:23.980992Z",
     "iopub.status.busy": "2025-03-15T07:49:23.980712Z",
     "iopub.status.idle": "2025-03-15T07:49:24.342510Z",
     "shell.execute_reply": "2025-03-15T07:49:24.341444Z"
    },
    "papermill": {
     "duration": 0.376477,
     "end_time": "2025-03-15T07:49:24.344140",
     "exception": false,
     "start_time": "2025-03-15T07:49:23.967663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10535,) (10535, 12)\n",
      "(2634,) (2634, 12)\n"
     ]
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
    "\n",
    "# Define the labels columns for multi-label classification\n",
    "label_columns = data.columns[1:]  # Assuming label columns start from the third column\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['Tweet'].values\n",
    "y_train = train_data[label_columns].values\n",
    "X_val = val_data['Tweet'].values\n",
    "y_val = val_data[label_columns].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b249c87",
   "metadata": {
    "papermill": {
     "duration": 0.011935,
     "end_time": "2025-03-15T07:49:24.369028",
     "exception": false,
     "start_time": "2025-03-15T07:49:24.357093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac5b350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:24.394744Z",
     "iopub.status.busy": "2025-03-15T07:49:24.394421Z",
     "iopub.status.idle": "2025-03-15T07:49:25.392908Z",
     "shell.execute_reply": "2025-03-15T07:49:25.391591Z"
    },
    "papermill": {
     "duration": 1.013874,
     "end_time": "2025-03-15T07:49:25.394862",
     "exception": false,
     "start_time": "2025-03-15T07:49:24.380988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469d790ca1934406a9f2b8ecb78f6db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371fdf0f94564fb0b90c9da9c36c8e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b4c9591c3641dfb8254878ce3b9c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de53d9bb3ad84b8d8156781279bb6c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts\n",
    "\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cbf69cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.426775Z",
     "iopub.status.busy": "2025-03-15T07:49:25.426422Z",
     "iopub.status.idle": "2025-03-15T07:49:25.431522Z",
     "shell.execute_reply": "2025-03-15T07:49:25.430622Z"
    },
    "papermill": {
     "duration": 0.022444,
     "end_time": "2025-03-15T07:49:25.432969",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.410525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(X_train, y_train, X_val, y_val, sequence_length=64, num_workers=4):\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_length=sequence_length)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fdf74",
   "metadata": {
    "papermill": {
     "duration": 0.014567,
     "end_time": "2025-03-15T07:49:25.462678",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.448111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c75a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.492909Z",
     "iopub.status.busy": "2025-03-15T07:49:25.492596Z",
     "iopub.status.idle": "2025-03-15T07:49:25.497106Z",
     "shell.execute_reply": "2025-03-15T07:49:25.496213Z"
    },
    "papermill": {
     "duration": 0.021181,
     "end_time": "2025-03-15T07:49:25.498506",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.477325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf89ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.528502Z",
     "iopub.status.busy": "2025-03-15T07:49:25.528196Z",
     "iopub.status.idle": "2025-03-15T07:49:25.533241Z",
     "shell.execute_reply": "2025-03-15T07:49:25.532609Z"
    },
    "papermill": {
     "duration": 0.021717,
     "end_time": "2025-03-15T07:49:25.534681",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.512964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        target_names=['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
    "        zero_division=0\n",
    "    )   \n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630821e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.563482Z",
     "iopub.status.busy": "2025-03-15T07:49:25.563242Z",
     "iopub.status.idle": "2025-03-15T07:49:25.574792Z",
     "shell.execute_reply": "2025-03-15T07:49:25.574211Z"
    },
    "papermill": {
     "duration": 0.02698,
     "end_time": "2025-03-15T07:49:25.575936",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.548956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, metrics, trials, i):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    train_loader, val_loader = get_dataloaders(current_X_train, current_y_train, X_val, y_val)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'indobenchmark/indobert-base-p1',\n",
    "            num_labels=len(label_columns),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze the first few layers of the encoder\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}))\n",
    "\n",
    "        if best_result is None or result['f1_micro'] >= best_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-{trials+1}-model-{i+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            best_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"Model {i+1} - Iteration {current_train_size}: Accuracy: {round(best_result['accuracy'], 4)}, F1 Micro: {round(best_result['f1_micro'], 4)}, F1 Macro: {round(best_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(best_result['report'])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    accelerator.print(f\"Training completed in {duration} s\")\n",
    "    \n",
    "    # Update the shared lists\n",
    "    if accelerator.is_local_main_process:\n",
    "        metrics[0].append(best_result['accuracy'])\n",
    "        metrics[1].append(best_result['f1_micro'])\n",
    "        metrics[2].append(best_result['f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a931f",
   "metadata": {
    "papermill": {
     "duration": 0.012595,
     "end_time": "2025-03-15T07:49:25.600760",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.588165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cb8f47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.628149Z",
     "iopub.status.busy": "2025-03-15T07:49:25.627854Z",
     "iopub.status.idle": "2025-03-15T07:49:25.633765Z",
     "shell.execute_reply": "2025-03-15T07:49:25.633096Z"
    },
    "papermill": {
     "duration": 0.021724,
     "end_time": "2025-03-15T07:49:25.635012",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.613288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475c5bd",
   "metadata": {
    "papermill": {
     "duration": 0.013253,
     "end_time": "2025-03-15T07:49:25.661402",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.648149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "366ceaa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.688506Z",
     "iopub.status.busy": "2025-03-15T07:49:25.688155Z",
     "iopub.status.idle": "2025-03-15T07:49:25.712141Z",
     "shell.execute_reply": "2025-03-15T07:49:25.711114Z"
    },
    "papermill": {
     "duration": 0.039267,
     "end_time": "2025-03-15T07:49:25.713546",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.674279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    # After data addition: use Bayesian update (posterior probability)\n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1\n",
    "\n",
    "# Function to compute Expected Score Change (∆Q)\n",
    "def besra_sampling(models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "    \n",
    "    dataset = HateSpeechDataset(X_pool, np.zeros((len(X_pool), 12)), tokenizer, max_length=sequence_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_dataset = HateSpeechDataset(current_X_train, current_y_train, tokenizer, max_length=sequence_length)\n",
    "    label_probs = labeled_dataset.get_global_probs()\n",
    "    class_probs = labeled_dataset.get_per_class_probs()\n",
    "\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    start_time = time.time()\n",
    "    score_changes = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        model_probs = []\n",
    "\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "        \n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(probs.shape[1]):\n",
    "                predicted_prob = probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx))\n",
    "            \n",
    "            score_changes.append(np.mean(score_diff))\n",
    "    \n",
    "    accelerator.wait_for_everyone()    \n",
    "    if accelerator.is_local_main_process:\n",
    "        score_changes = np.array(score_changes)\n",
    "        score_changes = score_changes.reshape(-1, 1)\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "    \n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'HS': [y_train[i][0] for i in temp],\n",
    "                'Abusive': [y_train[i][1] for i in temp],\n",
    "                'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                'HS_Group': [y_train[i][3] for i in temp],\n",
    "                'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                'HS_Race': [y_train[i][5] for i in temp],\n",
    "                'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                'HS_Other': [y_train[i][8] for i in temp],\n",
    "                'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                'HS_Strong': [y_train[i][11] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "\n",
    "        else:\n",
    "            # Cluster the data based on its embeddings\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 90)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances >= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # To handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'HS': [y_train[i][0] for i in temp],\n",
    "                    'Abusive': [y_train[i][1] for i in temp],\n",
    "                    'HS_Individual': [y_train[i][2] for i in temp],\n",
    "                    'HS_Group': [y_train[i][3] for i in temp],\n",
    "                    'HS_Religion': [y_train[i][4] for i in temp],\n",
    "                    'HS_Race': [y_train[i][5] for i in temp],\n",
    "                    'HS_Physical': [y_train[i][6] for i in temp],\n",
    "                    'HS_Gender': [y_train[i][7] for i in temp],\n",
    "                    'HS_Other': [y_train[i][8] for i in temp],\n",
    "                    'HS_Weak': [y_train[i][9] for i in temp],\n",
    "                    'HS_Moderate': [y_train[i][10] for i in temp],\n",
    "                    'HS_Strong': [y_train[i][11] for i in temp],\n",
    "                })\n",
    "        \n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            # print(f\"Thresholds: {thresholds}\")\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa634e7a",
   "metadata": {
    "papermill": {
     "duration": 0.012438,
     "end_time": "2025-03-15T07:49:25.738986",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.726548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01195abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.774875Z",
     "iopub.status.busy": "2025-03-15T07:49:25.774576Z",
     "iopub.status.idle": "2025-03-15T07:49:25.785221Z",
     "shell.execute_reply": "2025-03-15T07:49:25.784437Z"
    },
    "papermill": {
     "duration": 0.026219,
     "end_time": "2025-03-15T07:49:25.786546",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.760327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "\n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "        \n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        models = []\n",
    "        for j in range(3):\n",
    "            model = BertForSequenceClassification.from_pretrained(f'{filename}-{i+1}-model-{j+1}')\n",
    "            models.append(model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (models, [X_train[i] for i in remaining_indices], train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, i)\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    model_accuracies = manager.list()\n",
    "    model_f1_micros = manager.list()\n",
    "    model_f1_macros = manager.list()\n",
    "    \n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (current_train_size, train_indices, (model_accuracies, model_f1_micros, model_f1_macros), i, j)\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "        \n",
    "    data_used.append(current_train_size)\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "        \n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db32c8b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.811772Z",
     "iopub.status.busy": "2025-03-15T07:49:25.811555Z",
     "iopub.status.idle": "2025-03-15T07:49:25.815089Z",
     "shell.execute_reply": "2025-03-15T07:49:25.814286Z"
    },
    "papermill": {
     "duration": 0.017496,
     "end_time": "2025-03-15T07:49:25.816348",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.798852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12fa2e",
   "metadata": {
    "papermill": {
     "duration": 0.01283,
     "end_time": "2025-03-15T07:49:25.841509",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.828679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f0b2c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T07:49:25.869130Z",
     "iopub.status.busy": "2025-03-15T07:49:25.868815Z",
     "iopub.status.idle": "2025-03-15T13:24:23.009039Z",
     "shell.execute_reply": "2025-03-15T13:24:23.008281Z"
    },
    "papermill": {
     "duration": 20097.155566,
     "end_time": "2025-03-15T13:24:23.010596",
     "exception": false,
     "start_time": "2025-03-15T07:49:25.855030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5482, Accuracy: 0.8278, F1 Micro: 0.0187, F1 Macro: 0.0071\n",
      "Epoch 2/10, Train Loss: 0.4177, Accuracy: 0.8285, F1 Micro: 0.01, F1 Macro: 0.0046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3956, Accuracy: 0.8373, F1 Micro: 0.132, F1 Macro: 0.0461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3773, Accuracy: 0.8566, F1 Micro: 0.3534, F1 Macro: 0.1133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3452, Accuracy: 0.8762, F1 Micro: 0.5511, F1 Macro: 0.2507\n",
      "Epoch 6/10, Train Loss: 0.2866, Accuracy: 0.8735, F1 Micro: 0.4983, F1 Macro: 0.2345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2649, Accuracy: 0.8792, F1 Micro: 0.5822, F1 Macro: 0.3047\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2359, Accuracy: 0.8794, F1 Micro: 0.5921, F1 Macro: 0.325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1951, Accuracy: 0.8794, F1 Micro: 0.6097, F1 Macro: 0.3507\n",
      "Epoch 10/10, Train Loss: 0.1881, Accuracy: 0.88, F1 Micro: 0.5985, F1 Macro: 0.348\n",
      "Model 1 - Iteration 658: Accuracy: 0.8794, F1 Micro: 0.6097, F1 Macro: 0.3507\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.76      0.77      1134\n",
      "      Abusive       0.80      0.81      0.81       992\n",
      "HS_Individual       0.64      0.48      0.55       732\n",
      "     HS_Group       0.54      0.38      0.44       402\n",
      "  HS_Religion       0.53      0.11      0.19       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.60      0.62       762\n",
      "      HS_Weak       0.60      0.45      0.51       689\n",
      "  HS_Moderate       0.38      0.27      0.31       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.55      0.61      5556\n",
      "    macro avg       0.41      0.32      0.35      5556\n",
      " weighted avg       0.63      0.55      0.58      5556\n",
      "  samples avg       0.38      0.32      0.32      5556\n",
      "\n",
      "Training completed in 57.89021420478821 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.568, Accuracy: 0.8392, F1 Micro: 0.3169, F1 Macro: 0.0918\n",
      "Epoch 2/10, Train Loss: 0.4254, Accuracy: 0.8336, F1 Micro: 0.0793, F1 Macro: 0.0304\n",
      "Epoch 3/10, Train Loss: 0.3952, Accuracy: 0.8436, F1 Micro: 0.213, F1 Macro: 0.0739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3766, Accuracy: 0.8579, F1 Micro: 0.3773, F1 Macro: 0.1227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3432, Accuracy: 0.8733, F1 Micro: 0.5295, F1 Macro: 0.2306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2894, Accuracy: 0.8765, F1 Micro: 0.5482, F1 Macro: 0.2528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2627, Accuracy: 0.8719, F1 Micro: 0.6065, F1 Macro: 0.3202\n",
      "Epoch 8/10, Train Loss: 0.2414, Accuracy: 0.8799, F1 Micro: 0.5963, F1 Macro: 0.3142\n",
      "Epoch 9/10, Train Loss: 0.2006, Accuracy: 0.8797, F1 Micro: 0.5837, F1 Macro: 0.3153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1964, Accuracy: 0.8812, F1 Micro: 0.6182, F1 Macro: 0.3371\n",
      "Model 2 - Iteration 658: Accuracy: 0.8812, F1 Micro: 0.6182, F1 Macro: 0.3371\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.76      0.77      1134\n",
      "      Abusive       0.81      0.80      0.81       992\n",
      "HS_Individual       0.61      0.58      0.60       732\n",
      "     HS_Group       0.60      0.24      0.34       402\n",
      "  HS_Religion       0.50      0.01      0.02       157\n",
      "      HS_Race       1.00      0.03      0.05       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.63      0.64       762\n",
      "      HS_Weak       0.58      0.55      0.57       689\n",
      "  HS_Moderate       0.43      0.18      0.25       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.56      0.62      5556\n",
      "    macro avg       0.50      0.32      0.34      5556\n",
      " weighted avg       0.65      0.56      0.58      5556\n",
      "  samples avg       0.37      0.32      0.32      5556\n",
      "\n",
      "Training completed in 55.760502099990845 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5747, Accuracy: 0.8306, F1 Micro: 0.2662, F1 Macro: 0.0709\n",
      "Epoch 2/10, Train Loss: 0.4239, Accuracy: 0.8327, F1 Micro: 0.0798, F1 Macro: 0.0313\n",
      "Epoch 3/10, Train Loss: 0.3946, Accuracy: 0.8364, F1 Micro: 0.127, F1 Macro: 0.045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3747, Accuracy: 0.8531, F1 Micro: 0.3133, F1 Macro: 0.1038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3477, Accuracy: 0.8684, F1 Micro: 0.4764, F1 Macro: 0.1956\n",
      "Epoch 6/10, Train Loss: 0.294, Accuracy: 0.8707, F1 Micro: 0.4687, F1 Macro: 0.2145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2699, Accuracy: 0.8776, F1 Micro: 0.5892, F1 Macro: 0.2897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2444, Accuracy: 0.8803, F1 Micro: 0.6068, F1 Macro: 0.3164\n",
      "Epoch 9/10, Train Loss: 0.2046, Accuracy: 0.8806, F1 Micro: 0.6046, F1 Macro: 0.33\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1965, Accuracy: 0.8827, F1 Micro: 0.6107, F1 Macro: 0.3431\n",
      "Model 3 - Iteration 658: Accuracy: 0.8827, F1 Micro: 0.6107, F1 Macro: 0.3431\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.72      0.76      1134\n",
      "      Abusive       0.82      0.77      0.80       992\n",
      "HS_Individual       0.65      0.53      0.58       732\n",
      "     HS_Group       0.60      0.32      0.42       402\n",
      "  HS_Religion       0.64      0.06      0.11       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.58      0.63       762\n",
      "      HS_Weak       0.61      0.50      0.55       689\n",
      "  HS_Moderate       0.39      0.21      0.27       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.71      0.53      0.61      5556\n",
      "    macro avg       0.43      0.31      0.34      5556\n",
      " weighted avg       0.65      0.53      0.58      5556\n",
      "  samples avg       0.37      0.31      0.31      5556\n",
      "\n",
      "Training completed in 56.22684335708618 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8811, F1 Micro: 0.6129, F1 Macro: 0.3436\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 123.63182520866394 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4731, Accuracy: 0.8323, F1 Micro: 0.0595, F1 Macro: 0.0241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3675, Accuracy: 0.8593, F1 Micro: 0.3564, F1 Macro: 0.1452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3247, Accuracy: 0.8797, F1 Micro: 0.5453, F1 Macro: 0.2579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2646, Accuracy: 0.891, F1 Micro: 0.6415, F1 Macro: 0.3657\n",
      "Epoch 5/10, Train Loss: 0.2355, Accuracy: 0.889, F1 Micro: 0.6025, F1 Macro: 0.3728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2016, Accuracy: 0.8942, F1 Micro: 0.676, F1 Macro: 0.4784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1666, Accuracy: 0.8957, F1 Micro: 0.6889, F1 Macro: 0.4929\n",
      "Epoch 8/10, Train Loss: 0.1435, Accuracy: 0.8961, F1 Micro: 0.6659, F1 Macro: 0.4641\n",
      "Epoch 9/10, Train Loss: 0.1242, Accuracy: 0.8976, F1 Micro: 0.6716, F1 Macro: 0.48\n",
      "Epoch 10/10, Train Loss: 0.1063, Accuracy: 0.8974, F1 Micro: 0.6761, F1 Macro: 0.5209\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8957, F1 Micro: 0.6889, F1 Macro: 0.4929\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.84      0.81      1134\n",
      "      Abusive       0.83      0.84      0.83       992\n",
      "HS_Individual       0.67      0.64      0.66       732\n",
      "     HS_Group       0.58      0.58      0.58       402\n",
      "  HS_Religion       0.70      0.31      0.43       157\n",
      "      HS_Race       0.88      0.35      0.50       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.74      0.70       762\n",
      "      HS_Weak       0.65      0.59      0.62       689\n",
      "  HS_Moderate       0.48      0.47      0.48       331\n",
      "    HS_Strong       0.91      0.18      0.31       114\n",
      "\n",
      "    micro avg       0.71      0.67      0.69      5556\n",
      "    macro avg       0.60      0.46      0.49      5556\n",
      " weighted avg       0.70      0.67      0.67      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 79.71803641319275 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4834, Accuracy: 0.8386, F1 Micro: 0.1482, F1 Macro: 0.0542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3651, Accuracy: 0.8665, F1 Micro: 0.429, F1 Macro: 0.1783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3226, Accuracy: 0.8781, F1 Micro: 0.534, F1 Macro: 0.2456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2654, Accuracy: 0.8886, F1 Micro: 0.6207, F1 Macro: 0.3336\n",
      "Epoch 5/10, Train Loss: 0.2392, Accuracy: 0.8886, F1 Micro: 0.5926, F1 Macro: 0.3578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.207, Accuracy: 0.8932, F1 Micro: 0.6743, F1 Macro: 0.4469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1736, Accuracy: 0.8938, F1 Micro: 0.6844, F1 Macro: 0.495\n",
      "Epoch 8/10, Train Loss: 0.1481, Accuracy: 0.8908, F1 Micro: 0.6777, F1 Macro: 0.4786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1274, Accuracy: 0.8976, F1 Micro: 0.6941, F1 Macro: 0.511\n",
      "Epoch 10/10, Train Loss: 0.1088, Accuracy: 0.8918, F1 Micro: 0.6532, F1 Macro: 0.5047\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8976, F1 Micro: 0.6941, F1 Macro: 0.511\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.83      0.81      1134\n",
      "      Abusive       0.86      0.82      0.84       992\n",
      "HS_Individual       0.68      0.64      0.66       732\n",
      "     HS_Group       0.59      0.60      0.59       402\n",
      "  HS_Religion       0.69      0.31      0.43       157\n",
      "      HS_Race       0.85      0.43      0.57       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.75      0.72       762\n",
      "      HS_Weak       0.64      0.61      0.62       689\n",
      "  HS_Moderate       0.46      0.50      0.48       331\n",
      "    HS_Strong       1.00      0.24      0.38       114\n",
      "\n",
      "    micro avg       0.71      0.67      0.69      5556\n",
      "    macro avg       0.69      0.48      0.51      5556\n",
      " weighted avg       0.72      0.67      0.68      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 83.02823543548584 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4852, Accuracy: 0.8349, F1 Micro: 0.1105, F1 Macro: 0.0403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3636, Accuracy: 0.8602, F1 Micro: 0.3842, F1 Macro: 0.1492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3265, Accuracy: 0.8762, F1 Micro: 0.5089, F1 Macro: 0.2392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.269, Accuracy: 0.8894, F1 Micro: 0.6337, F1 Macro: 0.3527\n",
      "Epoch 5/10, Train Loss: 0.2405, Accuracy: 0.8852, F1 Micro: 0.5692, F1 Macro: 0.3127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2107, Accuracy: 0.8942, F1 Micro: 0.6788, F1 Macro: 0.4721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1771, Accuracy: 0.8973, F1 Micro: 0.6825, F1 Macro: 0.4907\n",
      "Epoch 8/10, Train Loss: 0.1509, Accuracy: 0.8973, F1 Micro: 0.675, F1 Macro: 0.4829\n",
      "Epoch 9/10, Train Loss: 0.1283, Accuracy: 0.8994, F1 Micro: 0.6824, F1 Macro: 0.5008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1099, Accuracy: 0.8983, F1 Micro: 0.6951, F1 Macro: 0.5301\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8983, F1 Micro: 0.6951, F1 Macro: 0.5301\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.81      1134\n",
      "      Abusive       0.83      0.86      0.84       992\n",
      "HS_Individual       0.67      0.60      0.64       732\n",
      "     HS_Group       0.58      0.61      0.60       402\n",
      "  HS_Religion       0.67      0.50      0.57       157\n",
      "      HS_Race       0.75      0.56      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.70      0.71       762\n",
      "      HS_Weak       0.64      0.57      0.61       689\n",
      "  HS_Moderate       0.45      0.53      0.49       331\n",
      "    HS_Strong       0.94      0.30      0.45       114\n",
      "\n",
      "    micro avg       0.72      0.67      0.70      5556\n",
      "    macro avg       0.59      0.50      0.53      5556\n",
      " weighted avg       0.71      0.67      0.68      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 81.892329454422 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8892, F1 Micro: 0.6528, F1 Macro: 0.4275\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 111.20058846473694 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4433, Accuracy: 0.8528, F1 Micro: 0.3789, F1 Macro: 0.1152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3311, Accuracy: 0.8841, F1 Micro: 0.619, F1 Macro: 0.3151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2786, Accuracy: 0.8918, F1 Micro: 0.6744, F1 Macro: 0.408\n",
      "Epoch 4/10, Train Loss: 0.2231, Accuracy: 0.8947, F1 Micro: 0.6221, F1 Macro: 0.365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1967, Accuracy: 0.8954, F1 Micro: 0.6986, F1 Macro: 0.4692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1593, Accuracy: 0.8976, F1 Micro: 0.7068, F1 Macro: 0.5262\n",
      "Epoch 7/10, Train Loss: 0.1351, Accuracy: 0.9013, F1 Micro: 0.6803, F1 Macro: 0.5066\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.9011, F1 Micro: 0.7057, F1 Macro: 0.5324\n",
      "Epoch 9/10, Train Loss: 0.0964, Accuracy: 0.9046, F1 Micro: 0.6903, F1 Macro: 0.5492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.076, Accuracy: 0.9048, F1 Micro: 0.7181, F1 Macro: 0.587\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9048, F1 Micro: 0.7181, F1 Macro: 0.587\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.85      0.89      0.87       992\n",
      "HS_Individual       0.67      0.68      0.67       732\n",
      "     HS_Group       0.62      0.53      0.57       402\n",
      "  HS_Religion       0.63      0.59      0.61       157\n",
      "      HS_Race       0.76      0.52      0.61       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.50      0.08      0.14        51\n",
      "     HS_Other       0.73      0.74      0.73       762\n",
      "      HS_Weak       0.65      0.64      0.65       689\n",
      "  HS_Moderate       0.52      0.43      0.47       331\n",
      "    HS_Strong       0.90      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.73      0.70      0.72      5556\n",
      "    macro avg       0.70      0.55      0.59      5556\n",
      " weighted avg       0.73      0.70      0.71      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 101.89185452461243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4493, Accuracy: 0.8541, F1 Micro: 0.3744, F1 Macro: 0.1179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3305, Accuracy: 0.8819, F1 Micro: 0.5971, F1 Macro: 0.2822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2795, Accuracy: 0.8886, F1 Micro: 0.669, F1 Macro: 0.386\n",
      "Epoch 4/10, Train Loss: 0.2247, Accuracy: 0.8968, F1 Micro: 0.6585, F1 Macro: 0.4279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2013, Accuracy: 0.8947, F1 Micro: 0.6812, F1 Macro: 0.4183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1649, Accuracy: 0.8909, F1 Micro: 0.7059, F1 Macro: 0.5372\n",
      "Epoch 7/10, Train Loss: 0.1411, Accuracy: 0.9008, F1 Micro: 0.701, F1 Macro: 0.5347\n",
      "Epoch 8/10, Train Loss: 0.1136, Accuracy: 0.8992, F1 Micro: 0.7041, F1 Macro: 0.5172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0955, Accuracy: 0.9047, F1 Micro: 0.7088, F1 Macro: 0.5582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9027, F1 Micro: 0.7114, F1 Macro: 0.5775\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9027, F1 Micro: 0.7114, F1 Macro: 0.5775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.81      0.81      1134\n",
      "      Abusive       0.83      0.87      0.85       992\n",
      "HS_Individual       0.67      0.66      0.66       732\n",
      "     HS_Group       0.61      0.57      0.59       402\n",
      "  HS_Religion       0.62      0.59      0.61       157\n",
      "      HS_Race       0.74      0.58      0.65       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.73      0.72      0.73       762\n",
      "      HS_Weak       0.65      0.62      0.63       689\n",
      "  HS_Moderate       0.52      0.46      0.49       331\n",
      "    HS_Strong       0.85      0.66      0.74       114\n",
      "\n",
      "    micro avg       0.73      0.70      0.71      5556\n",
      "    macro avg       0.72      0.55      0.58      5556\n",
      " weighted avg       0.73      0.70      0.70      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 103.34541487693787 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4491, Accuracy: 0.8539, F1 Micro: 0.3523, F1 Macro: 0.1084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3313, Accuracy: 0.8833, F1 Micro: 0.6122, F1 Macro: 0.299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2807, Accuracy: 0.8882, F1 Micro: 0.6727, F1 Macro: 0.3896\n",
      "Epoch 4/10, Train Loss: 0.2254, Accuracy: 0.8922, F1 Micro: 0.6066, F1 Macro: 0.3669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2002, Accuracy: 0.8968, F1 Micro: 0.6997, F1 Macro: 0.4916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1678, Accuracy: 0.8954, F1 Micro: 0.712, F1 Macro: 0.5395\n",
      "Epoch 7/10, Train Loss: 0.14, Accuracy: 0.901, F1 Micro: 0.6969, F1 Macro: 0.5328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1108, Accuracy: 0.9024, F1 Micro: 0.7141, F1 Macro: 0.5386\n",
      "Epoch 9/10, Train Loss: 0.0943, Accuracy: 0.9043, F1 Micro: 0.7001, F1 Macro: 0.5554\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9058, F1 Micro: 0.7117, F1 Macro: 0.5898\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9024, F1 Micro: 0.7141, F1 Macro: 0.5386\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.85      0.82      1134\n",
      "      Abusive       0.85      0.85      0.85       992\n",
      "HS_Individual       0.66      0.71      0.68       732\n",
      "     HS_Group       0.64      0.52      0.58       402\n",
      "  HS_Religion       0.71      0.46      0.56       157\n",
      "      HS_Race       0.76      0.51      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.79      0.74       762\n",
      "      HS_Weak       0.63      0.69      0.66       689\n",
      "  HS_Moderate       0.51      0.45      0.48       331\n",
      "    HS_Strong       0.93      0.33      0.49       114\n",
      "\n",
      "    micro avg       0.72      0.71      0.71      5556\n",
      "    macro avg       0.60      0.51      0.54      5556\n",
      " weighted avg       0.71      0.71      0.70      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 101.38835597038269 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8939, F1 Micro: 0.6734, F1 Macro: 0.4742\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 100.0362777709961 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4288, Accuracy: 0.8683, F1 Micro: 0.477, F1 Macro: 0.1979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3118, Accuracy: 0.8911, F1 Micro: 0.6355, F1 Macro: 0.3731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2502, Accuracy: 0.8996, F1 Micro: 0.6835, F1 Macro: 0.5\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.206, Accuracy: 0.9046, F1 Micro: 0.6912, F1 Macro: 0.4858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1635, Accuracy: 0.9059, F1 Micro: 0.7103, F1 Macro: 0.5308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1308, Accuracy: 0.9053, F1 Micro: 0.7185, F1 Macro: 0.5379\n",
      "Epoch 7/10, Train Loss: 0.1143, Accuracy: 0.9079, F1 Micro: 0.717, F1 Macro: 0.559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9069, F1 Micro: 0.7298, F1 Macro: 0.5996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9074, F1 Micro: 0.7351, F1 Macro: 0.6079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9136, F1 Micro: 0.74, F1 Macro: 0.6433\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9136, F1 Micro: 0.74, F1 Macro: 0.6433\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.87      0.88      0.88       992\n",
      "HS_Individual       0.72      0.66      0.69       732\n",
      "     HS_Group       0.66      0.60      0.63       402\n",
      "  HS_Religion       0.70      0.53      0.60       157\n",
      "      HS_Race       0.73      0.62      0.67       120\n",
      "  HS_Physical       0.50      0.18      0.27        72\n",
      "    HS_Gender       0.58      0.27      0.37        51\n",
      "     HS_Other       0.77      0.72      0.75       762\n",
      "      HS_Weak       0.70      0.63      0.66       689\n",
      "  HS_Moderate       0.57      0.53      0.55       331\n",
      "    HS_Strong       0.89      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5556\n",
      "    macro avg       0.71      0.60      0.64      5556\n",
      " weighted avg       0.76      0.71      0.74      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 126.44917631149292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4327, Accuracy: 0.8696, F1 Micro: 0.5235, F1 Macro: 0.2181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3155, Accuracy: 0.8896, F1 Micro: 0.6206, F1 Macro: 0.3674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2556, Accuracy: 0.8983, F1 Micro: 0.6659, F1 Macro: 0.4665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.211, Accuracy: 0.9036, F1 Micro: 0.6973, F1 Macro: 0.491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1689, Accuracy: 0.9106, F1 Micro: 0.7255, F1 Macro: 0.5538\n",
      "Epoch 6/10, Train Loss: 0.14, Accuracy: 0.9106, F1 Micro: 0.725, F1 Macro: 0.5467\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.9099, F1 Micro: 0.7234, F1 Macro: 0.5697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0958, Accuracy: 0.9122, F1 Micro: 0.7325, F1 Macro: 0.6162\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9076, F1 Micro: 0.7323, F1 Macro: 0.6171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0673, Accuracy: 0.9089, F1 Micro: 0.7379, F1 Macro: 0.6488\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9089, F1 Micro: 0.7379, F1 Macro: 0.6488\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1134\n",
      "      Abusive       0.84      0.89      0.87       992\n",
      "HS_Individual       0.69      0.70      0.70       732\n",
      "     HS_Group       0.60      0.62      0.61       402\n",
      "  HS_Religion       0.68      0.59      0.63       157\n",
      "      HS_Race       0.70      0.68      0.69       120\n",
      "  HS_Physical       0.67      0.19      0.30        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.74      0.76      0.75       762\n",
      "      HS_Weak       0.67      0.68      0.67       689\n",
      "  HS_Moderate       0.51      0.55      0.53       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.73      0.74      0.74      5556\n",
      "    macro avg       0.70      0.63      0.65      5556\n",
      " weighted avg       0.73      0.74      0.73      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 122.77100086212158 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4345, Accuracy: 0.8667, F1 Micro: 0.4688, F1 Macro: 0.1862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3152, Accuracy: 0.8911, F1 Micro: 0.6385, F1 Macro: 0.3701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2524, Accuracy: 0.9013, F1 Micro: 0.6865, F1 Macro: 0.5081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2068, Accuracy: 0.9052, F1 Micro: 0.6993, F1 Macro: 0.512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1658, Accuracy: 0.9092, F1 Micro: 0.7163, F1 Macro: 0.5532\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9076, F1 Micro: 0.7109, F1 Macro: 0.5405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1201, Accuracy: 0.9065, F1 Micro: 0.7188, F1 Macro: 0.5645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9062, F1 Micro: 0.7243, F1 Macro: 0.5885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9081, F1 Micro: 0.7373, F1 Macro: 0.6181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9102, F1 Micro: 0.7404, F1 Macro: 0.6356\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9102, F1 Micro: 0.7404, F1 Macro: 0.6356\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.84      0.90      0.87       992\n",
      "HS_Individual       0.70      0.67      0.69       732\n",
      "     HS_Group       0.60      0.66      0.63       402\n",
      "  HS_Religion       0.70      0.54      0.61       157\n",
      "      HS_Race       0.72      0.63      0.68       120\n",
      "  HS_Physical       0.64      0.12      0.21        72\n",
      "    HS_Gender       0.50      0.24      0.32        51\n",
      "     HS_Other       0.73      0.78      0.75       762\n",
      "      HS_Weak       0.69      0.65      0.67       689\n",
      "  HS_Moderate       0.53      0.59      0.56       331\n",
      "    HS_Strong       0.84      0.79      0.81       114\n",
      "\n",
      "    micro avg       0.74      0.74      0.74      5556\n",
      "    macro avg       0.69      0.62      0.64      5556\n",
      " weighted avg       0.73      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 126.33862257003784 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8981, F1 Micro: 0.6899, F1 Macro: 0.5163\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 91.01998281478882 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4169, Accuracy: 0.878, F1 Micro: 0.55, F1 Macro: 0.2503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2885, Accuracy: 0.8955, F1 Micro: 0.662, F1 Macro: 0.4249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2332, Accuracy: 0.9042, F1 Micro: 0.7077, F1 Macro: 0.4888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1888, Accuracy: 0.9076, F1 Micro: 0.7142, F1 Macro: 0.5489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9109, F1 Micro: 0.7289, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1219, Accuracy: 0.9125, F1 Micro: 0.7292, F1 Macro: 0.5937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1023, Accuracy: 0.9129, F1 Micro: 0.742, F1 Macro: 0.6175\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9146, F1 Micro: 0.7387, F1 Macro: 0.618\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.9104, F1 Micro: 0.7412, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.9146, F1 Micro: 0.7466, F1 Macro: 0.6543\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9146, F1 Micro: 0.7466, F1 Macro: 0.6543\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.72      0.67      0.69       732\n",
      "     HS_Group       0.65      0.65      0.65       402\n",
      "  HS_Religion       0.67      0.58      0.62       157\n",
      "      HS_Race       0.79      0.59      0.68       120\n",
      "  HS_Physical       0.71      0.21      0.32        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.77      0.73      0.75       762\n",
      "      HS_Weak       0.70      0.65      0.67       689\n",
      "  HS_Moderate       0.56      0.55      0.55       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5556\n",
      "    macro avg       0.73      0.62      0.65      5556\n",
      " weighted avg       0.76      0.73      0.74      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 142.29969143867493 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4187, Accuracy: 0.8778, F1 Micro: 0.5537, F1 Macro: 0.2454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2897, Accuracy: 0.8962, F1 Micro: 0.6607, F1 Macro: 0.4174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2349, Accuracy: 0.9024, F1 Micro: 0.7119, F1 Macro: 0.4887\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9054, F1 Micro: 0.6897, F1 Macro: 0.5347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1581, Accuracy: 0.9092, F1 Micro: 0.7309, F1 Macro: 0.5816\n",
      "Epoch 6/10, Train Loss: 0.1218, Accuracy: 0.9103, F1 Micro: 0.7213, F1 Macro: 0.6048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9108, F1 Micro: 0.7413, F1 Macro: 0.6388\n",
      "Epoch 8/10, Train Loss: 0.0877, Accuracy: 0.9129, F1 Micro: 0.7401, F1 Macro: 0.6501\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9142, F1 Micro: 0.7308, F1 Macro: 0.6352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9153, F1 Micro: 0.7417, F1 Macro: 0.6528\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9153, F1 Micro: 0.7417, F1 Macro: 0.6528\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1134\n",
      "      Abusive       0.89      0.85      0.87       992\n",
      "HS_Individual       0.70      0.70      0.70       732\n",
      "     HS_Group       0.73      0.53      0.61       402\n",
      "  HS_Religion       0.69      0.54      0.60       157\n",
      "      HS_Race       0.83      0.62      0.71       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.67      0.31      0.43        51\n",
      "     HS_Other       0.78      0.72      0.75       762\n",
      "      HS_Weak       0.68      0.67      0.68       689\n",
      "  HS_Moderate       0.64      0.44      0.52       331\n",
      "    HS_Strong       0.87      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.71      0.74      5556\n",
      "    macro avg       0.77      0.60      0.65      5556\n",
      " weighted avg       0.78      0.71      0.74      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 138.6101942062378 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4201, Accuracy: 0.8753, F1 Micro: 0.5261, F1 Macro: 0.2306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2923, Accuracy: 0.8949, F1 Micro: 0.6575, F1 Macro: 0.4222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2364, Accuracy: 0.9027, F1 Micro: 0.7112, F1 Macro: 0.4959\n",
      "Epoch 4/10, Train Loss: 0.1925, Accuracy: 0.908, F1 Micro: 0.7052, F1 Macro: 0.5412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1594, Accuracy: 0.911, F1 Micro: 0.7213, F1 Macro: 0.5691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1248, Accuracy: 0.9105, F1 Micro: 0.7232, F1 Macro: 0.5863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.912, F1 Micro: 0.7386, F1 Macro: 0.6118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.9138, F1 Micro: 0.7395, F1 Macro: 0.6408\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9133, F1 Micro: 0.7362, F1 Macro: 0.6376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.915, F1 Micro: 0.7505, F1 Macro: 0.6575\n",
      "Model 3 - Iteration 4055: Accuracy: 0.915, F1 Micro: 0.7505, F1 Macro: 0.6575\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.72      0.68      0.70       732\n",
      "     HS_Group       0.64      0.67      0.66       402\n",
      "  HS_Religion       0.66      0.64      0.65       157\n",
      "      HS_Race       0.74      0.67      0.70       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.76      0.73      0.75       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.56      0.60      0.58       331\n",
      "    HS_Strong       0.84      0.79      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.73      0.63      0.66      5556\n",
      " weighted avg       0.76      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 141.96930599212646 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9015, F1 Micro: 0.7012, F1 Macro: 0.544\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 81.5556435585022 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4053, Accuracy: 0.8805, F1 Micro: 0.6086, F1 Macro: 0.3017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2741, Accuracy: 0.8998, F1 Micro: 0.6633, F1 Macro: 0.4401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.9058, F1 Micro: 0.7251, F1 Macro: 0.5431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.9126, F1 Micro: 0.7295, F1 Macro: 0.5613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9156, F1 Micro: 0.7444, F1 Macro: 0.6299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9176, F1 Micro: 0.75, F1 Macro: 0.6107\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9145, F1 Micro: 0.7482, F1 Macro: 0.6445\n",
      "Epoch 8/10, Train Loss: 0.0815, Accuracy: 0.9081, F1 Micro: 0.7391, F1 Macro: 0.6389\n",
      "Epoch 9/10, Train Loss: 0.0684, Accuracy: 0.9152, F1 Micro: 0.7393, F1 Macro: 0.6543\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9169, F1 Micro: 0.7432, F1 Macro: 0.6504\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9176, F1 Micro: 0.75, F1 Macro: 0.6107\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.90      0.84      0.87       992\n",
      "HS_Individual       0.70      0.73      0.72       732\n",
      "     HS_Group       0.78      0.55      0.64       402\n",
      "  HS_Religion       0.76      0.45      0.57       157\n",
      "      HS_Race       0.84      0.58      0.69       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.50      0.08      0.14        51\n",
      "     HS_Other       0.75      0.78      0.77       762\n",
      "      HS_Weak       0.68      0.71      0.70       689\n",
      "  HS_Moderate       0.68      0.45      0.54       331\n",
      "    HS_Strong       0.90      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5556\n",
      "    macro avg       0.77      0.56      0.61      5556\n",
      " weighted avg       0.78      0.72      0.74      5556\n",
      "  samples avg       0.41      0.39      0.39      5556\n",
      "\n",
      "Training completed in 154.5072581768036 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4086, Accuracy: 0.8788, F1 Micro: 0.5967, F1 Macro: 0.2768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2764, Accuracy: 0.8996, F1 Micro: 0.6705, F1 Macro: 0.4421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2299, Accuracy: 0.9041, F1 Micro: 0.7245, F1 Macro: 0.546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1933, Accuracy: 0.914, F1 Micro: 0.7263, F1 Macro: 0.5593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9152, F1 Micro: 0.7342, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9165, F1 Micro: 0.747, F1 Macro: 0.6191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9143, F1 Micro: 0.7477, F1 Macro: 0.6351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9145, F1 Micro: 0.7518, F1 Macro: 0.6611\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9147, F1 Micro: 0.7514, F1 Macro: 0.6766\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.911, F1 Micro: 0.7482, F1 Macro: 0.6702\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9145, F1 Micro: 0.7518, F1 Macro: 0.6611\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.83      1134\n",
      "      Abusive       0.86      0.89      0.87       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.65      0.66      0.66       402\n",
      "  HS_Religion       0.71      0.52      0.60       157\n",
      "      HS_Race       0.73      0.71      0.72       120\n",
      "  HS_Physical       0.61      0.15      0.24        72\n",
      "    HS_Gender       0.67      0.35      0.46        51\n",
      "     HS_Other       0.74      0.78      0.76       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.55      0.58      0.56       331\n",
      "    HS_Strong       0.89      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.72      0.64      0.66      5556\n",
      " weighted avg       0.75      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 159.18103957176208 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4073, Accuracy: 0.8793, F1 Micro: 0.6096, F1 Macro: 0.2973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.8987, F1 Micro: 0.6531, F1 Macro: 0.4309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2317, Accuracy: 0.9059, F1 Micro: 0.7229, F1 Macro: 0.5442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9115, F1 Micro: 0.7272, F1 Macro: 0.5745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9158, F1 Micro: 0.7452, F1 Macro: 0.6043\n",
      "Epoch 6/10, Train Loss: 0.1193, Accuracy: 0.9146, F1 Micro: 0.7338, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9133, F1 Micro: 0.7491, F1 Macro: 0.6173\n",
      "Epoch 8/10, Train Loss: 0.0821, Accuracy: 0.9149, F1 Micro: 0.7422, F1 Macro: 0.6395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9162, F1 Micro: 0.7505, F1 Macro: 0.6505\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9181, F1 Micro: 0.7461, F1 Macro: 0.6516\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9162, F1 Micro: 0.7505, F1 Macro: 0.6505\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.86      0.89      0.88       992\n",
      "HS_Individual       0.72      0.69      0.70       732\n",
      "     HS_Group       0.68      0.62      0.65       402\n",
      "  HS_Religion       0.72      0.50      0.59       157\n",
      "      HS_Race       0.82      0.67      0.73       120\n",
      "  HS_Physical       0.75      0.17      0.27        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.75      0.77      0.76       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.58      0.56      0.57       331\n",
      "    HS_Strong       0.87      0.73      0.79       114\n",
      "\n",
      "    micro avg       0.77      0.73      0.75      5556\n",
      "    macro avg       0.74      0.61      0.65      5556\n",
      " weighted avg       0.77      0.73      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 158.36786079406738 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9039, F1 Micro: 0.7094, F1 Macro: 0.5601\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 74.0096845626831 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3871, Accuracy: 0.8835, F1 Micro: 0.6275, F1 Macro: 0.3025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2654, Accuracy: 0.9041, F1 Micro: 0.6934, F1 Macro: 0.4946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2195, Accuracy: 0.9076, F1 Micro: 0.7344, F1 Macro: 0.5621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.9122, F1 Micro: 0.7476, F1 Macro: 0.5969\n",
      "Epoch 5/10, Train Loss: 0.142, Accuracy: 0.9151, F1 Micro: 0.7432, F1 Macro: 0.6241\n",
      "Epoch 6/10, Train Loss: 0.1126, Accuracy: 0.9168, F1 Micro: 0.7425, F1 Macro: 0.6268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9157, F1 Micro: 0.7539, F1 Macro: 0.6487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9185, F1 Micro: 0.756, F1 Macro: 0.661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9169, F1 Micro: 0.7564, F1 Macro: 0.667\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9124, F1 Micro: 0.7525, F1 Macro: 0.6461\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9169, F1 Micro: 0.7564, F1 Macro: 0.667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.84      0.84      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.72      0.71       732\n",
      "     HS_Group       0.68      0.62      0.65       402\n",
      "  HS_Religion       0.62      0.67      0.64       157\n",
      "      HS_Race       0.74      0.61      0.67       120\n",
      "  HS_Physical       0.59      0.26      0.37        72\n",
      "    HS_Gender       0.68      0.29      0.41        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.59      0.54      0.56       331\n",
      "    HS_Strong       0.85      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.72      0.64      0.67      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 170.61522364616394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3904, Accuracy: 0.881, F1 Micro: 0.6184, F1 Macro: 0.2935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2697, Accuracy: 0.9021, F1 Micro: 0.6838, F1 Macro: 0.4514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9074, F1 Micro: 0.725, F1 Macro: 0.5435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9135, F1 Micro: 0.7429, F1 Macro: 0.5985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1464, Accuracy: 0.913, F1 Micro: 0.751, F1 Macro: 0.6223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1181, Accuracy: 0.9165, F1 Micro: 0.7562, F1 Macro: 0.6537\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9172, F1 Micro: 0.7507, F1 Macro: 0.6409\n",
      "Epoch 8/10, Train Loss: 0.0791, Accuracy: 0.9164, F1 Micro: 0.754, F1 Macro: 0.6545\n",
      "Epoch 9/10, Train Loss: 0.0677, Accuracy: 0.9161, F1 Micro: 0.7438, F1 Macro: 0.6472\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9183, F1 Micro: 0.7509, F1 Macro: 0.6439\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9165, F1 Micro: 0.7562, F1 Macro: 0.6537\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.84      0.92      0.88       992\n",
      "HS_Individual       0.71      0.72      0.71       732\n",
      "     HS_Group       0.68      0.63      0.65       402\n",
      "  HS_Religion       0.71      0.54      0.61       157\n",
      "      HS_Race       0.75      0.70      0.72       120\n",
      "  HS_Physical       0.67      0.14      0.23        72\n",
      "    HS_Gender       0.58      0.27      0.37        51\n",
      "     HS_Other       0.76      0.76      0.76       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.59      0.54      0.56       331\n",
      "    HS_Strong       0.86      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.72      0.63      0.65      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.44      0.43      0.41      5556\n",
      "\n",
      "Training completed in 169.71318316459656 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3908, Accuracy: 0.8805, F1 Micro: 0.6268, F1 Macro: 0.3053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2703, Accuracy: 0.9014, F1 Micro: 0.6754, F1 Macro: 0.4745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2219, Accuracy: 0.9085, F1 Micro: 0.7291, F1 Macro: 0.5688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9123, F1 Micro: 0.7422, F1 Macro: 0.5941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1469, Accuracy: 0.9131, F1 Micro: 0.7455, F1 Macro: 0.6168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9157, F1 Micro: 0.751, F1 Macro: 0.6398\n",
      "Epoch 7/10, Train Loss: 0.0932, Accuracy: 0.9151, F1 Micro: 0.7457, F1 Macro: 0.6365\n",
      "Epoch 8/10, Train Loss: 0.077, Accuracy: 0.9132, F1 Micro: 0.7419, F1 Macro: 0.6469\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9156, F1 Micro: 0.7481, F1 Macro: 0.6595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9183, F1 Micro: 0.752, F1 Macro: 0.6527\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9183, F1 Micro: 0.752, F1 Macro: 0.6527\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.81      0.83      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.73      0.57      0.64       402\n",
      "  HS_Religion       0.78      0.51      0.62       157\n",
      "      HS_Race       0.82      0.67      0.73       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.75      0.24      0.36        51\n",
      "     HS_Other       0.75      0.77      0.76       762\n",
      "      HS_Weak       0.70      0.66      0.68       689\n",
      "  HS_Moderate       0.64      0.47      0.55       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.72      0.75      5556\n",
      "    macro avg       0.78      0.60      0.65      5556\n",
      " weighted avg       0.79      0.72      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 170.63200092315674 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9058, F1 Micro: 0.7159, F1 Macro: 0.5741\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 67.82047057151794 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3813, Accuracy: 0.8857, F1 Micro: 0.6016, F1 Macro: 0.3015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2572, Accuracy: 0.8983, F1 Micro: 0.7149, F1 Macro: 0.5434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2111, Accuracy: 0.9131, F1 Micro: 0.7323, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9155, F1 Micro: 0.7512, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9124, F1 Micro: 0.7547, F1 Macro: 0.6294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9164, F1 Micro: 0.7591, F1 Macro: 0.6448\n",
      "Epoch 7/10, Train Loss: 0.085, Accuracy: 0.9185, F1 Micro: 0.7546, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.9198, F1 Micro: 0.7628, F1 Macro: 0.6642\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.919, F1 Micro: 0.7481, F1 Macro: 0.6771\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9193, F1 Micro: 0.757, F1 Macro: 0.6754\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9198, F1 Micro: 0.7628, F1 Macro: 0.6642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.75      0.58      0.65       157\n",
      "      HS_Race       0.85      0.59      0.70       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.59      0.25      0.36        51\n",
      "     HS_Other       0.74      0.78      0.76       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.86      0.77      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.76      0.62      0.66      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 183.01794958114624 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3814, Accuracy: 0.8846, F1 Micro: 0.6008, F1 Macro: 0.3023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.9001, F1 Micro: 0.7113, F1 Macro: 0.5277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2141, Accuracy: 0.9146, F1 Micro: 0.7338, F1 Macro: 0.5715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9176, F1 Micro: 0.7463, F1 Macro: 0.606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.9166, F1 Micro: 0.7515, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.9148, F1 Micro: 0.7519, F1 Macro: 0.6512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0874, Accuracy: 0.9147, F1 Micro: 0.752, F1 Macro: 0.6704\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9131, F1 Micro: 0.7484, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9191, F1 Micro: 0.7558, F1 Macro: 0.6873\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.915, F1 Micro: 0.7483, F1 Macro: 0.68\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9191, F1 Micro: 0.7558, F1 Macro: 0.6873\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.75      0.67      0.70       732\n",
      "     HS_Group       0.68      0.66      0.67       402\n",
      "  HS_Religion       0.76      0.59      0.67       157\n",
      "      HS_Race       0.78      0.61      0.69       120\n",
      "  HS_Physical       0.65      0.31      0.42        72\n",
      "    HS_Gender       0.65      0.43      0.52        51\n",
      "     HS_Other       0.78      0.73      0.76       762\n",
      "      HS_Weak       0.73      0.64      0.68       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.73      0.76      5556\n",
      "    macro avg       0.75      0.64      0.69      5556\n",
      " weighted avg       0.78      0.73      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 184.6838719844818 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3833, Accuracy: 0.8852, F1 Micro: 0.5983, F1 Macro: 0.2953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2596, Accuracy: 0.8977, F1 Micro: 0.7128, F1 Macro: 0.5487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2147, Accuracy: 0.9131, F1 Micro: 0.7313, F1 Macro: 0.5761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9164, F1 Micro: 0.7511, F1 Macro: 0.6055\n",
      "Epoch 5/10, Train Loss: 0.1378, Accuracy: 0.9142, F1 Micro: 0.7485, F1 Macro: 0.6208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9179, F1 Micro: 0.7571, F1 Macro: 0.6447\n",
      "Epoch 7/10, Train Loss: 0.0869, Accuracy: 0.9116, F1 Micro: 0.7517, F1 Macro: 0.6696\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9132, F1 Micro: 0.7512, F1 Macro: 0.6558\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9187, F1 Micro: 0.7545, F1 Macro: 0.6801\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9185, F1 Micro: 0.7532, F1 Macro: 0.6762\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9179, F1 Micro: 0.7571, F1 Macro: 0.6447\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.85      0.46      0.60       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.72      0.67      0.69       689\n",
      "  HS_Moderate       0.56      0.56      0.56       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5556\n",
      "    macro avg       0.75      0.61      0.64      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 180.4689588546753 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9075, F1 Micro: 0.7212, F1 Macro: 0.5855\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 60.36649417877197 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3783, Accuracy: 0.8875, F1 Micro: 0.6292, F1 Macro: 0.3408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2571, Accuracy: 0.9072, F1 Micro: 0.7242, F1 Macro: 0.5451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2099, Accuracy: 0.9116, F1 Micro: 0.7486, F1 Macro: 0.5926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1689, Accuracy: 0.9177, F1 Micro: 0.7492, F1 Macro: 0.624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.9188, F1 Micro: 0.7546, F1 Macro: 0.633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9147, F1 Micro: 0.7562, F1 Macro: 0.6667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9201, F1 Micro: 0.7624, F1 Macro: 0.6689\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.9195, F1 Micro: 0.7559, F1 Macro: 0.6768\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9214, F1 Micro: 0.762, F1 Macro: 0.6765\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9136, F1 Micro: 0.7542, F1 Macro: 0.6791\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9201, F1 Micro: 0.7624, F1 Macro: 0.6689\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.71      0.61      0.66       402\n",
      "  HS_Religion       0.82      0.43      0.56       157\n",
      "      HS_Race       0.78      0.65      0.71       120\n",
      "  HS_Physical       0.74      0.24      0.36        72\n",
      "    HS_Gender       0.64      0.35      0.46        51\n",
      "     HS_Other       0.77      0.78      0.77       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.63      0.52      0.57       331\n",
      "    HS_Strong       0.87      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.76      0.62      0.67      5556\n",
      " weighted avg       0.78      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 196.26843643188477 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3796, Accuracy: 0.8883, F1 Micro: 0.6346, F1 Macro: 0.3314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2606, Accuracy: 0.9065, F1 Micro: 0.7175, F1 Macro: 0.5291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.215, Accuracy: 0.9145, F1 Micro: 0.7466, F1 Macro: 0.5923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.172, Accuracy: 0.9188, F1 Micro: 0.7522, F1 Macro: 0.6264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1314, Accuracy: 0.9188, F1 Micro: 0.7529, F1 Macro: 0.635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9146, F1 Micro: 0.7544, F1 Macro: 0.666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9158, F1 Micro: 0.7587, F1 Macro: 0.6754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0687, Accuracy: 0.9203, F1 Micro: 0.7645, F1 Macro: 0.6894\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9212, F1 Micro: 0.7595, F1 Macro: 0.6712\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9156, F1 Micro: 0.7605, F1 Macro: 0.687\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9203, F1 Micro: 0.7645, F1 Macro: 0.6894\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.74      0.69      0.72       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.72      0.73      0.73       120\n",
      "  HS_Physical       0.79      0.26      0.40        72\n",
      "    HS_Gender       0.68      0.33      0.45        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.75      0.66      0.69      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 197.71222472190857 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.8875, F1 Micro: 0.6316, F1 Macro: 0.3259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2593, Accuracy: 0.9071, F1 Micro: 0.7098, F1 Macro: 0.5122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2137, Accuracy: 0.9099, F1 Micro: 0.7447, F1 Macro: 0.5887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.9173, F1 Micro: 0.7489, F1 Macro: 0.6215\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9181, F1 Micro: 0.7397, F1 Macro: 0.6199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9171, F1 Micro: 0.7585, F1 Macro: 0.6507\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9138, F1 Micro: 0.7572, F1 Macro: 0.65\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.9213, F1 Micro: 0.7603, F1 Macro: 0.6775\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9203, F1 Micro: 0.75, F1 Macro: 0.655\n",
      "Epoch 10/10, Train Loss: 0.0539, Accuracy: 0.9161, F1 Micro: 0.7499, F1 Macro: 0.6706\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9213, F1 Micro: 0.7603, F1 Macro: 0.6775\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.81      0.84      1134\n",
      "      Abusive       0.91      0.85      0.88       992\n",
      "HS_Individual       0.75      0.68      0.71       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.78      0.75      0.76       762\n",
      "      HS_Weak       0.72      0.66      0.69       689\n",
      "  HS_Moderate       0.62      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.72      0.76      5556\n",
      "    macro avg       0.77      0.63      0.68      5556\n",
      " weighted avg       0.80      0.72      0.76      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 196.38183498382568 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9089, F1 Micro: 0.7258, F1 Macro: 0.5958\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 54.83336567878723 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3827, Accuracy: 0.8814, F1 Micro: 0.667, F1 Macro: 0.3828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.9036, F1 Micro: 0.6997, F1 Macro: 0.4976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2118, Accuracy: 0.9144, F1 Micro: 0.7285, F1 Macro: 0.5633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9189, F1 Micro: 0.7525, F1 Macro: 0.6099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9193, F1 Micro: 0.756, F1 Macro: 0.6461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1045, Accuracy: 0.9179, F1 Micro: 0.7603, F1 Macro: 0.6519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9191, F1 Micro: 0.7676, F1 Macro: 0.6795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.6913\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9167, F1 Micro: 0.762, F1 Macro: 0.6933\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.9207, F1 Micro: 0.7674, F1 Macro: 0.6935\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.6913\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.68      0.65      0.67       402\n",
      "  HS_Religion       0.78      0.58      0.66       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       0.82      0.25      0.38        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.61      0.56      0.58       331\n",
      "    HS_Strong       0.84      0.84      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 204.50706958770752 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3834, Accuracy: 0.8858, F1 Micro: 0.6575, F1 Macro: 0.3638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2608, Accuracy: 0.9029, F1 Micro: 0.701, F1 Macro: 0.5093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2153, Accuracy: 0.9143, F1 Micro: 0.7338, F1 Macro: 0.5668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1737, Accuracy: 0.9203, F1 Micro: 0.7526, F1 Macro: 0.6163\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9197, F1 Micro: 0.7444, F1 Macro: 0.6299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1091, Accuracy: 0.919, F1 Micro: 0.7595, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.9147, F1 Micro: 0.7613, F1 Macro: 0.6763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9213, F1 Micro: 0.7685, F1 Macro: 0.6814\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.914, F1 Micro: 0.7594, F1 Macro: 0.688\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9215, F1 Micro: 0.7648, F1 Macro: 0.6864\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9213, F1 Micro: 0.7685, F1 Macro: 0.6814\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.70      0.75      0.72       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.75      0.57      0.64       157\n",
      "      HS_Race       0.82      0.68      0.74       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.59      0.37      0.46        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.67      0.50      0.57       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 203.25332260131836 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3862, Accuracy: 0.8817, F1 Micro: 0.6638, F1 Macro: 0.3763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2601, Accuracy: 0.9018, F1 Micro: 0.7043, F1 Macro: 0.5161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2136, Accuracy: 0.9136, F1 Micro: 0.7158, F1 Macro: 0.5527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1731, Accuracy: 0.9182, F1 Micro: 0.7429, F1 Macro: 0.5972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1334, Accuracy: 0.9181, F1 Micro: 0.7557, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9211, F1 Micro: 0.761, F1 Macro: 0.6545\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9114, F1 Micro: 0.7566, F1 Macro: 0.6671\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9164, F1 Micro: 0.7603, F1 Macro: 0.6673\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9156, F1 Micro: 0.7597, F1 Macro: 0.6764\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.921, F1 Micro: 0.7608, F1 Macro: 0.6773\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9211, F1 Micro: 0.761, F1 Macro: 0.6545\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.78      0.54      0.64       157\n",
      "      HS_Race       0.86      0.60      0.71       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.57      0.24      0.33        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.66      0.69       689\n",
      "  HS_Moderate       0.64      0.52      0.57       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.73      0.76      5556\n",
      "    macro avg       0.79      0.60      0.65      5556\n",
      " weighted avg       0.79      0.73      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 200.21548771858215 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9102, F1 Micro: 0.7299, F1 Macro: 0.6038\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 50.60560393333435 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.363, Accuracy: 0.8926, F1 Micro: 0.6413, F1 Macro: 0.3954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2455, Accuracy: 0.9077, F1 Micro: 0.6855, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9182, F1 Micro: 0.7513, F1 Macro: 0.5957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1623, Accuracy: 0.9192, F1 Micro: 0.761, F1 Macro: 0.6206\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9192, F1 Micro: 0.7577, F1 Macro: 0.6433\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9199, F1 Micro: 0.7523, F1 Macro: 0.634\n",
      "Epoch 7/10, Train Loss: 0.0878, Accuracy: 0.9192, F1 Micro: 0.761, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9147, F1 Micro: 0.7613, F1 Macro: 0.6846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9213, F1 Micro: 0.7674, F1 Macro: 0.6996\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9151, F1 Micro: 0.7621, F1 Macro: 0.6944\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9213, F1 Micro: 0.7674, F1 Macro: 0.6996\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.75      0.68      0.71       732\n",
      "     HS_Group       0.67      0.71      0.69       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.71      0.33      0.45        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.73      0.65      0.69       689\n",
      "  HS_Moderate       0.60      0.64      0.62       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.74      0.67      0.70      5556\n",
      " weighted avg       0.78      0.75      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 213.44682669639587 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.37, Accuracy: 0.8924, F1 Micro: 0.6379, F1 Macro: 0.3717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.9057, F1 Micro: 0.6783, F1 Macro: 0.4534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9191, F1 Micro: 0.7474, F1 Macro: 0.5917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9213, F1 Micro: 0.7667, F1 Macro: 0.6288\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9215, F1 Micro: 0.7636, F1 Macro: 0.6572\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9226, F1 Micro: 0.7653, F1 Macro: 0.6748\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9197, F1 Micro: 0.7667, F1 Macro: 0.6748\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9186, F1 Micro: 0.757, F1 Macro: 0.6815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9214, F1 Micro: 0.772, F1 Macro: 0.6988\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9177, F1 Micro: 0.764, F1 Macro: 0.7\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9214, F1 Micro: 0.772, F1 Macro: 0.6988\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.67      0.61      0.64       157\n",
      "      HS_Race       0.73      0.72      0.72       120\n",
      "  HS_Physical       0.67      0.33      0.44        72\n",
      "    HS_Gender       0.66      0.41      0.51        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.62      0.57      0.59       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.68      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 209.99833154678345 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3692, Accuracy: 0.8923, F1 Micro: 0.6525, F1 Macro: 0.3949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2506, Accuracy: 0.9062, F1 Micro: 0.6855, F1 Macro: 0.4819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.197, Accuracy: 0.9175, F1 Micro: 0.7414, F1 Macro: 0.5841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1664, Accuracy: 0.9202, F1 Micro: 0.7643, F1 Macro: 0.6194\n",
      "Epoch 5/10, Train Loss: 0.1276, Accuracy: 0.9213, F1 Micro: 0.7608, F1 Macro: 0.6597\n",
      "Epoch 6/10, Train Loss: 0.1043, Accuracy: 0.9213, F1 Micro: 0.7589, F1 Macro: 0.6441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9227, F1 Micro: 0.7736, F1 Macro: 0.6911\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.919, F1 Micro: 0.752, F1 Macro: 0.6798\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9217, F1 Micro: 0.7674, F1 Macro: 0.6979\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9166, F1 Micro: 0.765, F1 Macro: 0.6905\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9227, F1 Micro: 0.7736, F1 Macro: 0.6911\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.83      0.26      0.40        72\n",
      "    HS_Gender       0.58      0.35      0.44        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 210.71685600280762 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9112, F1 Micro: 0.7337, F1 Macro: 0.6123\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 45.055715799331665 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3666, Accuracy: 0.8929, F1 Micro: 0.6626, F1 Macro: 0.4136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2458, Accuracy: 0.9094, F1 Micro: 0.7279, F1 Macro: 0.5398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.9177, F1 Micro: 0.7358, F1 Macro: 0.5633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1567, Accuracy: 0.9208, F1 Micro: 0.7622, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9215, F1 Micro: 0.7722, F1 Macro: 0.6642\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9173, F1 Micro: 0.7651, F1 Macro: 0.6624\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9215, F1 Micro: 0.7692, F1 Macro: 0.6632\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9206, F1 Micro: 0.7669, F1 Macro: 0.689\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9182, F1 Micro: 0.7656, F1 Macro: 0.6951\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9197, F1 Micro: 0.7676, F1 Macro: 0.694\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9215, F1 Micro: 0.7722, F1 Macro: 0.6642\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.85      1134\n",
      "      Abusive       0.90      0.87      0.89       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.66      0.62      0.64       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.65      0.18      0.28        72\n",
      "    HS_Gender       0.53      0.16      0.24        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.55      0.60       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.72      0.65      0.66      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 218.4334499835968 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3684, Accuracy: 0.8932, F1 Micro: 0.6606, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2473, Accuracy: 0.9093, F1 Micro: 0.7299, F1 Macro: 0.5581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1989, Accuracy: 0.9184, F1 Micro: 0.7394, F1 Macro: 0.5764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.921, F1 Micro: 0.7612, F1 Macro: 0.6331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.132, Accuracy: 0.9225, F1 Micro: 0.7699, F1 Macro: 0.6732\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9164, F1 Micro: 0.7651, F1 Macro: 0.6682\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9198, F1 Micro: 0.7525, F1 Macro: 0.6259\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.921, F1 Micro: 0.7675, F1 Macro: 0.6877\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9199, F1 Micro: 0.7657, F1 Macro: 0.6961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9214, F1 Micro: 0.7709, F1 Macro: 0.7032\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9214, F1 Micro: 0.7709, F1 Macro: 0.7032\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.68      0.63      0.65       402\n",
      "  HS_Religion       0.71      0.66      0.68       157\n",
      "      HS_Race       0.74      0.69      0.72       120\n",
      "  HS_Physical       0.83      0.35      0.49        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 219.59049272537231 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3688, Accuracy: 0.8923, F1 Micro: 0.6696, F1 Macro: 0.4165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.249, Accuracy: 0.9093, F1 Micro: 0.7283, F1 Macro: 0.5477\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2017, Accuracy: 0.918, F1 Micro: 0.738, F1 Macro: 0.5813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1592, Accuracy: 0.9201, F1 Micro: 0.7658, F1 Macro: 0.6444\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9181, F1 Micro: 0.7641, F1 Macro: 0.6584\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.9156, F1 Micro: 0.7618, F1 Macro: 0.6582\n",
      "Epoch 7/10, Train Loss: 0.0875, Accuracy: 0.9207, F1 Micro: 0.7585, F1 Macro: 0.6417\n",
      "Epoch 8/10, Train Loss: 0.0694, Accuracy: 0.9136, F1 Micro: 0.757, F1 Macro: 0.666\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.918, F1 Micro: 0.7655, F1 Macro: 0.6838\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9206, F1 Micro: 0.7642, F1 Macro: 0.6944\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9201, F1 Micro: 0.7658, F1 Macro: 0.6444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.77      0.62      0.69       402\n",
      "  HS_Religion       0.77      0.57      0.66       157\n",
      "      HS_Race       0.83      0.75      0.79       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.62      0.10      0.17        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.66      0.50      0.57       331\n",
      "    HS_Strong       0.86      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.77      0.62      0.64      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 216.83950209617615 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.912, F1 Micro: 0.7367, F1 Macro: 0.6171\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 42.01283025741577 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3614, Accuracy: 0.8929, F1 Micro: 0.6762, F1 Macro: 0.4469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.248, Accuracy: 0.9104, F1 Micro: 0.7148, F1 Macro: 0.5582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1997, Accuracy: 0.9174, F1 Micro: 0.7535, F1 Macro: 0.5999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1583, Accuracy: 0.9189, F1 Micro: 0.7652, F1 Macro: 0.6386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9215, F1 Micro: 0.7695, F1 Macro: 0.6736\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9209, F1 Micro: 0.7695, F1 Macro: 0.6848\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9223, F1 Micro: 0.7686, F1 Macro: 0.6861\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9176, F1 Micro: 0.7693, F1 Macro: 0.6961\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.922, F1 Micro: 0.7669, F1 Macro: 0.6904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9208, F1 Micro: 0.7727, F1 Macro: 0.7033\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9208, F1 Micro: 0.7727, F1 Macro: 0.7033\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.91      0.91       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.64      0.68      0.66       157\n",
      "      HS_Race       0.67      0.80      0.73       120\n",
      "  HS_Physical       0.66      0.29      0.40        72\n",
      "    HS_Gender       0.60      0.55      0.57        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.72      0.70      0.70      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 230.27416348457336 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3618, Accuracy: 0.8942, F1 Micro: 0.6644, F1 Macro: 0.4223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2489, Accuracy: 0.9115, F1 Micro: 0.7161, F1 Macro: 0.5588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2031, Accuracy: 0.9154, F1 Micro: 0.745, F1 Macro: 0.5882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1566, Accuracy: 0.916, F1 Micro: 0.7625, F1 Macro: 0.6696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1275, Accuracy: 0.9187, F1 Micro: 0.7655, F1 Macro: 0.6811\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9158, F1 Micro: 0.7628, F1 Macro: 0.675\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9232, F1 Micro: 0.7653, F1 Macro: 0.6866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.9194, F1 Micro: 0.7668, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9198, F1 Micro: 0.7678, F1 Macro: 0.6955\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9197, F1 Micro: 0.7676, F1 Macro: 0.7016\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9198, F1 Micro: 0.7678, F1 Macro: 0.6955\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.68      0.66      0.67       402\n",
      "  HS_Religion       0.68      0.61      0.64       157\n",
      "      HS_Race       0.83      0.57      0.67       120\n",
      "  HS_Physical       0.66      0.29      0.40        72\n",
      "    HS_Gender       0.66      0.49      0.56        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.71      0.69       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.67      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 229.76936173439026 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3643, Accuracy: 0.8946, F1 Micro: 0.664, F1 Macro: 0.4354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2507, Accuracy: 0.9108, F1 Micro: 0.7196, F1 Macro: 0.5729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2003, Accuracy: 0.9181, F1 Micro: 0.7499, F1 Macro: 0.5989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1563, Accuracy: 0.9186, F1 Micro: 0.7637, F1 Macro: 0.6398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1266, Accuracy: 0.9216, F1 Micro: 0.7656, F1 Macro: 0.6654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0984, Accuracy: 0.9234, F1 Micro: 0.7716, F1 Macro: 0.677\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9218, F1 Micro: 0.7651, F1 Macro: 0.6819\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9205, F1 Micro: 0.7646, F1 Macro: 0.6938\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9224, F1 Micro: 0.7674, F1 Macro: 0.6951\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9212, F1 Micro: 0.7634, F1 Macro: 0.6956\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9234, F1 Micro: 0.7716, F1 Macro: 0.677\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.77      0.62      0.69       402\n",
      "  HS_Religion       0.69      0.61      0.65       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.68      0.33      0.45        51\n",
      "     HS_Other       0.78      0.77      0.77       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.69      0.53      0.60       331\n",
      "    HS_Strong       0.85      0.77      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 228.58852338790894 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9128, F1 Micro: 0.7393, F1 Macro: 0.6229\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 38.25348353385925 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.358, Accuracy: 0.8958, F1 Micro: 0.6769, F1 Macro: 0.4096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2449, Accuracy: 0.9116, F1 Micro: 0.7315, F1 Macro: 0.5291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1949, Accuracy: 0.9181, F1 Micro: 0.7554, F1 Macro: 0.6297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1504, Accuracy: 0.9196, F1 Micro: 0.7682, F1 Macro: 0.6379\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9233, F1 Micro: 0.7576, F1 Macro: 0.6521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0947, Accuracy: 0.9222, F1 Micro: 0.7718, F1 Macro: 0.6755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9253, F1 Micro: 0.7791, F1 Macro: 0.7066\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9248, F1 Micro: 0.7706, F1 Macro: 0.6886\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9228, F1 Micro: 0.7678, F1 Macro: 0.6892\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.9218, F1 Micro: 0.7672, F1 Macro: 0.6954\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9253, F1 Micro: 0.7791, F1 Macro: 0.7066\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.65       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.73      0.33      0.46        72\n",
      "    HS_Gender       0.67      0.43      0.52        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.76      0.67      0.71      5556\n",
      " weighted avg       0.79      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 234.050701379776 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.361, Accuracy: 0.8955, F1 Micro: 0.6657, F1 Macro: 0.4206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2476, Accuracy: 0.9122, F1 Micro: 0.7251, F1 Macro: 0.5308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1982, Accuracy: 0.9201, F1 Micro: 0.7584, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1518, Accuracy: 0.9203, F1 Micro: 0.7632, F1 Macro: 0.6351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1283, Accuracy: 0.9238, F1 Micro: 0.769, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.922, F1 Micro: 0.7692, F1 Macro: 0.6862\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9216, F1 Micro: 0.7681, F1 Macro: 0.689\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9204, F1 Micro: 0.7674, F1 Macro: 0.6923\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9181, F1 Micro: 0.7656, F1 Macro: 0.6932\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9249, F1 Micro: 0.7686, F1 Macro: 0.701\n",
      "Model 2 - Iteration 7901: Accuracy: 0.922, F1 Micro: 0.7692, F1 Macro: 0.6862\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.83      0.84      1134\n",
      "      Abusive       0.86      0.93      0.90       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.81      0.59      0.68       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.63      0.26      0.37        72\n",
      "    HS_Gender       0.49      0.45      0.47        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.61      0.55      0.58       331\n",
      "    HS_Strong       0.95      0.68      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.75      0.65      0.69      5556\n",
      " weighted avg       0.78      0.75      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 234.01927542686462 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3602, Accuracy: 0.8959, F1 Micro: 0.6777, F1 Macro: 0.4308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2457, Accuracy: 0.9129, F1 Micro: 0.7278, F1 Macro: 0.5455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.917, F1 Micro: 0.7546, F1 Macro: 0.6053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1517, Accuracy: 0.919, F1 Micro: 0.7654, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9238, F1 Micro: 0.7725, F1 Macro: 0.6705\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9227, F1 Micro: 0.7669, F1 Macro: 0.6622\n",
      "Epoch 7/10, Train Loss: 0.0807, Accuracy: 0.9206, F1 Micro: 0.7649, F1 Macro: 0.681\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9224, F1 Micro: 0.7674, F1 Macro: 0.6919\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9188, F1 Micro: 0.7696, F1 Macro: 0.6985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9238, F1 Micro: 0.7763, F1 Macro: 0.7065\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9238, F1 Micro: 0.7763, F1 Macro: 0.7065\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.71      0.68      0.69       157\n",
      "      HS_Race       0.67      0.73      0.70       120\n",
      "  HS_Physical       0.81      0.31      0.44        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.71      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 236.64149928092957 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9135, F1 Micro: 0.7418, F1 Macro: 0.6284\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 34.48261260986328 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.354, Accuracy: 0.8886, F1 Micro: 0.6831, F1 Macro: 0.4508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2434, Accuracy: 0.912, F1 Micro: 0.7297, F1 Macro: 0.5375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1896, Accuracy: 0.9202, F1 Micro: 0.7537, F1 Macro: 0.5976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1512, Accuracy: 0.9208, F1 Micro: 0.7688, F1 Macro: 0.6592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.9197, F1 Micro: 0.7697, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9235, F1 Micro: 0.7744, F1 Macro: 0.6613\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9225, F1 Micro: 0.7703, F1 Macro: 0.684\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9249, F1 Micro: 0.7742, F1 Macro: 0.7096\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9211, F1 Micro: 0.7706, F1 Macro: 0.6992\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9239, F1 Micro: 0.7733, F1 Macro: 0.6933\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9235, F1 Micro: 0.7744, F1 Macro: 0.6613\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.78      0.58      0.66       402\n",
      "  HS_Religion       0.76      0.48      0.59       157\n",
      "      HS_Race       0.85      0.62      0.71       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.65      0.22      0.32        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.62      0.66      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 240.90444231033325 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.358, Accuracy: 0.8914, F1 Micro: 0.6775, F1 Macro: 0.414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.245, Accuracy: 0.9122, F1 Micro: 0.7248, F1 Macro: 0.5221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1924, Accuracy: 0.9205, F1 Micro: 0.7602, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1567, Accuracy: 0.9198, F1 Micro: 0.7626, F1 Macro: 0.653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9231, F1 Micro: 0.7676, F1 Macro: 0.6735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9199, F1 Micro: 0.7682, F1 Macro: 0.6743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9262, F1 Micro: 0.7704, F1 Macro: 0.7002\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9232, F1 Micro: 0.7639, F1 Macro: 0.6941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9232, F1 Micro: 0.7724, F1 Macro: 0.6966\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9234, F1 Micro: 0.7693, F1 Macro: 0.7053\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9232, F1 Micro: 0.7724, F1 Macro: 0.6966\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.74      0.50      0.60       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.73      0.33      0.46        72\n",
      "    HS_Gender       0.60      0.49      0.54        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.63      0.53      0.58       331\n",
      "    HS_Strong       0.89      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.75      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 243.92185711860657 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3588, Accuracy: 0.8908, F1 Micro: 0.6802, F1 Macro: 0.4398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.244, Accuracy: 0.912, F1 Micro: 0.7301, F1 Macro: 0.552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1921, Accuracy: 0.92, F1 Micro: 0.7566, F1 Macro: 0.601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9214, F1 Micro: 0.764, F1 Macro: 0.6376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9184, F1 Micro: 0.7698, F1 Macro: 0.6695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0942, Accuracy: 0.9205, F1 Micro: 0.7703, F1 Macro: 0.6541\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.924, F1 Micro: 0.7698, F1 Macro: 0.6972\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9216, F1 Micro: 0.7678, F1 Macro: 0.6939\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9218, F1 Micro: 0.7668, F1 Macro: 0.6885\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.92, F1 Micro: 0.7625, F1 Macro: 0.6831\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9205, F1 Micro: 0.7703, F1 Macro: 0.6541\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.78      0.59      0.67       402\n",
      "  HS_Religion       0.71      0.52      0.60       157\n",
      "      HS_Race       0.81      0.66      0.73       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.56      0.20      0.29        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.66      0.78      0.72       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.88      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.62      0.65      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 242.4660623073578 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9141, F1 Micro: 0.7439, F1 Macro: 0.6312\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 31.172722578048706 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3562, Accuracy: 0.8985, F1 Micro: 0.6771, F1 Macro: 0.4466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2351, Accuracy: 0.9128, F1 Micro: 0.715, F1 Macro: 0.5349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1851, Accuracy: 0.9197, F1 Micro: 0.749, F1 Macro: 0.5998\n",
      "Epoch 4/10, Train Loss: 0.1474, Accuracy: 0.9199, F1 Micro: 0.7446, F1 Macro: 0.6181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9227, F1 Micro: 0.769, F1 Macro: 0.646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9213, F1 Micro: 0.7714, F1 Macro: 0.6916\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9225, F1 Micro: 0.7662, F1 Macro: 0.6816\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9207, F1 Micro: 0.7631, F1 Macro: 0.6928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9205, F1 Micro: 0.7731, F1 Macro: 0.7052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6978\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9222, F1 Micro: 0.7734, F1 Macro: 0.6978\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.75      0.57      0.65       402\n",
      "  HS_Religion       0.74      0.56      0.64       157\n",
      "      HS_Race       0.75      0.61      0.67       120\n",
      "  HS_Physical       0.63      0.47      0.54        72\n",
      "    HS_Gender       0.50      0.45      0.47        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.67      0.75      0.71       689\n",
      "  HS_Moderate       0.68      0.50      0.57       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.73      0.67      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 248.27375149726868 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3588, Accuracy: 0.8972, F1 Micro: 0.6561, F1 Macro: 0.4314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2372, Accuracy: 0.9106, F1 Micro: 0.7085, F1 Macro: 0.5337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1884, Accuracy: 0.9201, F1 Micro: 0.7566, F1 Macro: 0.6295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9236, F1 Micro: 0.7642, F1 Macro: 0.66\n",
      "Epoch 5/10, Train Loss: 0.1251, Accuracy: 0.922, F1 Micro: 0.7606, F1 Macro: 0.6208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.923, F1 Micro: 0.7687, F1 Macro: 0.6869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9204, F1 Micro: 0.7719, F1 Macro: 0.687\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.9214, F1 Micro: 0.7703, F1 Macro: 0.6985\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.917, F1 Micro: 0.7664, F1 Macro: 0.7033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9234, F1 Micro: 0.7744, F1 Macro: 0.7146\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9234, F1 Micro: 0.7744, F1 Macro: 0.7146\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.84      0.60      0.70       120\n",
      "  HS_Physical       0.78      0.43      0.55        72\n",
      "    HS_Gender       0.65      0.51      0.57        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.63      0.55      0.58       331\n",
      "    HS_Strong       0.87      0.89      0.88       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 248.7033486366272 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3589, Accuracy: 0.8979, F1 Micro: 0.668, F1 Macro: 0.4298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2359, Accuracy: 0.9124, F1 Micro: 0.7174, F1 Macro: 0.5476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.187, Accuracy: 0.9198, F1 Micro: 0.7462, F1 Macro: 0.5989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9248, F1 Micro: 0.7678, F1 Macro: 0.644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9224, F1 Micro: 0.7689, F1 Macro: 0.6332\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9245, F1 Micro: 0.768, F1 Macro: 0.6737\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9241, F1 Micro: 0.7639, F1 Macro: 0.6637\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.922, F1 Micro: 0.7675, F1 Macro: 0.6951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.922, F1 Micro: 0.7725, F1 Macro: 0.7045\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9231, F1 Micro: 0.7717, F1 Macro: 0.6996\n",
      "Model 3 - Iteration 8402: Accuracy: 0.922, F1 Micro: 0.7725, F1 Macro: 0.7045\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.67      0.66      0.66       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.66      0.72      0.69       120\n",
      "  HS_Physical       0.77      0.32      0.45        72\n",
      "    HS_Gender       0.61      0.53      0.57        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.59      0.59      0.59       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.69      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 247.2418372631073 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9147, F1 Micro: 0.7457, F1 Macro: 0.6358\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 27.722771406173706 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3523, Accuracy: 0.8988, F1 Micro: 0.6776, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2392, Accuracy: 0.9147, F1 Micro: 0.7412, F1 Macro: 0.562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1875, Accuracy: 0.919, F1 Micro: 0.7597, F1 Macro: 0.6055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9232, F1 Micro: 0.775, F1 Macro: 0.6637\n",
      "Epoch 5/10, Train Loss: 0.1156, Accuracy: 0.9207, F1 Micro: 0.7733, F1 Macro: 0.6589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9221, F1 Micro: 0.7764, F1 Macro: 0.6908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9269, F1 Micro: 0.7776, F1 Macro: 0.6865\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9247, F1 Micro: 0.7754, F1 Macro: 0.6813\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9213, F1 Micro: 0.7713, F1 Macro: 0.7016\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9222, F1 Micro: 0.7749, F1 Macro: 0.7106\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9269, F1 Micro: 0.7776, F1 Macro: 0.6865\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.78      0.60      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.79      0.60      0.68       120\n",
      "  HS_Physical       0.89      0.24      0.37        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.81      0.76      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.72      0.50      0.59       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.82      0.74      0.78      5556\n",
      "    macro avg       0.79      0.63      0.69      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 251.10637164115906 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3549, Accuracy: 0.8962, F1 Micro: 0.6539, F1 Macro: 0.3957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2421, Accuracy: 0.9125, F1 Micro: 0.7354, F1 Macro: 0.5641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1911, Accuracy: 0.92, F1 Micro: 0.7581, F1 Macro: 0.6001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.923, F1 Micro: 0.772, F1 Macro: 0.6681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.121, Accuracy: 0.9218, F1 Micro: 0.7774, F1 Macro: 0.6826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9249, F1 Micro: 0.7787, F1 Macro: 0.6934\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.925, F1 Micro: 0.7747, F1 Macro: 0.6963\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.925, F1 Micro: 0.7765, F1 Macro: 0.7013\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9247, F1 Micro: 0.7747, F1 Macro: 0.7067\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9228, F1 Micro: 0.7783, F1 Macro: 0.7188\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9249, F1 Micro: 0.7787, F1 Macro: 0.6934\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.69      0.70      0.70       402\n",
      "  HS_Religion       0.69      0.66      0.67       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.74      0.69      0.71       689\n",
      "  HS_Moderate       0.61      0.63      0.62       331\n",
      "    HS_Strong       0.90      0.80      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.69      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 253.78175473213196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3547, Accuracy: 0.8961, F1 Micro: 0.6499, F1 Macro: 0.3935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2435, Accuracy: 0.9123, F1 Micro: 0.7346, F1 Macro: 0.5598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.9191, F1 Micro: 0.7595, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9229, F1 Micro: 0.7692, F1 Macro: 0.6505\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9187, F1 Micro: 0.7669, F1 Macro: 0.6643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0922, Accuracy: 0.925, F1 Micro: 0.7761, F1 Macro: 0.6787\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9247, F1 Micro: 0.7696, F1 Macro: 0.6877\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9234, F1 Micro: 0.7751, F1 Macro: 0.698\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9195, F1 Micro: 0.7709, F1 Macro: 0.6986\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9219, F1 Micro: 0.7751, F1 Macro: 0.7007\n",
      "Model 3 - Iteration 8616: Accuracy: 0.925, F1 Micro: 0.7761, F1 Macro: 0.6787\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.81      0.68      0.74       120\n",
      "  HS_Physical       1.00      0.17      0.29        72\n",
      "    HS_Gender       0.56      0.27      0.37        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.55      0.60       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 249.6342329978943 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9153, F1 Micro: 0.7476, F1 Macro: 0.6388\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.664916515350342 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3501, Accuracy: 0.8966, F1 Micro: 0.6515, F1 Macro: 0.4293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.236, Accuracy: 0.9125, F1 Micro: 0.7426, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1881, Accuracy: 0.9141, F1 Micro: 0.7609, F1 Macro: 0.6233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1539, Accuracy: 0.9251, F1 Micro: 0.7719, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9202, F1 Micro: 0.7726, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9236, F1 Micro: 0.7756, F1 Macro: 0.684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9256, F1 Micro: 0.7822, F1 Macro: 0.7122\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9195, F1 Micro: 0.7692, F1 Macro: 0.7011\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9239, F1 Micro: 0.7765, F1 Macro: 0.701\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.921, F1 Micro: 0.7735, F1 Macro: 0.7046\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9256, F1 Micro: 0.7822, F1 Macro: 0.7122\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.73      0.64      0.69       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.81      0.35      0.49        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 258.07610535621643 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.353, Accuracy: 0.8961, F1 Micro: 0.6483, F1 Macro: 0.4347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2394, Accuracy: 0.9126, F1 Micro: 0.7403, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1921, Accuracy: 0.9136, F1 Micro: 0.7594, F1 Macro: 0.6388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1556, Accuracy: 0.9245, F1 Micro: 0.7687, F1 Macro: 0.6437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9199, F1 Micro: 0.7726, F1 Macro: 0.6816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.926, F1 Micro: 0.7762, F1 Macro: 0.6863\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.7002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9249, F1 Micro: 0.7769, F1 Macro: 0.7018\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9246, F1 Micro: 0.7766, F1 Macro: 0.705\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9265, F1 Micro: 0.7762, F1 Macro: 0.7096\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9249, F1 Micro: 0.7769, F1 Macro: 0.7018\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.83      0.55      0.66       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.95      0.29      0.45        72\n",
      "    HS_Gender       0.69      0.39      0.50        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.65      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 257.80801033973694 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3544, Accuracy: 0.8952, F1 Micro: 0.6448, F1 Macro: 0.4506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2391, Accuracy: 0.912, F1 Micro: 0.7405, F1 Macro: 0.5779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9149, F1 Micro: 0.7588, F1 Macro: 0.6143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1522, Accuracy: 0.9254, F1 Micro: 0.7647, F1 Macro: 0.6486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9261, F1 Micro: 0.7808, F1 Macro: 0.6761\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9239, F1 Micro: 0.7762, F1 Macro: 0.6765\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9256, F1 Micro: 0.7764, F1 Macro: 0.7028\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9238, F1 Micro: 0.7773, F1 Macro: 0.6939\n",
      "Epoch 9/10, Train Loss: 0.0495, Accuracy: 0.924, F1 Micro: 0.7805, F1 Macro: 0.7106\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9192, F1 Micro: 0.7734, F1 Macro: 0.6992\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9261, F1 Micro: 0.7808, F1 Macro: 0.6761\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.92      0.86      0.89       992\n",
      "HS_Individual       0.76      0.73      0.74       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.74      0.64      0.69       157\n",
      "      HS_Race       0.74      0.76      0.75       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.79      0.80      0.80       762\n",
      "      HS_Weak       0.73      0.72      0.72       689\n",
      "  HS_Moderate       0.63      0.60      0.61       331\n",
      "    HS_Strong       0.88      0.74      0.80       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 254.63857626914978 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9159, F1 Micro: 0.7494, F1 Macro: 0.642\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.494739532470703 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3471, Accuracy: 0.8994, F1 Micro: 0.6928, F1 Macro: 0.5008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2343, Accuracy: 0.9141, F1 Micro: 0.7207, F1 Macro: 0.5343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1876, Accuracy: 0.9207, F1 Micro: 0.7594, F1 Macro: 0.6271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1498, Accuracy: 0.923, F1 Micro: 0.7704, F1 Macro: 0.6598\n",
      "Epoch 5/10, Train Loss: 0.1216, Accuracy: 0.9178, F1 Micro: 0.7614, F1 Macro: 0.6564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0944, Accuracy: 0.9259, F1 Micro: 0.7744, F1 Macro: 0.689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9237, F1 Micro: 0.7769, F1 Macro: 0.6945\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9244, F1 Micro: 0.7683, F1 Macro: 0.7011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9249, F1 Micro: 0.7774, F1 Macro: 0.704\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9217, F1 Micro: 0.7766, F1 Macro: 0.7085\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9249, F1 Micro: 0.7774, F1 Macro: 0.704\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.77      0.70      0.73       732\n",
      "     HS_Group       0.69      0.68      0.69       402\n",
      "  HS_Religion       0.73      0.58      0.65       157\n",
      "      HS_Race       0.79      0.61      0.69       120\n",
      "  HS_Physical       0.71      0.35      0.47        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.75      0.66      0.70       689\n",
      "  HS_Moderate       0.62      0.62      0.62       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 262.79165172576904 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3496, Accuracy: 0.8982, F1 Micro: 0.6884, F1 Macro: 0.4753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2363, Accuracy: 0.9143, F1 Micro: 0.7299, F1 Macro: 0.5326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.189, Accuracy: 0.9222, F1 Micro: 0.7676, F1 Macro: 0.6343\n",
      "Epoch 4/10, Train Loss: 0.1537, Accuracy: 0.9225, F1 Micro: 0.767, F1 Macro: 0.6624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9203, F1 Micro: 0.7721, F1 Macro: 0.6829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9266, F1 Micro: 0.7804, F1 Macro: 0.702\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9226, F1 Micro: 0.775, F1 Macro: 0.7045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.927, F1 Micro: 0.7824, F1 Macro: 0.7099\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9237, F1 Micro: 0.7757, F1 Macro: 0.7009\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9255, F1 Micro: 0.7773, F1 Macro: 0.7098\n",
      "Model 2 - Iteration 9016: Accuracy: 0.927, F1 Micro: 0.7824, F1 Macro: 0.7099\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.95      0.28      0.43        72\n",
      "    HS_Gender       0.60      0.49      0.54        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.74      0.70      0.72       689\n",
      "  HS_Moderate       0.67      0.57      0.61       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.67      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 261.82151079177856 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3504, Accuracy: 0.8988, F1 Micro: 0.6921, F1 Macro: 0.4812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2366, Accuracy: 0.9141, F1 Micro: 0.7344, F1 Macro: 0.5575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1896, Accuracy: 0.9216, F1 Micro: 0.7664, F1 Macro: 0.6166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.152, Accuracy: 0.9234, F1 Micro: 0.7673, F1 Macro: 0.6443\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9182, F1 Micro: 0.7655, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9247, F1 Micro: 0.7765, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9256, F1 Micro: 0.7766, F1 Macro: 0.6938\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9219, F1 Micro: 0.7732, F1 Macro: 0.7016\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9228, F1 Micro: 0.7741, F1 Macro: 0.6918\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9257, F1 Micro: 0.7752, F1 Macro: 0.7052\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9256, F1 Micro: 0.7766, F1 Macro: 0.6938\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1134\n",
      "      Abusive       0.92      0.87      0.89       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.77      0.59      0.67       402\n",
      "  HS_Religion       0.79      0.54      0.64       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.77      0.24      0.36        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.70      0.49      0.58       331\n",
      "    HS_Strong       0.84      0.85      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 261.76384830474854 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9164, F1 Micro: 0.7509, F1 Macro: 0.6452\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.861517190933228 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3458, Accuracy: 0.8989, F1 Micro: 0.6775, F1 Macro: 0.4497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2325, Accuracy: 0.9173, F1 Micro: 0.7435, F1 Macro: 0.581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.9223, F1 Micro: 0.7634, F1 Macro: 0.6369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1464, Accuracy: 0.9242, F1 Micro: 0.7697, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1151, Accuracy: 0.9262, F1 Micro: 0.7806, F1 Macro: 0.6909\n",
      "Epoch 6/10, Train Loss: 0.0923, Accuracy: 0.9241, F1 Micro: 0.7768, F1 Macro: 0.6796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9239, F1 Micro: 0.7809, F1 Macro: 0.7081\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9239, F1 Micro: 0.7713, F1 Macro: 0.7065\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9233, F1 Micro: 0.7756, F1 Macro: 0.7079\n",
      "Epoch 10/10, Train Loss: 0.0452, Accuracy: 0.922, F1 Micro: 0.7785, F1 Macro: 0.7057\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9239, F1 Micro: 0.7809, F1 Macro: 0.7081\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.74      0.55      0.64       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.85      0.32      0.46        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.66      0.54      0.60       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.43      0.44      0.42      5556\n",
      "\n",
      "Training completed in 266.6952202320099 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3491, Accuracy: 0.8977, F1 Micro: 0.6851, F1 Macro: 0.4499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2375, Accuracy: 0.9164, F1 Micro: 0.7379, F1 Macro: 0.5689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1836, Accuracy: 0.9234, F1 Micro: 0.7696, F1 Macro: 0.6559\n",
      "Epoch 4/10, Train Loss: 0.1519, Accuracy: 0.9209, F1 Micro: 0.7691, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9264, F1 Micro: 0.7834, F1 Macro: 0.699\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9264, F1 Micro: 0.774, F1 Macro: 0.6949\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9263, F1 Micro: 0.7813, F1 Macro: 0.7064\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.927, F1 Micro: 0.7739, F1 Macro: 0.7103\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.9247, F1 Micro: 0.7821, F1 Macro: 0.714\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.9249, F1 Micro: 0.7797, F1 Macro: 0.7191\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9264, F1 Micro: 0.7834, F1 Macro: 0.699\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.77      0.75       732\n",
      "     HS_Group       0.77      0.62      0.68       402\n",
      "  HS_Religion       0.79      0.49      0.61       157\n",
      "      HS_Race       0.75      0.67      0.70       120\n",
      "  HS_Physical       0.75      0.25      0.38        72\n",
      "    HS_Gender       0.78      0.41      0.54        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.71      0.54      0.62       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 262.4831335544586 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3494, Accuracy: 0.899, F1 Micro: 0.6867, F1 Macro: 0.4742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2348, Accuracy: 0.9164, F1 Micro: 0.7405, F1 Macro: 0.5887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1821, Accuracy: 0.9227, F1 Micro: 0.7683, F1 Macro: 0.6341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9243, F1 Micro: 0.7692, F1 Macro: 0.6515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9274, F1 Micro: 0.7791, F1 Macro: 0.6804\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.9264, F1 Micro: 0.7786, F1 Macro: 0.6746\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9204, F1 Micro: 0.7755, F1 Macro: 0.6968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.925, F1 Micro: 0.782, F1 Macro: 0.7156\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.9229, F1 Micro: 0.7764, F1 Macro: 0.7018\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9231, F1 Micro: 0.7798, F1 Macro: 0.7098\n",
      "Model 3 - Iteration 9216: Accuracy: 0.925, F1 Micro: 0.782, F1 Macro: 0.7156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.69      0.72      0.70       402\n",
      "  HS_Religion       0.76      0.64      0.69       157\n",
      "      HS_Race       0.71      0.68      0.70       120\n",
      "  HS_Physical       0.72      0.36      0.48        72\n",
      "    HS_Gender       0.58      0.57      0.57        51\n",
      "     HS_Other       0.79      0.81      0.80       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.63      0.62      0.63       331\n",
      "    HS_Strong       0.78      0.87      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.74      0.70      0.72      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 265.8339536190033 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9168, F1 Micro: 0.7525, F1 Macro: 0.6483\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 17.901187658309937 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3443, Accuracy: 0.8995, F1 Micro: 0.6879, F1 Macro: 0.4293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2337, Accuracy: 0.9142, F1 Micro: 0.748, F1 Macro: 0.5841\n",
      "Epoch 3/10, Train Loss: 0.184, Accuracy: 0.9215, F1 Micro: 0.7445, F1 Macro: 0.6039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1475, Accuracy: 0.9247, F1 Micro: 0.7692, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9266, F1 Micro: 0.7756, F1 Macro: 0.6558\n",
      "Epoch 6/10, Train Loss: 0.0898, Accuracy: 0.921, F1 Micro: 0.7752, F1 Macro: 0.699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9248, F1 Micro: 0.7791, F1 Macro: 0.7081\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9246, F1 Micro: 0.7775, F1 Macro: 0.7155\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.921, F1 Micro: 0.7751, F1 Macro: 0.7063\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9246, F1 Micro: 0.7781, F1 Macro: 0.7014\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9248, F1 Micro: 0.7791, F1 Macro: 0.7081\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.76      0.61      0.68       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.85      0.32      0.46        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.69      0.55      0.61       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 265.1439166069031 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3481, Accuracy: 0.8987, F1 Micro: 0.6851, F1 Macro: 0.4337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2367, Accuracy: 0.9131, F1 Micro: 0.7492, F1 Macro: 0.5847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1869, Accuracy: 0.9222, F1 Micro: 0.7562, F1 Macro: 0.6189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9238, F1 Micro: 0.7711, F1 Macro: 0.6707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1195, Accuracy: 0.9251, F1 Micro: 0.777, F1 Macro: 0.6703\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9146, F1 Micro: 0.7669, F1 Macro: 0.6907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9249, F1 Micro: 0.7797, F1 Macro: 0.7135\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.9178, F1 Micro: 0.7708, F1 Macro: 0.6976\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9205, F1 Micro: 0.7745, F1 Macro: 0.7076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9243, F1 Micro: 0.78, F1 Macro: 0.7113\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9243, F1 Micro: 0.78, F1 Macro: 0.7113\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.77      0.57      0.65       157\n",
      "      HS_Race       0.80      0.61      0.69       120\n",
      "  HS_Physical       0.78      0.35      0.48        72\n",
      "    HS_Gender       0.61      0.59      0.60        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.63      0.59      0.61       331\n",
      "    HS_Strong       0.89      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 268.3586859703064 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3498, Accuracy: 0.8989, F1 Micro: 0.6851, F1 Macro: 0.4637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2355, Accuracy: 0.913, F1 Micro: 0.7418, F1 Macro: 0.5789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1864, Accuracy: 0.9219, F1 Micro: 0.749, F1 Macro: 0.605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1484, Accuracy: 0.9246, F1 Micro: 0.7776, F1 Macro: 0.6642\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9245, F1 Micro: 0.7739, F1 Macro: 0.6529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9246, F1 Micro: 0.7814, F1 Macro: 0.6977\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9244, F1 Micro: 0.7795, F1 Macro: 0.7076\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9239, F1 Micro: 0.7772, F1 Macro: 0.7041\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9221, F1 Micro: 0.778, F1 Macro: 0.7112\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9259, F1 Micro: 0.7774, F1 Macro: 0.7018\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9246, F1 Micro: 0.7814, F1 Macro: 0.6977\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.77      0.64      0.70       157\n",
      "      HS_Race       0.79      0.62      0.70       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.58      0.49      0.53        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.66      0.55      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 265.01922154426575 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9172, F1 Micro: 0.7538, F1 Macro: 0.6511\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.42128324508667 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3459, Accuracy: 0.8981, F1 Micro: 0.6846, F1 Macro: 0.4789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2313, Accuracy: 0.912, F1 Micro: 0.7421, F1 Macro: 0.5566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.9214, F1 Micro: 0.7596, F1 Macro: 0.6263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1469, Accuracy: 0.9263, F1 Micro: 0.7757, F1 Macro: 0.6725\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.924, F1 Micro: 0.7753, F1 Macro: 0.6669\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.923, F1 Micro: 0.7722, F1 Macro: 0.6793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0721, Accuracy: 0.9245, F1 Micro: 0.7778, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9247, F1 Micro: 0.7836, F1 Macro: 0.7133\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.919, F1 Micro: 0.7699, F1 Macro: 0.7034\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9242, F1 Micro: 0.7758, F1 Macro: 0.7092\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9247, F1 Micro: 0.7836, F1 Macro: 0.7133\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.76      0.71      0.73       120\n",
      "  HS_Physical       0.75      0.33      0.46        72\n",
      "    HS_Gender       0.56      0.49      0.52        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.75      0.73       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 272.49167132377625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3488, Accuracy: 0.8981, F1 Micro: 0.6873, F1 Macro: 0.4692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2323, Accuracy: 0.9157, F1 Micro: 0.7407, F1 Macro: 0.5681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9209, F1 Micro: 0.7624, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1465, Accuracy: 0.9213, F1 Micro: 0.7711, F1 Macro: 0.6719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9214, F1 Micro: 0.7748, F1 Macro: 0.6821\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.9221, F1 Micro: 0.774, F1 Macro: 0.6899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9231, F1 Micro: 0.7797, F1 Macro: 0.7144\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9225, F1 Micro: 0.7777, F1 Macro: 0.714\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9225, F1 Micro: 0.7714, F1 Macro: 0.7035\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9237, F1 Micro: 0.7724, F1 Macro: 0.7013\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9231, F1 Micro: 0.7797, F1 Macro: 0.7144\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.66      0.69      0.68       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.96      0.31      0.46        72\n",
      "    HS_Gender       0.66      0.53      0.59        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.91      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.77      0.69      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.43      0.44      0.42      5556\n",
      "\n",
      "Training completed in 271.8120262622833 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3511, Accuracy: 0.8975, F1 Micro: 0.688, F1 Macro: 0.4927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2342, Accuracy: 0.9142, F1 Micro: 0.7412, F1 Macro: 0.5713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9213, F1 Micro: 0.7595, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9253, F1 Micro: 0.7717, F1 Macro: 0.6624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9238, F1 Micro: 0.7833, F1 Macro: 0.6814\n",
      "Epoch 6/10, Train Loss: 0.0944, Accuracy: 0.9238, F1 Micro: 0.7762, F1 Macro: 0.6781\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9251, F1 Micro: 0.7789, F1 Macro: 0.6996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9271, F1 Micro: 0.7839, F1 Macro: 0.7177\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.9255, F1 Micro: 0.7796, F1 Macro: 0.7085\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9238, F1 Micro: 0.7737, F1 Macro: 0.706\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9271, F1 Micro: 0.7839, F1 Macro: 0.7177\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.79      0.58      0.67       402\n",
      "  HS_Religion       0.75      0.65      0.70       157\n",
      "      HS_Race       0.73      0.76      0.75       120\n",
      "  HS_Physical       0.82      0.32      0.46        72\n",
      "    HS_Gender       0.62      0.55      0.58        51\n",
      "     HS_Other       0.81      0.78      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.71      0.50      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5556\n",
      "    macro avg       0.78      0.69      0.72      5556\n",
      " weighted avg       0.80      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 272.5974087715149 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9175, F1 Micro: 0.7551, F1 Macro: 0.654\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.520424842834473 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3426, Accuracy: 0.9017, F1 Micro: 0.7025, F1 Macro: 0.5238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2265, Accuracy: 0.9166, F1 Micro: 0.7448, F1 Macro: 0.5693\n",
      "Epoch 3/10, Train Loss: 0.1805, Accuracy: 0.9206, F1 Micro: 0.7438, F1 Macro: 0.6126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.144, Accuracy: 0.9218, F1 Micro: 0.7699, F1 Macro: 0.6462\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9218, F1 Micro: 0.7729, F1 Macro: 0.6947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9266, F1 Micro: 0.7779, F1 Macro: 0.691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.926, F1 Micro: 0.7814, F1 Macro: 0.7102\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.924, F1 Micro: 0.7735, F1 Macro: 0.6988\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9261, F1 Micro: 0.7805, F1 Macro: 0.7135\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9252, F1 Micro: 0.7811, F1 Macro: 0.7183\n",
      "Model 1 - Iteration 9618: Accuracy: 0.926, F1 Micro: 0.7814, F1 Macro: 0.7102\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.76      0.68      0.72       732\n",
      "     HS_Group       0.69      0.73      0.71       402\n",
      "  HS_Religion       0.72      0.65      0.68       157\n",
      "      HS_Race       0.75      0.67      0.71       120\n",
      "  HS_Physical       0.82      0.32      0.46        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.75      0.66      0.70       689\n",
      "  HS_Moderate       0.62      0.65      0.63       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 277.113374710083 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3452, Accuracy: 0.9006, F1 Micro: 0.7017, F1 Macro: 0.5135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2286, Accuracy: 0.916, F1 Micro: 0.7457, F1 Macro: 0.5753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.9216, F1 Micro: 0.7459, F1 Macro: 0.6143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1435, Accuracy: 0.9186, F1 Micro: 0.769, F1 Macro: 0.6565\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9233, F1 Micro: 0.7816, F1 Macro: 0.6996\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9222, F1 Micro: 0.7733, F1 Macro: 0.696\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9209, F1 Micro: 0.7733, F1 Macro: 0.7029\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9242, F1 Micro: 0.7777, F1 Macro: 0.706\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9255, F1 Micro: 0.7804, F1 Macro: 0.7249\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9254, F1 Micro: 0.7746, F1 Macro: 0.7009\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9233, F1 Micro: 0.7816, F1 Macro: 0.6996\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.70      0.77      0.74       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.85      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.77      0.80      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 275.63569593429565 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3472, Accuracy: 0.9016, F1 Micro: 0.6936, F1 Macro: 0.5109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2279, Accuracy: 0.9162, F1 Micro: 0.747, F1 Macro: 0.5771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9224, F1 Micro: 0.7515, F1 Macro: 0.6046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1436, Accuracy: 0.9176, F1 Micro: 0.7635, F1 Macro: 0.6523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9173, F1 Micro: 0.7724, F1 Macro: 0.6817\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9221, F1 Micro: 0.7718, F1 Macro: 0.6799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9265, F1 Micro: 0.7818, F1 Macro: 0.7073\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9256, F1 Micro: 0.7799, F1 Macro: 0.7076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0489, Accuracy: 0.9264, F1 Micro: 0.7831, F1 Macro: 0.7111\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9242, F1 Micro: 0.7813, F1 Macro: 0.7117\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9264, F1 Micro: 0.7831, F1 Macro: 0.7111\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.89      0.93      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.74      0.64      0.68       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.54      0.38      0.44        72\n",
      "    HS_Gender       0.55      0.53      0.54        51\n",
      "     HS_Other       0.81      0.78      0.79       762\n",
      "      HS_Weak       0.74      0.70      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.61       331\n",
      "    HS_Strong       0.82      0.85      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.73      0.69      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 277.92196774482727 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9179, F1 Micro: 0.7563, F1 Macro: 0.6563\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.907151222229004 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3377, Accuracy: 0.9009, F1 Micro: 0.7054, F1 Macro: 0.5005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2233, Accuracy: 0.9156, F1 Micro: 0.7291, F1 Macro: 0.5458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1796, Accuracy: 0.9217, F1 Micro: 0.7607, F1 Macro: 0.6412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1448, Accuracy: 0.9266, F1 Micro: 0.7754, F1 Macro: 0.665\n",
      "Epoch 5/10, Train Loss: 0.1099, Accuracy: 0.925, F1 Micro: 0.7703, F1 Macro: 0.6732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0892, Accuracy: 0.9231, F1 Micro: 0.7805, F1 Macro: 0.7065\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9252, F1 Micro: 0.7788, F1 Macro: 0.709\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9262, F1 Micro: 0.7752, F1 Macro: 0.7023\n",
      "Epoch 9/10, Train Loss: 0.0498, Accuracy: 0.9243, F1 Micro: 0.7646, F1 Macro: 0.7072\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9191, F1 Micro: 0.7666, F1 Macro: 0.698\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9231, F1 Micro: 0.7805, F1 Macro: 0.7065\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.65      0.69      0.67       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       0.68      0.32      0.43        72\n",
      "    HS_Gender       0.57      0.45      0.51        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.64      0.55      0.59       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 279.63082933425903 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3408, Accuracy: 0.8991, F1 Micro: 0.7026, F1 Macro: 0.5128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2266, Accuracy: 0.9157, F1 Micro: 0.7247, F1 Macro: 0.5472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1791, Accuracy: 0.9213, F1 Micro: 0.7629, F1 Macro: 0.6427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1449, Accuracy: 0.9254, F1 Micro: 0.7651, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.112, Accuracy: 0.9256, F1 Micro: 0.7665, F1 Macro: 0.6693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0924, Accuracy: 0.9195, F1 Micro: 0.7745, F1 Macro: 0.7\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9247, F1 Micro: 0.7791, F1 Macro: 0.7139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9245, F1 Micro: 0.7826, F1 Macro: 0.7146\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.9245, F1 Micro: 0.7779, F1 Macro: 0.713\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.9247, F1 Micro: 0.774, F1 Macro: 0.7127\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9245, F1 Micro: 0.7826, F1 Macro: 0.7146\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.70      0.78      0.74       120\n",
      "  HS_Physical       0.63      0.33      0.44        72\n",
      "    HS_Gender       0.63      0.53      0.57        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.68      0.56      0.61       331\n",
      "    HS_Strong       0.89      0.86      0.88       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 287.7659034729004 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3399, Accuracy: 0.8992, F1 Micro: 0.7003, F1 Macro: 0.5063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2251, Accuracy: 0.9156, F1 Micro: 0.7379, F1 Macro: 0.5428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1808, Accuracy: 0.9218, F1 Micro: 0.7641, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1453, Accuracy: 0.9256, F1 Micro: 0.769, F1 Macro: 0.6578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1133, Accuracy: 0.9246, F1 Micro: 0.7719, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9228, F1 Micro: 0.7775, F1 Macro: 0.6964\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.923, F1 Micro: 0.7749, F1 Macro: 0.7088\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9227, F1 Micro: 0.7699, F1 Macro: 0.6988\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9231, F1 Micro: 0.7746, F1 Macro: 0.709\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.925, F1 Micro: 0.776, F1 Macro: 0.705\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9228, F1 Micro: 0.7775, F1 Macro: 0.6964\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.69      0.79      0.73       732\n",
      "     HS_Group       0.75      0.61      0.68       402\n",
      "  HS_Religion       0.73      0.67      0.70       157\n",
      "      HS_Race       0.74      0.66      0.70       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.51      0.43      0.47        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.67      0.78      0.72       689\n",
      "  HS_Moderate       0.68      0.50      0.58       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 283.70742416381836 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9181, F1 Micro: 0.7573, F1 Macro: 0.6583\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.58592414855957 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3369, Accuracy: 0.9, F1 Micro: 0.6931, F1 Macro: 0.508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2239, Accuracy: 0.9158, F1 Micro: 0.7528, F1 Macro: 0.5867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1777, Accuracy: 0.9194, F1 Micro: 0.7695, F1 Macro: 0.6368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1443, Accuracy: 0.9242, F1 Micro: 0.7805, F1 Macro: 0.6801\n",
      "Epoch 5/10, Train Loss: 0.1113, Accuracy: 0.9243, F1 Micro: 0.7774, F1 Macro: 0.6923\n",
      "Epoch 6/10, Train Loss: 0.088, Accuracy: 0.9218, F1 Micro: 0.776, F1 Macro: 0.7042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.9238, F1 Micro: 0.7819, F1 Macro: 0.7079\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9195, F1 Micro: 0.7691, F1 Macro: 0.7019\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9241, F1 Micro: 0.7817, F1 Macro: 0.7183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9249, F1 Micro: 0.7826, F1 Macro: 0.7156\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9249, F1 Micro: 0.7826, F1 Macro: 0.7156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.92      0.91       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.72      0.69      0.71       402\n",
      "  HS_Religion       0.69      0.59      0.64       157\n",
      "      HS_Race       0.74      0.68      0.70       120\n",
      "  HS_Physical       0.65      0.39      0.49        72\n",
      "    HS_Gender       0.61      0.53      0.57        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.63      0.64       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.74      0.70      0.72      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 286.88180112838745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3387, Accuracy: 0.9002, F1 Micro: 0.6869, F1 Macro: 0.4758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2267, Accuracy: 0.9138, F1 Micro: 0.7508, F1 Macro: 0.6043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1826, Accuracy: 0.9201, F1 Micro: 0.771, F1 Macro: 0.6598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9231, F1 Micro: 0.778, F1 Macro: 0.6804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9258, F1 Micro: 0.7821, F1 Macro: 0.688\n",
      "Epoch 6/10, Train Loss: 0.089, Accuracy: 0.9221, F1 Micro: 0.7749, F1 Macro: 0.7015\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9255, F1 Micro: 0.7758, F1 Macro: 0.7067\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9245, F1 Micro: 0.7734, F1 Macro: 0.711\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9237, F1 Micro: 0.7703, F1 Macro: 0.7038\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.922, F1 Micro: 0.7703, F1 Macro: 0.7101\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9258, F1 Micro: 0.7821, F1 Macro: 0.688\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.76      0.76      0.76       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.74      0.73       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 284.7781057357788 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3401, Accuracy: 0.8985, F1 Micro: 0.6941, F1 Macro: 0.4874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2252, Accuracy: 0.9129, F1 Micro: 0.7469, F1 Macro: 0.5927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1816, Accuracy: 0.9138, F1 Micro: 0.7627, F1 Macro: 0.6253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1477, Accuracy: 0.9227, F1 Micro: 0.777, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.9246, F1 Micro: 0.7811, F1 Macro: 0.7041\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9201, F1 Micro: 0.7662, F1 Macro: 0.6934\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9209, F1 Micro: 0.7734, F1 Macro: 0.7053\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9231, F1 Micro: 0.7807, F1 Macro: 0.7125\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9212, F1 Micro: 0.7753, F1 Macro: 0.7078\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.9227, F1 Micro: 0.7744, F1 Macro: 0.7023\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9246, F1 Micro: 0.7811, F1 Macro: 0.7041\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.85      0.94      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.75      0.65      0.70       402\n",
      "  HS_Religion       0.72      0.66      0.68       157\n",
      "      HS_Race       0.68      0.76      0.72       120\n",
      "  HS_Physical       0.81      0.24      0.37        72\n",
      "    HS_Gender       0.56      0.53      0.55        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.84      0.86      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 285.23439621925354 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9184, F1 Micro: 0.7583, F1 Macro: 0.6601\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.889779567718506 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3362, Accuracy: 0.8994, F1 Micro: 0.7062, F1 Macro: 0.4927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2241, Accuracy: 0.9187, F1 Micro: 0.7457, F1 Macro: 0.5845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1723, Accuracy: 0.9214, F1 Micro: 0.7618, F1 Macro: 0.6366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1376, Accuracy: 0.9236, F1 Micro: 0.7716, F1 Macro: 0.6698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1103, Accuracy: 0.9253, F1 Micro: 0.7724, F1 Macro: 0.6734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9262, F1 Micro: 0.7752, F1 Macro: 0.6975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.9264, F1 Micro: 0.7846, F1 Macro: 0.7166\n",
      "Epoch 8/10, Train Loss: 0.0578, Accuracy: 0.9233, F1 Micro: 0.7758, F1 Macro: 0.7094\n",
      "Epoch 9/10, Train Loss: 0.048, Accuracy: 0.9214, F1 Micro: 0.7782, F1 Macro: 0.7082\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9271, F1 Micro: 0.7819, F1 Macro: 0.7113\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9264, F1 Micro: 0.7846, F1 Macro: 0.7166\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.90      0.92      0.91       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.78      0.62      0.70       157\n",
      "      HS_Race       0.76      0.66      0.71       120\n",
      "  HS_Physical       0.77      0.38      0.50        72\n",
      "    HS_Gender       0.64      0.45      0.53        51\n",
      "     HS_Other       0.78      0.81      0.80       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.66      0.54      0.59       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.77      0.69      0.72      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 294.0899498462677 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3386, Accuracy: 0.9017, F1 Micro: 0.6942, F1 Macro: 0.4754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2258, Accuracy: 0.9184, F1 Micro: 0.7418, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1738, Accuracy: 0.9235, F1 Micro: 0.7641, F1 Macro: 0.6351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1434, Accuracy: 0.9228, F1 Micro: 0.7717, F1 Macro: 0.6764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1113, Accuracy: 0.9238, F1 Micro: 0.7736, F1 Macro: 0.6758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9227, F1 Micro: 0.7741, F1 Macro: 0.6972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9258, F1 Micro: 0.7781, F1 Macro: 0.7087\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9217, F1 Micro: 0.7701, F1 Macro: 0.6986\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9228, F1 Micro: 0.7723, F1 Macro: 0.7022\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.9225, F1 Micro: 0.77, F1 Macro: 0.6862\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9258, F1 Micro: 0.7781, F1 Macro: 0.7087\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.77      0.59      0.67       402\n",
      "  HS_Religion       0.78      0.64      0.70       157\n",
      "      HS_Race       0.83      0.58      0.69       120\n",
      "  HS_Physical       0.92      0.32      0.47        72\n",
      "    HS_Gender       0.65      0.43      0.52        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.71      0.51      0.59       331\n",
      "    HS_Strong       0.89      0.88      0.88       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.66      0.71      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 293.5586574077606 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3386, Accuracy: 0.9042, F1 Micro: 0.6981, F1 Macro: 0.4947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2266, Accuracy: 0.9187, F1 Micro: 0.7456, F1 Macro: 0.5898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1734, Accuracy: 0.9228, F1 Micro: 0.7616, F1 Macro: 0.6156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1408, Accuracy: 0.924, F1 Micro: 0.7749, F1 Macro: 0.6682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1124, Accuracy: 0.9249, F1 Micro: 0.7793, F1 Macro: 0.6755\n",
      "Epoch 6/10, Train Loss: 0.0891, Accuracy: 0.9262, F1 Micro: 0.7759, F1 Macro: 0.6885\n",
      "Epoch 7/10, Train Loss: 0.0687, Accuracy: 0.9252, F1 Micro: 0.7728, F1 Macro: 0.6998\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.9229, F1 Micro: 0.7743, F1 Macro: 0.7054\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9223, F1 Micro: 0.775, F1 Macro: 0.7085\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.923, F1 Micro: 0.7709, F1 Macro: 0.6967\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9249, F1 Micro: 0.7793, F1 Macro: 0.6755\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.71      0.70      0.71       402\n",
      "  HS_Religion       0.74      0.62      0.68       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.73      0.70      0.72       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.84      0.86      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 290.13468956947327 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9187, F1 Micro: 0.7591, F1 Macro: 0.6616\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.46286678314209 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3325, Accuracy: 0.9015, F1 Micro: 0.6724, F1 Macro: 0.4402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2171, Accuracy: 0.9175, F1 Micro: 0.7502, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1729, Accuracy: 0.9222, F1 Micro: 0.7694, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1394, Accuracy: 0.925, F1 Micro: 0.7721, F1 Macro: 0.6793\n",
      "Epoch 5/10, Train Loss: 0.1064, Accuracy: 0.9239, F1 Micro: 0.7656, F1 Macro: 0.6806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9231, F1 Micro: 0.7748, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9236, F1 Micro: 0.7785, F1 Macro: 0.7073\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.924, F1 Micro: 0.7696, F1 Macro: 0.7095\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9233, F1 Micro: 0.7762, F1 Macro: 0.7092\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.924, F1 Micro: 0.7704, F1 Macro: 0.7098\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9236, F1 Micro: 0.7785, F1 Macro: 0.7073\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.68      0.70      0.69       402\n",
      "  HS_Religion       0.66      0.67      0.66       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.79      0.31      0.44        72\n",
      "    HS_Gender       0.51      0.49      0.50        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.62      0.63      0.62       331\n",
      "    HS_Strong       0.88      0.87      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 295.99485421180725 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.335, Accuracy: 0.9026, F1 Micro: 0.6787, F1 Macro: 0.4392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2193, Accuracy: 0.9166, F1 Micro: 0.7473, F1 Macro: 0.5767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1767, Accuracy: 0.9209, F1 Micro: 0.7665, F1 Macro: 0.6496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1405, Accuracy: 0.9253, F1 Micro: 0.7735, F1 Macro: 0.689\n",
      "Epoch 5/10, Train Loss: 0.1116, Accuracy: 0.9265, F1 Micro: 0.7712, F1 Macro: 0.691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9255, F1 Micro: 0.7806, F1 Macro: 0.7063\n",
      "Epoch 7/10, Train Loss: 0.0707, Accuracy: 0.922, F1 Micro: 0.7759, F1 Macro: 0.7112\n",
      "Epoch 8/10, Train Loss: 0.0569, Accuracy: 0.924, F1 Micro: 0.776, F1 Macro: 0.714\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9231, F1 Micro: 0.7787, F1 Macro: 0.7183\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9218, F1 Micro: 0.7747, F1 Macro: 0.7183\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9255, F1 Micro: 0.7806, F1 Macro: 0.7063\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.86      0.25      0.39        72\n",
      "    HS_Gender       0.75      0.41      0.53        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.68      0.56      0.62       331\n",
      "    HS_Strong       0.88      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 293.7235369682312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3361, Accuracy: 0.9013, F1 Micro: 0.6728, F1 Macro: 0.4223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9167, F1 Micro: 0.7437, F1 Macro: 0.5741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9209, F1 Micro: 0.7679, F1 Macro: 0.6239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.141, Accuracy: 0.9258, F1 Micro: 0.778, F1 Macro: 0.6691\n",
      "Epoch 5/10, Train Loss: 0.1081, Accuracy: 0.9231, F1 Micro: 0.7727, F1 Macro: 0.6923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0843, Accuracy: 0.9263, F1 Micro: 0.7801, F1 Macro: 0.6934\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9189, F1 Micro: 0.7699, F1 Macro: 0.7031\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9227, F1 Micro: 0.769, F1 Macro: 0.7031\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.9219, F1 Micro: 0.7709, F1 Macro: 0.7051\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9227, F1 Micro: 0.7773, F1 Macro: 0.7129\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9263, F1 Micro: 0.7801, F1 Macro: 0.6934\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.84      0.63      0.72       120\n",
      "  HS_Physical       1.00      0.24      0.38        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.80      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 294.16134881973267 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9189, F1 Micro: 0.7599, F1 Macro: 0.6632\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.188983201980591 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3254, Accuracy: 0.9016, F1 Micro: 0.7042, F1 Macro: 0.4957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2127, Accuracy: 0.915, F1 Micro: 0.7429, F1 Macro: 0.5731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1659, Accuracy: 0.9223, F1 Micro: 0.7717, F1 Macro: 0.65\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9222, F1 Micro: 0.7727, F1 Macro: 0.6879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.9236, F1 Micro: 0.7734, F1 Macro: 0.6853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9238, F1 Micro: 0.7757, F1 Macro: 0.6926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9237, F1 Micro: 0.7763, F1 Macro: 0.7102\n",
      "Epoch 8/10, Train Loss: 0.0572, Accuracy: 0.9199, F1 Micro: 0.7695, F1 Macro: 0.7061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0461, Accuracy: 0.9235, F1 Micro: 0.7766, F1 Macro: 0.715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.04, Accuracy: 0.9237, F1 Micro: 0.7769, F1 Macro: 0.7119\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9237, F1 Micro: 0.7769, F1 Macro: 0.7119\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.68      0.55      0.61       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.61      0.46      0.52        72\n",
      "    HS_Gender       0.60      0.53      0.56        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.73      0.69      0.71      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 304.1400189399719 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3276, Accuracy: 0.8992, F1 Micro: 0.6943, F1 Macro: 0.4625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2166, Accuracy: 0.9137, F1 Micro: 0.7416, F1 Macro: 0.5742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1699, Accuracy: 0.9221, F1 Micro: 0.768, F1 Macro: 0.6465\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9202, F1 Micro: 0.7641, F1 Macro: 0.6798\n",
      "Epoch 5/10, Train Loss: 0.1137, Accuracy: 0.9226, F1 Micro: 0.7652, F1 Macro: 0.6514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.9246, F1 Micro: 0.77, F1 Macro: 0.7032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0709, Accuracy: 0.9233, F1 Micro: 0.7721, F1 Macro: 0.7056\n",
      "Epoch 8/10, Train Loss: 0.0572, Accuracy: 0.9167, F1 Micro: 0.7654, F1 Macro: 0.6996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0471, Accuracy: 0.92, F1 Micro: 0.774, F1 Macro: 0.7149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9253, F1 Micro: 0.779, F1 Macro: 0.7166\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9253, F1 Micro: 0.779, F1 Macro: 0.7166\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.91      0.91       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.74      0.62      0.68       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.67      0.39      0.49        72\n",
      "    HS_Gender       0.63      0.57      0.60        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.68      0.56      0.61       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.76      0.68      0.72      5556\n",
      " weighted avg       0.79      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 300.84795570373535 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3295, Accuracy: 0.8998, F1 Micro: 0.699, F1 Macro: 0.4884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2164, Accuracy: 0.913, F1 Micro: 0.7374, F1 Macro: 0.556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1679, Accuracy: 0.9204, F1 Micro: 0.7642, F1 Macro: 0.6204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1393, Accuracy: 0.9247, F1 Micro: 0.7751, F1 Macro: 0.6787\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.9197, F1 Micro: 0.7699, F1 Macro: 0.6758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9253, F1 Micro: 0.7793, F1 Macro: 0.6998\n",
      "Epoch 7/10, Train Loss: 0.0727, Accuracy: 0.9247, F1 Micro: 0.7782, F1 Macro: 0.7079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9237, F1 Micro: 0.7806, F1 Macro: 0.7143\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9214, F1 Micro: 0.7702, F1 Macro: 0.7117\n",
      "Epoch 10/10, Train Loss: 0.0395, Accuracy: 0.9206, F1 Micro: 0.7742, F1 Macro: 0.705\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9237, F1 Micro: 0.7806, F1 Macro: 0.7143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.69      0.70      0.69       402\n",
      "  HS_Religion       0.68      0.66      0.67       157\n",
      "      HS_Race       0.73      0.77      0.75       120\n",
      "  HS_Physical       0.76      0.31      0.44        72\n",
      "    HS_Gender       0.66      0.49      0.56        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.60      0.63      0.62       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 299.5548343658447 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9191, F1 Micro: 0.7606, F1 Macro: 0.665\n",
      "Total sampling time: 1187.68 seconds\n",
      "Total runtime: 20095.81672525406 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906858ba",
   "metadata": {
    "papermill": {
     "duration": 0.181068,
     "end_time": "2025-03-15T13:24:23.383658",
     "exception": false,
     "start_time": "2025-03-15T13:24:23.202590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5334, Accuracy: 0.832, F1 Micro: 0.281, F1 Macro: 0.0625\n",
      "Epoch 2/10, Train Loss: 0.4141, Accuracy: 0.8325, F1 Micro: 0.0725, F1 Macro: 0.0287\n",
      "Epoch 3/10, Train Loss: 0.3891, Accuracy: 0.8395, F1 Micro: 0.166, F1 Macro: 0.0599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3683, Accuracy: 0.8596, F1 Micro: 0.3961, F1 Macro: 0.1571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3357, Accuracy: 0.874, F1 Micro: 0.5616, F1 Macro: 0.2616\n",
      "Epoch 6/10, Train Loss: 0.2863, Accuracy: 0.8772, F1 Micro: 0.5346, F1 Macro: 0.2474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2586, Accuracy: 0.8802, F1 Micro: 0.6089, F1 Macro: 0.3077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2319, Accuracy: 0.8818, F1 Micro: 0.6161, F1 Macro: 0.3284\n",
      "Epoch 9/10, Train Loss: 0.1943, Accuracy: 0.8809, F1 Micro: 0.591, F1 Macro: 0.3223\n",
      "Epoch 10/10, Train Loss: 0.1843, Accuracy: 0.8815, F1 Micro: 0.5782, F1 Macro: 0.3293\n",
      "Model 1 - Iteration 658: Accuracy: 0.8818, F1 Micro: 0.6161, F1 Macro: 0.3284\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.79      0.78      1134\n",
      "      Abusive       0.82      0.78      0.80       992\n",
      "HS_Individual       0.63      0.52      0.57       732\n",
      "     HS_Group       0.56      0.37      0.44       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.63      0.64       762\n",
      "      HS_Weak       0.58      0.51      0.54       689\n",
      "  HS_Moderate       0.43      0.10      0.16       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.55      0.62      5556\n",
      "    macro avg       0.37      0.31      0.33      5556\n",
      " weighted avg       0.62      0.55      0.57      5556\n",
      "  samples avg       0.39      0.32      0.32      5556\n",
      "\n",
      "Training completed in 53.7275288105011 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5488, Accuracy: 0.8365, F1 Micro: 0.2242, F1 Macro: 0.0759\n",
      "Epoch 2/10, Train Loss: 0.4157, Accuracy: 0.8332, F1 Micro: 0.0781, F1 Macro: 0.0299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3877, Accuracy: 0.8443, F1 Micro: 0.2305, F1 Macro: 0.0771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3652, Accuracy: 0.8601, F1 Micro: 0.3923, F1 Macro: 0.1362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3337, Accuracy: 0.8761, F1 Micro: 0.5521, F1 Macro: 0.2466\n",
      "Epoch 6/10, Train Loss: 0.2781, Accuracy: 0.8744, F1 Micro: 0.5094, F1 Macro: 0.2358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2583, Accuracy: 0.8786, F1 Micro: 0.5916, F1 Macro: 0.3072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2301, Accuracy: 0.8803, F1 Micro: 0.5931, F1 Macro: 0.3214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1923, Accuracy: 0.8787, F1 Micro: 0.6097, F1 Macro: 0.3466\n",
      "Epoch 10/10, Train Loss: 0.1816, Accuracy: 0.8808, F1 Micro: 0.5953, F1 Macro: 0.3417\n",
      "Model 2 - Iteration 658: Accuracy: 0.8787, F1 Micro: 0.6097, F1 Macro: 0.3466\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.77      0.77      1134\n",
      "      Abusive       0.81      0.79      0.80       992\n",
      "HS_Individual       0.64      0.51      0.57       732\n",
      "     HS_Group       0.55      0.41      0.47       402\n",
      "  HS_Religion       0.50      0.07      0.12       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.58      0.60       762\n",
      "      HS_Weak       0.62      0.47      0.53       689\n",
      "  HS_Moderate       0.38      0.24      0.30       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.68      0.55      0.61      5556\n",
      "    macro avg       0.41      0.32      0.35      5556\n",
      " weighted avg       0.62      0.55      0.58      5556\n",
      "  samples avg       0.39      0.32      0.32      5556\n",
      "\n",
      "Training completed in 57.10281944274902 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5584, Accuracy: 0.8301, F1 Micro: 0.0762, F1 Macro: 0.0308\n",
      "Epoch 2/10, Train Loss: 0.4164, Accuracy: 0.8297, F1 Micro: 0.0269, F1 Macro: 0.0117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3921, Accuracy: 0.8388, F1 Micro: 0.1615, F1 Macro: 0.0557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3787, Accuracy: 0.8458, F1 Micro: 0.2457, F1 Macro: 0.0818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.354, Accuracy: 0.8607, F1 Micro: 0.3895, F1 Macro: 0.1422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3012, Accuracy: 0.8716, F1 Micro: 0.4643, F1 Macro: 0.2092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2725, Accuracy: 0.8785, F1 Micro: 0.5728, F1 Macro: 0.2936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2457, Accuracy: 0.8807, F1 Micro: 0.5917, F1 Macro: 0.3243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1994, Accuracy: 0.8811, F1 Micro: 0.6141, F1 Macro: 0.3584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1899, Accuracy: 0.8813, F1 Micro: 0.6142, F1 Macro: 0.3692\n",
      "Model 3 - Iteration 658: Accuracy: 0.8813, F1 Micro: 0.6142, F1 Macro: 0.3692\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.75      0.77      1134\n",
      "      Abusive       0.83      0.78      0.80       992\n",
      "HS_Individual       0.67      0.45      0.53       732\n",
      "     HS_Group       0.57      0.49      0.52       402\n",
      "  HS_Religion       0.57      0.08      0.14       157\n",
      "      HS_Race       1.00      0.06      0.11       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.59      0.62       762\n",
      "      HS_Weak       0.62      0.42      0.50       689\n",
      "  HS_Moderate       0.41      0.42      0.42       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.55      0.61      5556\n",
      "    macro avg       0.51      0.34      0.37      5556\n",
      " weighted avg       0.67      0.55      0.59      5556\n",
      "  samples avg       0.37      0.31      0.32      5556\n",
      "\n",
      "Training completed in 60.753944635391235 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8806, F1 Micro: 0.6133, F1 Macro: 0.3481\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 127.89497303962708 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.459, Accuracy: 0.8388, F1 Micro: 0.1682, F1 Macro: 0.0611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3607, Accuracy: 0.8646, F1 Micro: 0.4295, F1 Macro: 0.192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3108, Accuracy: 0.884, F1 Micro: 0.6154, F1 Macro: 0.3002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2589, Accuracy: 0.8896, F1 Micro: 0.625, F1 Macro: 0.3455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.236, Accuracy: 0.8955, F1 Micro: 0.6645, F1 Macro: 0.4383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1941, Accuracy: 0.8948, F1 Micro: 0.6881, F1 Macro: 0.4581\n",
      "Epoch 7/10, Train Loss: 0.1638, Accuracy: 0.8956, F1 Micro: 0.6809, F1 Macro: 0.4596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1432, Accuracy: 0.899, F1 Micro: 0.6883, F1 Macro: 0.5027\n",
      "Epoch 9/10, Train Loss: 0.1172, Accuracy: 0.9, F1 Micro: 0.6817, F1 Macro: 0.4954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1023, Accuracy: 0.8984, F1 Micro: 0.7003, F1 Macro: 0.5322\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8984, F1 Micro: 0.7003, F1 Macro: 0.5322\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.83      0.81      1134\n",
      "      Abusive       0.84      0.86      0.85       992\n",
      "HS_Individual       0.64      0.68      0.66       732\n",
      "     HS_Group       0.62      0.53      0.57       402\n",
      "  HS_Religion       0.68      0.48      0.57       157\n",
      "      HS_Race       0.77      0.59      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.71      0.70       762\n",
      "      HS_Weak       0.61      0.65      0.63       689\n",
      "  HS_Moderate       0.51      0.44      0.47       331\n",
      "    HS_Strong       0.92      0.30      0.45       114\n",
      "\n",
      "    micro avg       0.71      0.69      0.70      5556\n",
      "    macro avg       0.59      0.51      0.53      5556\n",
      " weighted avg       0.70      0.69      0.69      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 83.42755150794983 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4609, Accuracy: 0.8455, F1 Micro: 0.2499, F1 Macro: 0.0857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3596, Accuracy: 0.8713, F1 Micro: 0.4861, F1 Macro: 0.213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3107, Accuracy: 0.8815, F1 Micro: 0.5891, F1 Macro: 0.2751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2615, Accuracy: 0.8893, F1 Micro: 0.6278, F1 Macro: 0.3354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2327, Accuracy: 0.8957, F1 Micro: 0.6644, F1 Macro: 0.469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1935, Accuracy: 0.8941, F1 Micro: 0.6906, F1 Macro: 0.4793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1674, Accuracy: 0.8956, F1 Micro: 0.6968, F1 Macro: 0.5075\n",
      "Epoch 8/10, Train Loss: 0.1478, Accuracy: 0.896, F1 Micro: 0.6776, F1 Macro: 0.497\n",
      "Epoch 9/10, Train Loss: 0.1228, Accuracy: 0.8992, F1 Micro: 0.6875, F1 Macro: 0.5117\n",
      "Epoch 10/10, Train Loss: 0.1064, Accuracy: 0.9014, F1 Micro: 0.6934, F1 Macro: 0.5241\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8956, F1 Micro: 0.6968, F1 Macro: 0.5075\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.86      0.80      1134\n",
      "      Abusive       0.86      0.82      0.84       992\n",
      "HS_Individual       0.63      0.72      0.67       732\n",
      "     HS_Group       0.63      0.53      0.58       402\n",
      "  HS_Religion       0.73      0.33      0.46       157\n",
      "      HS_Race       0.81      0.48      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.80      0.71       762\n",
      "      HS_Weak       0.60      0.67      0.63       689\n",
      "  HS_Moderate       0.53      0.44      0.48       331\n",
      "    HS_Strong       0.76      0.19      0.31       114\n",
      "\n",
      "    micro avg       0.70      0.70      0.70      5556\n",
      "    macro avg       0.58      0.49      0.51      5556\n",
      " weighted avg       0.69      0.70      0.68      5556\n",
      "  samples avg       0.39      0.38      0.37      5556\n",
      "\n",
      "Training completed in 81.8914840221405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4684, Accuracy: 0.8345, F1 Micro: 0.0974, F1 Macro: 0.0359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.372, Accuracy: 0.857, F1 Micro: 0.3705, F1 Macro: 0.131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3241, Accuracy: 0.881, F1 Micro: 0.5741, F1 Macro: 0.2672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2668, Accuracy: 0.8898, F1 Micro: 0.6312, F1 Macro: 0.3279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2427, Accuracy: 0.8943, F1 Micro: 0.6438, F1 Macro: 0.442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1976, Accuracy: 0.8948, F1 Micro: 0.6916, F1 Macro: 0.477\n",
      "Epoch 7/10, Train Loss: 0.1684, Accuracy: 0.9007, F1 Micro: 0.6862, F1 Macro: 0.4786\n",
      "Epoch 8/10, Train Loss: 0.1463, Accuracy: 0.8951, F1 Micro: 0.6874, F1 Macro: 0.5168\n",
      "Epoch 9/10, Train Loss: 0.1196, Accuracy: 0.896, F1 Micro: 0.6855, F1 Macro: 0.5069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1026, Accuracy: 0.9008, F1 Micro: 0.6963, F1 Macro: 0.5317\n",
      "Model 3 - Iteration 1646: Accuracy: 0.9008, F1 Micro: 0.6963, F1 Macro: 0.5317\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.80      0.81      1134\n",
      "      Abusive       0.88      0.83      0.85       992\n",
      "HS_Individual       0.67      0.62      0.64       732\n",
      "     HS_Group       0.63      0.52      0.57       402\n",
      "  HS_Religion       0.73      0.49      0.59       157\n",
      "      HS_Race       0.71      0.67      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.69      0.70       762\n",
      "      HS_Weak       0.64      0.60      0.62       689\n",
      "  HS_Moderate       0.52      0.43      0.47       331\n",
      "    HS_Strong       0.87      0.29      0.43       114\n",
      "\n",
      "    micro avg       0.74      0.66      0.70      5556\n",
      "    macro avg       0.60      0.50      0.53      5556\n",
      " weighted avg       0.72      0.66      0.68      5556\n",
      "  samples avg       0.40      0.37      0.37      5556\n",
      "\n",
      "Training completed in 81.88704133033752 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8894, F1 Micro: 0.6556, F1 Macro: 0.4359\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 115.44829392433167 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4325, Accuracy: 0.858, F1 Micro: 0.4051, F1 Macro: 0.1497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3259, Accuracy: 0.8799, F1 Micro: 0.5317, F1 Macro: 0.2599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2699, Accuracy: 0.8932, F1 Micro: 0.682, F1 Macro: 0.4475\n",
      "Epoch 4/10, Train Loss: 0.221, Accuracy: 0.9019, F1 Micro: 0.6801, F1 Macro: 0.4967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1835, Accuracy: 0.9048, F1 Micro: 0.7068, F1 Macro: 0.483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1515, Accuracy: 0.907, F1 Micro: 0.7162, F1 Macro: 0.5044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1348, Accuracy: 0.9084, F1 Micro: 0.7191, F1 Macro: 0.5564\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.901, F1 Micro: 0.7157, F1 Macro: 0.5509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0896, Accuracy: 0.908, F1 Micro: 0.7234, F1 Macro: 0.5614\n",
      "Epoch 10/10, Train Loss: 0.0717, Accuracy: 0.9078, F1 Micro: 0.7189, F1 Macro: 0.5687\n",
      "Model 1 - Iteration 2535: Accuracy: 0.908, F1 Micro: 0.7234, F1 Macro: 0.5614\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.81      0.82      1134\n",
      "      Abusive       0.85      0.86      0.86       992\n",
      "HS_Individual       0.68      0.72      0.70       732\n",
      "     HS_Group       0.71      0.51      0.59       402\n",
      "  HS_Religion       0.80      0.48      0.60       157\n",
      "      HS_Race       0.84      0.51      0.63       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.74      0.72       762\n",
      "      HS_Weak       0.65      0.70      0.67       689\n",
      "  HS_Moderate       0.59      0.41      0.49       331\n",
      "    HS_Strong       0.92      0.50      0.65       114\n",
      "\n",
      "    micro avg       0.75      0.70      0.72      5556\n",
      "    macro avg       0.63      0.52      0.56      5556\n",
      " weighted avg       0.74      0.70      0.71      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 103.4673445224762 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4364, Accuracy: 0.8564, F1 Micro: 0.4094, F1 Macro: 0.1237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.324, Accuracy: 0.8796, F1 Micro: 0.5332, F1 Macro: 0.2549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2706, Accuracy: 0.8915, F1 Micro: 0.6742, F1 Macro: 0.4363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2252, Accuracy: 0.9005, F1 Micro: 0.6797, F1 Macro: 0.4918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9024, F1 Micro: 0.7087, F1 Macro: 0.5099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1541, Accuracy: 0.9015, F1 Micro: 0.7173, F1 Macro: 0.5324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1336, Accuracy: 0.9069, F1 Micro: 0.7245, F1 Macro: 0.5558\n",
      "Epoch 8/10, Train Loss: 0.1049, Accuracy: 0.9069, F1 Micro: 0.7192, F1 Macro: 0.5607\n",
      "Epoch 9/10, Train Loss: 0.0918, Accuracy: 0.9078, F1 Micro: 0.7013, F1 Macro: 0.5526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.078, Accuracy: 0.9075, F1 Micro: 0.726, F1 Macro: 0.5759\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9075, F1 Micro: 0.726, F1 Macro: 0.5759\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.87      0.85      0.86       992\n",
      "HS_Individual       0.69      0.68      0.69       732\n",
      "     HS_Group       0.64      0.63      0.63       402\n",
      "  HS_Religion       0.67      0.58      0.62       157\n",
      "      HS_Race       0.74      0.62      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.72      0.73      0.73       762\n",
      "      HS_Weak       0.67      0.65      0.66       689\n",
      "  HS_Moderate       0.54      0.54      0.54       331\n",
      "    HS_Strong       0.84      0.58      0.68       114\n",
      "\n",
      "    micro avg       0.74      0.71      0.73      5556\n",
      "    macro avg       0.60      0.56      0.58      5556\n",
      " weighted avg       0.72      0.71      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 104.78343105316162 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4433, Accuracy: 0.8502, F1 Micro: 0.3171, F1 Macro: 0.1027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.337, Accuracy: 0.8806, F1 Micro: 0.5514, F1 Macro: 0.266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2755, Accuracy: 0.8948, F1 Micro: 0.6828, F1 Macro: 0.4712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.226, Accuracy: 0.9019, F1 Micro: 0.6911, F1 Macro: 0.5188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1872, Accuracy: 0.9049, F1 Micro: 0.7082, F1 Macro: 0.4925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1512, Accuracy: 0.9052, F1 Micro: 0.7219, F1 Macro: 0.5236\n",
      "Epoch 7/10, Train Loss: 0.129, Accuracy: 0.9055, F1 Micro: 0.7031, F1 Macro: 0.5469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0984, Accuracy: 0.9069, F1 Micro: 0.7253, F1 Macro: 0.5659\n",
      "Epoch 9/10, Train Loss: 0.0833, Accuracy: 0.9098, F1 Micro: 0.7193, F1 Macro: 0.5646\n",
      "Epoch 10/10, Train Loss: 0.0708, Accuracy: 0.9098, F1 Micro: 0.7207, F1 Macro: 0.5768\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9069, F1 Micro: 0.7253, F1 Macro: 0.5659\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.83      1134\n",
      "      Abusive       0.85      0.88      0.86       992\n",
      "HS_Individual       0.73      0.62      0.67       732\n",
      "     HS_Group       0.60      0.66      0.63       402\n",
      "  HS_Religion       0.74      0.53      0.62       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.77      0.74       762\n",
      "      HS_Weak       0.70      0.61      0.65       689\n",
      "  HS_Moderate       0.49      0.60      0.54       331\n",
      "    HS_Strong       0.92      0.39      0.55       114\n",
      "\n",
      "    micro avg       0.74      0.71      0.73      5556\n",
      "    macro avg       0.61      0.55      0.57      5556\n",
      " weighted avg       0.73      0.71      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 103.36589860916138 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8954, F1 Micro: 0.6787, F1 Macro: 0.4799\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 104.08650517463684 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4183, Accuracy: 0.8701, F1 Micro: 0.5761, F1 Macro: 0.2622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3013, Accuracy: 0.8923, F1 Micro: 0.662, F1 Macro: 0.4305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.9013, F1 Micro: 0.6696, F1 Macro: 0.4607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2034, Accuracy: 0.9061, F1 Micro: 0.7238, F1 Macro: 0.5522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1626, Accuracy: 0.9102, F1 Micro: 0.7246, F1 Macro: 0.5622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1298, Accuracy: 0.9091, F1 Micro: 0.737, F1 Macro: 0.5767\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9076, F1 Micro: 0.7284, F1 Macro: 0.5786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9108, F1 Micro: 0.7373, F1 Macro: 0.5894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9137, F1 Micro: 0.7428, F1 Macro: 0.6072\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9139, F1 Micro: 0.7361, F1 Macro: 0.5972\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9137, F1 Micro: 0.7428, F1 Macro: 0.6072\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.83      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.72      0.68      0.70       732\n",
      "     HS_Group       0.66      0.64      0.65       402\n",
      "  HS_Religion       0.75      0.54      0.63       157\n",
      "      HS_Race       0.71      0.71      0.71       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.74      0.76      0.75       762\n",
      "      HS_Weak       0.69      0.65      0.67       689\n",
      "  HS_Moderate       0.58      0.55      0.57       331\n",
      "    HS_Strong       0.92      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.68      0.58      0.61      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 125.92466902732849 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4205, Accuracy: 0.8704, F1 Micro: 0.5304, F1 Macro: 0.2171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3, Accuracy: 0.8908, F1 Micro: 0.6518, F1 Macro: 0.3965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2396, Accuracy: 0.9019, F1 Micro: 0.6994, F1 Macro: 0.4818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2058, Accuracy: 0.9046, F1 Micro: 0.7189, F1 Macro: 0.5347\n",
      "Epoch 5/10, Train Loss: 0.1644, Accuracy: 0.9095, F1 Micro: 0.717, F1 Macro: 0.5489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1319, Accuracy: 0.9084, F1 Micro: 0.724, F1 Macro: 0.5737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9056, F1 Micro: 0.7346, F1 Macro: 0.5638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9103, F1 Micro: 0.7397, F1 Macro: 0.594\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9116, F1 Micro: 0.7362, F1 Macro: 0.6086\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9121, F1 Micro: 0.7332, F1 Macro: 0.6007\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9103, F1 Micro: 0.7397, F1 Macro: 0.594\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.86      0.83      1134\n",
      "      Abusive       0.88      0.85      0.87       992\n",
      "HS_Individual       0.67      0.75      0.70       732\n",
      "     HS_Group       0.69      0.61      0.65       402\n",
      "  HS_Religion       0.72      0.57      0.64       157\n",
      "      HS_Race       0.80      0.58      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.70      0.80      0.75       762\n",
      "      HS_Weak       0.64      0.71      0.67       689\n",
      "  HS_Moderate       0.60      0.51      0.55       331\n",
      "    HS_Strong       0.88      0.55      0.68       114\n",
      "\n",
      "    micro avg       0.74      0.74      0.74      5556\n",
      "    macro avg       0.70      0.57      0.59      5556\n",
      " weighted avg       0.74      0.74      0.73      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 124.0880811214447 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4256, Accuracy: 0.8603, F1 Micro: 0.4321, F1 Macro: 0.1652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3068, Accuracy: 0.8916, F1 Micro: 0.6542, F1 Macro: 0.4268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2407, Accuracy: 0.9032, F1 Micro: 0.6907, F1 Macro: 0.4892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.9034, F1 Micro: 0.7249, F1 Macro: 0.5487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.162, Accuracy: 0.9105, F1 Micro: 0.7341, F1 Macro: 0.5738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1288, Accuracy: 0.9087, F1 Micro: 0.7374, F1 Macro: 0.5822\n",
      "Epoch 7/10, Train Loss: 0.1062, Accuracy: 0.9093, F1 Micro: 0.7366, F1 Macro: 0.5929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9114, F1 Micro: 0.7429, F1 Macro: 0.5972\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9078, F1 Micro: 0.7374, F1 Macro: 0.6072\n",
      "Epoch 10/10, Train Loss: 0.0644, Accuracy: 0.915, F1 Micro: 0.7397, F1 Macro: 0.6149\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9114, F1 Micro: 0.7429, F1 Macro: 0.5972\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.86      0.83      1134\n",
      "      Abusive       0.89      0.86      0.87       992\n",
      "HS_Individual       0.68      0.74      0.71       732\n",
      "     HS_Group       0.68      0.59      0.63       402\n",
      "  HS_Religion       0.72      0.60      0.65       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.70      0.80      0.75       762\n",
      "      HS_Weak       0.65      0.72      0.69       689\n",
      "  HS_Moderate       0.58      0.50      0.54       331\n",
      "    HS_Strong       0.87      0.67      0.76       114\n",
      "\n",
      "    micro avg       0.74      0.74      0.74      5556\n",
      "    macro avg       0.65      0.59      0.60      5556\n",
      " weighted avg       0.73      0.74      0.73      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 124.43286442756653 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8995, F1 Micro: 0.6945, F1 Macro: 0.5098\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 93.60043215751648 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.407, Accuracy: 0.8792, F1 Micro: 0.5783, F1 Macro: 0.2704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.8925, F1 Micro: 0.6393, F1 Macro: 0.3732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2304, Accuracy: 0.906, F1 Micro: 0.7231, F1 Macro: 0.525\n",
      "Epoch 4/10, Train Loss: 0.1854, Accuracy: 0.9092, F1 Micro: 0.7113, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1542, Accuracy: 0.9132, F1 Micro: 0.7362, F1 Macro: 0.5701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9158, F1 Micro: 0.7435, F1 Macro: 0.5939\n",
      "Epoch 7/10, Train Loss: 0.1019, Accuracy: 0.914, F1 Micro: 0.7409, F1 Macro: 0.5875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9163, F1 Micro: 0.747, F1 Macro: 0.6121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0701, Accuracy: 0.9117, F1 Micro: 0.7494, F1 Macro: 0.6179\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9142, F1 Micro: 0.7474, F1 Macro: 0.6142\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9117, F1 Micro: 0.7494, F1 Macro: 0.6179\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.69      0.74      0.71       732\n",
      "     HS_Group       0.64      0.66      0.65       402\n",
      "  HS_Religion       0.63      0.65      0.64       157\n",
      "      HS_Race       0.66      0.69      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.70      0.14      0.23        51\n",
      "     HS_Other       0.71      0.80      0.75       762\n",
      "      HS_Weak       0.66      0.72      0.69       689\n",
      "  HS_Moderate       0.56      0.57      0.57       331\n",
      "    HS_Strong       0.90      0.69      0.78       114\n",
      "\n",
      "    micro avg       0.73      0.77      0.75      5556\n",
      "    macro avg       0.65      0.62      0.62      5556\n",
      " weighted avg       0.72      0.77      0.74      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 141.38590788841248 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4072, Accuracy: 0.8747, F1 Micro: 0.6049, F1 Macro: 0.2805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.8943, F1 Micro: 0.6542, F1 Macro: 0.3899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2298, Accuracy: 0.9067, F1 Micro: 0.7197, F1 Macro: 0.5277\n",
      "Epoch 4/10, Train Loss: 0.1875, Accuracy: 0.9097, F1 Micro: 0.7152, F1 Macro: 0.5445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1565, Accuracy: 0.9137, F1 Micro: 0.7401, F1 Macro: 0.5783\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.913, F1 Micro: 0.7384, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.9098, F1 Micro: 0.7455, F1 Macro: 0.5872\n",
      "Epoch 8/10, Train Loss: 0.0824, Accuracy: 0.9171, F1 Micro: 0.744, F1 Macro: 0.6059\n",
      "Epoch 9/10, Train Loss: 0.0687, Accuracy: 0.9157, F1 Micro: 0.7424, F1 Macro: 0.6036\n",
      "Epoch 10/10, Train Loss: 0.06, Accuracy: 0.9137, F1 Micro: 0.7341, F1 Macro: 0.6011\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9098, F1 Micro: 0.7455, F1 Macro: 0.5872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.89      0.84      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.67      0.77      0.72       732\n",
      "     HS_Group       0.64      0.66      0.65       402\n",
      "  HS_Religion       0.66      0.57      0.61       157\n",
      "      HS_Race       0.70      0.65      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.50      0.02      0.04        51\n",
      "     HS_Other       0.70      0.80      0.75       762\n",
      "      HS_Weak       0.64      0.73      0.68       689\n",
      "  HS_Moderate       0.55      0.59      0.57       331\n",
      "    HS_Strong       0.90      0.49      0.64       114\n",
      "\n",
      "    micro avg       0.73      0.77      0.75      5556\n",
      "    macro avg       0.64      0.59      0.59      5556\n",
      " weighted avg       0.72      0.77      0.74      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 138.54971551895142 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4154, Accuracy: 0.8751, F1 Micro: 0.5227, F1 Macro: 0.2378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2873, Accuracy: 0.8955, F1 Micro: 0.6493, F1 Macro: 0.3935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2303, Accuracy: 0.907, F1 Micro: 0.719, F1 Macro: 0.5331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1844, Accuracy: 0.9118, F1 Micro: 0.7258, F1 Macro: 0.549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1524, Accuracy: 0.913, F1 Micro: 0.7387, F1 Macro: 0.5818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9153, F1 Micro: 0.7423, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9113, F1 Micro: 0.7454, F1 Macro: 0.597\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9164, F1 Micro: 0.7448, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0678, Accuracy: 0.9128, F1 Micro: 0.7507, F1 Macro: 0.6144\n",
      "Epoch 10/10, Train Loss: 0.0551, Accuracy: 0.9148, F1 Micro: 0.7489, F1 Macro: 0.6199\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9128, F1 Micro: 0.7507, F1 Macro: 0.6144\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.87      0.88      0.88       992\n",
      "HS_Individual       0.67      0.76      0.71       732\n",
      "     HS_Group       0.69      0.60      0.64       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.67      0.75      0.71       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.60      0.06      0.11        51\n",
      "     HS_Other       0.72      0.81      0.76       762\n",
      "      HS_Weak       0.64      0.74      0.69       689\n",
      "  HS_Moderate       0.58      0.51      0.55       331\n",
      "    HS_Strong       0.91      0.71      0.80       114\n",
      "\n",
      "    micro avg       0.74      0.76      0.75      5556\n",
      "    macro avg       0.70      0.61      0.61      5556\n",
      " weighted avg       0.74      0.76      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 143.3416407108307 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9019, F1 Micro: 0.7053, F1 Macro: 0.5291\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 84.88375568389893 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.8828, F1 Micro: 0.6065, F1 Macro: 0.2945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2765, Accuracy: 0.9008, F1 Micro: 0.6874, F1 Macro: 0.4305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.225, Accuracy: 0.9072, F1 Micro: 0.7115, F1 Macro: 0.5385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1824, Accuracy: 0.9135, F1 Micro: 0.739, F1 Macro: 0.5596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.9142, F1 Micro: 0.7475, F1 Macro: 0.5906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9171, F1 Micro: 0.7489, F1 Macro: 0.606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0958, Accuracy: 0.914, F1 Micro: 0.7549, F1 Macro: 0.6172\n",
      "Epoch 8/10, Train Loss: 0.0785, Accuracy: 0.9132, F1 Micro: 0.7491, F1 Macro: 0.6162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.9145, F1 Micro: 0.7551, F1 Macro: 0.6362\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.9134, F1 Micro: 0.7525, F1 Macro: 0.6274\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9145, F1 Micro: 0.7551, F1 Macro: 0.6362\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.87      0.83      1134\n",
      "      Abusive       0.90      0.86      0.88       992\n",
      "HS_Individual       0.69      0.75      0.72       732\n",
      "     HS_Group       0.66      0.65      0.66       402\n",
      "  HS_Religion       0.65      0.65      0.65       157\n",
      "      HS_Race       0.66      0.68      0.67       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.73      0.80      0.76       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.59      0.57      0.58       331\n",
      "    HS_Strong       0.86      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.74      0.77      0.76      5556\n",
      "    macro avg       0.71      0.63      0.64      5556\n",
      " weighted avg       0.75      0.77      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 158.5930151939392 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3899, Accuracy: 0.8804, F1 Micro: 0.6014, F1 Macro: 0.2896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.8996, F1 Micro: 0.688, F1 Macro: 0.435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.225, Accuracy: 0.9072, F1 Micro: 0.7122, F1 Macro: 0.5524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9143, F1 Micro: 0.7393, F1 Macro: 0.5634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1499, Accuracy: 0.9146, F1 Micro: 0.7408, F1 Macro: 0.589\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.9158, F1 Micro: 0.7393, F1 Macro: 0.5998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.915, F1 Micro: 0.7541, F1 Macro: 0.6211\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.9107, F1 Micro: 0.7443, F1 Macro: 0.6279\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9141, F1 Micro: 0.753, F1 Macro: 0.6362\n",
      "Epoch 10/10, Train Loss: 0.0553, Accuracy: 0.9178, F1 Micro: 0.7523, F1 Macro: 0.6309\n",
      "Model 2 - Iteration 4703: Accuracy: 0.915, F1 Micro: 0.7541, F1 Macro: 0.6211\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.84      0.91      0.88       992\n",
      "HS_Individual       0.69      0.74      0.71       732\n",
      "     HS_Group       0.70      0.63      0.66       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.72      0.69      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.62      0.10      0.17        51\n",
      "     HS_Other       0.75      0.78      0.77       762\n",
      "      HS_Weak       0.66      0.71      0.69       689\n",
      "  HS_Moderate       0.61      0.55      0.57       331\n",
      "    HS_Strong       0.86      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.75      0.76      0.75      5556\n",
      "    macro avg       0.66      0.61      0.62      5556\n",
      " weighted avg       0.74      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 156.26198554039001 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4008, Accuracy: 0.8795, F1 Micro: 0.5915, F1 Macro: 0.2754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.9007, F1 Micro: 0.6852, F1 Macro: 0.4355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2259, Accuracy: 0.909, F1 Micro: 0.7168, F1 Macro: 0.5507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.9144, F1 Micro: 0.7396, F1 Macro: 0.5759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1485, Accuracy: 0.9136, F1 Micro: 0.7472, F1 Macro: 0.6041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9174, F1 Micro: 0.7508, F1 Macro: 0.6156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.9183, F1 Micro: 0.7557, F1 Macro: 0.6181\n",
      "Epoch 8/10, Train Loss: 0.0744, Accuracy: 0.9151, F1 Micro: 0.7518, F1 Macro: 0.6378\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9143, F1 Micro: 0.7539, F1 Macro: 0.6465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9164, F1 Micro: 0.7562, F1 Macro: 0.6443\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9164, F1 Micro: 0.7562, F1 Macro: 0.6443\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.66      0.63      0.65       402\n",
      "  HS_Religion       0.71      0.60      0.65       157\n",
      "      HS_Race       0.70      0.67      0.68       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.76      0.77      0.76       762\n",
      "      HS_Weak       0.69      0.70      0.70       689\n",
      "  HS_Moderate       0.57      0.56      0.56       331\n",
      "    HS_Strong       0.89      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.75      0.76      5556\n",
      "    macro avg       0.75      0.62      0.64      5556\n",
      " weighted avg       0.76      0.75      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 158.63271498680115 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9041, F1 Micro: 0.7136, F1 Macro: 0.5466\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 76.36960887908936 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3836, Accuracy: 0.8869, F1 Micro: 0.6235, F1 Macro: 0.31\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2658, Accuracy: 0.905, F1 Micro: 0.6952, F1 Macro: 0.4883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2139, Accuracy: 0.913, F1 Micro: 0.7345, F1 Macro: 0.5594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1753, Accuracy: 0.9161, F1 Micro: 0.7463, F1 Macro: 0.5938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1404, Accuracy: 0.9109, F1 Micro: 0.7497, F1 Macro: 0.5984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1155, Accuracy: 0.9179, F1 Micro: 0.7545, F1 Macro: 0.609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9183, F1 Micro: 0.7553, F1 Macro: 0.6229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.92, F1 Micro: 0.7574, F1 Macro: 0.6255\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9185, F1 Micro: 0.7538, F1 Macro: 0.6384\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9173, F1 Micro: 0.7507, F1 Macro: 0.6374\n",
      "Model 1 - Iteration 5287: Accuracy: 0.92, F1 Micro: 0.7574, F1 Macro: 0.6255\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.84      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.77      0.54      0.64       402\n",
      "  HS_Religion       0.85      0.44      0.58       157\n",
      "      HS_Race       0.87      0.50      0.63       120\n",
      "  HS_Physical       1.00      0.08      0.15        72\n",
      "    HS_Gender       0.58      0.14      0.22        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.70      0.45      0.55       331\n",
      "    HS_Strong       0.93      0.72      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.72      0.76      5556\n",
      "    macro avg       0.80      0.57      0.63      5556\n",
      " weighted avg       0.80      0.72      0.75      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 173.44203448295593 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3836, Accuracy: 0.8832, F1 Micro: 0.6124, F1 Macro: 0.2981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2667, Accuracy: 0.9057, F1 Micro: 0.6968, F1 Macro: 0.499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2169, Accuracy: 0.9114, F1 Micro: 0.7314, F1 Macro: 0.552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1801, Accuracy: 0.9155, F1 Micro: 0.7434, F1 Macro: 0.5933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9132, F1 Micro: 0.7528, F1 Macro: 0.609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9162, F1 Micro: 0.7546, F1 Macro: 0.6142\n",
      "Epoch 7/10, Train Loss: 0.099, Accuracy: 0.915, F1 Micro: 0.7433, F1 Macro: 0.6346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0737, Accuracy: 0.9172, F1 Micro: 0.7585, F1 Macro: 0.6394\n",
      "Epoch 9/10, Train Loss: 0.0678, Accuracy: 0.9156, F1 Micro: 0.7538, F1 Macro: 0.6421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9195, F1 Micro: 0.7615, F1 Macro: 0.6436\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9195, F1 Micro: 0.7615, F1 Macro: 0.6436\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.75      0.58      0.65       402\n",
      "  HS_Religion       0.77      0.55      0.64       157\n",
      "      HS_Race       0.76      0.62      0.68       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.71      0.20      0.31        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.68      0.71      0.70       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.77      0.61      0.64      5556\n",
      " weighted avg       0.78      0.75      0.75      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 173.56687927246094 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3924, Accuracy: 0.8819, F1 Micro: 0.6094, F1 Macro: 0.3092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2687, Accuracy: 0.9053, F1 Micro: 0.6926, F1 Macro: 0.4753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2158, Accuracy: 0.9116, F1 Micro: 0.7284, F1 Macro: 0.5519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9169, F1 Micro: 0.7458, F1 Macro: 0.5955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9164, F1 Micro: 0.7578, F1 Macro: 0.6136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1149, Accuracy: 0.9173, F1 Micro: 0.7584, F1 Macro: 0.6225\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9174, F1 Micro: 0.7573, F1 Macro: 0.6295\n",
      "Epoch 8/10, Train Loss: 0.0734, Accuracy: 0.9182, F1 Micro: 0.7487, F1 Macro: 0.6299\n",
      "Epoch 9/10, Train Loss: 0.0632, Accuracy: 0.9183, F1 Micro: 0.7551, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.9142, F1 Micro: 0.7588, F1 Macro: 0.6552\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9142, F1 Micro: 0.7588, F1 Macro: 0.6552\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.88      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.65      0.64      0.65       402\n",
      "  HS_Religion       0.65      0.66      0.65       157\n",
      "      HS_Race       0.71      0.68      0.69       120\n",
      "  HS_Physical       0.58      0.10      0.17        72\n",
      "    HS_Gender       0.55      0.31      0.40        51\n",
      "     HS_Other       0.72      0.83      0.77       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.56      0.56      0.56       331\n",
      "    HS_Strong       0.85      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5556\n",
      "    macro avg       0.69      0.66      0.66      5556\n",
      " weighted avg       0.73      0.78      0.75      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 171.8349814414978 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9061, F1 Micro: 0.7201, F1 Macro: 0.5601\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 69.47131252288818 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3726, Accuracy: 0.8885, F1 Micro: 0.6285, F1 Macro: 0.3273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2592, Accuracy: 0.9018, F1 Micro: 0.7131, F1 Macro: 0.4926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.912, F1 Micro: 0.7299, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.168, Accuracy: 0.9156, F1 Micro: 0.7366, F1 Macro: 0.568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9208, F1 Micro: 0.764, F1 Macro: 0.6238\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9173, F1 Micro: 0.7554, F1 Macro: 0.6219\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9178, F1 Micro: 0.7586, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0708, Accuracy: 0.9204, F1 Micro: 0.7647, F1 Macro: 0.6462\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9178, F1 Micro: 0.7588, F1 Macro: 0.6559\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9191, F1 Micro: 0.7611, F1 Macro: 0.6585\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9204, F1 Micro: 0.7647, F1 Macro: 0.6462\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.75      0.65      0.70       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.89      0.16      0.27        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.69      0.70      0.70       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.76      5556\n",
      "    macro avg       0.79      0.62      0.65      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 182.6325101852417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3722, Accuracy: 0.8867, F1 Micro: 0.6425, F1 Macro: 0.3361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2606, Accuracy: 0.8965, F1 Micro: 0.7066, F1 Macro: 0.4991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.211, Accuracy: 0.9099, F1 Micro: 0.7232, F1 Macro: 0.5295\n",
      "Epoch 4/10, Train Loss: 0.1718, Accuracy: 0.914, F1 Micro: 0.7232, F1 Macro: 0.5657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1378, Accuracy: 0.9183, F1 Micro: 0.7566, F1 Macro: 0.6009\n",
      "Epoch 6/10, Train Loss: 0.1057, Accuracy: 0.916, F1 Micro: 0.7562, F1 Macro: 0.6285\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9192, F1 Micro: 0.755, F1 Macro: 0.6556\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9191, F1 Micro: 0.7552, F1 Macro: 0.6401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9164, F1 Micro: 0.7582, F1 Macro: 0.6543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9184, F1 Micro: 0.7595, F1 Macro: 0.6591\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9184, F1 Micro: 0.7595, F1 Macro: 0.6591\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.76      0.54      0.63       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.76      0.63      0.69       120\n",
      "  HS_Physical       0.50      0.10      0.16        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.66      0.74      0.70       689\n",
      "  HS_Moderate       0.69      0.44      0.54       331\n",
      "    HS_Strong       0.89      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.73      0.63      0.66      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 183.88709378242493 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3808, Accuracy: 0.8867, F1 Micro: 0.6217, F1 Macro: 0.3523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.262, Accuracy: 0.9033, F1 Micro: 0.7146, F1 Macro: 0.5053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2098, Accuracy: 0.9121, F1 Micro: 0.7315, F1 Macro: 0.5461\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9147, F1 Micro: 0.727, F1 Macro: 0.5716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9199, F1 Micro: 0.7469, F1 Macro: 0.6052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.9137, F1 Micro: 0.7571, F1 Macro: 0.6238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0858, Accuracy: 0.9188, F1 Micro: 0.7642, F1 Macro: 0.6573\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9151, F1 Micro: 0.7595, F1 Macro: 0.6487\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9181, F1 Micro: 0.7626, F1 Macro: 0.6745\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9176, F1 Micro: 0.759, F1 Macro: 0.6596\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9188, F1 Micro: 0.7642, F1 Macro: 0.6573\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.77      0.59      0.66       157\n",
      "      HS_Race       0.83      0.58      0.69       120\n",
      "  HS_Physical       0.54      0.10      0.16        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.65      0.51      0.57       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.74      0.63      0.66      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 182.91657996177673 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9077, F1 Micro: 0.7254, F1 Macro: 0.5719\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 62.54543447494507 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3648, Accuracy: 0.8841, F1 Micro: 0.5597, F1 Macro: 0.3029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2531, Accuracy: 0.908, F1 Micro: 0.7143, F1 Macro: 0.5221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2049, Accuracy: 0.9153, F1 Micro: 0.7497, F1 Macro: 0.5948\n",
      "Epoch 4/10, Train Loss: 0.1647, Accuracy: 0.9165, F1 Micro: 0.748, F1 Macro: 0.5847\n",
      "Epoch 5/10, Train Loss: 0.1296, Accuracy: 0.919, F1 Micro: 0.7429, F1 Macro: 0.6049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9195, F1 Micro: 0.7634, F1 Macro: 0.6435\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9192, F1 Micro: 0.7518, F1 Macro: 0.6402\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9168, F1 Micro: 0.7552, F1 Macro: 0.6631\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9176, F1 Micro: 0.7605, F1 Macro: 0.6494\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9119, F1 Micro: 0.7495, F1 Macro: 0.6509\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9195, F1 Micro: 0.7634, F1 Macro: 0.6435\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.65       157\n",
      "      HS_Race       0.77      0.60      0.68       120\n",
      "  HS_Physical       0.50      0.01      0.03        72\n",
      "    HS_Gender       0.73      0.22      0.33        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.71      0.68      0.70       689\n",
      "  HS_Moderate       0.60      0.59      0.60       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.73      0.62      0.64      5556\n",
      " weighted avg       0.77      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 192.33866429328918 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3637, Accuracy: 0.8826, F1 Micro: 0.5442, F1 Macro: 0.2932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2561, Accuracy: 0.9079, F1 Micro: 0.7194, F1 Macro: 0.5421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2074, Accuracy: 0.9137, F1 Micro: 0.7428, F1 Macro: 0.5821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9172, F1 Micro: 0.7538, F1 Macro: 0.5942\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9194, F1 Micro: 0.7482, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9163, F1 Micro: 0.7578, F1 Macro: 0.6389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0874, Accuracy: 0.9172, F1 Micro: 0.7595, F1 Macro: 0.656\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.909, F1 Micro: 0.7532, F1 Macro: 0.6588\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9182, F1 Micro: 0.7581, F1 Macro: 0.6535\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9177, F1 Micro: 0.7578, F1 Macro: 0.6483\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9172, F1 Micro: 0.7595, F1 Macro: 0.656\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.73      0.57      0.64       157\n",
      "      HS_Race       0.68      0.63      0.66       120\n",
      "  HS_Physical       0.50      0.08      0.14        72\n",
      "    HS_Gender       0.57      0.39      0.47        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.58      0.56      0.57       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.71      0.64      0.66      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 196.56724786758423 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.374, Accuracy: 0.8842, F1 Micro: 0.5696, F1 Macro: 0.3451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2575, Accuracy: 0.9085, F1 Micro: 0.7165, F1 Macro: 0.5314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9094, F1 Micro: 0.7476, F1 Macro: 0.5922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1635, Accuracy: 0.9176, F1 Micro: 0.7588, F1 Macro: 0.6085\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9199, F1 Micro: 0.7498, F1 Macro: 0.6231\n",
      "Epoch 6/10, Train Loss: 0.1017, Accuracy: 0.9208, F1 Micro: 0.7577, F1 Macro: 0.6518\n",
      "Epoch 7/10, Train Loss: 0.0848, Accuracy: 0.9204, F1 Micro: 0.7582, F1 Macro: 0.6599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9192, F1 Micro: 0.7622, F1 Macro: 0.6644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9185, F1 Micro: 0.7624, F1 Macro: 0.6596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9187, F1 Micro: 0.7636, F1 Macro: 0.6721\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9187, F1 Micro: 0.7636, F1 Macro: 0.6721\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.69      0.71       732\n",
      "     HS_Group       0.66      0.67      0.66       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.63      0.33      0.44        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.68      0.69       689\n",
      "  HS_Moderate       0.56      0.61      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.44811534881592 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9089, F1 Micro: 0.7295, F1 Macro: 0.5814\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 56.81345272064209 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3696, Accuracy: 0.8907, F1 Micro: 0.6625, F1 Macro: 0.3607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2507, Accuracy: 0.9054, F1 Micro: 0.7127, F1 Macro: 0.4998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.916, F1 Micro: 0.7399, F1 Macro: 0.5554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1612, Accuracy: 0.919, F1 Micro: 0.7524, F1 Macro: 0.6123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9191, F1 Micro: 0.755, F1 Macro: 0.6191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.9166, F1 Micro: 0.7576, F1 Macro: 0.6479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.9188, F1 Micro: 0.7579, F1 Macro: 0.6396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.9173, F1 Micro: 0.7642, F1 Macro: 0.6663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9178, F1 Micro: 0.7662, F1 Macro: 0.6676\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9205, F1 Micro: 0.7654, F1 Macro: 0.6852\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9178, F1 Micro: 0.7662, F1 Macro: 0.6676\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.67      0.63      0.65       402\n",
      "  HS_Religion       0.66      0.63      0.65       157\n",
      "      HS_Race       0.72      0.65      0.68       120\n",
      "  HS_Physical       0.80      0.11      0.20        72\n",
      "    HS_Gender       0.54      0.39      0.45        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.67      0.75      0.71       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.92      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.77      5556\n",
      "    macro avg       0.72      0.66      0.67      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 207.3582808971405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3693, Accuracy: 0.8894, F1 Micro: 0.6664, F1 Macro: 0.3742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.906, F1 Micro: 0.7208, F1 Macro: 0.5331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2055, Accuracy: 0.913, F1 Micro: 0.7417, F1 Macro: 0.5611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1621, Accuracy: 0.9177, F1 Micro: 0.751, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.9117, F1 Micro: 0.7542, F1 Macro: 0.6373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9174, F1 Micro: 0.759, F1 Macro: 0.6599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9168, F1 Micro: 0.759, F1 Macro: 0.6418\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9195, F1 Micro: 0.7541, F1 Macro: 0.6564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.917, F1 Micro: 0.7642, F1 Macro: 0.6729\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9169, F1 Micro: 0.7618, F1 Macro: 0.6724\n",
      "Model 2 - Iteration 6584: Accuracy: 0.917, F1 Micro: 0.7642, F1 Macro: 0.6729\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.71      0.59      0.65       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.68      0.64      0.66       120\n",
      "  HS_Physical       0.45      0.14      0.21        72\n",
      "    HS_Gender       0.54      0.53      0.53        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.65      0.77      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.78      0.76      5556\n",
      "    macro avg       0.70      0.67      0.67      5556\n",
      " weighted avg       0.75      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 205.443097114563 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3803, Accuracy: 0.8877, F1 Micro: 0.6599, F1 Macro: 0.3764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2525, Accuracy: 0.9078, F1 Micro: 0.7223, F1 Macro: 0.53\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.202, Accuracy: 0.9164, F1 Micro: 0.7488, F1 Macro: 0.5766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1597, Accuracy: 0.9194, F1 Micro: 0.7514, F1 Macro: 0.609\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9176, F1 Micro: 0.763, F1 Macro: 0.647\n",
      "Epoch 6/10, Train Loss: 0.102, Accuracy: 0.9171, F1 Micro: 0.7554, F1 Macro: 0.6526\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9195, F1 Micro: 0.7672, F1 Macro: 0.6752\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.9233, F1 Micro: 0.766, F1 Macro: 0.6727\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.921, F1 Micro: 0.7595, F1 Macro: 0.6748\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9182, F1 Micro: 0.762, F1 Macro: 0.6872\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9195, F1 Micro: 0.7672, F1 Macro: 0.6752\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.69      0.75      0.72       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.76      0.74      0.75       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.74      0.33      0.46        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.68      0.72      0.70       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.84      0.86      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 202.3216905593872 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9099, F1 Micro: 0.7332, F1 Macro: 0.5904\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 52.928372621536255 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3584, Accuracy: 0.8949, F1 Micro: 0.6642, F1 Macro: 0.3877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2503, Accuracy: 0.9052, F1 Micro: 0.7186, F1 Macro: 0.5107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2033, Accuracy: 0.9179, F1 Micro: 0.7467, F1 Macro: 0.5968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1586, Accuracy: 0.9161, F1 Micro: 0.7574, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9197, F1 Micro: 0.761, F1 Macro: 0.6187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1021, Accuracy: 0.92, F1 Micro: 0.7685, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9201, F1 Micro: 0.7709, F1 Macro: 0.6811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9209, F1 Micro: 0.7743, F1 Macro: 0.6947\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9173, F1 Micro: 0.762, F1 Macro: 0.6688\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9221, F1 Micro: 0.7655, F1 Macro: 0.6962\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9209, F1 Micro: 0.7743, F1 Macro: 0.6947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.69      0.67      0.68       157\n",
      "      HS_Race       0.71      0.76      0.73       120\n",
      "  HS_Physical       0.64      0.19      0.30        72\n",
      "    HS_Gender       0.60      0.49      0.54        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.74      0.72       689\n",
      "  HS_Moderate       0.60      0.59      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.72      0.69      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 216.44761896133423 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.359, Accuracy: 0.8941, F1 Micro: 0.6506, F1 Macro: 0.3572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.25, Accuracy: 0.9046, F1 Micro: 0.7144, F1 Macro: 0.4938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2059, Accuracy: 0.9178, F1 Micro: 0.7472, F1 Macro: 0.5905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1605, Accuracy: 0.9192, F1 Micro: 0.7627, F1 Macro: 0.6245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1285, Accuracy: 0.9188, F1 Micro: 0.7639, F1 Macro: 0.6442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9208, F1 Micro: 0.7642, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0839, Accuracy: 0.9212, F1 Micro: 0.7669, F1 Macro: 0.6763\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9206, F1 Micro: 0.7661, F1 Macro: 0.6823\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.9206, F1 Micro: 0.7662, F1 Macro: 0.6723\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9213, F1 Micro: 0.76, F1 Macro: 0.6821\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9212, F1 Micro: 0.7669, F1 Macro: 0.6763\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.69      0.77      0.73       732\n",
      "     HS_Group       0.78      0.55      0.65       402\n",
      "  HS_Religion       0.76      0.59      0.66       157\n",
      "      HS_Race       0.81      0.61      0.70       120\n",
      "  HS_Physical       0.36      0.18      0.24        72\n",
      "    HS_Gender       0.49      0.49      0.49        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.67      0.75      0.71       689\n",
      "  HS_Moderate       0.72      0.47      0.57       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.73      0.64      0.68      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 215.06238079071045 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3669, Accuracy: 0.8943, F1 Micro: 0.6465, F1 Macro: 0.3887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.251, Accuracy: 0.9061, F1 Micro: 0.7188, F1 Macro: 0.499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2022, Accuracy: 0.9187, F1 Micro: 0.7521, F1 Macro: 0.6042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1563, Accuracy: 0.9173, F1 Micro: 0.7588, F1 Macro: 0.6184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1267, Accuracy: 0.9159, F1 Micro: 0.7613, F1 Macro: 0.626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.9168, F1 Micro: 0.7634, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9196, F1 Micro: 0.7673, F1 Macro: 0.6769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9199, F1 Micro: 0.7702, F1 Macro: 0.6896\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9142, F1 Micro: 0.76, F1 Macro: 0.6844\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9214, F1 Micro: 0.7669, F1 Macro: 0.697\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9199, F1 Micro: 0.7702, F1 Macro: 0.6896\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.70      0.61      0.66       402\n",
      "  HS_Religion       0.69      0.63      0.66       157\n",
      "      HS_Race       0.73      0.80      0.76       120\n",
      "  HS_Physical       0.71      0.21      0.32        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.66      0.75      0.70       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 216.59680461883545 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9108, F1 Micro: 0.7366, F1 Macro: 0.5992\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 48.26569175720215 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.361, Accuracy: 0.893, F1 Micro: 0.6611, F1 Macro: 0.3894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2472, Accuracy: 0.9105, F1 Micro: 0.7174, F1 Macro: 0.5389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1968, Accuracy: 0.9161, F1 Micro: 0.7541, F1 Macro: 0.5833\n",
      "Epoch 4/10, Train Loss: 0.1577, Accuracy: 0.9195, F1 Micro: 0.7429, F1 Macro: 0.6097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9155, F1 Micro: 0.7607, F1 Macro: 0.6356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.102, Accuracy: 0.9231, F1 Micro: 0.7754, F1 Macro: 0.6815\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.918, F1 Micro: 0.7652, F1 Macro: 0.6447\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9147, F1 Micro: 0.7663, F1 Macro: 0.6806\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9215, F1 Micro: 0.7614, F1 Macro: 0.685\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9177, F1 Micro: 0.7587, F1 Macro: 0.6935\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9231, F1 Micro: 0.7754, F1 Macro: 0.6815\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.77      0.60      0.68       402\n",
      "  HS_Religion       0.74      0.59      0.65       157\n",
      "      HS_Race       0.76      0.68      0.71       120\n",
      "  HS_Physical       0.50      0.11      0.18        72\n",
      "    HS_Gender       0.65      0.43      0.52        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.71      0.51      0.60       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.74      0.65      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 220.22924542427063 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3609, Accuracy: 0.8923, F1 Micro: 0.6496, F1 Macro: 0.3715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2472, Accuracy: 0.9103, F1 Micro: 0.709, F1 Macro: 0.5352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1986, Accuracy: 0.9168, F1 Micro: 0.7571, F1 Macro: 0.6012\n",
      "Epoch 4/10, Train Loss: 0.1574, Accuracy: 0.9183, F1 Micro: 0.7356, F1 Macro: 0.6107\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9143, F1 Micro: 0.7555, F1 Macro: 0.6172\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.921, F1 Micro: 0.7535, F1 Macro: 0.6561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9176, F1 Micro: 0.7639, F1 Macro: 0.6477\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9172, F1 Micro: 0.7629, F1 Macro: 0.6801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9195, F1 Micro: 0.7645, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9217, F1 Micro: 0.7686, F1 Macro: 0.6979\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9217, F1 Micro: 0.7686, F1 Macro: 0.6979\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.70      0.75      0.73       732\n",
      "     HS_Group       0.77      0.60      0.67       402\n",
      "  HS_Religion       0.70      0.59      0.64       157\n",
      "      HS_Race       0.75      0.67      0.70       120\n",
      "  HS_Physical       0.75      0.29      0.42        72\n",
      "    HS_Gender       0.60      0.55      0.57        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.69      0.51      0.59       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 223.52912616729736 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3652, Accuracy: 0.8898, F1 Micro: 0.6729, F1 Macro: 0.4436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2461, Accuracy: 0.9102, F1 Micro: 0.716, F1 Macro: 0.5474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1963, Accuracy: 0.9156, F1 Micro: 0.7577, F1 Macro: 0.6046\n",
      "Epoch 4/10, Train Loss: 0.1562, Accuracy: 0.9181, F1 Micro: 0.7328, F1 Macro: 0.6169\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9191, F1 Micro: 0.7568, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.9195, F1 Micro: 0.7669, F1 Macro: 0.6784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9252, F1 Micro: 0.772, F1 Macro: 0.6742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9216, F1 Micro: 0.7725, F1 Macro: 0.694\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9222, F1 Micro: 0.7649, F1 Macro: 0.6887\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9216, F1 Micro: 0.7682, F1 Macro: 0.6929\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9216, F1 Micro: 0.7725, F1 Macro: 0.694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.89      0.24      0.37        72\n",
      "    HS_Gender       0.54      0.43      0.48        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.59      0.60      0.59       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.68      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 222.2316393852234 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9118, F1 Micro: 0.7395, F1 Macro: 0.6068\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 43.540931701660156 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3567, Accuracy: 0.8931, F1 Micro: 0.6641, F1 Macro: 0.4274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2429, Accuracy: 0.9089, F1 Micro: 0.7264, F1 Macro: 0.5597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9171, F1 Micro: 0.7391, F1 Macro: 0.5827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1536, Accuracy: 0.9167, F1 Micro: 0.7634, F1 Macro: 0.641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9219, F1 Micro: 0.7682, F1 Macro: 0.6326\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.9177, F1 Micro: 0.7656, F1 Macro: 0.6382\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9171, F1 Micro: 0.7664, F1 Macro: 0.6611\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9166, F1 Micro: 0.7649, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9219, F1 Micro: 0.7689, F1 Macro: 0.6852\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.918, F1 Micro: 0.7684, F1 Macro: 0.687\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9219, F1 Micro: 0.7689, F1 Macro: 0.6852\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.70      0.71       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.62      0.47      0.53        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.67      0.69       689\n",
      "  HS_Moderate       0.63      0.60      0.61       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.76      0.65      0.69      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 228.82674384117126 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3559, Accuracy: 0.8923, F1 Micro: 0.669, F1 Macro: 0.4209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9079, F1 Micro: 0.7265, F1 Macro: 0.5559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.9183, F1 Micro: 0.7403, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9199, F1 Micro: 0.7679, F1 Macro: 0.6537\n",
      "Epoch 5/10, Train Loss: 0.1212, Accuracy: 0.9203, F1 Micro: 0.7587, F1 Macro: 0.6348\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9174, F1 Micro: 0.7632, F1 Macro: 0.6302\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9175, F1 Micro: 0.7646, F1 Macro: 0.6561\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.9216, F1 Micro: 0.7667, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9225, F1 Micro: 0.7681, F1 Macro: 0.6841\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9211, F1 Micro: 0.7669, F1 Macro: 0.6798\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9225, F1 Micro: 0.7681, F1 Macro: 0.6841\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.75      0.60      0.66       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.74      0.66      0.70       120\n",
      "  HS_Physical       0.74      0.19      0.31        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.74      0.77      5556\n",
      "    macro avg       0.76      0.64      0.68      5556\n",
      " weighted avg       0.79      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 227.9472906589508 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3609, Accuracy: 0.8938, F1 Micro: 0.6728, F1 Macro: 0.4399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9102, F1 Micro: 0.7241, F1 Macro: 0.5611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1932, Accuracy: 0.9185, F1 Micro: 0.7413, F1 Macro: 0.5918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1512, Accuracy: 0.9206, F1 Micro: 0.768, F1 Macro: 0.6549\n",
      "Epoch 5/10, Train Loss: 0.1177, Accuracy: 0.9196, F1 Micro: 0.7625, F1 Macro: 0.6518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.921, F1 Micro: 0.7706, F1 Macro: 0.6531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9182, F1 Micro: 0.7708, F1 Macro: 0.6719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.9221, F1 Micro: 0.774, F1 Macro: 0.6918\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9196, F1 Micro: 0.7701, F1 Macro: 0.6879\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9178, F1 Micro: 0.7667, F1 Macro: 0.6752\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9221, F1 Micro: 0.774, F1 Macro: 0.6918\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.68      0.69      0.68       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.87      0.62      0.72       120\n",
      "  HS_Physical       0.92      0.17      0.28        72\n",
      "    HS_Gender       0.56      0.47      0.51        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 230.54186177253723 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9126, F1 Micro: 0.7419, F1 Macro: 0.613\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 38.66590619087219 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3522, Accuracy: 0.8919, F1 Micro: 0.6069, F1 Macro: 0.365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2342, Accuracy: 0.912, F1 Micro: 0.729, F1 Macro: 0.5301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1894, Accuracy: 0.9191, F1 Micro: 0.7457, F1 Macro: 0.5993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1495, Accuracy: 0.9194, F1 Micro: 0.7661, F1 Macro: 0.6288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9203, F1 Micro: 0.7685, F1 Macro: 0.662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9225, F1 Micro: 0.7731, F1 Macro: 0.6733\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.9237, F1 Micro: 0.7693, F1 Macro: 0.671\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.9222, F1 Micro: 0.7656, F1 Macro: 0.6567\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9198, F1 Micro: 0.7711, F1 Macro: 0.6845\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9194, F1 Micro: 0.7675, F1 Macro: 0.6939\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9225, F1 Micro: 0.7731, F1 Macro: 0.6733\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.71      0.61      0.66       157\n",
      "      HS_Race       0.69      0.75      0.72       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.84      0.84      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.65      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 235.48285031318665 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.352, Accuracy: 0.8966, F1 Micro: 0.6553, F1 Macro: 0.4188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2353, Accuracy: 0.9122, F1 Micro: 0.7235, F1 Macro: 0.5406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1911, Accuracy: 0.92, F1 Micro: 0.7512, F1 Macro: 0.606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1532, Accuracy: 0.9209, F1 Micro: 0.7664, F1 Macro: 0.6505\n",
      "Epoch 5/10, Train Loss: 0.1181, Accuracy: 0.9182, F1 Micro: 0.7658, F1 Macro: 0.6637\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9221, F1 Micro: 0.7632, F1 Macro: 0.6685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9204, F1 Micro: 0.7701, F1 Macro: 0.6614\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9183, F1 Micro: 0.7603, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9242, F1 Micro: 0.772, F1 Macro: 0.6819\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9178, F1 Micro: 0.7635, F1 Macro: 0.6871\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9242, F1 Micro: 0.772, F1 Macro: 0.6819\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.75      0.59      0.66       402\n",
      "  HS_Religion       0.82      0.50      0.62       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.59      0.47      0.52        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.67      0.50      0.57       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 237.13448929786682 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3593, Accuracy: 0.8927, F1 Micro: 0.6171, F1 Macro: 0.3723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2368, Accuracy: 0.9133, F1 Micro: 0.7312, F1 Macro: 0.5363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1877, Accuracy: 0.9202, F1 Micro: 0.7566, F1 Macro: 0.619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.919, F1 Micro: 0.766, F1 Macro: 0.6561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1178, Accuracy: 0.9187, F1 Micro: 0.7709, F1 Macro: 0.675\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9162, F1 Micro: 0.7643, F1 Macro: 0.6659\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9214, F1 Micro: 0.7651, F1 Macro: 0.6753\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9216, F1 Micro: 0.7689, F1 Macro: 0.6858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9219, F1 Micro: 0.7725, F1 Macro: 0.6872\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9203, F1 Micro: 0.7721, F1 Macro: 0.696\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9219, F1 Micro: 0.7725, F1 Macro: 0.6872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.76      0.55      0.64       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.51      0.49      0.50        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.59       331\n",
      "    HS_Strong       0.80      0.87      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 236.15887331962585 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9133, F1 Micro: 0.7441, F1 Macro: 0.6179\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 35.90159606933594 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3503, Accuracy: 0.8969, F1 Micro: 0.653, F1 Macro: 0.411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2396, Accuracy: 0.9131, F1 Micro: 0.7273, F1 Macro: 0.566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1921, Accuracy: 0.9156, F1 Micro: 0.7581, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1524, Accuracy: 0.9222, F1 Micro: 0.7718, F1 Macro: 0.635\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9172, F1 Micro: 0.7679, F1 Macro: 0.6474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.9244, F1 Micro: 0.7729, F1 Macro: 0.6732\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9215, F1 Micro: 0.7629, F1 Macro: 0.6614\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9231, F1 Micro: 0.7683, F1 Macro: 0.6816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9241, F1 Micro: 0.7749, F1 Macro: 0.7013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9249, F1 Micro: 0.7762, F1 Macro: 0.7031\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9249, F1 Micro: 0.7762, F1 Macro: 0.7031\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.73      0.59      0.65       157\n",
      "      HS_Race       0.89      0.53      0.67       120\n",
      "  HS_Physical       0.79      0.32      0.46        72\n",
      "    HS_Gender       0.61      0.49      0.54        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.69      0.53      0.60       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 243.41835641860962 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3521, Accuracy: 0.8955, F1 Micro: 0.6441, F1 Macro: 0.4055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2392, Accuracy: 0.9125, F1 Micro: 0.7269, F1 Macro: 0.5725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1931, Accuracy: 0.9169, F1 Micro: 0.7608, F1 Macro: 0.6163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1533, Accuracy: 0.9246, F1 Micro: 0.7689, F1 Macro: 0.6345\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9157, F1 Micro: 0.7683, F1 Macro: 0.655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9217, F1 Micro: 0.7751, F1 Macro: 0.6708\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9218, F1 Micro: 0.7635, F1 Macro: 0.6617\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9202, F1 Micro: 0.7727, F1 Macro: 0.6819\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.922, F1 Micro: 0.7735, F1 Macro: 0.6997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9253, F1 Micro: 0.7767, F1 Macro: 0.7061\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9253, F1 Micro: 0.7767, F1 Macro: 0.7061\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.75      0.64      0.69       120\n",
      "  HS_Physical       0.77      0.32      0.45        72\n",
      "    HS_Gender       0.60      0.51      0.55        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.54      0.59       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.76      0.67      0.71      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 242.79914784431458 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3572, Accuracy: 0.8973, F1 Micro: 0.6485, F1 Macro: 0.4314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2402, Accuracy: 0.9133, F1 Micro: 0.7396, F1 Macro: 0.593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1919, Accuracy: 0.92, F1 Micro: 0.7603, F1 Macro: 0.6163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9252, F1 Micro: 0.777, F1 Macro: 0.6514\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.9177, F1 Micro: 0.7704, F1 Macro: 0.662\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9214, F1 Micro: 0.7723, F1 Macro: 0.676\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9222, F1 Micro: 0.7675, F1 Macro: 0.6809\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9198, F1 Micro: 0.7729, F1 Macro: 0.6845\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9254, F1 Micro: 0.7683, F1 Macro: 0.6941\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.923, F1 Micro: 0.775, F1 Macro: 0.699\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9252, F1 Micro: 0.777, F1 Macro: 0.6514\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.76      0.61      0.68       402\n",
      "  HS_Religion       0.75      0.63      0.69       157\n",
      "      HS_Race       0.74      0.74      0.74       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.58      0.14      0.22        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.75      0.73       689\n",
      "  HS_Moderate       0.71      0.50      0.58       331\n",
      "    HS_Strong       0.86      0.75      0.80       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.62      0.65      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 238.77888941764832 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9141, F1 Micro: 0.7462, F1 Macro: 0.6225\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 32.14878749847412 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3448, Accuracy: 0.8977, F1 Micro: 0.6521, F1 Macro: 0.4029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2345, Accuracy: 0.9122, F1 Micro: 0.7304, F1 Macro: 0.5545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1927, Accuracy: 0.9212, F1 Micro: 0.7586, F1 Macro: 0.6071\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9212, F1 Micro: 0.755, F1 Macro: 0.6359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.9224, F1 Micro: 0.7675, F1 Macro: 0.6568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9236, F1 Micro: 0.7763, F1 Macro: 0.6793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0772, Accuracy: 0.924, F1 Micro: 0.7794, F1 Macro: 0.6924\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.9255, F1 Micro: 0.7758, F1 Macro: 0.6943\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9208, F1 Micro: 0.7703, F1 Macro: 0.692\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9224, F1 Micro: 0.7735, F1 Macro: 0.7043\n",
      "Model 1 - Iteration 8402: Accuracy: 0.924, F1 Micro: 0.7794, F1 Macro: 0.6924\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.77      0.65      0.71       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.56      0.39      0.46        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.67      0.69      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 248.41080808639526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3439, Accuracy: 0.8969, F1 Micro: 0.6568, F1 Macro: 0.3944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2344, Accuracy: 0.913, F1 Micro: 0.7349, F1 Macro: 0.5611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1919, Accuracy: 0.9208, F1 Micro: 0.7598, F1 Macro: 0.6125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1488, Accuracy: 0.9233, F1 Micro: 0.7602, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9213, F1 Micro: 0.7707, F1 Macro: 0.66\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9185, F1 Micro: 0.7683, F1 Macro: 0.6582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9207, F1 Micro: 0.771, F1 Macro: 0.693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0633, Accuracy: 0.9215, F1 Micro: 0.7745, F1 Macro: 0.6891\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9213, F1 Micro: 0.7711, F1 Macro: 0.685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9224, F1 Micro: 0.7746, F1 Macro: 0.7014\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9224, F1 Micro: 0.7746, F1 Macro: 0.7014\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.69      0.65      0.67       402\n",
      "  HS_Religion       0.68      0.63      0.66       157\n",
      "      HS_Race       0.72      0.66      0.69       120\n",
      "  HS_Physical       0.70      0.32      0.44        72\n",
      "    HS_Gender       0.58      0.51      0.54        51\n",
      "     HS_Other       0.77      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.73      0.68      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 251.7063434123993 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3504, Accuracy: 0.8973, F1 Micro: 0.652, F1 Macro: 0.4096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2333, Accuracy: 0.9136, F1 Micro: 0.7349, F1 Macro: 0.5602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1908, Accuracy: 0.92, F1 Micro: 0.756, F1 Macro: 0.6107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1449, Accuracy: 0.9218, F1 Micro: 0.7602, F1 Macro: 0.6594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9223, F1 Micro: 0.7711, F1 Macro: 0.6725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9224, F1 Micro: 0.7764, F1 Macro: 0.685\n",
      "Epoch 7/10, Train Loss: 0.0745, Accuracy: 0.922, F1 Micro: 0.7758, F1 Macro: 0.7001\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9183, F1 Micro: 0.7706, F1 Macro: 0.6941\n",
      "Epoch 9/10, Train Loss: 0.0493, Accuracy: 0.9235, F1 Micro: 0.776, F1 Macro: 0.6945\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9216, F1 Micro: 0.7748, F1 Macro: 0.7064\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9224, F1 Micro: 0.7764, F1 Macro: 0.685\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.74      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.57      0.33      0.42        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 249.23646998405457 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9147, F1 Micro: 0.7482, F1 Macro: 0.6269\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 28.504218816757202 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3456, Accuracy: 0.896, F1 Micro: 0.6285, F1 Macro: 0.3833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.233, Accuracy: 0.9145, F1 Micro: 0.7389, F1 Macro: 0.5603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1863, Accuracy: 0.9201, F1 Micro: 0.7485, F1 Macro: 0.6057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9218, F1 Micro: 0.7747, F1 Macro: 0.6633\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9248, F1 Micro: 0.7723, F1 Macro: 0.6719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9254, F1 Micro: 0.7751, F1 Macro: 0.6744\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9255, F1 Micro: 0.7739, F1 Macro: 0.6866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9185, F1 Micro: 0.7759, F1 Macro: 0.6962\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9204, F1 Micro: 0.7757, F1 Macro: 0.7014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9225, F1 Micro: 0.7773, F1 Macro: 0.7105\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9225, F1 Micro: 0.7773, F1 Macro: 0.7105\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.73      0.66      0.69       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.85      0.31      0.45        72\n",
      "    HS_Gender       0.56      0.55      0.55        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.83      0.89      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 254.4848039150238 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3461, Accuracy: 0.8954, F1 Micro: 0.6218, F1 Macro: 0.383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2358, Accuracy: 0.9134, F1 Micro: 0.7386, F1 Macro: 0.5688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1876, Accuracy: 0.9195, F1 Micro: 0.7492, F1 Macro: 0.6028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.918, F1 Micro: 0.7662, F1 Macro: 0.6566\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9222, F1 Micro: 0.7661, F1 Macro: 0.6728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9245, F1 Micro: 0.7749, F1 Macro: 0.6772\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9224, F1 Micro: 0.7726, F1 Macro: 0.7027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9243, F1 Micro: 0.7809, F1 Macro: 0.6877\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9192, F1 Micro: 0.7728, F1 Macro: 0.6979\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9195, F1 Micro: 0.7698, F1 Macro: 0.7021\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9243, F1 Micro: 0.7809, F1 Macro: 0.6877\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.73      0.67      0.70       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.77      0.60      0.68       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.66      0.45      0.53        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.66      0.61      0.63       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 252.9061815738678 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3516, Accuracy: 0.8961, F1 Micro: 0.6354, F1 Macro: 0.4128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.234, Accuracy: 0.9154, F1 Micro: 0.7452, F1 Macro: 0.5793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9227, F1 Micro: 0.7626, F1 Macro: 0.646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9254, F1 Micro: 0.78, F1 Macro: 0.6816\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9239, F1 Micro: 0.7771, F1 Macro: 0.6809\n",
      "Epoch 6/10, Train Loss: 0.0877, Accuracy: 0.9252, F1 Micro: 0.7775, F1 Macro: 0.6922\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9233, F1 Micro: 0.7771, F1 Macro: 0.7015\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9229, F1 Micro: 0.7765, F1 Macro: 0.6926\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9233, F1 Micro: 0.7781, F1 Macro: 0.7064\n",
      "Epoch 10/10, Train Loss: 0.0418, Accuracy: 0.9239, F1 Micro: 0.7775, F1 Macro: 0.7055\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9254, F1 Micro: 0.78, F1 Macro: 0.6816\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.77      0.67      0.72       157\n",
      "      HS_Race       0.77      0.73      0.75       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.65      0.33      0.44        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.74      0.68      0.71       689\n",
      "  HS_Moderate       0.65      0.59      0.62       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.66      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 249.96551990509033 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9152, F1 Micro: 0.75, F1 Macro: 0.6308\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.92521333694458 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3469, Accuracy: 0.8993, F1 Micro: 0.6773, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2347, Accuracy: 0.9148, F1 Micro: 0.7339, F1 Macro: 0.5722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.187, Accuracy: 0.9154, F1 Micro: 0.7596, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1479, Accuracy: 0.9236, F1 Micro: 0.7703, F1 Macro: 0.6408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9242, F1 Micro: 0.7757, F1 Macro: 0.6648\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.918, F1 Micro: 0.7721, F1 Macro: 0.6649\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.9159, F1 Micro: 0.7672, F1 Macro: 0.6706\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.9248, F1 Micro: 0.7727, F1 Macro: 0.6759\n",
      "Epoch 9/10, Train Loss: 0.0553, Accuracy: 0.923, F1 Micro: 0.7729, F1 Macro: 0.6959\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9216, F1 Micro: 0.7729, F1 Macro: 0.6972\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9242, F1 Micro: 0.7757, F1 Macro: 0.6648\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.78      0.67      0.72       732\n",
      "     HS_Group       0.67      0.73      0.70       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.75      0.65      0.70       689\n",
      "  HS_Moderate       0.60      0.66      0.63       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.64      0.66      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 256.2362630367279 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3466, Accuracy: 0.8991, F1 Micro: 0.676, F1 Macro: 0.4392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2352, Accuracy: 0.9123, F1 Micro: 0.709, F1 Macro: 0.5365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1869, Accuracy: 0.9151, F1 Micro: 0.7596, F1 Macro: 0.6467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9206, F1 Micro: 0.7693, F1 Macro: 0.6532\n",
      "Epoch 5/10, Train Loss: 0.1161, Accuracy: 0.922, F1 Micro: 0.768, F1 Macro: 0.6553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.9229, F1 Micro: 0.7783, F1 Macro: 0.6613\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9222, F1 Micro: 0.7745, F1 Macro: 0.6701\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9149, F1 Micro: 0.7618, F1 Macro: 0.6812\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9214, F1 Micro: 0.7697, F1 Macro: 0.6892\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9209, F1 Micro: 0.7722, F1 Macro: 0.7042\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9229, F1 Micro: 0.7783, F1 Macro: 0.6613\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.77      0.63      0.69       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.65      0.22      0.32        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.73       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.77      0.65      0.66      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 256.14175486564636 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3522, Accuracy: 0.8986, F1 Micro: 0.6793, F1 Macro: 0.4727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2343, Accuracy: 0.9156, F1 Micro: 0.7325, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1863, Accuracy: 0.9186, F1 Micro: 0.7668, F1 Macro: 0.6272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1477, Accuracy: 0.924, F1 Micro: 0.7709, F1 Macro: 0.6631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1167, Accuracy: 0.9224, F1 Micro: 0.7781, F1 Macro: 0.6691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9243, F1 Micro: 0.7804, F1 Macro: 0.673\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9212, F1 Micro: 0.7752, F1 Macro: 0.6871\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9249, F1 Micro: 0.7761, F1 Macro: 0.6889\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9257, F1 Micro: 0.7792, F1 Macro: 0.713\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9203, F1 Micro: 0.7719, F1 Macro: 0.6968\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9243, F1 Micro: 0.7804, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.72      0.64      0.68       402\n",
      "  HS_Religion       0.80      0.58      0.67       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       1.00      0.10      0.18        72\n",
      "    HS_Gender       0.78      0.27      0.41        51\n",
      "     HS_Other       0.74      0.85      0.79       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.79      0.64      0.67      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 257.6955597400665 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9157, F1 Micro: 0.7516, F1 Macro: 0.6327\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 23.593400478363037 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3487, Accuracy: 0.8989, F1 Micro: 0.6791, F1 Macro: 0.4718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2359, Accuracy: 0.9132, F1 Micro: 0.7417, F1 Macro: 0.5748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.183, Accuracy: 0.9223, F1 Micro: 0.7554, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1472, Accuracy: 0.9231, F1 Micro: 0.7734, F1 Macro: 0.6367\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9223, F1 Micro: 0.7576, F1 Macro: 0.6439\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.9239, F1 Micro: 0.7726, F1 Macro: 0.675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0726, Accuracy: 0.9247, F1 Micro: 0.7811, F1 Macro: 0.693\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9238, F1 Micro: 0.7764, F1 Macro: 0.6978\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9226, F1 Micro: 0.7745, F1 Macro: 0.6951\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9237, F1 Micro: 0.7782, F1 Macro: 0.7062\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9247, F1 Micro: 0.7811, F1 Macro: 0.693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.70      0.79      0.74       732\n",
      "     HS_Group       0.79      0.61      0.69       402\n",
      "  HS_Religion       0.82      0.58      0.68       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.72      0.52      0.61       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 261.0946135520935 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3498, Accuracy: 0.8973, F1 Micro: 0.6804, F1 Macro: 0.4772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2366, Accuracy: 0.9144, F1 Micro: 0.7418, F1 Macro: 0.5774\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.9201, F1 Micro: 0.7383, F1 Macro: 0.5632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1487, Accuracy: 0.9231, F1 Micro: 0.7726, F1 Macro: 0.6453\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9202, F1 Micro: 0.763, F1 Macro: 0.6543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9221, F1 Micro: 0.7774, F1 Macro: 0.6756\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9201, F1 Micro: 0.7731, F1 Macro: 0.6708\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9256, F1 Micro: 0.7731, F1 Macro: 0.6964\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9206, F1 Micro: 0.7731, F1 Macro: 0.6956\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9212, F1 Micro: 0.7717, F1 Macro: 0.6952\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9221, F1 Micro: 0.7774, F1 Macro: 0.6756\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.68      0.69      0.69       157\n",
      "      HS_Race       0.78      0.60      0.68       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.49      0.43      0.46        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.72       689\n",
      "  HS_Moderate       0.63      0.58      0.61       331\n",
      "    HS_Strong       0.84      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 259.4797348976135 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3538, Accuracy: 0.8996, F1 Micro: 0.6904, F1 Macro: 0.4892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2367, Accuracy: 0.9159, F1 Micro: 0.7449, F1 Macro: 0.5765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1822, Accuracy: 0.9206, F1 Micro: 0.7547, F1 Macro: 0.5977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1477, Accuracy: 0.9248, F1 Micro: 0.7732, F1 Macro: 0.6546\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9208, F1 Micro: 0.765, F1 Macro: 0.6484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.9258, F1 Micro: 0.7796, F1 Macro: 0.6909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0731, Accuracy: 0.9252, F1 Micro: 0.7806, F1 Macro: 0.695\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9257, F1 Micro: 0.7732, F1 Macro: 0.7005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.926, F1 Micro: 0.7822, F1 Macro: 0.7099\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9225, F1 Micro: 0.7752, F1 Macro: 0.7071\n",
      "Model 3 - Iteration 9016: Accuracy: 0.926, F1 Micro: 0.7822, F1 Macro: 0.7099\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.75      0.75       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.80      0.62      0.70       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.90      0.25      0.39        72\n",
      "    HS_Gender       0.59      0.51      0.55        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 265.0003035068512 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9161, F1 Micro: 0.7531, F1 Macro: 0.6359\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.47336483001709 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3399, Accuracy: 0.8982, F1 Micro: 0.6932, F1 Macro: 0.4651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2292, Accuracy: 0.9167, F1 Micro: 0.7447, F1 Macro: 0.5722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.187, Accuracy: 0.9213, F1 Micro: 0.7691, F1 Macro: 0.6166\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9229, F1 Micro: 0.7647, F1 Macro: 0.6275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9223, F1 Micro: 0.7749, F1 Macro: 0.644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9235, F1 Micro: 0.7765, F1 Macro: 0.6717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.9223, F1 Micro: 0.7776, F1 Macro: 0.6907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9264, F1 Micro: 0.7798, F1 Macro: 0.7091\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9264, F1 Micro: 0.7779, F1 Macro: 0.7088\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9241, F1 Micro: 0.778, F1 Macro: 0.7068\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9264, F1 Micro: 0.7798, F1 Macro: 0.7091\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.74      0.63      0.68       157\n",
      "      HS_Race       0.82      0.69      0.75       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.55      0.59      0.57        51\n",
      "     HS_Other       0.83      0.76      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.69      0.51      0.59       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 269.5906617641449 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3369, Accuracy: 0.8998, F1 Micro: 0.6964, F1 Macro: 0.4767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9155, F1 Micro: 0.7455, F1 Macro: 0.5763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1863, Accuracy: 0.9211, F1 Micro: 0.7632, F1 Macro: 0.6093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1514, Accuracy: 0.9237, F1 Micro: 0.7655, F1 Macro: 0.6434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9232, F1 Micro: 0.7686, F1 Macro: 0.6261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9236, F1 Micro: 0.7749, F1 Macro: 0.6715\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.9228, F1 Micro: 0.7739, F1 Macro: 0.689\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9224, F1 Micro: 0.7731, F1 Macro: 0.7015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.924, F1 Micro: 0.7754, F1 Macro: 0.7074\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9203, F1 Micro: 0.7682, F1 Macro: 0.6981\n",
      "Model 2 - Iteration 9216: Accuracy: 0.924, F1 Micro: 0.7754, F1 Macro: 0.7074\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.75      0.68      0.71       732\n",
      "     HS_Group       0.68      0.71      0.70       402\n",
      "  HS_Religion       0.71      0.64      0.68       157\n",
      "      HS_Race       0.74      0.69      0.72       120\n",
      "  HS_Physical       0.78      0.29      0.42        72\n",
      "    HS_Gender       0.61      0.49      0.54        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.65      0.69       689\n",
      "  HS_Moderate       0.62      0.64      0.63       331\n",
      "    HS_Strong       0.86      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.68      0.71      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 270.49902963638306 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3428, Accuracy: 0.8998, F1 Micro: 0.6948, F1 Macro: 0.4845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2273, Accuracy: 0.9178, F1 Micro: 0.7518, F1 Macro: 0.5866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1842, Accuracy: 0.9213, F1 Micro: 0.7704, F1 Macro: 0.6242\n",
      "Epoch 4/10, Train Loss: 0.145, Accuracy: 0.9247, F1 Micro: 0.7692, F1 Macro: 0.6475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.923, F1 Micro: 0.7748, F1 Macro: 0.6625\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9247, F1 Micro: 0.7728, F1 Macro: 0.6905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9251, F1 Micro: 0.7795, F1 Macro: 0.6946\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9215, F1 Micro: 0.7776, F1 Macro: 0.7127\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9244, F1 Micro: 0.7761, F1 Macro: 0.7118\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9232, F1 Micro: 0.7771, F1 Macro: 0.7023\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9251, F1 Micro: 0.7795, F1 Macro: 0.6946\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.72      0.78      0.75       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.75      0.73       689\n",
      "  HS_Moderate       0.67      0.50      0.58       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 265.45911860466003 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9166, F1 Micro: 0.7543, F1 Macro: 0.6393\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 17.478184938430786 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3381, Accuracy: 0.8987, F1 Micro: 0.6733, F1 Macro: 0.4288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2346, Accuracy: 0.9151, F1 Micro: 0.7425, F1 Macro: 0.5616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1813, Accuracy: 0.9195, F1 Micro: 0.7694, F1 Macro: 0.6163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1465, Accuracy: 0.9215, F1 Micro: 0.7776, F1 Macro: 0.666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1151, Accuracy: 0.923, F1 Micro: 0.7808, F1 Macro: 0.6908\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9251, F1 Micro: 0.7801, F1 Macro: 0.6999\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.924, F1 Micro: 0.7701, F1 Macro: 0.6882\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.9237, F1 Micro: 0.7759, F1 Macro: 0.7059\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9233, F1 Micro: 0.7766, F1 Macro: 0.7028\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.9231, F1 Micro: 0.7771, F1 Macro: 0.7108\n",
      "Model 1 - Iteration 9218: Accuracy: 0.923, F1 Micro: 0.7808, F1 Macro: 0.6908\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.71      0.70      0.70       402\n",
      "  HS_Religion       0.68      0.66      0.67       157\n",
      "      HS_Race       0.71      0.78      0.74       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.56      0.47      0.51        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.60      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5556\n",
      "    macro avg       0.74      0.69      0.69      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 267.7021236419678 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.338, Accuracy: 0.8988, F1 Micro: 0.6724, F1 Macro: 0.4334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2363, Accuracy: 0.9154, F1 Micro: 0.7397, F1 Macro: 0.5636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1829, Accuracy: 0.9141, F1 Micro: 0.7611, F1 Macro: 0.61\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9224, F1 Micro: 0.7773, F1 Macro: 0.6772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9263, F1 Micro: 0.783, F1 Macro: 0.6839\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.919, F1 Micro: 0.7733, F1 Macro: 0.6979\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9234, F1 Micro: 0.7777, F1 Macro: 0.6947\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9172, F1 Micro: 0.7618, F1 Macro: 0.6873\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9199, F1 Micro: 0.7732, F1 Macro: 0.7016\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9226, F1 Micro: 0.7777, F1 Macro: 0.7112\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9263, F1 Micro: 0.783, F1 Macro: 0.6839\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.77      0.75       732\n",
      "     HS_Group       0.77      0.63      0.69       402\n",
      "  HS_Religion       0.77      0.64      0.70       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.66      0.37      0.48        51\n",
      "     HS_Other       0.78      0.81      0.80       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.68      0.51      0.58       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 267.4809124469757 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3434, Accuracy: 0.8987, F1 Micro: 0.6686, F1 Macro: 0.4222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2339, Accuracy: 0.917, F1 Micro: 0.7434, F1 Macro: 0.5685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1807, Accuracy: 0.9218, F1 Micro: 0.7692, F1 Macro: 0.6176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1454, Accuracy: 0.918, F1 Micro: 0.7717, F1 Macro: 0.6671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9256, F1 Micro: 0.777, F1 Macro: 0.6818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9224, F1 Micro: 0.7781, F1 Macro: 0.7013\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9248, F1 Micro: 0.7723, F1 Macro: 0.7007\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.925, F1 Micro: 0.7767, F1 Macro: 0.7083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9249, F1 Micro: 0.7843, F1 Macro: 0.7144\n",
      "Epoch 10/10, Train Loss: 0.0399, Accuracy: 0.9241, F1 Micro: 0.782, F1 Macro: 0.7151\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9249, F1 Micro: 0.7843, F1 Macro: 0.7144\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.88      0.94      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.79      0.76      0.77       120\n",
      "  HS_Physical       0.69      0.31      0.42        72\n",
      "    HS_Gender       0.54      0.55      0.54        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 270.5365045070648 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.917, F1 Micro: 0.7557, F1 Macro: 0.642\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.84168314933777 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3371, Accuracy: 0.9008, F1 Micro: 0.7014, F1 Macro: 0.4851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2286, Accuracy: 0.9133, F1 Micro: 0.7446, F1 Macro: 0.5744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1836, Accuracy: 0.921, F1 Micro: 0.7552, F1 Macro: 0.6054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1441, Accuracy: 0.9203, F1 Micro: 0.7754, F1 Macro: 0.6402\n",
      "Epoch 5/10, Train Loss: 0.114, Accuracy: 0.9231, F1 Micro: 0.7753, F1 Macro: 0.6634\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9175, F1 Micro: 0.7659, F1 Macro: 0.671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.923, F1 Micro: 0.7774, F1 Macro: 0.6859\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9236, F1 Micro: 0.7749, F1 Macro: 0.6966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9241, F1 Micro: 0.7779, F1 Macro: 0.7096\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9236, F1 Micro: 0.7735, F1 Macro: 0.6999\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9241, F1 Micro: 0.7779, F1 Macro: 0.7096\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.76      0.64      0.69       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.72      0.71      0.71       120\n",
      "  HS_Physical       0.83      0.33      0.48        72\n",
      "    HS_Gender       0.57      0.49      0.53        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.68      0.57      0.62       331\n",
      "    HS_Strong       0.91      0.81      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.78      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 274.3702709674835 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3384, Accuracy: 0.899, F1 Micro: 0.6991, F1 Macro: 0.4876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2296, Accuracy: 0.9131, F1 Micro: 0.7453, F1 Macro: 0.5828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1852, Accuracy: 0.9198, F1 Micro: 0.7463, F1 Macro: 0.5964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.147, Accuracy: 0.9235, F1 Micro: 0.7766, F1 Macro: 0.6599\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9201, F1 Micro: 0.7746, F1 Macro: 0.6754\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.919, F1 Micro: 0.772, F1 Macro: 0.6821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9253, F1 Micro: 0.7771, F1 Macro: 0.7079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9248, F1 Micro: 0.7778, F1 Macro: 0.7077\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9212, F1 Micro: 0.7735, F1 Macro: 0.703\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9246, F1 Micro: 0.7716, F1 Macro: 0.7021\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9248, F1 Micro: 0.7778, F1 Macro: 0.7077\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.76      0.72      0.74       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.70      0.65      0.68       157\n",
      "      HS_Race       0.79      0.72      0.75       120\n",
      "  HS_Physical       0.76      0.31      0.44        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.74      0.69      0.72       689\n",
      "  HS_Moderate       0.62      0.61      0.61       331\n",
      "    HS_Strong       0.83      0.84      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.68      0.71      5556\n",
      " weighted avg       0.79      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 273.8343162536621 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3421, Accuracy: 0.8995, F1 Micro: 0.7023, F1 Macro: 0.5036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2279, Accuracy: 0.9153, F1 Micro: 0.7491, F1 Macro: 0.586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1819, Accuracy: 0.9211, F1 Micro: 0.7627, F1 Macro: 0.6287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.145, Accuracy: 0.9243, F1 Micro: 0.7812, F1 Macro: 0.6483\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.9249, F1 Micro: 0.7746, F1 Macro: 0.6806\n",
      "Epoch 6/10, Train Loss: 0.0863, Accuracy: 0.9187, F1 Micro: 0.7708, F1 Macro: 0.6885\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.924, F1 Micro: 0.7707, F1 Macro: 0.6978\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9243, F1 Micro: 0.7805, F1 Macro: 0.7061\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9235, F1 Micro: 0.7626, F1 Macro: 0.6873\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9244, F1 Micro: 0.7751, F1 Macro: 0.7057\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9243, F1 Micro: 0.7812, F1 Macro: 0.6483\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.73      0.76      0.75       732\n",
      "     HS_Group       0.71      0.68      0.70       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.78      0.72      0.75       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.73      0.85      0.79       762\n",
      "      HS_Weak       0.72      0.74      0.73       689\n",
      "  HS_Moderate       0.65      0.58      0.62       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.64      0.65      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 270.20624685287476 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9173, F1 Micro: 0.7567, F1 Macro: 0.6441\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.796833038330078 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3343, Accuracy: 0.9009, F1 Micro: 0.688, F1 Macro: 0.4814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2257, Accuracy: 0.9163, F1 Micro: 0.7374, F1 Macro: 0.5576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1799, Accuracy: 0.922, F1 Micro: 0.7633, F1 Macro: 0.6339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9257, F1 Micro: 0.773, F1 Macro: 0.6422\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.9197, F1 Micro: 0.7715, F1 Macro: 0.6653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9219, F1 Micro: 0.7749, F1 Macro: 0.6937\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9222, F1 Micro: 0.7715, F1 Macro: 0.68\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9172, F1 Micro: 0.7705, F1 Macro: 0.6933\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9227, F1 Micro: 0.7713, F1 Macro: 0.699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.924, F1 Micro: 0.7775, F1 Macro: 0.7134\n",
      "Model 1 - Iteration 9618: Accuracy: 0.924, F1 Micro: 0.7775, F1 Macro: 0.7134\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.71      0.68      0.70       157\n",
      "      HS_Race       0.72      0.76      0.74       120\n",
      "  HS_Physical       0.79      0.32      0.46        72\n",
      "    HS_Gender       0.60      0.53      0.56        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.61      0.62      0.62       331\n",
      "    HS_Strong       0.82      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.78      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 279.2088027000427 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3354, Accuracy: 0.8999, F1 Micro: 0.6928, F1 Macro: 0.4939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2273, Accuracy: 0.9152, F1 Micro: 0.7229, F1 Macro: 0.5547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1808, Accuracy: 0.9216, F1 Micro: 0.7635, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1469, Accuracy: 0.9267, F1 Micro: 0.7736, F1 Macro: 0.6501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.12, Accuracy: 0.9215, F1 Micro: 0.7754, F1 Macro: 0.6795\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9191, F1 Micro: 0.7639, F1 Macro: 0.6904\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9181, F1 Micro: 0.7697, F1 Macro: 0.6894\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9216, F1 Micro: 0.7751, F1 Macro: 0.6994\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9235, F1 Micro: 0.7723, F1 Macro: 0.7061\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.923, F1 Micro: 0.7751, F1 Macro: 0.702\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9215, F1 Micro: 0.7754, F1 Macro: 0.6795\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.67      0.68      0.67       402\n",
      "  HS_Religion       0.76      0.50      0.60       157\n",
      "      HS_Race       0.72      0.73      0.73       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.67      0.39      0.49        51\n",
      "     HS_Other       0.73      0.84      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.61      0.61      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 277.7887907028198 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3409, Accuracy: 0.9006, F1 Micro: 0.688, F1 Macro: 0.4801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2273, Accuracy: 0.9161, F1 Micro: 0.7219, F1 Macro: 0.5727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1802, Accuracy: 0.924, F1 Micro: 0.7665, F1 Macro: 0.6432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.143, Accuracy: 0.9258, F1 Micro: 0.7792, F1 Macro: 0.6491\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9195, F1 Micro: 0.7748, F1 Macro: 0.6771\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9195, F1 Micro: 0.7675, F1 Macro: 0.6939\n",
      "Epoch 7/10, Train Loss: 0.0719, Accuracy: 0.9173, F1 Micro: 0.77, F1 Macro: 0.68\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9209, F1 Micro: 0.7771, F1 Macro: 0.6989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9271, F1 Micro: 0.7802, F1 Macro: 0.7055\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9193, F1 Micro: 0.7703, F1 Macro: 0.7014\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9271, F1 Micro: 0.7802, F1 Macro: 0.7055\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.76      0.72      0.74       732\n",
      "     HS_Group       0.76      0.65      0.70       402\n",
      "  HS_Religion       0.77      0.63      0.69       157\n",
      "      HS_Race       0.86      0.63      0.73       120\n",
      "  HS_Physical       0.73      0.26      0.39        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.74      0.70      0.72       689\n",
      "  HS_Moderate       0.68      0.55      0.61       331\n",
      "    HS_Strong       0.88      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.78      0.66      0.71      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 277.35059356689453 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9176, F1 Micro: 0.7576, F1 Macro: 0.6465\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 13.10834002494812 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3385, Accuracy: 0.8978, F1 Micro: 0.662, F1 Macro: 0.3913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2246, Accuracy: 0.9121, F1 Micro: 0.7372, F1 Macro: 0.5681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1784, Accuracy: 0.9227, F1 Micro: 0.7653, F1 Macro: 0.6111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1472, Accuracy: 0.9252, F1 Micro: 0.77, F1 Macro: 0.6714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9215, F1 Micro: 0.7734, F1 Macro: 0.6697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0895, Accuracy: 0.9251, F1 Micro: 0.7777, F1 Macro: 0.6868\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9245, F1 Micro: 0.7759, F1 Macro: 0.6997\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9209, F1 Micro: 0.7734, F1 Macro: 0.6983\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9237, F1 Micro: 0.7776, F1 Macro: 0.7068\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9241, F1 Micro: 0.772, F1 Macro: 0.7105\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9251, F1 Micro: 0.7777, F1 Macro: 0.6868\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.78      0.60      0.68       157\n",
      "      HS_Race       0.87      0.68      0.77       120\n",
      "  HS_Physical       1.00      0.14      0.24        72\n",
      "    HS_Gender       0.54      0.39      0.45        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 284.21666979789734 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3368, Accuracy: 0.8986, F1 Micro: 0.6686, F1 Macro: 0.4099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2252, Accuracy: 0.9132, F1 Micro: 0.7417, F1 Macro: 0.5844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1793, Accuracy: 0.9216, F1 Micro: 0.7603, F1 Macro: 0.5981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1466, Accuracy: 0.9249, F1 Micro: 0.7746, F1 Macro: 0.6814\n",
      "Epoch 5/10, Train Loss: 0.1152, Accuracy: 0.9216, F1 Micro: 0.7694, F1 Macro: 0.6738\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9209, F1 Micro: 0.7743, F1 Macro: 0.6827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0746, Accuracy: 0.9246, F1 Micro: 0.7786, F1 Macro: 0.7045\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9185, F1 Micro: 0.7679, F1 Macro: 0.691\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9229, F1 Micro: 0.7764, F1 Macro: 0.7087\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.9243, F1 Micro: 0.7724, F1 Macro: 0.712\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9246, F1 Micro: 0.7786, F1 Macro: 0.7045\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.70      0.79      0.74       732\n",
      "     HS_Group       0.76      0.54      0.63       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.81      0.35      0.49        72\n",
      "    HS_Gender       0.53      0.55      0.54        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.70      0.46      0.55       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 281.627876996994 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3416, Accuracy: 0.8983, F1 Micro: 0.6635, F1 Macro: 0.3961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2226, Accuracy: 0.9134, F1 Micro: 0.7449, F1 Macro: 0.596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.177, Accuracy: 0.9241, F1 Micro: 0.7704, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1443, Accuracy: 0.9255, F1 Micro: 0.7764, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.114, Accuracy: 0.9226, F1 Micro: 0.7786, F1 Macro: 0.6889\n",
      "Epoch 6/10, Train Loss: 0.0887, Accuracy: 0.9236, F1 Micro: 0.777, F1 Macro: 0.6854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9254, F1 Micro: 0.7823, F1 Macro: 0.7148\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9212, F1 Micro: 0.7747, F1 Macro: 0.7035\n",
      "Epoch 9/10, Train Loss: 0.0517, Accuracy: 0.9253, F1 Micro: 0.7813, F1 Macro: 0.7174\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9243, F1 Micro: 0.775, F1 Macro: 0.7145\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9254, F1 Micro: 0.7823, F1 Macro: 0.7148\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.67      0.67      0.67       157\n",
      "      HS_Race       0.80      0.71      0.75       120\n",
      "  HS_Physical       0.85      0.32      0.46        72\n",
      "    HS_Gender       0.58      0.51      0.54        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.84      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.69      0.71      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 283.8678765296936 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9179, F1 Micro: 0.7586, F1 Macro: 0.6488\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.62597370147705 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3323, Accuracy: 0.8974, F1 Micro: 0.7047, F1 Macro: 0.5209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2268, Accuracy: 0.9154, F1 Micro: 0.7533, F1 Macro: 0.6044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1827, Accuracy: 0.9206, F1 Micro: 0.771, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1407, Accuracy: 0.9251, F1 Micro: 0.7817, F1 Macro: 0.6624\n",
      "Epoch 5/10, Train Loss: 0.1107, Accuracy: 0.9213, F1 Micro: 0.7778, F1 Macro: 0.6743\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9263, F1 Micro: 0.7781, F1 Macro: 0.696\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.9247, F1 Micro: 0.7808, F1 Macro: 0.7062\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.924, F1 Micro: 0.7805, F1 Macro: 0.7137\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9162, F1 Micro: 0.7667, F1 Macro: 0.6975\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9163, F1 Micro: 0.7719, F1 Macro: 0.7042\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9251, F1 Micro: 0.7817, F1 Macro: 0.6624\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.78      0.75       732\n",
      "     HS_Group       0.76      0.61      0.68       402\n",
      "  HS_Religion       0.68      0.67      0.67       157\n",
      "      HS_Race       0.88      0.61      0.72       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.64      0.18      0.28        51\n",
      "     HS_Other       0.77      0.83      0.80       762\n",
      "      HS_Weak       0.70      0.76      0.73       689\n",
      "  HS_Moderate       0.69      0.52      0.59       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.64      0.66      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 284.6475727558136 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3328, Accuracy: 0.8984, F1 Micro: 0.7067, F1 Macro: 0.5286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2263, Accuracy: 0.9151, F1 Micro: 0.7499, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.183, Accuracy: 0.9199, F1 Micro: 0.7683, F1 Macro: 0.6463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1408, Accuracy: 0.9241, F1 Micro: 0.7755, F1 Macro: 0.6574\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.921, F1 Micro: 0.7751, F1 Macro: 0.6854\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9236, F1 Micro: 0.7753, F1 Macro: 0.6942\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9179, F1 Micro: 0.7692, F1 Macro: 0.6897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0575, Accuracy: 0.9256, F1 Micro: 0.7823, F1 Macro: 0.717\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9175, F1 Micro: 0.7716, F1 Macro: 0.7018\n",
      "Epoch 10/10, Train Loss: 0.0427, Accuracy: 0.9227, F1 Micro: 0.7783, F1 Macro: 0.705\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9256, F1 Micro: 0.7823, F1 Macro: 0.717\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.79      0.63      0.70       157\n",
      "      HS_Race       0.74      0.75      0.75       120\n",
      "  HS_Physical       0.68      0.36      0.47        72\n",
      "    HS_Gender       0.55      0.55      0.55        51\n",
      "     HS_Other       0.80      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.75      0.70      0.72      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 288.0246000289917 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3366, Accuracy: 0.8968, F1 Micro: 0.7036, F1 Macro: 0.5275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.9178, F1 Micro: 0.7568, F1 Macro: 0.6045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1817, Accuracy: 0.9218, F1 Micro: 0.7725, F1 Macro: 0.6599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1383, Accuracy: 0.9253, F1 Micro: 0.7771, F1 Macro: 0.6669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1066, Accuracy: 0.9239, F1 Micro: 0.7809, F1 Macro: 0.6963\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.9238, F1 Micro: 0.7768, F1 Macro: 0.7022\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9239, F1 Micro: 0.7804, F1 Macro: 0.7038\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.9253, F1 Micro: 0.7708, F1 Macro: 0.7087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.925, F1 Micro: 0.7823, F1 Macro: 0.72\n",
      "Epoch 10/10, Train Loss: 0.0425, Accuracy: 0.9242, F1 Micro: 0.7779, F1 Macro: 0.713\n",
      "Model 3 - Iteration 10018: Accuracy: 0.925, F1 Micro: 0.7823, F1 Macro: 0.72\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.91      0.91       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.68      0.69      0.69       402\n",
      "  HS_Religion       0.77      0.54      0.63       157\n",
      "      HS_Race       0.72      0.78      0.74       120\n",
      "  HS_Physical       0.69      0.35      0.46        72\n",
      "    HS_Gender       0.66      0.61      0.63        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.74      0.71      0.72       689\n",
      "  HS_Moderate       0.61      0.62      0.62       331\n",
      "    HS_Strong       0.89      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.70      0.72      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 288.14368319511414 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9182, F1 Micro: 0.7595, F1 Macro: 0.6509\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.119313478469849 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3257, Accuracy: 0.9012, F1 Micro: 0.7018, F1 Macro: 0.475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2228, Accuracy: 0.9167, F1 Micro: 0.7399, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1752, Accuracy: 0.9246, F1 Micro: 0.7679, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1419, Accuracy: 0.9209, F1 Micro: 0.7775, F1 Macro: 0.6794\n",
      "Epoch 5/10, Train Loss: 0.1133, Accuracy: 0.9199, F1 Micro: 0.7702, F1 Macro: 0.6756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9262, F1 Micro: 0.7837, F1 Macro: 0.7015\n",
      "Epoch 7/10, Train Loss: 0.0733, Accuracy: 0.9244, F1 Micro: 0.7771, F1 Macro: 0.7077\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9252, F1 Micro: 0.7786, F1 Macro: 0.7168\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9259, F1 Micro: 0.7795, F1 Macro: 0.7122\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.923, F1 Micro: 0.7668, F1 Macro: 0.7009\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9262, F1 Micro: 0.7837, F1 Macro: 0.7015\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.73      0.66      0.70       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.56      0.49      0.52        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.66      0.59      0.62       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 292.51667523384094 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3277, Accuracy: 0.9015, F1 Micro: 0.7005, F1 Macro: 0.4682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.225, Accuracy: 0.9184, F1 Micro: 0.743, F1 Macro: 0.5898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1759, Accuracy: 0.9223, F1 Micro: 0.762, F1 Macro: 0.618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1415, Accuracy: 0.9215, F1 Micro: 0.7766, F1 Macro: 0.6832\n",
      "Epoch 5/10, Train Loss: 0.1141, Accuracy: 0.9244, F1 Micro: 0.7737, F1 Macro: 0.6776\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9229, F1 Micro: 0.7749, F1 Macro: 0.704\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9241, F1 Micro: 0.7649, F1 Macro: 0.6942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9239, F1 Micro: 0.7772, F1 Macro: 0.7073\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9223, F1 Micro: 0.7568, F1 Macro: 0.6886\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9247, F1 Micro: 0.7732, F1 Macro: 0.7094\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9239, F1 Micro: 0.7772, F1 Macro: 0.7073\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.91      0.91       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.77      0.63      0.69       120\n",
      "  HS_Physical       0.75      0.33      0.46        72\n",
      "    HS_Gender       0.59      0.51      0.55        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.71      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 291.73899507522583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3309, Accuracy: 0.9025, F1 Micro: 0.7017, F1 Macro: 0.4801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2233, Accuracy: 0.9177, F1 Micro: 0.7405, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1732, Accuracy: 0.9225, F1 Micro: 0.7654, F1 Macro: 0.6317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1414, Accuracy: 0.9208, F1 Micro: 0.7781, F1 Macro: 0.683\n",
      "Epoch 5/10, Train Loss: 0.1106, Accuracy: 0.9165, F1 Micro: 0.7707, F1 Macro: 0.6851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.922, F1 Micro: 0.7793, F1 Macro: 0.7101\n",
      "Epoch 7/10, Train Loss: 0.0697, Accuracy: 0.9263, F1 Micro: 0.7787, F1 Macro: 0.705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0592, Accuracy: 0.9252, F1 Micro: 0.7814, F1 Macro: 0.7156\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9217, F1 Micro: 0.7788, F1 Macro: 0.714\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9251, F1 Micro: 0.7705, F1 Macro: 0.7122\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9252, F1 Micro: 0.7814, F1 Macro: 0.7156\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.74      0.62      0.67       402\n",
      "  HS_Religion       0.79      0.60      0.68       157\n",
      "      HS_Race       0.81      0.69      0.74       120\n",
      "  HS_Physical       0.68      0.36      0.47        72\n",
      "    HS_Gender       0.58      0.57      0.57        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.69      0.72      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 293.192421913147 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9185, F1 Micro: 0.7603, F1 Macro: 0.6531\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.8699095249176025 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3248, Accuracy: 0.9031, F1 Micro: 0.6771, F1 Macro: 0.4552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2127, Accuracy: 0.9184, F1 Micro: 0.7499, F1 Macro: 0.5849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1698, Accuracy: 0.9234, F1 Micro: 0.7676, F1 Macro: 0.6229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9226, F1 Micro: 0.7754, F1 Macro: 0.651\n",
      "Epoch 5/10, Train Loss: 0.1084, Accuracy: 0.9182, F1 Micro: 0.7713, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0826, Accuracy: 0.9228, F1 Micro: 0.7765, F1 Macro: 0.6885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0712, Accuracy: 0.9224, F1 Micro: 0.7785, F1 Macro: 0.7093\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9217, F1 Micro: 0.7744, F1 Macro: 0.7113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9235, F1 Micro: 0.7796, F1 Macro: 0.7057\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9246, F1 Micro: 0.7749, F1 Macro: 0.7026\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9235, F1 Micro: 0.7796, F1 Macro: 0.7057\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.68      0.68      0.68       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.81      0.29      0.43        72\n",
      "    HS_Gender       0.53      0.49      0.51        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.73      0.72       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 300.298002243042 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3249, Accuracy: 0.9039, F1 Micro: 0.6835, F1 Macro: 0.4544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2136, Accuracy: 0.9189, F1 Micro: 0.7526, F1 Macro: 0.5883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1689, Accuracy: 0.9231, F1 Micro: 0.7676, F1 Macro: 0.6334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1411, Accuracy: 0.9206, F1 Micro: 0.7708, F1 Macro: 0.6388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.111, Accuracy: 0.9214, F1 Micro: 0.7714, F1 Macro: 0.6661\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.92, F1 Micro: 0.77, F1 Macro: 0.6876\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.9218, F1 Micro: 0.7706, F1 Macro: 0.6957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.927, F1 Micro: 0.7804, F1 Macro: 0.7167\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9234, F1 Micro: 0.7717, F1 Macro: 0.7081\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.9221, F1 Micro: 0.7751, F1 Macro: 0.7111\n",
      "Model 2 - Iteration 10418: Accuracy: 0.927, F1 Micro: 0.7804, F1 Macro: 0.7167\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.76      0.71      0.74       732\n",
      "     HS_Group       0.76      0.65      0.70       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.80      0.33      0.47        72\n",
      "    HS_Gender       0.57      0.57      0.57        51\n",
      "     HS_Other       0.82      0.74      0.78       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.68      0.58      0.63       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.77      0.68      0.72      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 298.3105673789978 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3309, Accuracy: 0.9025, F1 Micro: 0.6738, F1 Macro: 0.4544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2136, Accuracy: 0.9185, F1 Micro: 0.7548, F1 Macro: 0.5936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1676, Accuracy: 0.9249, F1 Micro: 0.771, F1 Macro: 0.6194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.9254, F1 Micro: 0.7767, F1 Macro: 0.6736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1088, Accuracy: 0.9239, F1 Micro: 0.7799, F1 Macro: 0.6948\n",
      "Epoch 6/10, Train Loss: 0.0826, Accuracy: 0.9237, F1 Micro: 0.7773, F1 Macro: 0.687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0692, Accuracy: 0.9226, F1 Micro: 0.7817, F1 Macro: 0.7138\n",
      "Epoch 8/10, Train Loss: 0.0544, Accuracy: 0.9246, F1 Micro: 0.7753, F1 Macro: 0.71\n",
      "Epoch 9/10, Train Loss: 0.0466, Accuracy: 0.9257, F1 Micro: 0.7785, F1 Macro: 0.7111\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.9229, F1 Micro: 0.7762, F1 Macro: 0.7102\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9226, F1 Micro: 0.7817, F1 Macro: 0.7138\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.65      0.69      0.67       157\n",
      "      HS_Race       0.77      0.76      0.76       120\n",
      "  HS_Physical       0.77      0.32      0.45        72\n",
      "    HS_Gender       0.54      0.51      0.53        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.67      0.77      0.72       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.73      0.71      0.71      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.44      5556\n",
      "\n",
      "Training completed in 297.91416668891907 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9187, F1 Micro: 0.7611, F1 Macro: 0.6553\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.2103641033172607 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.322, Accuracy: 0.8999, F1 Micro: 0.6586, F1 Macro: 0.3794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2194, Accuracy: 0.9171, F1 Micro: 0.7469, F1 Macro: 0.5808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1713, Accuracy: 0.9223, F1 Micro: 0.7552, F1 Macro: 0.6091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.136, Accuracy: 0.9269, F1 Micro: 0.7813, F1 Macro: 0.6625\n",
      "Epoch 5/10, Train Loss: 0.1112, Accuracy: 0.9241, F1 Micro: 0.771, F1 Macro: 0.6521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9257, F1 Micro: 0.7821, F1 Macro: 0.695\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9244, F1 Micro: 0.7799, F1 Macro: 0.6938\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9252, F1 Micro: 0.7703, F1 Macro: 0.6978\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9235, F1 Micro: 0.7768, F1 Macro: 0.7065\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.9187, F1 Micro: 0.7677, F1 Macro: 0.6934\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9257, F1 Micro: 0.7821, F1 Macro: 0.695\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.71      0.68      0.69       402\n",
      "  HS_Religion       0.74      0.64      0.68       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.58      0.35      0.44        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.73      0.72      0.73       689\n",
      "  HS_Moderate       0.63      0.58      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 299.3354160785675 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3231, Accuracy: 0.899, F1 Micro: 0.6547, F1 Macro: 0.3739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2211, Accuracy: 0.916, F1 Micro: 0.7412, F1 Macro: 0.5746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1726, Accuracy: 0.9229, F1 Micro: 0.758, F1 Macro: 0.6178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1397, Accuracy: 0.9251, F1 Micro: 0.7705, F1 Macro: 0.639\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.9211, F1 Micro: 0.77, F1 Macro: 0.6628\n",
      "Epoch 6/10, Train Loss: 0.0874, Accuracy: 0.918, F1 Micro: 0.7682, F1 Macro: 0.6801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9228, F1 Micro: 0.7719, F1 Macro: 0.6939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.9238, F1 Micro: 0.7778, F1 Macro: 0.712\n",
      "Epoch 9/10, Train Loss: 0.0457, Accuracy: 0.9245, F1 Micro: 0.7732, F1 Macro: 0.7024\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9218, F1 Micro: 0.7755, F1 Macro: 0.7073\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9238, F1 Micro: 0.7778, F1 Macro: 0.712\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.68      0.68      0.68       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.79      0.32      0.46        72\n",
      "    HS_Gender       0.57      0.55      0.56        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.70      0.71      5556\n",
      " weighted avg       0.78      0.77      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 301.0537962913513 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3263, Accuracy: 0.8976, F1 Micro: 0.6573, F1 Macro: 0.3714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2187, Accuracy: 0.916, F1 Micro: 0.741, F1 Macro: 0.5814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1717, Accuracy: 0.9229, F1 Micro: 0.7522, F1 Macro: 0.6219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1356, Accuracy: 0.926, F1 Micro: 0.7717, F1 Macro: 0.6657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.9247, F1 Micro: 0.7792, F1 Macro: 0.685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9243, F1 Micro: 0.7838, F1 Macro: 0.7107\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.9252, F1 Micro: 0.783, F1 Macro: 0.7077\n",
      "Epoch 8/10, Train Loss: 0.0526, Accuracy: 0.9249, F1 Micro: 0.7697, F1 Macro: 0.7084\n",
      "Epoch 9/10, Train Loss: 0.0468, Accuracy: 0.9235, F1 Micro: 0.7755, F1 Macro: 0.7035\n",
      "Epoch 10/10, Train Loss: 0.0409, Accuracy: 0.9214, F1 Micro: 0.7757, F1 Macro: 0.7145\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9243, F1 Micro: 0.7838, F1 Macro: 0.7107\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.90      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.75      0.74      0.74       732\n",
      "     HS_Group       0.65      0.73      0.69       402\n",
      "  HS_Religion       0.69      0.66      0.68       157\n",
      "      HS_Race       0.78      0.73      0.76       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.61      0.49      0.54        51\n",
      "     HS_Other       0.77      0.82      0.80       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.59      0.66      0.62       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5556\n",
      "    macro avg       0.76      0.70      0.71      5556\n",
      " weighted avg       0.77      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 301.87245202064514 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9189, F1 Micro: 0.7618, F1 Macro: 0.6571\n",
      "Total sampling time: 1231.11 seconds\n",
      "Total runtime: 20204.94789791107 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a84e8",
   "metadata": {
    "papermill": {
     "duration": 0.174018,
     "end_time": "2025-03-15T13:24:24.096891",
     "exception": false,
     "start_time": "2025-03-15T13:24:23.922873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3650420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.8281, F1 Micro: 0.0314, F1 Macro: 0.0116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4135, Accuracy: 0.8311, F1 Micro: 0.0509, F1 Macro: 0.0214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3878, Accuracy: 0.8455, F1 Micro: 0.2462, F1 Macro: 0.085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3631, Accuracy: 0.861, F1 Micro: 0.4165, F1 Macro: 0.1586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3274, Accuracy: 0.8707, F1 Micro: 0.5642, F1 Macro: 0.269\n",
      "Epoch 6/10, Train Loss: 0.2692, Accuracy: 0.8748, F1 Micro: 0.5337, F1 Macro: 0.2575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2499, Accuracy: 0.8752, F1 Micro: 0.6045, F1 Macro: 0.3351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2231, Accuracy: 0.8783, F1 Micro: 0.6121, F1 Macro: 0.3459\n",
      "Epoch 9/10, Train Loss: 0.1835, Accuracy: 0.8792, F1 Micro: 0.5733, F1 Macro: 0.3269\n",
      "Epoch 10/10, Train Loss: 0.1776, Accuracy: 0.8814, F1 Micro: 0.581, F1 Macro: 0.341\n",
      "Model 1 - Iteration 658: Accuracy: 0.8783, F1 Micro: 0.6121, F1 Macro: 0.3459\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.79      0.78      1134\n",
      "      Abusive       0.83      0.73      0.78       992\n",
      "HS_Individual       0.64      0.51      0.57       732\n",
      "     HS_Group       0.54      0.45      0.49       402\n",
      "  HS_Religion       0.75      0.02      0.04       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.62      0.63       762\n",
      "      HS_Weak       0.58      0.49      0.54       689\n",
      "  HS_Moderate       0.38      0.30      0.33       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.68      0.56      0.61      5556\n",
      "    macro avg       0.43      0.33      0.35      5556\n",
      " weighted avg       0.63      0.56      0.58      5556\n",
      "  samples avg       0.37      0.31      0.31      5556\n",
      "\n",
      "Training completed in 56.67655372619629 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5923, Accuracy: 0.8372, F1 Micro: 0.3946, F1 Macro: 0.1088\n",
      "Epoch 2/10, Train Loss: 0.4278, Accuracy: 0.8351, F1 Micro: 0.1029, F1 Macro: 0.04\n",
      "Epoch 3/10, Train Loss: 0.3942, Accuracy: 0.8379, F1 Micro: 0.1413, F1 Macro: 0.0519\n",
      "Epoch 4/10, Train Loss: 0.3747, Accuracy: 0.8553, F1 Micro: 0.3433, F1 Macro: 0.112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3417, Accuracy: 0.8726, F1 Micro: 0.5182, F1 Macro: 0.2254\n",
      "Epoch 6/10, Train Loss: 0.2919, Accuracy: 0.8729, F1 Micro: 0.497, F1 Macro: 0.2291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.27, Accuracy: 0.876, F1 Micro: 0.6096, F1 Macro: 0.3093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2396, Accuracy: 0.8806, F1 Micro: 0.6149, F1 Macro: 0.3259\n",
      "Epoch 9/10, Train Loss: 0.203, Accuracy: 0.8803, F1 Micro: 0.6002, F1 Macro: 0.3225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1953, Accuracy: 0.8814, F1 Micro: 0.6171, F1 Macro: 0.3354\n",
      "Model 2 - Iteration 658: Accuracy: 0.8814, F1 Micro: 0.6171, F1 Macro: 0.3354\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.77      0.78      1134\n",
      "      Abusive       0.80      0.79      0.80       992\n",
      "HS_Individual       0.63      0.54      0.58       732\n",
      "     HS_Group       0.60      0.33      0.42       402\n",
      "  HS_Religion       0.33      0.01      0.01       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.63      0.64       762\n",
      "      HS_Weak       0.58      0.53      0.55       689\n",
      "  HS_Moderate       0.41      0.17      0.24       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.55      0.62      5556\n",
      "    macro avg       0.40      0.31      0.34      5556\n",
      " weighted avg       0.63      0.55      0.58      5556\n",
      "  samples avg       0.38      0.32      0.32      5556\n",
      "\n",
      "Training completed in 54.07783102989197 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.8304, F1 Micro: 0.0856, F1 Macro: 0.0322\n",
      "Epoch 2/10, Train Loss: 0.4221, Accuracy: 0.8323, F1 Micro: 0.0663, F1 Macro: 0.0261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3932, Accuracy: 0.842, F1 Micro: 0.2123, F1 Macro: 0.0678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3734, Accuracy: 0.8531, F1 Micro: 0.3384, F1 Macro: 0.1079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3428, Accuracy: 0.8694, F1 Micro: 0.4773, F1 Macro: 0.2023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2886, Accuracy: 0.8748, F1 Micro: 0.5235, F1 Macro: 0.2548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.262, Accuracy: 0.8767, F1 Micro: 0.6086, F1 Macro: 0.3292\n",
      "Epoch 8/10, Train Loss: 0.2371, Accuracy: 0.8816, F1 Micro: 0.6037, F1 Macro: 0.3434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1951, Accuracy: 0.8802, F1 Micro: 0.6101, F1 Macro: 0.364\n",
      "Epoch 10/10, Train Loss: 0.1868, Accuracy: 0.8808, F1 Micro: 0.6095, F1 Macro: 0.3499\n",
      "Model 3 - Iteration 658: Accuracy: 0.8802, F1 Micro: 0.6101, F1 Macro: 0.364\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.75      0.77      1134\n",
      "      Abusive       0.84      0.75      0.79       992\n",
      "HS_Individual       0.67      0.45      0.54       732\n",
      "     HS_Group       0.51      0.48      0.49       402\n",
      "  HS_Religion       0.52      0.15      0.24       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.60      0.63       762\n",
      "      HS_Weak       0.66      0.42      0.51       689\n",
      "  HS_Moderate       0.38      0.40      0.39       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.54      0.61      5556\n",
      "    macro avg       0.42      0.33      0.36      5556\n",
      " weighted avg       0.65      0.54      0.59      5556\n",
      "  samples avg       0.37      0.31      0.31      5556\n",
      "\n",
      "Training completed in 57.146459102630615 s\n",
      "Averaged - Iteration 658: Accuracy: 0.88, F1 Micro: 0.6131, F1 Macro: 0.3484\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 129.5409209728241 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4649, Accuracy: 0.8401, F1 Micro: 0.2008, F1 Macro: 0.0669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3629, Accuracy: 0.8659, F1 Micro: 0.4355, F1 Macro: 0.1813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3106, Accuracy: 0.8832, F1 Micro: 0.5723, F1 Macro: 0.2879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2641, Accuracy: 0.8919, F1 Micro: 0.6371, F1 Macro: 0.3956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2231, Accuracy: 0.8942, F1 Micro: 0.6654, F1 Macro: 0.4491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1896, Accuracy: 0.896, F1 Micro: 0.6777, F1 Macro: 0.4755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.154, Accuracy: 0.8969, F1 Micro: 0.6812, F1 Macro: 0.4859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1404, Accuracy: 0.8982, F1 Micro: 0.6877, F1 Macro: 0.5212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1185, Accuracy: 0.897, F1 Micro: 0.6929, F1 Macro: 0.5292\n",
      "Epoch 10/10, Train Loss: 0.1003, Accuracy: 0.8986, F1 Micro: 0.6892, F1 Macro: 0.5291\n",
      "Model 1 - Iteration 1646: Accuracy: 0.897, F1 Micro: 0.6929, F1 Macro: 0.5292\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.81      0.81      1134\n",
      "      Abusive       0.84      0.86      0.85       992\n",
      "HS_Individual       0.66      0.61      0.64       732\n",
      "     HS_Group       0.60      0.61      0.60       402\n",
      "  HS_Religion       0.60      0.55      0.57       157\n",
      "      HS_Race       0.81      0.47      0.59       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.70      0.69      0.69       762\n",
      "      HS_Weak       0.63      0.60      0.61       689\n",
      "  HS_Moderate       0.47      0.52      0.49       331\n",
      "    HS_Strong       0.93      0.33      0.49       114\n",
      "\n",
      "    micro avg       0.71      0.67      0.69      5556\n",
      "    macro avg       0.59      0.50      0.53      5556\n",
      " weighted avg       0.70      0.67      0.68      5556\n",
      "  samples avg       0.39      0.38      0.37      5556\n",
      "\n",
      "Training completed in 85.23712491989136 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4926, Accuracy: 0.8468, F1 Micro: 0.2711, F1 Macro: 0.0886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3705, Accuracy: 0.8649, F1 Micro: 0.4295, F1 Macro: 0.1655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3224, Accuracy: 0.8787, F1 Micro: 0.5383, F1 Macro: 0.2521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.276, Accuracy: 0.8871, F1 Micro: 0.6157, F1 Macro: 0.3395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2362, Accuracy: 0.8903, F1 Micro: 0.664, F1 Macro: 0.4334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2047, Accuracy: 0.8929, F1 Micro: 0.6744, F1 Macro: 0.4318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1712, Accuracy: 0.8938, F1 Micro: 0.6772, F1 Macro: 0.4391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1546, Accuracy: 0.8968, F1 Micro: 0.6809, F1 Macro: 0.4884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1263, Accuracy: 0.8966, F1 Micro: 0.6838, F1 Macro: 0.5213\n",
      "Epoch 10/10, Train Loss: 0.1085, Accuracy: 0.8972, F1 Micro: 0.6758, F1 Macro: 0.5089\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8966, F1 Micro: 0.6838, F1 Macro: 0.5213\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.79      0.81      1134\n",
      "      Abusive       0.83      0.85      0.84       992\n",
      "HS_Individual       0.68      0.55      0.61       732\n",
      "     HS_Group       0.60      0.60      0.60       402\n",
      "  HS_Religion       0.60      0.55      0.57       157\n",
      "      HS_Race       0.79      0.46      0.58       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.74      0.65      0.69       762\n",
      "      HS_Weak       0.65      0.55      0.60       689\n",
      "  HS_Moderate       0.45      0.51      0.48       331\n",
      "    HS_Strong       0.95      0.32      0.48       114\n",
      "\n",
      "    micro avg       0.72      0.65      0.68      5556\n",
      "    macro avg       0.59      0.49      0.52      5556\n",
      " weighted avg       0.71      0.65      0.67      5556\n",
      "  samples avg       0.39      0.36      0.36      5556\n",
      "\n",
      "Training completed in 85.96268773078918 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4766, Accuracy: 0.8395, F1 Micro: 0.1975, F1 Macro: 0.0591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3717, Accuracy: 0.8612, F1 Micro: 0.4199, F1 Macro: 0.147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3207, Accuracy: 0.8804, F1 Micro: 0.5431, F1 Macro: 0.2683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2715, Accuracy: 0.8904, F1 Micro: 0.6371, F1 Macro: 0.3772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2303, Accuracy: 0.8925, F1 Micro: 0.666, F1 Macro: 0.4479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1964, Accuracy: 0.8916, F1 Micro: 0.6887, F1 Macro: 0.4986\n",
      "Epoch 7/10, Train Loss: 0.1647, Accuracy: 0.895, F1 Micro: 0.6831, F1 Macro: 0.4817\n",
      "Epoch 8/10, Train Loss: 0.1464, Accuracy: 0.8978, F1 Micro: 0.6805, F1 Macro: 0.4885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1204, Accuracy: 0.897, F1 Micro: 0.6936, F1 Macro: 0.5153\n",
      "Epoch 10/10, Train Loss: 0.105, Accuracy: 0.898, F1 Micro: 0.6877, F1 Macro: 0.5245\n",
      "Model 3 - Iteration 1646: Accuracy: 0.897, F1 Micro: 0.6936, F1 Macro: 0.5153\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.81      0.81      1134\n",
      "      Abusive       0.85      0.84      0.85       992\n",
      "HS_Individual       0.65      0.66      0.66       732\n",
      "     HS_Group       0.63      0.52      0.57       402\n",
      "  HS_Religion       0.61      0.53      0.56       157\n",
      "      HS_Race       0.79      0.40      0.53       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.68      0.72      0.70       762\n",
      "      HS_Weak       0.61      0.66      0.63       689\n",
      "  HS_Moderate       0.46      0.43      0.45       331\n",
      "    HS_Strong       0.94      0.27      0.42       114\n",
      "\n",
      "    micro avg       0.71      0.68      0.69      5556\n",
      "    macro avg       0.59      0.49      0.52      5556\n",
      " weighted avg       0.70      0.68      0.68      5556\n",
      "  samples avg       0.40      0.38      0.37      5556\n",
      "\n",
      "Training completed in 82.37962508201599 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8884, F1 Micro: 0.6516, F1 Macro: 0.4352\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 116.03459811210632 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.436, Accuracy: 0.8536, F1 Micro: 0.3412, F1 Macro: 0.1057\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3213, Accuracy: 0.8784, F1 Micro: 0.5208, F1 Macro: 0.25\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.266, Accuracy: 0.8964, F1 Micro: 0.6666, F1 Macro: 0.4233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2181, Accuracy: 0.9017, F1 Micro: 0.6843, F1 Macro: 0.4796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1835, Accuracy: 0.904, F1 Micro: 0.7, F1 Macro: 0.5251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1522, Accuracy: 0.8996, F1 Micro: 0.7059, F1 Macro: 0.5426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1226, Accuracy: 0.9053, F1 Micro: 0.7061, F1 Macro: 0.5506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1023, Accuracy: 0.9055, F1 Micro: 0.7089, F1 Macro: 0.547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9078, F1 Micro: 0.7163, F1 Macro: 0.5694\n",
      "Epoch 10/10, Train Loss: 0.0749, Accuracy: 0.9073, F1 Micro: 0.714, F1 Macro: 0.5707\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9078, F1 Micro: 0.7163, F1 Macro: 0.5694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.79      0.81      1134\n",
      "      Abusive       0.87      0.83      0.85       992\n",
      "HS_Individual       0.69      0.67      0.68       732\n",
      "     HS_Group       0.70      0.52      0.60       402\n",
      "  HS_Religion       0.64      0.52      0.57       157\n",
      "      HS_Race       0.82      0.55      0.66       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.75      0.71      0.73       762\n",
      "      HS_Weak       0.65      0.62      0.63       689\n",
      "  HS_Moderate       0.61      0.43      0.50       331\n",
      "    HS_Strong       0.89      0.72      0.80       114\n",
      "\n",
      "    micro avg       0.76      0.68      0.72      5556\n",
      "    macro avg       0.62      0.53      0.57      5556\n",
      " weighted avg       0.74      0.68      0.71      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 109.74415063858032 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4568, Accuracy: 0.8535, F1 Micro: 0.3415, F1 Macro: 0.106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3339, Accuracy: 0.8779, F1 Micro: 0.5285, F1 Macro: 0.2512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2787, Accuracy: 0.8923, F1 Micro: 0.6514, F1 Macro: 0.3733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2309, Accuracy: 0.8985, F1 Micro: 0.6691, F1 Macro: 0.4324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1955, Accuracy: 0.8978, F1 Micro: 0.6992, F1 Macro: 0.5223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1638, Accuracy: 0.9009, F1 Micro: 0.7003, F1 Macro: 0.5426\n",
      "Epoch 7/10, Train Loss: 0.1351, Accuracy: 0.9037, F1 Micro: 0.6866, F1 Macro: 0.5213\n",
      "Epoch 8/10, Train Loss: 0.1118, Accuracy: 0.9041, F1 Micro: 0.6877, F1 Macro: 0.5243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0936, Accuracy: 0.9028, F1 Micro: 0.7025, F1 Macro: 0.5442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0833, Accuracy: 0.9072, F1 Micro: 0.7135, F1 Macro: 0.5687\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9072, F1 Micro: 0.7135, F1 Macro: 0.5687\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.79      0.82      1134\n",
      "      Abusive       0.84      0.85      0.85       992\n",
      "HS_Individual       0.67      0.67      0.67       732\n",
      "     HS_Group       0.75      0.50      0.60       402\n",
      "  HS_Religion       0.69      0.52      0.59       157\n",
      "      HS_Race       0.84      0.48      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.77      0.67      0.72       762\n",
      "      HS_Weak       0.65      0.65      0.65       689\n",
      "  HS_Moderate       0.63      0.37      0.47       331\n",
      "    HS_Strong       0.87      0.61      0.72       114\n",
      "\n",
      "    micro avg       0.76      0.67      0.71      5556\n",
      "    macro avg       0.69      0.52      0.57      5556\n",
      " weighted avg       0.75      0.67      0.70      5556\n",
      "  samples avg       0.41      0.38      0.37      5556\n",
      "\n",
      "Training completed in 106.51092267036438 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4454, Accuracy: 0.8528, F1 Micro: 0.3569, F1 Macro: 0.109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.33, Accuracy: 0.8796, F1 Micro: 0.5405, F1 Macro: 0.259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2731, Accuracy: 0.8943, F1 Micro: 0.6477, F1 Macro: 0.3792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2238, Accuracy: 0.9021, F1 Micro: 0.6905, F1 Macro: 0.4825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.187, Accuracy: 0.8985, F1 Micro: 0.7027, F1 Macro: 0.5148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.152, Accuracy: 0.9, F1 Micro: 0.7095, F1 Macro: 0.546\n",
      "Epoch 7/10, Train Loss: 0.1302, Accuracy: 0.9047, F1 Micro: 0.6991, F1 Macro: 0.5368\n",
      "Epoch 8/10, Train Loss: 0.1079, Accuracy: 0.9063, F1 Micro: 0.7073, F1 Macro: 0.5524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0907, Accuracy: 0.9066, F1 Micro: 0.7255, F1 Macro: 0.5849\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9068, F1 Micro: 0.7233, F1 Macro: 0.5724\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9066, F1 Micro: 0.7255, F1 Macro: 0.5849\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.84      0.82      1134\n",
      "      Abusive       0.87      0.83      0.85       992\n",
      "HS_Individual       0.65      0.72      0.68       732\n",
      "     HS_Group       0.69      0.53      0.60       402\n",
      "  HS_Religion       0.66      0.59      0.63       157\n",
      "      HS_Race       0.77      0.60      0.68       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.04      0.08        51\n",
      "     HS_Other       0.71      0.78      0.74       762\n",
      "      HS_Weak       0.63      0.69      0.66       689\n",
      "  HS_Moderate       0.59      0.43      0.50       331\n",
      "    HS_Strong       0.86      0.72      0.78       114\n",
      "\n",
      "    micro avg       0.73      0.72      0.73      5556\n",
      "    macro avg       0.69      0.56      0.58      5556\n",
      " weighted avg       0.73      0.72      0.72      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 105.71144723892212 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8947, F1 Micro: 0.6739, F1 Macro: 0.4816\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 104.00116109848022 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4169, Accuracy: 0.8738, F1 Micro: 0.5656, F1 Macro: 0.2589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3034, Accuracy: 0.8926, F1 Micro: 0.6331, F1 Macro: 0.3778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2455, Accuracy: 0.8977, F1 Micro: 0.6986, F1 Macro: 0.5219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2046, Accuracy: 0.903, F1 Micro: 0.7118, F1 Macro: 0.554\n",
      "Epoch 5/10, Train Loss: 0.1674, Accuracy: 0.9078, F1 Micro: 0.7065, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1384, Accuracy: 0.9038, F1 Micro: 0.7264, F1 Macro: 0.5699\n",
      "Epoch 7/10, Train Loss: 0.1126, Accuracy: 0.9079, F1 Micro: 0.7228, F1 Macro: 0.5678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0957, Accuracy: 0.9107, F1 Micro: 0.7363, F1 Macro: 0.5949\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9124, F1 Micro: 0.7343, F1 Macro: 0.6182\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9101, F1 Micro: 0.7183, F1 Macro: 0.6046\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9107, F1 Micro: 0.7363, F1 Macro: 0.5949\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.83      1134\n",
      "      Abusive       0.87      0.86      0.86       992\n",
      "HS_Individual       0.68      0.73      0.70       732\n",
      "     HS_Group       0.69      0.53      0.60       402\n",
      "  HS_Religion       0.72      0.49      0.58       157\n",
      "      HS_Race       0.76      0.63      0.69       120\n",
      "  HS_Physical       0.60      0.04      0.08        72\n",
      "    HS_Gender       0.50      0.06      0.11        51\n",
      "     HS_Other       0.72      0.79      0.75       762\n",
      "      HS_Weak       0.65      0.70      0.68       689\n",
      "  HS_Moderate       0.61      0.42      0.50       331\n",
      "    HS_Strong       0.86      0.67      0.75       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.74      5556\n",
      "    macro avg       0.71      0.56      0.59      5556\n",
      " weighted avg       0.74      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.40      5556\n",
      "\n",
      "Training completed in 123.88883924484253 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4335, Accuracy: 0.8712, F1 Micro: 0.5035, F1 Macro: 0.2141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3099, Accuracy: 0.8895, F1 Micro: 0.6401, F1 Macro: 0.3366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2518, Accuracy: 0.8959, F1 Micro: 0.6915, F1 Macro: 0.5137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2097, Accuracy: 0.9007, F1 Micro: 0.7078, F1 Macro: 0.5443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1769, Accuracy: 0.9063, F1 Micro: 0.7092, F1 Macro: 0.5379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.14, Accuracy: 0.9047, F1 Micro: 0.7109, F1 Macro: 0.5374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1172, Accuracy: 0.9035, F1 Micro: 0.721, F1 Macro: 0.5686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0976, Accuracy: 0.9088, F1 Micro: 0.7259, F1 Macro: 0.5852\n",
      "Epoch 9/10, Train Loss: 0.0862, Accuracy: 0.9098, F1 Micro: 0.7227, F1 Macro: 0.5868\n",
      "Epoch 10/10, Train Loss: 0.0729, Accuracy: 0.9078, F1 Micro: 0.7026, F1 Macro: 0.5997\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9088, F1 Micro: 0.7259, F1 Macro: 0.5852\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.82      0.82      1134\n",
      "      Abusive       0.87      0.84      0.85       992\n",
      "HS_Individual       0.68      0.68      0.68       732\n",
      "     HS_Group       0.70      0.58      0.64       402\n",
      "  HS_Religion       0.72      0.49      0.58       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.67      0.04      0.07        51\n",
      "     HS_Other       0.72      0.75      0.74       762\n",
      "      HS_Weak       0.66      0.65      0.66       689\n",
      "  HS_Moderate       0.60      0.45      0.51       331\n",
      "    HS_Strong       0.89      0.69      0.78       114\n",
      "\n",
      "    micro avg       0.75      0.70      0.73      5556\n",
      "    macro avg       0.67      0.55      0.59      5556\n",
      " weighted avg       0.74      0.70      0.72      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 126.21291017532349 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4273, Accuracy: 0.8702, F1 Micro: 0.508, F1 Macro: 0.218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3076, Accuracy: 0.8916, F1 Micro: 0.6264, F1 Macro: 0.3474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2492, Accuracy: 0.8968, F1 Micro: 0.6948, F1 Macro: 0.5258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2123, Accuracy: 0.9042, F1 Micro: 0.7112, F1 Macro: 0.5518\n",
      "Epoch 5/10, Train Loss: 0.1732, Accuracy: 0.9071, F1 Micro: 0.7096, F1 Macro: 0.54\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1416, Accuracy: 0.9055, F1 Micro: 0.716, F1 Macro: 0.5461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1137, Accuracy: 0.902, F1 Micro: 0.7269, F1 Macro: 0.5691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9013, F1 Micro: 0.7318, F1 Macro: 0.6112\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.9115, F1 Micro: 0.7271, F1 Macro: 0.5976\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9113, F1 Micro: 0.7308, F1 Macro: 0.6169\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9013, F1 Micro: 0.7318, F1 Macro: 0.6112\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.92      0.82      1134\n",
      "      Abusive       0.82      0.90      0.86       992\n",
      "HS_Individual       0.62      0.76      0.69       732\n",
      "     HS_Group       0.61      0.62      0.61       402\n",
      "  HS_Religion       0.59      0.59      0.59       157\n",
      "      HS_Race       0.66      0.70      0.68       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.71      0.20      0.31        51\n",
      "     HS_Other       0.69      0.85      0.76       762\n",
      "      HS_Weak       0.60      0.74      0.66       689\n",
      "  HS_Moderate       0.52      0.53      0.52       331\n",
      "    HS_Strong       0.87      0.71      0.78       114\n",
      "\n",
      "    micro avg       0.69      0.78      0.73      5556\n",
      "    macro avg       0.66      0.63      0.61      5556\n",
      " weighted avg       0.69      0.78      0.72      5556\n",
      "  samples avg       0.43      0.44      0.42      5556\n",
      "\n",
      "Training completed in 124.38071155548096 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.8977, F1 Micro: 0.6882, F1 Macro: 0.5104\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 94.19827008247375 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4019, Accuracy: 0.8796, F1 Micro: 0.58, F1 Macro: 0.274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2839, Accuracy: 0.8977, F1 Micro: 0.667, F1 Macro: 0.4116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2233, Accuracy: 0.9058, F1 Micro: 0.71, F1 Macro: 0.5351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1864, Accuracy: 0.9097, F1 Micro: 0.7154, F1 Macro: 0.5492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1576, Accuracy: 0.9102, F1 Micro: 0.7325, F1 Macro: 0.578\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.9133, F1 Micro: 0.7225, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9129, F1 Micro: 0.7437, F1 Macro: 0.6059\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9098, F1 Micro: 0.7356, F1 Macro: 0.6276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0691, Accuracy: 0.9131, F1 Micro: 0.7461, F1 Macro: 0.6245\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9124, F1 Micro: 0.7427, F1 Macro: 0.6327\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9131, F1 Micro: 0.7461, F1 Macro: 0.6245\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.83      1134\n",
      "      Abusive       0.86      0.86      0.86       992\n",
      "HS_Individual       0.67      0.75      0.71       732\n",
      "     HS_Group       0.70      0.57      0.63       402\n",
      "  HS_Religion       0.68      0.60      0.64       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       0.33      0.03      0.05        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.75      0.78      0.76       762\n",
      "      HS_Weak       0.66      0.73      0.69       689\n",
      "  HS_Moderate       0.62      0.47      0.53       331\n",
      "    HS_Strong       0.86      0.77      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.75      5556\n",
      "    macro avg       0.69      0.60      0.62      5556\n",
      " weighted avg       0.74      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 142.03286027908325 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4164, Accuracy: 0.8774, F1 Micro: 0.54, F1 Macro: 0.2419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2922, Accuracy: 0.8946, F1 Micro: 0.6611, F1 Macro: 0.3673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2323, Accuracy: 0.9032, F1 Micro: 0.6889, F1 Macro: 0.5018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1988, Accuracy: 0.907, F1 Micro: 0.7018, F1 Macro: 0.5196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1639, Accuracy: 0.9088, F1 Micro: 0.7181, F1 Macro: 0.5588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1353, Accuracy: 0.9083, F1 Micro: 0.7246, F1 Macro: 0.5892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9054, F1 Micro: 0.7348, F1 Macro: 0.5911\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9094, F1 Micro: 0.7355, F1 Macro: 0.5996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.912, F1 Micro: 0.7433, F1 Macro: 0.6187\n",
      "Epoch 10/10, Train Loss: 0.0646, Accuracy: 0.9101, F1 Micro: 0.7347, F1 Macro: 0.6097\n",
      "Model 2 - Iteration 4055: Accuracy: 0.912, F1 Micro: 0.7433, F1 Macro: 0.6187\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.85      0.83      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.67      0.74      0.70       732\n",
      "     HS_Group       0.72      0.57      0.64       402\n",
      "  HS_Religion       0.63      0.54      0.58       157\n",
      "      HS_Race       0.71      0.69      0.70       120\n",
      "  HS_Physical       0.17      0.01      0.03        72\n",
      "    HS_Gender       0.56      0.20      0.29        51\n",
      "     HS_Other       0.74      0.78      0.76       762\n",
      "      HS_Weak       0.65      0.72      0.68       689\n",
      "  HS_Moderate       0.62      0.46      0.53       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.75      0.74      0.74      5556\n",
      "    macro avg       0.67      0.60      0.62      5556\n",
      " weighted avg       0.74      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 144.84669256210327 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4115, Accuracy: 0.878, F1 Micro: 0.5331, F1 Macro: 0.2459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2885, Accuracy: 0.8967, F1 Micro: 0.6632, F1 Macro: 0.374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2269, Accuracy: 0.9035, F1 Micro: 0.7075, F1 Macro: 0.5219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.9076, F1 Micro: 0.7112, F1 Macro: 0.5312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1585, Accuracy: 0.908, F1 Micro: 0.7327, F1 Macro: 0.5701\n",
      "Epoch 6/10, Train Loss: 0.1282, Accuracy: 0.9114, F1 Micro: 0.7176, F1 Macro: 0.5826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1021, Accuracy: 0.9085, F1 Micro: 0.7388, F1 Macro: 0.6039\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9114, F1 Micro: 0.7375, F1 Macro: 0.6169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0684, Accuracy: 0.9108, F1 Micro: 0.7442, F1 Macro: 0.6125\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9085, F1 Micro: 0.742, F1 Macro: 0.626\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9108, F1 Micro: 0.7442, F1 Macro: 0.6125\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.85      0.89      0.87       992\n",
      "HS_Individual       0.68      0.74      0.71       732\n",
      "     HS_Group       0.65      0.58      0.61       402\n",
      "  HS_Religion       0.65      0.54      0.59       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       1.00      0.01      0.03        72\n",
      "    HS_Gender       0.57      0.16      0.25        51\n",
      "     HS_Other       0.71      0.82      0.76       762\n",
      "      HS_Weak       0.66      0.72      0.69       689\n",
      "  HS_Moderate       0.55      0.49      0.52       331\n",
      "    HS_Strong       0.86      0.72      0.78       114\n",
      "\n",
      "    micro avg       0.74      0.75      0.74      5556\n",
      "    macro avg       0.73      0.60      0.61      5556\n",
      " weighted avg       0.74      0.75      0.74      5556\n",
      "  samples avg       0.42      0.42      0.41      5556\n",
      "\n",
      "Training completed in 141.55088925361633 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9006, F1 Micro: 0.6995, F1 Macro: 0.5321\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 85.12711691856384 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3887, Accuracy: 0.8806, F1 Micro: 0.5661, F1 Macro: 0.2761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.9, F1 Micro: 0.677, F1 Macro: 0.4711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.9098, F1 Micro: 0.7182, F1 Macro: 0.5361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1817, Accuracy: 0.9144, F1 Micro: 0.7372, F1 Macro: 0.5829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1438, Accuracy: 0.9138, F1 Micro: 0.7433, F1 Macro: 0.5849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.9124, F1 Micro: 0.7444, F1 Macro: 0.5978\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.914, F1 Micro: 0.7375, F1 Macro: 0.6029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0802, Accuracy: 0.9117, F1 Micro: 0.7497, F1 Macro: 0.647\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.9137, F1 Micro: 0.7398, F1 Macro: 0.6326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0582, Accuracy: 0.9151, F1 Micro: 0.7545, F1 Macro: 0.6558\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9151, F1 Micro: 0.7545, F1 Macro: 0.6558\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.69      0.76      0.72       732\n",
      "     HS_Group       0.68      0.60      0.64       402\n",
      "  HS_Religion       0.64      0.68      0.66       157\n",
      "      HS_Race       0.66      0.69      0.68       120\n",
      "  HS_Physical       0.67      0.14      0.23        72\n",
      "    HS_Gender       0.64      0.31      0.42        51\n",
      "     HS_Other       0.75      0.78      0.77       762\n",
      "      HS_Weak       0.67      0.72      0.70       689\n",
      "  HS_Moderate       0.58      0.50      0.53       331\n",
      "    HS_Strong       0.84      0.77      0.80       114\n",
      "\n",
      "    micro avg       0.75      0.76      0.75      5556\n",
      "    macro avg       0.71      0.64      0.66      5556\n",
      " weighted avg       0.75      0.76      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 158.8222930431366 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.405, Accuracy: 0.8797, F1 Micro: 0.5699, F1 Macro: 0.2681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2759, Accuracy: 0.8971, F1 Micro: 0.659, F1 Macro: 0.4062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2311, Accuracy: 0.9043, F1 Micro: 0.7141, F1 Macro: 0.5299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1882, Accuracy: 0.9119, F1 Micro: 0.728, F1 Macro: 0.5456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9113, F1 Micro: 0.7441, F1 Macro: 0.5789\n",
      "Epoch 6/10, Train Loss: 0.1199, Accuracy: 0.9122, F1 Micro: 0.7425, F1 Macro: 0.5952\n",
      "Epoch 7/10, Train Loss: 0.0993, Accuracy: 0.9138, F1 Micro: 0.7288, F1 Macro: 0.5897\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9163, F1 Micro: 0.7397, F1 Macro: 0.6185\n",
      "Epoch 9/10, Train Loss: 0.0684, Accuracy: 0.9138, F1 Micro: 0.7343, F1 Macro: 0.5987\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.917, F1 Micro: 0.741, F1 Macro: 0.638\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9113, F1 Micro: 0.7441, F1 Macro: 0.5789\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.82      0.91      0.86       992\n",
      "HS_Individual       0.67      0.77      0.72       732\n",
      "     HS_Group       0.70      0.58      0.63       402\n",
      "  HS_Religion       0.69      0.54      0.60       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.04      0.08        51\n",
      "     HS_Other       0.74      0.79      0.77       762\n",
      "      HS_Weak       0.65      0.74      0.69       689\n",
      "  HS_Moderate       0.58      0.47      0.52       331\n",
      "    HS_Strong       0.94      0.39      0.56       114\n",
      "\n",
      "    micro avg       0.74      0.75      0.74      5556\n",
      "    macro avg       0.70      0.56      0.58      5556\n",
      " weighted avg       0.73      0.75      0.73      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 154.49859499931335 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3974, Accuracy: 0.8816, F1 Micro: 0.5855, F1 Macro: 0.2964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.272, Accuracy: 0.8992, F1 Micro: 0.6707, F1 Macro: 0.469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2288, Accuracy: 0.9078, F1 Micro: 0.7176, F1 Macro: 0.5297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1862, Accuracy: 0.9125, F1 Micro: 0.7301, F1 Macro: 0.5649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1498, Accuracy: 0.9159, F1 Micro: 0.7409, F1 Macro: 0.5788\n",
      "Epoch 6/10, Train Loss: 0.117, Accuracy: 0.9113, F1 Micro: 0.7406, F1 Macro: 0.5892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.9158, F1 Micro: 0.7473, F1 Macro: 0.6068\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.916, F1 Micro: 0.7339, F1 Macro: 0.6136\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9146, F1 Micro: 0.7381, F1 Macro: 0.6132\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9155, F1 Micro: 0.747, F1 Macro: 0.648\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9158, F1 Micro: 0.7473, F1 Macro: 0.6068\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.82      0.83      1134\n",
      "      Abusive       0.84      0.91      0.87       992\n",
      "HS_Individual       0.75      0.66      0.70       732\n",
      "     HS_Group       0.66      0.63      0.64       402\n",
      "  HS_Religion       0.73      0.50      0.60       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.60      0.06      0.11        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.73      0.64      0.68       689\n",
      "  HS_Moderate       0.57      0.54      0.55       331\n",
      "    HS_Strong       0.85      0.80      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.72      0.75      5556\n",
      "    macro avg       0.68      0.58      0.61      5556\n",
      " weighted avg       0.76      0.72      0.74      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 155.7203552722931 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9028, F1 Micro: 0.7077, F1 Macro: 0.5457\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 76.90022802352905 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3821, Accuracy: 0.8846, F1 Micro: 0.6165, F1 Macro: 0.3043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2706, Accuracy: 0.9027, F1 Micro: 0.7092, F1 Macro: 0.5066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9107, F1 Micro: 0.7232, F1 Macro: 0.5484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1752, Accuracy: 0.9137, F1 Micro: 0.729, F1 Macro: 0.5794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9136, F1 Micro: 0.7412, F1 Macro: 0.5948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.9115, F1 Micro: 0.745, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9157, F1 Micro: 0.7518, F1 Macro: 0.6381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9159, F1 Micro: 0.753, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9167, F1 Micro: 0.7544, F1 Macro: 0.6525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.9137, F1 Micro: 0.7575, F1 Macro: 0.6598\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9137, F1 Micro: 0.7575, F1 Macro: 0.6598\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.88      0.84      1134\n",
      "      Abusive       0.83      0.93      0.88       992\n",
      "HS_Individual       0.69      0.76      0.73       732\n",
      "     HS_Group       0.65      0.64      0.65       402\n",
      "  HS_Religion       0.65      0.62      0.63       157\n",
      "      HS_Race       0.69      0.73      0.71       120\n",
      "  HS_Physical       0.53      0.12      0.20        72\n",
      "    HS_Gender       0.54      0.37      0.44        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.67      0.74      0.70       689\n",
      "  HS_Moderate       0.56      0.55      0.55       331\n",
      "    HS_Strong       0.83      0.80      0.82       114\n",
      "\n",
      "    micro avg       0.73      0.78      0.76      5556\n",
      "    macro avg       0.68      0.66      0.66      5556\n",
      " weighted avg       0.73      0.78      0.75      5556\n",
      "  samples avg       0.43      0.44      0.42      5556\n",
      "\n",
      "Training completed in 177.0855221748352 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3977, Accuracy: 0.8826, F1 Micro: 0.6194, F1 Macro: 0.2987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2779, Accuracy: 0.9016, F1 Micro: 0.6886, F1 Macro: 0.473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9084, F1 Micro: 0.7097, F1 Macro: 0.5306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9094, F1 Micro: 0.7254, F1 Macro: 0.5714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1449, Accuracy: 0.9129, F1 Micro: 0.7392, F1 Macro: 0.5812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.9156, F1 Micro: 0.7446, F1 Macro: 0.6031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.911, F1 Micro: 0.7446, F1 Macro: 0.625\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.913, F1 Micro: 0.7442, F1 Macro: 0.6218\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9151, F1 Micro: 0.7431, F1 Macro: 0.6207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9147, F1 Micro: 0.7469, F1 Macro: 0.6467\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9147, F1 Micro: 0.7469, F1 Macro: 0.6467\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.71      0.69      0.70       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.68      0.56      0.62       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.50      0.10      0.16        72\n",
      "    HS_Gender       0.47      0.37      0.42        51\n",
      "     HS_Other       0.77      0.75      0.76       762\n",
      "      HS_Weak       0.69      0.66      0.67       689\n",
      "  HS_Moderate       0.58      0.52      0.55       331\n",
      "    HS_Strong       0.86      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.76      0.73      0.75      5556\n",
      "    macro avg       0.70      0.62      0.65      5556\n",
      " weighted avg       0.76      0.73      0.74      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 173.898108959198 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3895, Accuracy: 0.8836, F1 Micro: 0.6317, F1 Macro: 0.312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2735, Accuracy: 0.903, F1 Micro: 0.7061, F1 Macro: 0.5084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2149, Accuracy: 0.9102, F1 Micro: 0.7134, F1 Macro: 0.5475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1772, Accuracy: 0.9097, F1 Micro: 0.7229, F1 Macro: 0.5775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1426, Accuracy: 0.9119, F1 Micro: 0.7299, F1 Macro: 0.5754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9142, F1 Micro: 0.7419, F1 Macro: 0.5982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.094, Accuracy: 0.9148, F1 Micro: 0.7446, F1 Macro: 0.6235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0751, Accuracy: 0.9163, F1 Micro: 0.7447, F1 Macro: 0.6457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9158, F1 Micro: 0.7459, F1 Macro: 0.6351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9169, F1 Micro: 0.7543, F1 Macro: 0.6567\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9169, F1 Micro: 0.7543, F1 Macro: 0.6567\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.85      0.90      0.88       992\n",
      "HS_Individual       0.73      0.70      0.72       732\n",
      "     HS_Group       0.68      0.61      0.64       402\n",
      "  HS_Religion       0.78      0.52      0.62       157\n",
      "      HS_Race       0.72      0.68      0.70       120\n",
      "  HS_Physical       0.56      0.12      0.20        72\n",
      "    HS_Gender       0.51      0.37      0.43        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.70      0.68      0.69       689\n",
      "  HS_Moderate       0.59      0.55      0.57       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.75      5556\n",
      "    macro avg       0.72      0.63      0.66      5556\n",
      " weighted avg       0.76      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 177.30564188957214 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9046, F1 Micro: 0.7141, F1 Macro: 0.5612\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 68.87429475784302 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3698, Accuracy: 0.8857, F1 Micro: 0.6555, F1 Macro: 0.3837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.9041, F1 Micro: 0.7159, F1 Macro: 0.5124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2067, Accuracy: 0.9119, F1 Micro: 0.7281, F1 Macro: 0.574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.9142, F1 Micro: 0.7442, F1 Macro: 0.5865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.136, Accuracy: 0.9167, F1 Micro: 0.7485, F1 Macro: 0.6327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1065, Accuracy: 0.9171, F1 Micro: 0.7527, F1 Macro: 0.6238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0845, Accuracy: 0.9184, F1 Micro: 0.7641, F1 Macro: 0.6445\n",
      "Epoch 8/10, Train Loss: 0.0719, Accuracy: 0.9113, F1 Micro: 0.7531, F1 Macro: 0.649\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9178, F1 Micro: 0.7589, F1 Macro: 0.6645\n",
      "Epoch 10/10, Train Loss: 0.0532, Accuracy: 0.9188, F1 Micro: 0.7542, F1 Macro: 0.6421\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9184, F1 Micro: 0.7641, F1 Macro: 0.6445\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.84      0.92      0.88       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.67      0.65      0.66       402\n",
      "  HS_Religion       0.65      0.68      0.66       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.83      0.10      0.18        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.58      0.58      0.58       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.76      0.63      0.64      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 184.91536140441895 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3853, Accuracy: 0.8853, F1 Micro: 0.6424, F1 Macro: 0.3318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.9025, F1 Micro: 0.6997, F1 Macro: 0.4663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2132, Accuracy: 0.9082, F1 Micro: 0.7033, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9131, F1 Micro: 0.7303, F1 Macro: 0.5715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9155, F1 Micro: 0.7349, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.111, Accuracy: 0.9151, F1 Micro: 0.7392, F1 Macro: 0.5935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0893, Accuracy: 0.9158, F1 Micro: 0.7542, F1 Macro: 0.6259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9168, F1 Micro: 0.7558, F1 Macro: 0.6471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0654, Accuracy: 0.9189, F1 Micro: 0.7564, F1 Macro: 0.6524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.9186, F1 Micro: 0.761, F1 Macro: 0.6633\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9186, F1 Micro: 0.761, F1 Macro: 0.6633\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.71      0.63      0.66       402\n",
      "  HS_Religion       0.75      0.55      0.63       157\n",
      "      HS_Race       0.79      0.66      0.72       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.62      0.31      0.42        51\n",
      "     HS_Other       0.75      0.78      0.77       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.63      0.55      0.59       331\n",
      "    HS_Strong       0.85      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.75      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 189.63968539237976 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.379, Accuracy: 0.8873, F1 Micro: 0.6244, F1 Macro: 0.3428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2596, Accuracy: 0.9035, F1 Micro: 0.7151, F1 Macro: 0.5285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2078, Accuracy: 0.911, F1 Micro: 0.7226, F1 Macro: 0.5604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1739, Accuracy: 0.9141, F1 Micro: 0.7319, F1 Macro: 0.5801\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.9147, F1 Micro: 0.7429, F1 Macro: 0.5989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9158, F1 Micro: 0.7549, F1 Macro: 0.6209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0857, Accuracy: 0.9181, F1 Micro: 0.7637, F1 Macro: 0.638\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9117, F1 Micro: 0.7536, F1 Macro: 0.6471\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9175, F1 Micro: 0.7588, F1 Macro: 0.6565\n",
      "Epoch 10/10, Train Loss: 0.054, Accuracy: 0.9152, F1 Micro: 0.7579, F1 Macro: 0.6638\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9181, F1 Micro: 0.7637, F1 Macro: 0.638\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.90      0.88       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.67      0.69      0.68       402\n",
      "  HS_Religion       0.66      0.64      0.65       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.67      0.03      0.05        72\n",
      "    HS_Gender       0.54      0.14      0.22        51\n",
      "     HS_Other       0.73      0.81      0.77       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.59      0.60      0.59       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.72      0.63      0.64      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 184.99675059318542 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9063, F1 Micro: 0.7202, F1 Macro: 0.5721\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 62.71479344367981 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3655, Accuracy: 0.8898, F1 Micro: 0.6471, F1 Macro: 0.3887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2525, Accuracy: 0.9053, F1 Micro: 0.7185, F1 Macro: 0.5425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2019, Accuracy: 0.9124, F1 Micro: 0.743, F1 Macro: 0.585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1633, Accuracy: 0.9161, F1 Micro: 0.751, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.9122, F1 Micro: 0.755, F1 Macro: 0.638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9208, F1 Micro: 0.7657, F1 Macro: 0.6588\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9204, F1 Micro: 0.7619, F1 Macro: 0.6486\n",
      "Epoch 8/10, Train Loss: 0.0703, Accuracy: 0.9195, F1 Micro: 0.7611, F1 Macro: 0.6665\n",
      "Epoch 9/10, Train Loss: 0.0589, Accuracy: 0.918, F1 Micro: 0.763, F1 Macro: 0.6679\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9168, F1 Micro: 0.7644, F1 Macro: 0.6773\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9208, F1 Micro: 0.7657, F1 Macro: 0.6588\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.84      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.69      0.69      0.69       402\n",
      "  HS_Religion       0.78      0.61      0.68       157\n",
      "      HS_Race       0.73      0.72      0.72       120\n",
      "  HS_Physical       0.64      0.10      0.17        72\n",
      "    HS_Gender       0.53      0.20      0.29        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.73      0.66      0.69       689\n",
      "  HS_Moderate       0.61      0.59      0.60       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.73      0.63      0.66      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 196.44152975082397 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3802, Accuracy: 0.8865, F1 Micro: 0.6396, F1 Macro: 0.3327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2612, Accuracy: 0.9047, F1 Micro: 0.703, F1 Macro: 0.4996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9097, F1 Micro: 0.7355, F1 Macro: 0.5648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9171, F1 Micro: 0.7491, F1 Macro: 0.582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.9152, F1 Micro: 0.7519, F1 Macro: 0.6378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1149, Accuracy: 0.9162, F1 Micro: 0.757, F1 Macro: 0.6423\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9198, F1 Micro: 0.7449, F1 Macro: 0.6277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9197, F1 Micro: 0.7621, F1 Macro: 0.6585\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9159, F1 Micro: 0.7614, F1 Macro: 0.6548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.918, F1 Micro: 0.7641, F1 Macro: 0.675\n",
      "Model 2 - Iteration 6285: Accuracy: 0.918, F1 Micro: 0.7641, F1 Macro: 0.675\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.86      0.90      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.65      0.69      0.67       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.69      0.70      0.69       120\n",
      "  HS_Physical       0.61      0.15      0.24        72\n",
      "    HS_Gender       0.55      0.43      0.48        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.58      0.62      0.60       331\n",
      "    HS_Strong       0.83      0.82      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.71      0.67      0.68      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 199.555171251297 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3737, Accuracy: 0.8889, F1 Micro: 0.6417, F1 Macro: 0.3585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2564, Accuracy: 0.9055, F1 Micro: 0.7179, F1 Macro: 0.5313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2049, Accuracy: 0.9102, F1 Micro: 0.7432, F1 Macro: 0.5767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9151, F1 Micro: 0.7487, F1 Macro: 0.5799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9174, F1 Micro: 0.7601, F1 Macro: 0.6472\n",
      "Epoch 6/10, Train Loss: 0.109, Accuracy: 0.9171, F1 Micro: 0.7596, F1 Macro: 0.6516\n",
      "Epoch 7/10, Train Loss: 0.0914, Accuracy: 0.9186, F1 Micro: 0.7553, F1 Macro: 0.6446\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9187, F1 Micro: 0.7592, F1 Macro: 0.6597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9178, F1 Micro: 0.7634, F1 Macro: 0.6712\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9124, F1 Micro: 0.7574, F1 Macro: 0.6673\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9178, F1 Micro: 0.7634, F1 Macro: 0.6712\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.74      0.69      0.71       732\n",
      "     HS_Group       0.64      0.72      0.68       402\n",
      "  HS_Religion       0.69      0.66      0.68       157\n",
      "      HS_Race       0.66      0.74      0.70       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.53      0.35      0.42        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.71      0.67      0.69       689\n",
      "  HS_Moderate       0.56      0.65      0.60       331\n",
      "    HS_Strong       0.84      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.72      0.67      0.67      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 195.90465879440308 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9077, F1 Micro: 0.7252, F1 Macro: 0.5828\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 56.88826942443848 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3621, Accuracy: 0.8906, F1 Micro: 0.6557, F1 Macro: 0.3828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2496, Accuracy: 0.9034, F1 Micro: 0.6734, F1 Macro: 0.5112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2035, Accuracy: 0.9132, F1 Micro: 0.723, F1 Macro: 0.5655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1595, Accuracy: 0.9192, F1 Micro: 0.7401, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9166, F1 Micro: 0.7575, F1 Macro: 0.6345\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9142, F1 Micro: 0.761, F1 Macro: 0.6602\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9194, F1 Micro: 0.7593, F1 Macro: 0.6616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9171, F1 Micro: 0.7632, F1 Macro: 0.6617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.9185, F1 Micro: 0.7649, F1 Macro: 0.6693\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9212, F1 Micro: 0.7624, F1 Macro: 0.6822\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9185, F1 Micro: 0.7649, F1 Macro: 0.6693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.69      0.62      0.66       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.75      0.67      0.71       120\n",
      "  HS_Physical       0.57      0.17      0.26        72\n",
      "    HS_Gender       0.57      0.33      0.42        51\n",
      "     HS_Other       0.75      0.80      0.77       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.60      0.53      0.56       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.72      0.65      0.67      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 205.3036756515503 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3762, Accuracy: 0.8866, F1 Micro: 0.6206, F1 Macro: 0.3167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.9031, F1 Micro: 0.6842, F1 Macro: 0.5104\n",
      "Epoch 3/10, Train Loss: 0.209, Accuracy: 0.9064, F1 Micro: 0.6802, F1 Macro: 0.5281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.163, Accuracy: 0.9163, F1 Micro: 0.7329, F1 Macro: 0.5741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1334, Accuracy: 0.9139, F1 Micro: 0.759, F1 Macro: 0.6308\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9138, F1 Micro: 0.7554, F1 Macro: 0.6538\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.916, F1 Micro: 0.7566, F1 Macro: 0.6554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0762, Accuracy: 0.9153, F1 Micro: 0.7625, F1 Macro: 0.6672\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.919, F1 Micro: 0.7594, F1 Macro: 0.6734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9195, F1 Micro: 0.7665, F1 Macro: 0.6842\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9195, F1 Micro: 0.7665, F1 Macro: 0.6842\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.85      0.92      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.68      0.63      0.66       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.68      0.24      0.35        72\n",
      "    HS_Gender       0.52      0.45      0.48        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.85      0.82      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.66      0.68      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 202.19824814796448 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3711, Accuracy: 0.8892, F1 Micro: 0.6349, F1 Macro: 0.3453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2546, Accuracy: 0.9043, F1 Micro: 0.6769, F1 Macro: 0.4968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9108, F1 Micro: 0.7148, F1 Macro: 0.5613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9188, F1 Micro: 0.7487, F1 Macro: 0.5818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9158, F1 Micro: 0.7614, F1 Macro: 0.6206\n",
      "Epoch 6/10, Train Loss: 0.1072, Accuracy: 0.9171, F1 Micro: 0.7534, F1 Macro: 0.652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9174, F1 Micro: 0.7627, F1 Macro: 0.6692\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.919, F1 Micro: 0.7614, F1 Macro: 0.6656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9175, F1 Micro: 0.7632, F1 Macro: 0.6727\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.918, F1 Micro: 0.7553, F1 Macro: 0.6619\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9175, F1 Micro: 0.7632, F1 Macro: 0.6727\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.66      0.65      0.66       402\n",
      "  HS_Religion       0.65      0.65      0.65       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.63      0.17      0.26        72\n",
      "    HS_Gender       0.54      0.41      0.47        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.70       689\n",
      "  HS_Moderate       0.56      0.57      0.57       331\n",
      "    HS_Strong       0.84      0.79      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.71      0.66      0.67      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 203.7903938293457 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9088, F1 Micro: 0.7291, F1 Macro: 0.5921\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 53.47484064102173 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3525, Accuracy: 0.8934, F1 Micro: 0.6499, F1 Macro: 0.3799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2432, Accuracy: 0.9025, F1 Micro: 0.7258, F1 Macro: 0.5517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.198, Accuracy: 0.9166, F1 Micro: 0.7441, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1576, Accuracy: 0.9167, F1 Micro: 0.7633, F1 Macro: 0.6215\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9179, F1 Micro: 0.7624, F1 Macro: 0.6616\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9213, F1 Micro: 0.7628, F1 Macro: 0.6703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9238, F1 Micro: 0.774, F1 Macro: 0.6749\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.924, F1 Micro: 0.7715, F1 Macro: 0.6687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9226, F1 Micro: 0.7745, F1 Macro: 0.6897\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9161, F1 Micro: 0.7686, F1 Macro: 0.6895\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9226, F1 Micro: 0.7745, F1 Macro: 0.6897\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.70      0.57      0.62       157\n",
      "      HS_Race       0.77      0.69      0.73       120\n",
      "  HS_Physical       0.80      0.22      0.35        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.83      0.88      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 213.07509684562683 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3656, Accuracy: 0.8895, F1 Micro: 0.6509, F1 Macro: 0.3499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9017, F1 Micro: 0.719, F1 Macro: 0.5336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2052, Accuracy: 0.9148, F1 Micro: 0.7328, F1 Macro: 0.5661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.918, F1 Micro: 0.7594, F1 Macro: 0.6175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.9209, F1 Micro: 0.7595, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.105, Accuracy: 0.9198, F1 Micro: 0.7618, F1 Macro: 0.6588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9195, F1 Micro: 0.7707, F1 Macro: 0.6731\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.916, F1 Micro: 0.7651, F1 Macro: 0.6696\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9227, F1 Micro: 0.7702, F1 Macro: 0.6794\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.92, F1 Micro: 0.7669, F1 Macro: 0.6826\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9195, F1 Micro: 0.7707, F1 Macro: 0.6731\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.66      0.62      0.64       157\n",
      "      HS_Race       0.74      0.70      0.72       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.59      0.31      0.41        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.68      0.76      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.73      0.66      0.67      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 215.60583877563477 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.362, Accuracy: 0.8908, F1 Micro: 0.6491, F1 Macro: 0.3717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2478, Accuracy: 0.9029, F1 Micro: 0.7233, F1 Macro: 0.5509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2006, Accuracy: 0.9164, F1 Micro: 0.7426, F1 Macro: 0.5823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1592, Accuracy: 0.918, F1 Micro: 0.7671, F1 Macro: 0.6392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1304, Accuracy: 0.9217, F1 Micro: 0.7674, F1 Macro: 0.6511\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9209, F1 Micro: 0.7588, F1 Macro: 0.6577\n",
      "Epoch 7/10, Train Loss: 0.0848, Accuracy: 0.9177, F1 Micro: 0.7654, F1 Macro: 0.6692\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9214, F1 Micro: 0.7606, F1 Macro: 0.6595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9204, F1 Micro: 0.7729, F1 Macro: 0.693\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9214, F1 Micro: 0.771, F1 Macro: 0.6937\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9204, F1 Micro: 0.7729, F1 Macro: 0.693\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.71      0.62      0.66       402\n",
      "  HS_Religion       0.73      0.57      0.64       157\n",
      "      HS_Race       0.71      0.63      0.67       120\n",
      "  HS_Physical       0.83      0.28      0.42        72\n",
      "    HS_Gender       0.54      0.51      0.53        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.82      0.86      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.77      5556\n",
      "    macro avg       0.73      0.68      0.69      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 213.2371642589569 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9099, F1 Micro: 0.7331, F1 Macro: 0.6006\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 47.8806266784668 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3578, Accuracy: 0.888, F1 Micro: 0.5987, F1 Macro: 0.3324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2488, Accuracy: 0.9078, F1 Micro: 0.7151, F1 Macro: 0.5471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.9161, F1 Micro: 0.7497, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1596, Accuracy: 0.9214, F1 Micro: 0.7555, F1 Macro: 0.6154\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.9113, F1 Micro: 0.7552, F1 Macro: 0.6357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.9194, F1 Micro: 0.7653, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9229, F1 Micro: 0.7715, F1 Macro: 0.6816\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.92, F1 Micro: 0.7646, F1 Macro: 0.6814\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9202, F1 Micro: 0.7673, F1 Macro: 0.6823\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9195, F1 Micro: 0.7703, F1 Macro: 0.6895\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9229, F1 Micro: 0.7715, F1 Macro: 0.6816\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.60      0.35      0.44        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.56      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.77      0.65      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 222.06032586097717 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.368, Accuracy: 0.8864, F1 Micro: 0.6011, F1 Macro: 0.3038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2545, Accuracy: 0.9067, F1 Micro: 0.7031, F1 Macro: 0.5077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2009, Accuracy: 0.9122, F1 Micro: 0.7406, F1 Macro: 0.5753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9182, F1 Micro: 0.7487, F1 Macro: 0.6129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9165, F1 Micro: 0.7562, F1 Macro: 0.6286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9187, F1 Micro: 0.7642, F1 Macro: 0.6402\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9192, F1 Micro: 0.7622, F1 Macro: 0.6543\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.9201, F1 Micro: 0.7625, F1 Macro: 0.6661\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9222, F1 Micro: 0.7614, F1 Macro: 0.6733\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9156, F1 Micro: 0.7604, F1 Macro: 0.6695\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9187, F1 Micro: 0.7642, F1 Macro: 0.6402\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.89      0.88       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.76      0.41      0.53       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.57      0.16      0.25        51\n",
      "     HS_Other       0.72      0.84      0.78       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.61      0.55      0.58       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.75      0.62      0.64      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 222.17075300216675 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3657, Accuracy: 0.8866, F1 Micro: 0.5964, F1 Macro: 0.3168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2529, Accuracy: 0.9077, F1 Micro: 0.6999, F1 Macro: 0.5246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2004, Accuracy: 0.9168, F1 Micro: 0.752, F1 Macro: 0.5877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.9214, F1 Micro: 0.7583, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1263, Accuracy: 0.92, F1 Micro: 0.7655, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.105, Accuracy: 0.9226, F1 Micro: 0.7741, F1 Macro: 0.6664\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9202, F1 Micro: 0.7708, F1 Macro: 0.6755\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9216, F1 Micro: 0.773, F1 Macro: 0.6982\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9195, F1 Micro: 0.7719, F1 Macro: 0.6955\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9163, F1 Micro: 0.7655, F1 Macro: 0.6966\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9226, F1 Micro: 0.7741, F1 Macro: 0.6664\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.76      0.54      0.63       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.68      0.25      0.37        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.77      0.64      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 220.9667477607727 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9108, F1 Micro: 0.7362, F1 Macro: 0.6057\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 44.1330885887146 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3587, Accuracy: 0.8896, F1 Micro: 0.6901, F1 Macro: 0.4549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2419, Accuracy: 0.91, F1 Micro: 0.7416, F1 Macro: 0.5798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1908, Accuracy: 0.9178, F1 Micro: 0.7474, F1 Macro: 0.5861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1571, Accuracy: 0.9218, F1 Micro: 0.7672, F1 Macro: 0.6263\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9214, F1 Micro: 0.767, F1 Macro: 0.657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.92, F1 Micro: 0.7704, F1 Macro: 0.6648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0801, Accuracy: 0.9216, F1 Micro: 0.7706, F1 Macro: 0.6702\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9201, F1 Micro: 0.7679, F1 Macro: 0.6762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9235, F1 Micro: 0.7767, F1 Macro: 0.6953\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.922, F1 Micro: 0.7677, F1 Macro: 0.6832\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9235, F1 Micro: 0.7767, F1 Macro: 0.6953\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.70      0.31      0.43        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.83      0.89      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 234.22093224525452 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3696, Accuracy: 0.8861, F1 Micro: 0.6772, F1 Macro: 0.399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2489, Accuracy: 0.9076, F1 Micro: 0.7355, F1 Macro: 0.5646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.198, Accuracy: 0.9152, F1 Micro: 0.7385, F1 Macro: 0.5651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.157, Accuracy: 0.9208, F1 Micro: 0.7588, F1 Macro: 0.6118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1267, Accuracy: 0.92, F1 Micro: 0.768, F1 Macro: 0.658\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9221, F1 Micro: 0.7672, F1 Macro: 0.6441\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9203, F1 Micro: 0.7672, F1 Macro: 0.6725\n",
      "Epoch 8/10, Train Loss: 0.0694, Accuracy: 0.9195, F1 Micro: 0.7645, F1 Macro: 0.6704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9235, F1 Micro: 0.7697, F1 Macro: 0.6753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9204, F1 Micro: 0.7705, F1 Macro: 0.6839\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9204, F1 Micro: 0.7705, F1 Macro: 0.6839\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.69      0.78      0.73       732\n",
      "     HS_Group       0.73      0.56      0.64       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.74      0.28      0.40        72\n",
      "    HS_Gender       0.75      0.29      0.42        51\n",
      "     HS_Other       0.75      0.81      0.78       762\n",
      "      HS_Weak       0.67      0.76      0.71       689\n",
      "  HS_Moderate       0.66      0.48      0.56       331\n",
      "    HS_Strong       0.83      0.83      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.76      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 232.71994185447693 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3643, Accuracy: 0.8845, F1 Micro: 0.6834, F1 Macro: 0.4313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2455, Accuracy: 0.9085, F1 Micro: 0.7343, F1 Macro: 0.5644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.917, F1 Micro: 0.7374, F1 Macro: 0.5712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1575, Accuracy: 0.92, F1 Micro: 0.7623, F1 Macro: 0.6219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9222, F1 Micro: 0.7739, F1 Macro: 0.6663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9229, F1 Micro: 0.7739, F1 Macro: 0.6675\n",
      "Epoch 7/10, Train Loss: 0.0855, Accuracy: 0.9204, F1 Micro: 0.7648, F1 Macro: 0.6745\n",
      "Epoch 8/10, Train Loss: 0.0685, Accuracy: 0.9196, F1 Micro: 0.7701, F1 Macro: 0.6832\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9208, F1 Micro: 0.7682, F1 Macro: 0.6898\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9225, F1 Micro: 0.7723, F1 Macro: 0.6952\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9229, F1 Micro: 0.7739, F1 Macro: 0.6675\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.74      0.66      0.70       402\n",
      "  HS_Religion       0.80      0.45      0.58       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.79      0.15      0.26        72\n",
      "    HS_Gender       0.60      0.24      0.34        51\n",
      "     HS_Other       0.73      0.83      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.67      0.57      0.62       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.76      0.63      0.67      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 230.99237155914307 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9117, F1 Micro: 0.739, F1 Macro: 0.6116\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 39.35582947731018 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3493, Accuracy: 0.8979, F1 Micro: 0.6679, F1 Macro: 0.4011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2462, Accuracy: 0.9131, F1 Micro: 0.7353, F1 Macro: 0.5734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1924, Accuracy: 0.9194, F1 Micro: 0.7525, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.9169, F1 Micro: 0.7623, F1 Macro: 0.6279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9234, F1 Micro: 0.7649, F1 Macro: 0.6648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1028, Accuracy: 0.9213, F1 Micro: 0.765, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9231, F1 Micro: 0.767, F1 Macro: 0.6659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9226, F1 Micro: 0.772, F1 Macro: 0.6827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9219, F1 Micro: 0.7742, F1 Macro: 0.6995\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9222, F1 Micro: 0.7701, F1 Macro: 0.6914\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9219, F1 Micro: 0.7742, F1 Macro: 0.6995\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.76      0.55      0.64       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.84      0.29      0.43        72\n",
      "    HS_Gender       0.57      0.45      0.51        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.59       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 240.76993942260742 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3629, Accuracy: 0.8924, F1 Micro: 0.6315, F1 Macro: 0.3466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.91, F1 Micro: 0.7213, F1 Macro: 0.5534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9174, F1 Micro: 0.7416, F1 Macro: 0.5826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1586, Accuracy: 0.9175, F1 Micro: 0.7509, F1 Macro: 0.6229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9236, F1 Micro: 0.7679, F1 Macro: 0.6716\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9193, F1 Micro: 0.7649, F1 Macro: 0.6802\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.9164, F1 Micro: 0.7574, F1 Macro: 0.6592\n",
      "Epoch 8/10, Train Loss: 0.0671, Accuracy: 0.9196, F1 Micro: 0.7605, F1 Macro: 0.6622\n",
      "Epoch 9/10, Train Loss: 0.0609, Accuracy: 0.9198, F1 Micro: 0.7618, F1 Macro: 0.6859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9222, F1 Micro: 0.7726, F1 Macro: 0.69\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9222, F1 Micro: 0.7726, F1 Macro: 0.69\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.76      0.59      0.66       157\n",
      "      HS_Race       0.81      0.64      0.72       120\n",
      "  HS_Physical       0.88      0.21      0.34        72\n",
      "    HS_Gender       0.65      0.43      0.52        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.73      0.71       689\n",
      "  HS_Moderate       0.66      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 235.31714749336243 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.355, Accuracy: 0.8956, F1 Micro: 0.6651, F1 Macro: 0.3705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2476, Accuracy: 0.9123, F1 Micro: 0.7271, F1 Macro: 0.5563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1936, Accuracy: 0.9205, F1 Micro: 0.76, F1 Macro: 0.6061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1582, Accuracy: 0.9219, F1 Micro: 0.7636, F1 Macro: 0.6268\n",
      "Epoch 5/10, Train Loss: 0.1224, Accuracy: 0.922, F1 Micro: 0.7502, F1 Macro: 0.6382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.922, F1 Micro: 0.7677, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9228, F1 Micro: 0.7708, F1 Macro: 0.6783\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9235, F1 Micro: 0.7691, F1 Macro: 0.7002\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9182, F1 Micro: 0.7708, F1 Macro: 0.6972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.925, F1 Micro: 0.7765, F1 Macro: 0.7011\n",
      "Model 3 - Iteration 7901: Accuracy: 0.925, F1 Micro: 0.7765, F1 Macro: 0.7011\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.72      0.73       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.77      0.64      0.70       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.62      0.51      0.56        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.57      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 236.77686762809753 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9125, F1 Micro: 0.7416, F1 Macro: 0.6177\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 36.754148960113525 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.35, Accuracy: 0.8982, F1 Micro: 0.6611, F1 Macro: 0.4348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2356, Accuracy: 0.9128, F1 Micro: 0.7348, F1 Macro: 0.5547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9136, F1 Micro: 0.7501, F1 Macro: 0.5791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1487, Accuracy: 0.9213, F1 Micro: 0.7606, F1 Macro: 0.6309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9222, F1 Micro: 0.7709, F1 Macro: 0.6778\n",
      "Epoch 6/10, Train Loss: 0.0947, Accuracy: 0.9236, F1 Micro: 0.7696, F1 Macro: 0.6746\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.9235, F1 Micro: 0.7693, F1 Macro: 0.6853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9226, F1 Micro: 0.7742, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9242, F1 Micro: 0.7756, F1 Macro: 0.7037\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9245, F1 Micro: 0.7739, F1 Macro: 0.7017\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9242, F1 Micro: 0.7756, F1 Macro: 0.7037\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.82      0.60      0.69       157\n",
      "      HS_Race       0.77      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.33      0.47        72\n",
      "    HS_Gender       0.51      0.43      0.47        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 243.85375142097473 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3625, Accuracy: 0.8954, F1 Micro: 0.6575, F1 Macro: 0.3816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2423, Accuracy: 0.9106, F1 Micro: 0.7292, F1 Macro: 0.5452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9141, F1 Micro: 0.7458, F1 Macro: 0.5471\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1522, Accuracy: 0.9202, F1 Micro: 0.7639, F1 Macro: 0.6426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9209, F1 Micro: 0.767, F1 Macro: 0.6603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9191, F1 Micro: 0.7675, F1 Macro: 0.663\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.9217, F1 Micro: 0.7602, F1 Macro: 0.6706\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9228, F1 Micro: 0.765, F1 Macro: 0.6634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9234, F1 Micro: 0.7683, F1 Macro: 0.6897\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9227, F1 Micro: 0.7678, F1 Macro: 0.6899\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9234, F1 Micro: 0.7683, F1 Macro: 0.6897\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.74      0.69      0.71       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.75      0.57      0.65       157\n",
      "      HS_Race       0.84      0.66      0.74       120\n",
      "  HS_Physical       0.94      0.21      0.34        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.79      0.78      0.78       762\n",
      "      HS_Weak       0.72      0.67      0.70       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.78      0.64      0.69      5556\n",
      " weighted avg       0.80      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 243.7738630771637 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3551, Accuracy: 0.897, F1 Micro: 0.6683, F1 Macro: 0.424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2376, Accuracy: 0.9119, F1 Micro: 0.7288, F1 Macro: 0.5429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1928, Accuracy: 0.9142, F1 Micro: 0.7527, F1 Macro: 0.576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9209, F1 Micro: 0.766, F1 Macro: 0.636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9212, F1 Micro: 0.7677, F1 Macro: 0.6702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0949, Accuracy: 0.9206, F1 Micro: 0.7726, F1 Macro: 0.6819\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9219, F1 Micro: 0.7665, F1 Macro: 0.6888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.922, F1 Micro: 0.773, F1 Macro: 0.6888\n",
      "Epoch 9/10, Train Loss: 0.0559, Accuracy: 0.9226, F1 Micro: 0.7685, F1 Macro: 0.7001\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9234, F1 Micro: 0.7706, F1 Macro: 0.7079\n",
      "Model 3 - Iteration 8165: Accuracy: 0.922, F1 Micro: 0.773, F1 Macro: 0.6888\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.72      0.65      0.69       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.81      0.62      0.70       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.64      0.41      0.50        51\n",
      "     HS_Other       0.73      0.83      0.77       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.60      0.62       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 243.2703139781952 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9132, F1 Micro: 0.7436, F1 Macro: 0.6228\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 33.427228927612305 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3441, Accuracy: 0.8938, F1 Micro: 0.6139, F1 Macro: 0.3842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2381, Accuracy: 0.9075, F1 Micro: 0.6855, F1 Macro: 0.5241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1871, Accuracy: 0.9206, F1 Micro: 0.7633, F1 Macro: 0.6259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1525, Accuracy: 0.9233, F1 Micro: 0.7667, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.9202, F1 Micro: 0.772, F1 Macro: 0.6729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9244, F1 Micro: 0.777, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9243, F1 Micro: 0.7806, F1 Macro: 0.7014\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9172, F1 Micro: 0.7701, F1 Macro: 0.6904\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.921, F1 Micro: 0.7729, F1 Macro: 0.7017\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9187, F1 Micro: 0.7597, F1 Macro: 0.6959\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9243, F1 Micro: 0.7806, F1 Macro: 0.7014\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.70      0.61      0.65       157\n",
      "      HS_Race       0.73      0.68      0.70       120\n",
      "  HS_Physical       0.90      0.25      0.39        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.85      0.87      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 250.26986455917358 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3557, Accuracy: 0.8907, F1 Micro: 0.6022, F1 Macro: 0.3357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2461, Accuracy: 0.9033, F1 Micro: 0.6676, F1 Macro: 0.4929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1918, Accuracy: 0.919, F1 Micro: 0.7582, F1 Macro: 0.6324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1558, Accuracy: 0.9225, F1 Micro: 0.763, F1 Macro: 0.6613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9223, F1 Micro: 0.7744, F1 Macro: 0.6716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9217, F1 Micro: 0.7757, F1 Macro: 0.6796\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9226, F1 Micro: 0.7715, F1 Macro: 0.6859\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9127, F1 Micro: 0.7641, F1 Macro: 0.6845\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9214, F1 Micro: 0.7747, F1 Macro: 0.6961\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9237, F1 Micro: 0.7743, F1 Macro: 0.7011\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9217, F1 Micro: 0.7757, F1 Macro: 0.6796\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.72      0.55      0.62       157\n",
      "      HS_Race       0.72      0.76      0.74       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.75      0.84      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.60      0.62      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 248.38941955566406 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3512, Accuracy: 0.8921, F1 Micro: 0.6103, F1 Macro: 0.3568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2419, Accuracy: 0.9051, F1 Micro: 0.6801, F1 Macro: 0.5121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1905, Accuracy: 0.9162, F1 Micro: 0.7547, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1524, Accuracy: 0.9235, F1 Micro: 0.7671, F1 Macro: 0.6569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9212, F1 Micro: 0.7706, F1 Macro: 0.6793\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9184, F1 Micro: 0.7685, F1 Macro: 0.68\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9217, F1 Micro: 0.7761, F1 Macro: 0.7047\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9199, F1 Micro: 0.7735, F1 Macro: 0.7038\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9187, F1 Micro: 0.7721, F1 Macro: 0.7048\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9219, F1 Micro: 0.7745, F1 Macro: 0.7043\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9217, F1 Micro: 0.7761, F1 Macro: 0.7047\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.71      0.76      0.73       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.66      0.69      0.67       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.90      0.26      0.41        72\n",
      "    HS_Gender       0.55      0.57      0.56        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.70      5556\n",
      " weighted avg       0.76      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 248.51700377464294 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9138, F1 Micro: 0.7457, F1 Macro: 0.6273\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 28.380839586257935 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3401, Accuracy: 0.8992, F1 Micro: 0.6802, F1 Macro: 0.448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2306, Accuracy: 0.9146, F1 Micro: 0.7407, F1 Macro: 0.5705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1844, Accuracy: 0.9214, F1 Micro: 0.7619, F1 Macro: 0.6189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1456, Accuracy: 0.9189, F1 Micro: 0.7695, F1 Macro: 0.6708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1215, Accuracy: 0.9212, F1 Micro: 0.7709, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9243, F1 Micro: 0.7778, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0807, Accuracy: 0.9242, F1 Micro: 0.7803, F1 Macro: 0.6978\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.922, F1 Micro: 0.7779, F1 Macro: 0.7158\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.924, F1 Micro: 0.7758, F1 Macro: 0.706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9252, F1 Micro: 0.7822, F1 Macro: 0.7118\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9252, F1 Micro: 0.7822, F1 Macro: 0.7118\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.72      0.66      0.69       120\n",
      "  HS_Physical       0.79      0.32      0.46        72\n",
      "    HS_Gender       0.60      0.53      0.56        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.69      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 256.52955627441406 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3521, Accuracy: 0.8929, F1 Micro: 0.6239, F1 Macro: 0.3664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2358, Accuracy: 0.9134, F1 Micro: 0.7292, F1 Macro: 0.5405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1891, Accuracy: 0.9206, F1 Micro: 0.7577, F1 Macro: 0.6017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.15, Accuracy: 0.9197, F1 Micro: 0.7693, F1 Macro: 0.678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9225, F1 Micro: 0.7696, F1 Macro: 0.6709\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9243, F1 Micro: 0.7669, F1 Macro: 0.6735\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9233, F1 Micro: 0.7683, F1 Macro: 0.6822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9234, F1 Micro: 0.7701, F1 Macro: 0.6877\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9202, F1 Micro: 0.768, F1 Macro: 0.6826\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.9199, F1 Micro: 0.7692, F1 Macro: 0.6997\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9234, F1 Micro: 0.7701, F1 Macro: 0.6877\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.91      0.87      0.89       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.77      0.56      0.65       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.77      0.28      0.41        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.66      0.53      0.58       331\n",
      "    HS_Strong       0.86      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.76      0.65      0.69      5556\n",
      " weighted avg       0.80      0.74      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 254.2757806777954 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3476, Accuracy: 0.899, F1 Micro: 0.675, F1 Macro: 0.4374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2334, Accuracy: 0.9152, F1 Micro: 0.7401, F1 Macro: 0.5585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1889, Accuracy: 0.9211, F1 Micro: 0.7657, F1 Macro: 0.6237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1478, Accuracy: 0.9231, F1 Micro: 0.77, F1 Macro: 0.6733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9249, F1 Micro: 0.7729, F1 Macro: 0.6524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9255, F1 Micro: 0.7739, F1 Macro: 0.6851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9216, F1 Micro: 0.7744, F1 Macro: 0.6929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.924, F1 Micro: 0.7796, F1 Macro: 0.7138\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9244, F1 Micro: 0.7785, F1 Macro: 0.7091\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9241, F1 Micro: 0.7767, F1 Macro: 0.6992\n",
      "Model 3 - Iteration 8616: Accuracy: 0.924, F1 Micro: 0.7796, F1 Macro: 0.7138\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.68      0.68      0.68       402\n",
      "  HS_Religion       0.71      0.61      0.65       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.88      0.32      0.47        72\n",
      "    HS_Gender       0.61      0.53      0.57        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.61      0.60      0.61       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.70      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 257.7789297103882 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9144, F1 Micro: 0.7476, F1 Macro: 0.6319\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 25.26454734802246 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3445, Accuracy: 0.8972, F1 Micro: 0.6897, F1 Macro: 0.4636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2359, Accuracy: 0.9108, F1 Micro: 0.7473, F1 Macro: 0.5812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1878, Accuracy: 0.9203, F1 Micro: 0.7516, F1 Macro: 0.6066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1506, Accuracy: 0.9225, F1 Micro: 0.7771, F1 Macro: 0.6712\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9136, F1 Micro: 0.7624, F1 Macro: 0.6494\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9227, F1 Micro: 0.774, F1 Macro: 0.6918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9231, F1 Micro: 0.7778, F1 Macro: 0.7015\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9233, F1 Micro: 0.7771, F1 Macro: 0.7098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9248, F1 Micro: 0.7786, F1 Macro: 0.708\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9248, F1 Micro: 0.7719, F1 Macro: 0.6993\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9248, F1 Micro: 0.7786, F1 Macro: 0.708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.89      0.92      0.91       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.68      0.68      0.68       120\n",
      "  HS_Physical       0.83      0.33      0.48        72\n",
      "    HS_Gender       0.63      0.47      0.54        51\n",
      "     HS_Other       0.80      0.77      0.78       762\n",
      "      HS_Weak       0.73      0.70      0.72       689\n",
      "  HS_Moderate       0.62      0.59      0.60       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 258.73028898239136 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3526, Accuracy: 0.8956, F1 Micro: 0.6784, F1 Macro: 0.4009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9084, F1 Micro: 0.7391, F1 Macro: 0.5656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.9192, F1 Micro: 0.7556, F1 Macro: 0.6146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9248, F1 Micro: 0.7693, F1 Macro: 0.649\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9178, F1 Micro: 0.7671, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9234, F1 Micro: 0.7724, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9256, F1 Micro: 0.7776, F1 Macro: 0.6937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0674, Accuracy: 0.9238, F1 Micro: 0.7785, F1 Macro: 0.701\n",
      "Epoch 9/10, Train Loss: 0.0562, Accuracy: 0.9224, F1 Micro: 0.7758, F1 Macro: 0.7008\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9235, F1 Micro: 0.7693, F1 Macro: 0.7004\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9238, F1 Micro: 0.7785, F1 Macro: 0.701\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.85      0.94      0.89       992\n",
      "HS_Individual       0.75      0.70      0.73       732\n",
      "     HS_Group       0.68      0.73      0.71       402\n",
      "  HS_Religion       0.68      0.66      0.67       157\n",
      "      HS_Race       0.70      0.71      0.70       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.55      0.47      0.51        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.71       689\n",
      "  HS_Moderate       0.61      0.65      0.63       331\n",
      "    HS_Strong       0.86      0.89      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.69      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 260.85178685188293 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3498, Accuracy: 0.8944, F1 Micro: 0.6899, F1 Macro: 0.455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2372, Accuracy: 0.9096, F1 Micro: 0.7427, F1 Macro: 0.5684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1893, Accuracy: 0.9215, F1 Micro: 0.7684, F1 Macro: 0.6216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9248, F1 Micro: 0.7763, F1 Macro: 0.6657\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9189, F1 Micro: 0.761, F1 Macro: 0.6211\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.9225, F1 Micro: 0.7728, F1 Macro: 0.6859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9237, F1 Micro: 0.7766, F1 Macro: 0.6991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9248, F1 Micro: 0.7792, F1 Macro: 0.7092\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9243, F1 Micro: 0.7777, F1 Macro: 0.7107\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9229, F1 Micro: 0.7744, F1 Macro: 0.7072\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9248, F1 Micro: 0.7792, F1 Macro: 0.7092\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.87      0.92      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.79      0.59      0.68       157\n",
      "      HS_Race       0.72      0.67      0.69       120\n",
      "  HS_Physical       0.91      0.29      0.44        72\n",
      "    HS_Gender       0.63      0.53      0.57        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 257.9834668636322 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.915, F1 Micro: 0.7493, F1 Macro: 0.636\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.524275064468384 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3452, Accuracy: 0.8969, F1 Micro: 0.6737, F1 Macro: 0.4519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2335, Accuracy: 0.9158, F1 Micro: 0.7452, F1 Macro: 0.5728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.189, Accuracy: 0.9213, F1 Micro: 0.7633, F1 Macro: 0.6261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1533, Accuracy: 0.9248, F1 Micro: 0.7672, F1 Macro: 0.6671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1172, Accuracy: 0.9209, F1 Micro: 0.7703, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9252, F1 Micro: 0.7717, F1 Macro: 0.6933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9248, F1 Micro: 0.7765, F1 Macro: 0.6987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9252, F1 Micro: 0.7837, F1 Macro: 0.7085\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9245, F1 Micro: 0.778, F1 Macro: 0.7099\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9238, F1 Micro: 0.7734, F1 Macro: 0.707\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9252, F1 Micro: 0.7837, F1 Macro: 0.7085\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.73      0.67      0.70       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.61      0.43      0.51        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.67      0.58      0.62       331\n",
      "    HS_Strong       0.85      0.88      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.77      0.69      0.71      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 265.9854290485382 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3539, Accuracy: 0.8949, F1 Micro: 0.6597, F1 Macro: 0.4142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2379, Accuracy: 0.9132, F1 Micro: 0.7265, F1 Macro: 0.5227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1944, Accuracy: 0.9206, F1 Micro: 0.7558, F1 Macro: 0.6196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1531, Accuracy: 0.9224, F1 Micro: 0.7697, F1 Macro: 0.6447\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9229, F1 Micro: 0.7649, F1 Macro: 0.6708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1006, Accuracy: 0.925, F1 Micro: 0.7766, F1 Macro: 0.6884\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9183, F1 Micro: 0.7727, F1 Macro: 0.6864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9235, F1 Micro: 0.7778, F1 Macro: 0.6877\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9205, F1 Micro: 0.7663, F1 Macro: 0.6855\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.9221, F1 Micro: 0.773, F1 Macro: 0.6955\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9235, F1 Micro: 0.7778, F1 Macro: 0.6877\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.69      0.60      0.64       157\n",
      "      HS_Race       0.73      0.69      0.71       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.70      0.37      0.49        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.67      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 262.96778297424316 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3496, Accuracy: 0.8981, F1 Micro: 0.6723, F1 Macro: 0.4535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2333, Accuracy: 0.9148, F1 Micro: 0.7388, F1 Macro: 0.5625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.192, Accuracy: 0.9208, F1 Micro: 0.7637, F1 Macro: 0.6257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1519, Accuracy: 0.9242, F1 Micro: 0.7752, F1 Macro: 0.6533\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9232, F1 Micro: 0.7738, F1 Macro: 0.6825\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.924, F1 Micro: 0.7733, F1 Macro: 0.6877\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9236, F1 Micro: 0.7733, F1 Macro: 0.6926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9253, F1 Micro: 0.7815, F1 Macro: 0.7027\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9241, F1 Micro: 0.7717, F1 Macro: 0.6991\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.925, F1 Micro: 0.7767, F1 Macro: 0.7093\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9253, F1 Micro: 0.7815, F1 Macro: 0.7027\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.71      0.76      0.74       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       1.00      0.25      0.40        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.67      0.53      0.59       331\n",
      "    HS_Strong       0.82      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 262.4866409301758 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9155, F1 Micro: 0.751, F1 Macro: 0.6393\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 20.272541284561157 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.336, Accuracy: 0.8989, F1 Micro: 0.6875, F1 Macro: 0.4654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2343, Accuracy: 0.9169, F1 Micro: 0.7379, F1 Macro: 0.5611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.9194, F1 Micro: 0.7611, F1 Macro: 0.6127\n",
      "Epoch 4/10, Train Loss: 0.148, Accuracy: 0.9226, F1 Micro: 0.7575, F1 Macro: 0.6398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9247, F1 Micro: 0.7738, F1 Macro: 0.6735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0925, Accuracy: 0.9266, F1 Micro: 0.7765, F1 Macro: 0.6742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9244, F1 Micro: 0.7787, F1 Macro: 0.7041\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9234, F1 Micro: 0.7781, F1 Macro: 0.7156\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9253, F1 Micro: 0.7769, F1 Macro: 0.7165\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.9233, F1 Micro: 0.7765, F1 Macro: 0.7159\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9244, F1 Micro: 0.7787, F1 Macro: 0.7041\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.75      0.65      0.70       157\n",
      "      HS_Race       0.80      0.61      0.69       120\n",
      "  HS_Physical       1.00      0.28      0.43        72\n",
      "    HS_Gender       0.73      0.37      0.49        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.85      0.88      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 270.6306481361389 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3452, Accuracy: 0.8971, F1 Micro: 0.6725, F1 Macro: 0.4169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2385, Accuracy: 0.914, F1 Micro: 0.7254, F1 Macro: 0.5451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1866, Accuracy: 0.9171, F1 Micro: 0.7573, F1 Macro: 0.6065\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9216, F1 Micro: 0.7538, F1 Macro: 0.6291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.924, F1 Micro: 0.7659, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9271, F1 Micro: 0.7786, F1 Macro: 0.6658\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9219, F1 Micro: 0.7742, F1 Macro: 0.685\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.9234, F1 Micro: 0.7768, F1 Macro: 0.6936\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9228, F1 Micro: 0.7753, F1 Macro: 0.7058\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9232, F1 Micro: 0.7749, F1 Macro: 0.7066\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9271, F1 Micro: 0.7786, F1 Macro: 0.6658\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.84      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.77      0.69      0.73       732\n",
      "     HS_Group       0.75      0.65      0.69       402\n",
      "  HS_Religion       0.76      0.61      0.68       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       1.00      0.06      0.11        72\n",
      "    HS_Gender       0.92      0.22      0.35        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.76      0.67      0.71       689\n",
      "  HS_Moderate       0.66      0.54      0.59       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.82      0.74      0.78      5556\n",
      "    macro avg       0.82      0.62      0.67      5556\n",
      " weighted avg       0.82      0.74      0.77      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 266.4098210334778 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3411, Accuracy: 0.8973, F1 Micro: 0.6947, F1 Macro: 0.4753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2362, Accuracy: 0.9171, F1 Micro: 0.74, F1 Macro: 0.5637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1872, Accuracy: 0.9198, F1 Micro: 0.7619, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1494, Accuracy: 0.925, F1 Micro: 0.7672, F1 Macro: 0.6474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9266, F1 Micro: 0.7791, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.9274, F1 Micro: 0.7796, F1 Macro: 0.6877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0761, Accuracy: 0.9258, F1 Micro: 0.7831, F1 Macro: 0.7164\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9271, F1 Micro: 0.7795, F1 Macro: 0.7152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9259, F1 Micro: 0.7866, F1 Macro: 0.7217\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9261, F1 Micro: 0.7816, F1 Macro: 0.7151\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9259, F1 Micro: 0.7866, F1 Macro: 0.7217\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.76      0.74      0.75       732\n",
      "     HS_Group       0.68      0.73      0.70       402\n",
      "  HS_Religion       0.71      0.66      0.68       157\n",
      "      HS_Race       0.65      0.78      0.71       120\n",
      "  HS_Physical       0.62      0.36      0.46        72\n",
      "    HS_Gender       0.58      0.61      0.60        51\n",
      "     HS_Other       0.81      0.78      0.79       762\n",
      "      HS_Weak       0.75      0.72      0.73       689\n",
      "  HS_Moderate       0.61      0.66      0.63       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5556\n",
      "    macro avg       0.73      0.72      0.72      5556\n",
      " weighted avg       0.78      0.79      0.79      5556\n",
      "  samples avg       0.46      0.44      0.43      5556\n",
      "\n",
      "Training completed in 272.2915885448456 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.916, F1 Micro: 0.7525, F1 Macro: 0.6422\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 17.240750074386597 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3405, Accuracy: 0.9006, F1 Micro: 0.6741, F1 Macro: 0.4739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2313, Accuracy: 0.9152, F1 Micro: 0.7268, F1 Macro: 0.5515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9226, F1 Micro: 0.7659, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9257, F1 Micro: 0.7764, F1 Macro: 0.6683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9263, F1 Micro: 0.7797, F1 Macro: 0.6888\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9206, F1 Micro: 0.777, F1 Macro: 0.6967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.924, F1 Micro: 0.7802, F1 Macro: 0.7073\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9227, F1 Micro: 0.7764, F1 Macro: 0.7009\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9231, F1 Micro: 0.7795, F1 Macro: 0.7185\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9268, F1 Micro: 0.7786, F1 Macro: 0.7139\n",
      "Model 1 - Iteration 9218: Accuracy: 0.924, F1 Micro: 0.7802, F1 Macro: 0.7073\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.78      0.74       732\n",
      "     HS_Group       0.72      0.60      0.66       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.80      0.70      0.75       120\n",
      "  HS_Physical       0.55      0.36      0.44        72\n",
      "    HS_Gender       0.57      0.51      0.54        51\n",
      "     HS_Other       0.78      0.82      0.80       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.65      0.52      0.58       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.74      0.69      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 270.52655386924744 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3535, Accuracy: 0.896, F1 Micro: 0.652, F1 Macro: 0.4084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2386, Accuracy: 0.9097, F1 Micro: 0.7022, F1 Macro: 0.5113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.9222, F1 Micro: 0.7565, F1 Macro: 0.6046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.148, Accuracy: 0.9241, F1 Micro: 0.7673, F1 Macro: 0.6633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.9244, F1 Micro: 0.7697, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.9254, F1 Micro: 0.7813, F1 Macro: 0.6998\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9247, F1 Micro: 0.7664, F1 Macro: 0.689\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9236, F1 Micro: 0.7757, F1 Macro: 0.7004\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.9177, F1 Micro: 0.7706, F1 Macro: 0.6977\n",
      "Epoch 10/10, Train Loss: 0.045, Accuracy: 0.9242, F1 Micro: 0.7666, F1 Macro: 0.6957\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9254, F1 Micro: 0.7813, F1 Macro: 0.6998\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.76      0.64      0.70       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.68      0.24      0.35        72\n",
      "    HS_Gender       0.58      0.41      0.48        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.68      0.53      0.60       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 271.2976441383362 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3451, Accuracy: 0.9017, F1 Micro: 0.6846, F1 Macro: 0.4704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2327, Accuracy: 0.9147, F1 Micro: 0.72, F1 Macro: 0.5417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1829, Accuracy: 0.9243, F1 Micro: 0.7681, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.924, F1 Micro: 0.7758, F1 Macro: 0.6748\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.926, F1 Micro: 0.7732, F1 Macro: 0.6806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0915, Accuracy: 0.9225, F1 Micro: 0.7765, F1 Macro: 0.7039\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9239, F1 Micro: 0.7714, F1 Macro: 0.6956\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9225, F1 Micro: 0.7703, F1 Macro: 0.6978\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9213, F1 Micro: 0.7753, F1 Macro: 0.7087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9224, F1 Micro: 0.7773, F1 Macro: 0.7162\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9224, F1 Micro: 0.7773, F1 Macro: 0.7162\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.71      0.68      0.70       402\n",
      "  HS_Religion       0.65      0.72      0.68       157\n",
      "      HS_Race       0.71      0.74      0.72       120\n",
      "  HS_Physical       0.67      0.36      0.47        72\n",
      "    HS_Gender       0.57      0.59      0.58        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.61      0.63       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.73      0.72      0.72      5556\n",
      " weighted avg       0.77      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 270.12901401519775 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9164, F1 Micro: 0.7538, F1 Macro: 0.6454\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.388420820236206 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3369, Accuracy: 0.9008, F1 Micro: 0.6908, F1 Macro: 0.4976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2228, Accuracy: 0.9147, F1 Micro: 0.7461, F1 Macro: 0.5954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1815, Accuracy: 0.9232, F1 Micro: 0.7633, F1 Macro: 0.6095\n",
      "Epoch 4/10, Train Loss: 0.1432, Accuracy: 0.9229, F1 Micro: 0.7632, F1 Macro: 0.6614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1152, Accuracy: 0.9235, F1 Micro: 0.7736, F1 Macro: 0.6886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0929, Accuracy: 0.925, F1 Micro: 0.7746, F1 Macro: 0.6941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9245, F1 Micro: 0.781, F1 Macro: 0.7063\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.9257, F1 Micro: 0.7719, F1 Macro: 0.7057\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9226, F1 Micro: 0.7719, F1 Macro: 0.6996\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9244, F1 Micro: 0.7783, F1 Macro: 0.7105\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9245, F1 Micro: 0.781, F1 Macro: 0.7063\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.80      0.63      0.71       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.88      0.31      0.45        72\n",
      "    HS_Gender       0.55      0.43      0.48        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.69      0.76      0.73       689\n",
      "  HS_Moderate       0.69      0.52      0.59       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 275.42058801651 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3469, Accuracy: 0.8939, F1 Micro: 0.6289, F1 Macro: 0.355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2293, Accuracy: 0.9131, F1 Micro: 0.7331, F1 Macro: 0.5764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.183, Accuracy: 0.9206, F1 Micro: 0.7484, F1 Macro: 0.6012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9225, F1 Micro: 0.7699, F1 Macro: 0.6569\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9236, F1 Micro: 0.766, F1 Macro: 0.6684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.9249, F1 Micro: 0.7738, F1 Macro: 0.6858\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.921, F1 Micro: 0.7731, F1 Macro: 0.7036\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9216, F1 Micro: 0.7729, F1 Macro: 0.7044\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9219, F1 Micro: 0.7651, F1 Macro: 0.6969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9249, F1 Micro: 0.7763, F1 Macro: 0.7057\n",
      "Model 2 - Iteration 9418: Accuracy: 0.9249, F1 Micro: 0.7763, F1 Macro: 0.7057\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.90      0.91      0.91       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.74      0.61      0.66       157\n",
      "      HS_Race       0.82      0.57      0.67       120\n",
      "  HS_Physical       0.73      0.33      0.46        72\n",
      "    HS_Gender       0.61      0.53      0.57        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.68      0.52      0.59       331\n",
      "    HS_Strong       0.88      0.86      0.87       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.67      0.71      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 275.596892118454 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3425, Accuracy: 0.8994, F1 Micro: 0.6889, F1 Macro: 0.4671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2249, Accuracy: 0.9142, F1 Micro: 0.7427, F1 Macro: 0.5928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.9237, F1 Micro: 0.7651, F1 Macro: 0.6173\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9215, F1 Micro: 0.7615, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1199, Accuracy: 0.9244, F1 Micro: 0.7766, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.9252, F1 Micro: 0.7787, F1 Macro: 0.6976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.9258, F1 Micro: 0.7793, F1 Macro: 0.7059\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9239, F1 Micro: 0.7773, F1 Macro: 0.7096\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9221, F1 Micro: 0.7765, F1 Macro: 0.7121\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9245, F1 Micro: 0.7714, F1 Macro: 0.7071\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9258, F1 Micro: 0.7793, F1 Macro: 0.7059\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.77      0.57      0.66       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.83      0.28      0.42        72\n",
      "    HS_Gender       0.64      0.49      0.56        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 275.9658043384552 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9168, F1 Micro: 0.7549, F1 Macro: 0.6481\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.849703788757324 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3398, Accuracy: 0.901, F1 Micro: 0.7009, F1 Macro: 0.4929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2295, Accuracy: 0.9161, F1 Micro: 0.7546, F1 Macro: 0.5913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1808, Accuracy: 0.9135, F1 Micro: 0.7621, F1 Macro: 0.6386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9269, F1 Micro: 0.7776, F1 Macro: 0.6815\n",
      "Epoch 5/10, Train Loss: 0.114, Accuracy: 0.9259, F1 Micro: 0.7753, F1 Macro: 0.6782\n",
      "Epoch 6/10, Train Loss: 0.0922, Accuracy: 0.9221, F1 Micro: 0.7776, F1 Macro: 0.6929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9248, F1 Micro: 0.7784, F1 Macro: 0.6977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.925, F1 Micro: 0.7791, F1 Macro: 0.7074\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9244, F1 Micro: 0.7751, F1 Macro: 0.7095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.926, F1 Micro: 0.7815, F1 Macro: 0.7135\n",
      "Model 1 - Iteration 9618: Accuracy: 0.926, F1 Micro: 0.7815, F1 Macro: 0.7135\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.92      0.89      0.90       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.72      0.55      0.62       157\n",
      "      HS_Race       0.74      0.65      0.69       120\n",
      "  HS_Physical       0.72      0.39      0.50        72\n",
      "    HS_Gender       0.60      0.51      0.55        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.66      0.59      0.63       331\n",
      "    HS_Strong       0.87      0.87      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 281.65656423568726 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3492, Accuracy: 0.8975, F1 Micro: 0.6961, F1 Macro: 0.4719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2322, Accuracy: 0.9152, F1 Micro: 0.7433, F1 Macro: 0.5827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1838, Accuracy: 0.9098, F1 Micro: 0.7579, F1 Macro: 0.6293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9245, F1 Micro: 0.771, F1 Macro: 0.6668\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9243, F1 Micro: 0.7705, F1 Macro: 0.6663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9231, F1 Micro: 0.7775, F1 Macro: 0.6926\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9232, F1 Micro: 0.7704, F1 Macro: 0.6903\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9244, F1 Micro: 0.772, F1 Macro: 0.6895\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9232, F1 Micro: 0.7643, F1 Macro: 0.7002\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9228, F1 Micro: 0.7737, F1 Macro: 0.6978\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9231, F1 Micro: 0.7775, F1 Macro: 0.6926\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.75      0.59      0.66       402\n",
      "  HS_Religion       0.72      0.61      0.66       157\n",
      "      HS_Race       0.82      0.70      0.75       120\n",
      "  HS_Physical       0.62      0.22      0.33        72\n",
      "    HS_Gender       0.49      0.49      0.49        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.73      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 277.8141305446625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3439, Accuracy: 0.8987, F1 Micro: 0.6996, F1 Macro: 0.4885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2322, Accuracy: 0.9136, F1 Micro: 0.7485, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9134, F1 Micro: 0.7622, F1 Macro: 0.6289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1463, Accuracy: 0.9234, F1 Micro: 0.7707, F1 Macro: 0.6609\n",
      "Epoch 5/10, Train Loss: 0.1155, Accuracy: 0.9254, F1 Micro: 0.7676, F1 Macro: 0.6483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9218, F1 Micro: 0.7743, F1 Macro: 0.6939\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9251, F1 Micro: 0.7737, F1 Macro: 0.7016\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9188, F1 Micro: 0.7712, F1 Macro: 0.699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9241, F1 Micro: 0.7764, F1 Macro: 0.7002\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9239, F1 Micro: 0.7738, F1 Macro: 0.7054\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9241, F1 Micro: 0.7764, F1 Macro: 0.7002\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.75      0.55      0.64       157\n",
      "      HS_Race       0.80      0.61      0.69       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.62      0.49      0.55        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.77      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 280.42757868766785 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9171, F1 Micro: 0.756, F1 Macro: 0.6505\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.302652597427368 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3314, Accuracy: 0.9023, F1 Micro: 0.692, F1 Macro: 0.511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2222, Accuracy: 0.9151, F1 Micro: 0.7529, F1 Macro: 0.5928\n",
      "Epoch 3/10, Train Loss: 0.1815, Accuracy: 0.9201, F1 Micro: 0.7401, F1 Macro: 0.6201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1422, Accuracy: 0.9218, F1 Micro: 0.7764, F1 Macro: 0.684\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.923, F1 Micro: 0.774, F1 Macro: 0.6851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9236, F1 Micro: 0.78, F1 Macro: 0.6986\n",
      "Epoch 7/10, Train Loss: 0.0734, Accuracy: 0.9243, F1 Micro: 0.7766, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0576, Accuracy: 0.9272, F1 Micro: 0.7882, F1 Macro: 0.7209\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9168, F1 Micro: 0.7717, F1 Macro: 0.6992\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.926, F1 Micro: 0.7824, F1 Macro: 0.7171\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9272, F1 Micro: 0.7882, F1 Macro: 0.7209\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.74      0.76      0.75       732\n",
      "     HS_Group       0.72      0.66      0.69       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.71      0.80      0.75       120\n",
      "  HS_Physical       0.75      0.33      0.46        72\n",
      "    HS_Gender       0.57      0.59      0.58        51\n",
      "     HS_Other       0.81      0.79      0.80       762\n",
      "      HS_Weak       0.73      0.74      0.74       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.84      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.79      0.79      5556\n",
      "    macro avg       0.75      0.71      0.72      5556\n",
      " weighted avg       0.79      0.79      0.79      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 283.98376965522766 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3429, Accuracy: 0.9003, F1 Micro: 0.6808, F1 Macro: 0.4723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9135, F1 Micro: 0.7435, F1 Macro: 0.5744\n",
      "Epoch 3/10, Train Loss: 0.1854, Accuracy: 0.9191, F1 Micro: 0.7405, F1 Macro: 0.6138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9226, F1 Micro: 0.7712, F1 Macro: 0.6722\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.9221, F1 Micro: 0.7614, F1 Macro: 0.6712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9227, F1 Micro: 0.7765, F1 Macro: 0.6903\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9241, F1 Micro: 0.7729, F1 Macro: 0.6827\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9244, F1 Micro: 0.7719, F1 Macro: 0.7051\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9244, F1 Micro: 0.775, F1 Macro: 0.7082\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9224, F1 Micro: 0.7756, F1 Macro: 0.7034\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9227, F1 Micro: 0.7765, F1 Macro: 0.6903\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.77      0.74       732\n",
      "     HS_Group       0.72      0.63      0.67       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.59      0.47      0.52        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.55      0.59       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 281.16890358924866 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3369, Accuracy: 0.9009, F1 Micro: 0.6816, F1 Macro: 0.4966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.9133, F1 Micro: 0.7508, F1 Macro: 0.5886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.9213, F1 Micro: 0.7555, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.147, Accuracy: 0.923, F1 Micro: 0.7732, F1 Macro: 0.6814\n",
      "Epoch 5/10, Train Loss: 0.1167, Accuracy: 0.9235, F1 Micro: 0.7653, F1 Macro: 0.6734\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9163, F1 Micro: 0.7688, F1 Macro: 0.6801\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9258, F1 Micro: 0.77, F1 Macro: 0.6985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9252, F1 Micro: 0.7773, F1 Macro: 0.7075\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.9218, F1 Micro: 0.772, F1 Macro: 0.7017\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9166, F1 Micro: 0.7664, F1 Macro: 0.6963\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9252, F1 Micro: 0.7773, F1 Macro: 0.7075\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.72      0.74       732\n",
      "     HS_Group       0.71      0.66      0.69       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.81      0.29      0.43        72\n",
      "    HS_Gender       0.54      0.53      0.53        51\n",
      "     HS_Other       0.81      0.74      0.77       762\n",
      "      HS_Weak       0.73      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.57      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 283.40524530410767 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9174, F1 Micro: 0.757, F1 Macro: 0.6528\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.542095184326172 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3294, Accuracy: 0.8999, F1 Micro: 0.6914, F1 Macro: 0.4744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2262, Accuracy: 0.9148, F1 Micro: 0.7517, F1 Macro: 0.5961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1788, Accuracy: 0.9214, F1 Micro: 0.7604, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1362, Accuracy: 0.9226, F1 Micro: 0.773, F1 Macro: 0.6799\n",
      "Epoch 5/10, Train Loss: 0.1133, Accuracy: 0.9239, F1 Micro: 0.7704, F1 Macro: 0.6738\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9235, F1 Micro: 0.7634, F1 Macro: 0.6733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9263, F1 Micro: 0.7848, F1 Macro: 0.7141\n",
      "Epoch 8/10, Train Loss: 0.0577, Accuracy: 0.921, F1 Micro: 0.7771, F1 Macro: 0.7032\n",
      "Epoch 9/10, Train Loss: 0.0503, Accuracy: 0.9208, F1 Micro: 0.7804, F1 Macro: 0.7117\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9241, F1 Micro: 0.7778, F1 Macro: 0.7135\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9263, F1 Micro: 0.7848, F1 Macro: 0.7141\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.71      0.79      0.75       732\n",
      "     HS_Group       0.76      0.59      0.66       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.83      0.65      0.73       120\n",
      "  HS_Physical       0.92      0.31      0.46        72\n",
      "    HS_Gender       0.71      0.47      0.56        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.78      0.73       689\n",
      "  HS_Moderate       0.67      0.52      0.58       331\n",
      "    HS_Strong       0.90      0.84      0.87       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.79      0.68      0.71      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 288.4479250907898 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3408, Accuracy: 0.899, F1 Micro: 0.6795, F1 Macro: 0.4359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2307, Accuracy: 0.9161, F1 Micro: 0.7432, F1 Macro: 0.5876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1821, Accuracy: 0.9225, F1 Micro: 0.7614, F1 Macro: 0.6104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1396, Accuracy: 0.9208, F1 Micro: 0.7648, F1 Macro: 0.6702\n",
      "Epoch 5/10, Train Loss: 0.1151, Accuracy: 0.9176, F1 Micro: 0.7628, F1 Macro: 0.6454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9203, F1 Micro: 0.7701, F1 Macro: 0.6832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9249, F1 Micro: 0.7725, F1 Macro: 0.6945\n",
      "Epoch 8/10, Train Loss: 0.0598, Accuracy: 0.9199, F1 Micro: 0.7692, F1 Macro: 0.6919\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9209, F1 Micro: 0.7719, F1 Macro: 0.6989\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.9231, F1 Micro: 0.7663, F1 Macro: 0.6869\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9249, F1 Micro: 0.7725, F1 Macro: 0.6945\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.84      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.78      0.62      0.69       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.86      0.65      0.74       120\n",
      "  HS_Physical       0.84      0.22      0.35        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.66      0.54      0.60       331\n",
      "    HS_Strong       0.92      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.79      0.64      0.69      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 289.82347440719604 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3352, Accuracy: 0.9004, F1 Micro: 0.6917, F1 Macro: 0.4729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2286, Accuracy: 0.9165, F1 Micro: 0.7471, F1 Macro: 0.5886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1831, Accuracy: 0.9221, F1 Micro: 0.7594, F1 Macro: 0.6076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1427, Accuracy: 0.9216, F1 Micro: 0.7718, F1 Macro: 0.6826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.115, Accuracy: 0.9245, F1 Micro: 0.7769, F1 Macro: 0.6768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0934, Accuracy: 0.9254, F1 Micro: 0.7798, F1 Macro: 0.6939\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.923, F1 Micro: 0.7758, F1 Macro: 0.6967\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9209, F1 Micro: 0.7738, F1 Macro: 0.7064\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9244, F1 Micro: 0.7736, F1 Macro: 0.7047\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9247, F1 Micro: 0.7752, F1 Macro: 0.7147\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9254, F1 Micro: 0.7798, F1 Macro: 0.6939\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.73      0.77      0.75       732\n",
      "     HS_Group       0.74      0.61      0.67       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.55      0.51      0.53        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.75      0.73       689\n",
      "  HS_Moderate       0.66      0.53      0.59       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.69      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 292.73113107681274 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9178, F1 Micro: 0.7579, F1 Macro: 0.6547\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.454941511154175 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3231, Accuracy: 0.9011, F1 Micro: 0.6571, F1 Macro: 0.4555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2224, Accuracy: 0.9164, F1 Micro: 0.7328, F1 Macro: 0.5669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1754, Accuracy: 0.923, F1 Micro: 0.7581, F1 Macro: 0.6309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1402, Accuracy: 0.924, F1 Micro: 0.7784, F1 Macro: 0.6572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1154, Accuracy: 0.9246, F1 Micro: 0.7796, F1 Macro: 0.6789\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9217, F1 Micro: 0.7779, F1 Macro: 0.6964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9263, F1 Micro: 0.7799, F1 Macro: 0.7127\n",
      "Epoch 8/10, Train Loss: 0.0585, Accuracy: 0.9255, F1 Micro: 0.7789, F1 Macro: 0.7127\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9213, F1 Micro: 0.7792, F1 Macro: 0.7168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0418, Accuracy: 0.9268, F1 Micro: 0.781, F1 Macro: 0.7243\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9268, F1 Micro: 0.781, F1 Macro: 0.7243\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.83      0.85      1134\n",
      "      Abusive       0.89      0.93      0.91       992\n",
      "HS_Individual       0.77      0.70      0.74       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.75      0.62      0.68       157\n",
      "      HS_Race       0.70      0.82      0.76       120\n",
      "  HS_Physical       0.59      0.46      0.52        72\n",
      "    HS_Gender       0.64      0.59      0.61        51\n",
      "     HS_Other       0.83      0.72      0.77       762\n",
      "      HS_Weak       0.75      0.68      0.71       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.81      0.76      0.78      5556\n",
      "    macro avg       0.75      0.70      0.72      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 297.84388303756714 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.334, Accuracy: 0.9002, F1 Micro: 0.6792, F1 Macro: 0.4418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2274, Accuracy: 0.9125, F1 Micro: 0.711, F1 Macro: 0.5119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1785, Accuracy: 0.9217, F1 Micro: 0.7514, F1 Macro: 0.6158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1424, Accuracy: 0.9238, F1 Micro: 0.7738, F1 Macro: 0.6362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9247, F1 Micro: 0.7807, F1 Macro: 0.6714\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.923, F1 Micro: 0.7747, F1 Macro: 0.6882\n",
      "Epoch 7/10, Train Loss: 0.0715, Accuracy: 0.9218, F1 Micro: 0.7754, F1 Macro: 0.6908\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9237, F1 Micro: 0.776, F1 Macro: 0.6999\n",
      "Epoch 9/10, Train Loss: 0.0497, Accuracy: 0.9232, F1 Micro: 0.7729, F1 Macro: 0.7034\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9228, F1 Micro: 0.7652, F1 Macro: 0.6938\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9247, F1 Micro: 0.7807, F1 Macro: 0.6714\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.76      0.62      0.68       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.69      0.76      0.73       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.70      0.76      0.73       689\n",
      "  HS_Moderate       0.70      0.53      0.60       331\n",
      "    HS_Strong       0.86      0.81      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.65      0.67      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 293.05187344551086 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3298, Accuracy: 0.9012, F1 Micro: 0.6648, F1 Macro: 0.4718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.225, Accuracy: 0.9153, F1 Micro: 0.7275, F1 Macro: 0.5489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1798, Accuracy: 0.9247, F1 Micro: 0.7647, F1 Macro: 0.63\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.143, Accuracy: 0.9254, F1 Micro: 0.7781, F1 Macro: 0.6452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9251, F1 Micro: 0.7799, F1 Macro: 0.676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0911, Accuracy: 0.925, F1 Micro: 0.7812, F1 Macro: 0.7039\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9226, F1 Micro: 0.7775, F1 Macro: 0.7026\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.924, F1 Micro: 0.7779, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.926, F1 Micro: 0.7832, F1 Macro: 0.7165\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.9256, F1 Micro: 0.7749, F1 Macro: 0.7158\n",
      "Model 3 - Iteration 10218: Accuracy: 0.926, F1 Micro: 0.7832, F1 Macro: 0.7165\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.91      0.90      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.69      0.64      0.66       157\n",
      "      HS_Race       0.70      0.69      0.70       120\n",
      "  HS_Physical       0.63      0.40      0.49        72\n",
      "    HS_Gender       0.60      0.55      0.57        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.68      0.56      0.61       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.74      0.70      0.72      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 296.4956600666046 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9181, F1 Micro: 0.7588, F1 Macro: 0.6566\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 5.598972797393799 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3236, Accuracy: 0.9022, F1 Micro: 0.6954, F1 Macro: 0.4817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2163, Accuracy: 0.9179, F1 Micro: 0.7563, F1 Macro: 0.6035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1733, Accuracy: 0.9216, F1 Micro: 0.7735, F1 Macro: 0.6635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1435, Accuracy: 0.9219, F1 Micro: 0.7749, F1 Macro: 0.6608\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9227, F1 Micro: 0.774, F1 Macro: 0.6891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9248, F1 Micro: 0.7764, F1 Macro: 0.6913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.9258, F1 Micro: 0.7823, F1 Macro: 0.6984\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9247, F1 Micro: 0.7758, F1 Macro: 0.705\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9223, F1 Micro: 0.773, F1 Macro: 0.7039\n",
      "Epoch 10/10, Train Loss: 0.0413, Accuracy: 0.9215, F1 Micro: 0.7754, F1 Macro: 0.7022\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9258, F1 Micro: 0.7823, F1 Macro: 0.6984\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.77      0.74       732\n",
      "     HS_Group       0.76      0.59      0.67       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.79      0.68      0.73       120\n",
      "  HS_Physical       1.00      0.25      0.40        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.70      0.50      0.58       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.79      0.66      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 299.38151931762695 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3352, Accuracy: 0.8967, F1 Micro: 0.6938, F1 Macro: 0.469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2216, Accuracy: 0.9139, F1 Micro: 0.7528, F1 Macro: 0.6077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1754, Accuracy: 0.9235, F1 Micro: 0.7712, F1 Macro: 0.6389\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9229, F1 Micro: 0.7705, F1 Macro: 0.6401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1125, Accuracy: 0.9202, F1 Micro: 0.7717, F1 Macro: 0.6838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.9241, F1 Micro: 0.7756, F1 Macro: 0.6891\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9198, F1 Micro: 0.7731, F1 Macro: 0.6847\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9238, F1 Micro: 0.7731, F1 Macro: 0.698\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.924, F1 Micro: 0.7749, F1 Macro: 0.702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9225, F1 Micro: 0.7758, F1 Macro: 0.7032\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9225, F1 Micro: 0.7758, F1 Macro: 0.7032\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.91      0.91       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.68      0.62      0.65       157\n",
      "      HS_Race       0.75      0.77      0.76       120\n",
      "  HS_Physical       0.69      0.33      0.45        72\n",
      "    HS_Gender       0.59      0.43      0.50        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.74      0.68      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 299.7135558128357 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3303, Accuracy: 0.9008, F1 Micro: 0.7056, F1 Macro: 0.4975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2183, Accuracy: 0.9153, F1 Micro: 0.7525, F1 Macro: 0.6025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1754, Accuracy: 0.9227, F1 Micro: 0.7738, F1 Macro: 0.6538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1444, Accuracy: 0.9263, F1 Micro: 0.7807, F1 Macro: 0.6664\n",
      "Epoch 5/10, Train Loss: 0.1131, Accuracy: 0.9167, F1 Micro: 0.7657, F1 Macro: 0.6747\n",
      "Epoch 6/10, Train Loss: 0.09, Accuracy: 0.9255, F1 Micro: 0.7769, F1 Macro: 0.6993\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.9242, F1 Micro: 0.7762, F1 Macro: 0.6983\n",
      "Epoch 8/10, Train Loss: 0.0583, Accuracy: 0.9244, F1 Micro: 0.7702, F1 Macro: 0.7032\n",
      "Epoch 9/10, Train Loss: 0.0489, Accuracy: 0.9254, F1 Micro: 0.7762, F1 Macro: 0.7105\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.9241, F1 Micro: 0.7741, F1 Macro: 0.7135\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9263, F1 Micro: 0.7807, F1 Macro: 0.6664\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.86      0.87      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.74      0.74      0.74       732\n",
      "     HS_Group       0.77      0.62      0.69       402\n",
      "  HS_Religion       0.87      0.52      0.65       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.69      0.56      0.62       331\n",
      "    HS_Strong       0.88      0.73      0.80       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.62      0.67      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 299.2377681732178 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9183, F1 Micro: 0.7596, F1 Macro: 0.6578\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.6246755123138428 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3229, Accuracy: 0.9015, F1 Micro: 0.6988, F1 Macro: 0.4965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2199, Accuracy: 0.9182, F1 Micro: 0.7562, F1 Macro: 0.5896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1682, Accuracy: 0.9234, F1 Micro: 0.7706, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1329, Accuracy: 0.9219, F1 Micro: 0.7748, F1 Macro: 0.6827\n",
      "Epoch 5/10, Train Loss: 0.1052, Accuracy: 0.925, F1 Micro: 0.7729, F1 Macro: 0.686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0883, Accuracy: 0.9247, F1 Micro: 0.776, F1 Macro: 0.693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9231, F1 Micro: 0.7786, F1 Macro: 0.6948\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.9234, F1 Micro: 0.7749, F1 Macro: 0.704\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.9237, F1 Micro: 0.7746, F1 Macro: 0.7072\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9211, F1 Micro: 0.7713, F1 Macro: 0.7128\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9231, F1 Micro: 0.7786, F1 Macro: 0.6948\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.73      0.63      0.67       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.76      0.75      0.75       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.68      0.76      0.72       689\n",
      "  HS_Moderate       0.67      0.54      0.60       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 302.2463321685791 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3319, Accuracy: 0.8994, F1 Micro: 0.6755, F1 Macro: 0.4016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.915, F1 Micro: 0.7377, F1 Macro: 0.551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1719, Accuracy: 0.9213, F1 Micro: 0.7567, F1 Macro: 0.6488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1376, Accuracy: 0.9247, F1 Micro: 0.7717, F1 Macro: 0.6659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1068, Accuracy: 0.9254, F1 Micro: 0.7746, F1 Macro: 0.6785\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9233, F1 Micro: 0.7742, F1 Macro: 0.6816\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9241, F1 Micro: 0.7692, F1 Macro: 0.6825\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9239, F1 Micro: 0.7728, F1 Macro: 0.6993\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9226, F1 Micro: 0.7717, F1 Macro: 0.7056\n",
      "Epoch 10/10, Train Loss: 0.0403, Accuracy: 0.9217, F1 Micro: 0.7737, F1 Macro: 0.7037\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9254, F1 Micro: 0.7746, F1 Macro: 0.6785\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.76      0.67      0.71       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.81      0.60      0.69       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.75      0.65      0.70       689\n",
      "  HS_Moderate       0.65      0.61      0.63       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 300.6502673625946 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3265, Accuracy: 0.9034, F1 Micro: 0.6929, F1 Macro: 0.4663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2219, Accuracy: 0.9171, F1 Micro: 0.7496, F1 Macro: 0.568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1674, Accuracy: 0.9218, F1 Micro: 0.7685, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1343, Accuracy: 0.9244, F1 Micro: 0.7792, F1 Macro: 0.6754\n",
      "Epoch 5/10, Train Loss: 0.1088, Accuracy: 0.9257, F1 Micro: 0.7687, F1 Macro: 0.6726\n",
      "Epoch 6/10, Train Loss: 0.0888, Accuracy: 0.9265, F1 Micro: 0.7774, F1 Macro: 0.6968\n",
      "Epoch 7/10, Train Loss: 0.0716, Accuracy: 0.925, F1 Micro: 0.7729, F1 Macro: 0.6952\n",
      "Epoch 8/10, Train Loss: 0.0584, Accuracy: 0.9238, F1 Micro: 0.7771, F1 Macro: 0.7101\n",
      "Epoch 9/10, Train Loss: 0.0498, Accuracy: 0.9206, F1 Micro: 0.7763, F1 Macro: 0.7126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.04, Accuracy: 0.9256, F1 Micro: 0.7803, F1 Macro: 0.7203\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9256, F1 Micro: 0.7803, F1 Macro: 0.7203\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.77      0.69      0.73       732\n",
      "     HS_Group       0.68      0.70      0.69       402\n",
      "  HS_Religion       0.73      0.69      0.71       157\n",
      "      HS_Race       0.70      0.77      0.73       120\n",
      "  HS_Physical       0.73      0.38      0.50        72\n",
      "    HS_Gender       0.57      0.53      0.55        51\n",
      "     HS_Other       0.80      0.77      0.78       762\n",
      "      HS_Weak       0.76      0.66      0.71       689\n",
      "  HS_Moderate       0.60      0.65      0.63       331\n",
      "    HS_Strong       0.90      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.75      0.70      0.72      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 301.2433133125305 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9186, F1 Micro: 0.7602, F1 Macro: 0.6592\n",
      "Total sampling time: 1232.75 seconds\n",
      "Total runtime: 20388.086096286774 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407329d9",
   "metadata": {
    "papermill": {
     "duration": 0.17161,
     "end_time": "2025-03-15T13:24:24.790393",
     "exception": false,
     "start_time": "2025-03-15T13:24:24.618783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5491, Accuracy: 0.8289, F1 Micro: 0.0306, F1 Macro: 0.0129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4208, Accuracy: 0.834, F1 Micro: 0.1005, F1 Macro: 0.0359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3922, Accuracy: 0.8387, F1 Micro: 0.1585, F1 Macro: 0.0546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3713, Accuracy: 0.8584, F1 Micro: 0.3786, F1 Macro: 0.1292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3385, Accuracy: 0.8753, F1 Micro: 0.543, F1 Macro: 0.2484\n",
      "Epoch 6/10, Train Loss: 0.2825, Accuracy: 0.8763, F1 Micro: 0.5213, F1 Macro: 0.2401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2593, Accuracy: 0.876, F1 Micro: 0.6157, F1 Macro: 0.3145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2339, Accuracy: 0.8811, F1 Micro: 0.6249, F1 Macro: 0.3345\n",
      "Epoch 9/10, Train Loss: 0.1932, Accuracy: 0.8825, F1 Micro: 0.5928, F1 Macro: 0.3207\n",
      "Epoch 10/10, Train Loss: 0.1839, Accuracy: 0.8827, F1 Micro: 0.5855, F1 Macro: 0.3239\n",
      "Model 1 - Iteration 658: Accuracy: 0.8811, F1 Micro: 0.6249, F1 Macro: 0.3345\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.81      0.78      1134\n",
      "      Abusive       0.82      0.79      0.80       992\n",
      "HS_Individual       0.62      0.57      0.59       732\n",
      "     HS_Group       0.57      0.27      0.37       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.70      0.66       762\n",
      "      HS_Weak       0.57      0.55      0.56       689\n",
      "  HS_Moderate       0.39      0.17      0.24       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.68      0.57      0.62      5556\n",
      "    macro avg       0.36      0.32      0.33      5556\n",
      " weighted avg       0.61      0.57      0.58      5556\n",
      "  samples avg       0.39      0.33      0.33      5556\n",
      "\n",
      "Training completed in 58.6789665222168 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.8322, F1 Micro: 0.1081, F1 Macro: 0.0369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4119, Accuracy: 0.8348, F1 Micro: 0.1161, F1 Macro: 0.041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3853, Accuracy: 0.8421, F1 Micro: 0.1873, F1 Macro: 0.0689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3658, Accuracy: 0.8578, F1 Micro: 0.3663, F1 Macro: 0.1246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3301, Accuracy: 0.8746, F1 Micro: 0.5543, F1 Macro: 0.2532\n",
      "Epoch 6/10, Train Loss: 0.2728, Accuracy: 0.876, F1 Micro: 0.5368, F1 Macro: 0.2593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2509, Accuracy: 0.8686, F1 Micro: 0.6215, F1 Macro: 0.3486\n",
      "Epoch 8/10, Train Loss: 0.2243, Accuracy: 0.8785, F1 Micro: 0.6117, F1 Macro: 0.3553\n",
      "Epoch 9/10, Train Loss: 0.1855, Accuracy: 0.8783, F1 Micro: 0.5886, F1 Macro: 0.3502\n",
      "Epoch 10/10, Train Loss: 0.1751, Accuracy: 0.8801, F1 Micro: 0.5869, F1 Macro: 0.3511\n",
      "Model 2 - Iteration 658: Accuracy: 0.8686, F1 Micro: 0.6215, F1 Macro: 0.3486\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.71      0.86      0.78      1134\n",
      "      Abusive       0.76      0.78      0.77       992\n",
      "HS_Individual       0.58      0.64      0.60       732\n",
      "     HS_Group       0.49      0.48      0.48       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.57      0.73      0.64       762\n",
      "      HS_Weak       0.56      0.58      0.57       689\n",
      "  HS_Moderate       0.33      0.35      0.34       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.62      0.63      0.62      5556\n",
      "    macro avg       0.33      0.37      0.35      5556\n",
      " weighted avg       0.56      0.63      0.59      5556\n",
      "  samples avg       0.37      0.35      0.33      5556\n",
      "\n",
      "Training completed in 56.173216342926025 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.543, Accuracy: 0.829, F1 Micro: 0.0251, F1 Macro: 0.0108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4348, Accuracy: 0.8315, F1 Micro: 0.0536, F1 Macro: 0.0218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.399, Accuracy: 0.838, F1 Micro: 0.1576, F1 Macro: 0.0517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3805, Accuracy: 0.8491, F1 Micro: 0.272, F1 Macro: 0.0938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3531, Accuracy: 0.8665, F1 Micro: 0.4398, F1 Macro: 0.1871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2961, Accuracy: 0.8737, F1 Micro: 0.5013, F1 Macro: 0.2322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2693, Accuracy: 0.8773, F1 Micro: 0.5913, F1 Macro: 0.296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2417, Accuracy: 0.8767, F1 Micro: 0.5973, F1 Macro: 0.3192\n",
      "Epoch 9/10, Train Loss: 0.201, Accuracy: 0.8762, F1 Micro: 0.5913, F1 Macro: 0.3232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1901, Accuracy: 0.8794, F1 Micro: 0.5991, F1 Macro: 0.3436\n",
      "Model 3 - Iteration 658: Accuracy: 0.8794, F1 Micro: 0.5991, F1 Macro: 0.3436\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.70      0.75      1134\n",
      "      Abusive       0.83      0.74      0.78       992\n",
      "HS_Individual       0.64      0.52      0.57       732\n",
      "     HS_Group       0.58      0.31      0.40       402\n",
      "  HS_Religion       0.67      0.10      0.18       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.58      0.61       762\n",
      "      HS_Weak       0.61      0.50      0.55       689\n",
      "  HS_Moderate       0.37      0.22      0.28       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.52      0.60      5556\n",
      "    macro avg       0.43      0.31      0.34      5556\n",
      " weighted avg       0.64      0.52      0.57      5556\n",
      "  samples avg       0.36      0.30      0.30      5556\n",
      "\n",
      "Training completed in 63.36462116241455 s\n",
      "Averaged - Iteration 658: Accuracy: 0.8764, F1 Micro: 0.6152, F1 Macro: 0.3422\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 108.88345098495483 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4658, Accuracy: 0.8376, F1 Micro: 0.1565, F1 Macro: 0.0541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3676, Accuracy: 0.8574, F1 Micro: 0.3495, F1 Macro: 0.1302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3123, Accuracy: 0.8811, F1 Micro: 0.5817, F1 Macro: 0.2912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.261, Accuracy: 0.8901, F1 Micro: 0.6259, F1 Macro: 0.3431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2237, Accuracy: 0.8921, F1 Micro: 0.6557, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.191, Accuracy: 0.8933, F1 Micro: 0.6772, F1 Macro: 0.4643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1643, Accuracy: 0.8928, F1 Micro: 0.6902, F1 Macro: 0.5068\n",
      "Epoch 8/10, Train Loss: 0.1406, Accuracy: 0.8911, F1 Micro: 0.6411, F1 Macro: 0.4666\n",
      "Epoch 9/10, Train Loss: 0.1161, Accuracy: 0.8955, F1 Micro: 0.6832, F1 Macro: 0.512\n",
      "Epoch 10/10, Train Loss: 0.1056, Accuracy: 0.8953, F1 Micro: 0.6835, F1 Macro: 0.5206\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8928, F1 Micro: 0.6902, F1 Macro: 0.5068\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.85      0.81      1134\n",
      "      Abusive       0.83      0.86      0.84       992\n",
      "HS_Individual       0.63      0.67      0.65       732\n",
      "     HS_Group       0.60      0.59      0.60       402\n",
      "  HS_Religion       0.61      0.36      0.46       157\n",
      "      HS_Race       0.73      0.51      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.76      0.70       762\n",
      "      HS_Weak       0.61      0.64      0.62       689\n",
      "  HS_Moderate       0.46      0.47      0.46       331\n",
      "    HS_Strong       0.81      0.22      0.34       114\n",
      "\n",
      "    micro avg       0.69      0.69      0.69      5556\n",
      "    macro avg       0.56      0.49      0.51      5556\n",
      " weighted avg       0.67      0.69      0.68      5556\n",
      "  samples avg       0.39      0.39      0.37      5556\n",
      "\n",
      "Training completed in 82.31200885772705 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4556, Accuracy: 0.8468, F1 Micro: 0.2687, F1 Macro: 0.091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.364, Accuracy: 0.8606, F1 Micro: 0.3887, F1 Macro: 0.1506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3089, Accuracy: 0.8825, F1 Micro: 0.5932, F1 Macro: 0.3161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2581, Accuracy: 0.8909, F1 Micro: 0.6308, F1 Macro: 0.3849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2212, Accuracy: 0.8924, F1 Micro: 0.6632, F1 Macro: 0.4724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1895, Accuracy: 0.8932, F1 Micro: 0.665, F1 Macro: 0.4769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1652, Accuracy: 0.8956, F1 Micro: 0.6819, F1 Macro: 0.5055\n",
      "Epoch 8/10, Train Loss: 0.1381, Accuracy: 0.8912, F1 Micro: 0.6396, F1 Macro: 0.4832\n",
      "Epoch 9/10, Train Loss: 0.1138, Accuracy: 0.895, F1 Micro: 0.6568, F1 Macro: 0.4803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.105, Accuracy: 0.8955, F1 Micro: 0.6912, F1 Macro: 0.5354\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8955, F1 Micro: 0.6912, F1 Macro: 0.5354\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.81      0.80      1134\n",
      "      Abusive       0.84      0.87      0.86       992\n",
      "HS_Individual       0.66      0.61      0.63       732\n",
      "     HS_Group       0.55      0.62      0.59       402\n",
      "  HS_Religion       0.58      0.54      0.56       157\n",
      "      HS_Race       0.67      0.57      0.62       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.70      0.71       762\n",
      "      HS_Weak       0.64      0.55      0.59       689\n",
      "  HS_Moderate       0.45      0.55      0.50       331\n",
      "    HS_Strong       0.92      0.42      0.58       114\n",
      "\n",
      "    micro avg       0.70      0.68      0.69      5556\n",
      "    macro avg       0.57      0.52      0.54      5556\n",
      " weighted avg       0.69      0.68      0.68      5556\n",
      "  samples avg       0.41      0.38      0.38      5556\n",
      "\n",
      "Training completed in 84.18838667869568 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4703, Accuracy: 0.8316, F1 Micro: 0.0517, F1 Macro: 0.0215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3739, Accuracy: 0.8565, F1 Micro: 0.3474, F1 Macro: 0.1317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3212, Accuracy: 0.879, F1 Micro: 0.5605, F1 Macro: 0.2684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2673, Accuracy: 0.888, F1 Micro: 0.6198, F1 Macro: 0.3561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2259, Accuracy: 0.8908, F1 Micro: 0.6496, F1 Macro: 0.4531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1903, Accuracy: 0.8918, F1 Micro: 0.6714, F1 Macro: 0.4798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1663, Accuracy: 0.8926, F1 Micro: 0.683, F1 Macro: 0.4942\n",
      "Epoch 8/10, Train Loss: 0.144, Accuracy: 0.8932, F1 Micro: 0.6604, F1 Macro: 0.4964\n",
      "Epoch 9/10, Train Loss: 0.1172, Accuracy: 0.897, F1 Micro: 0.67, F1 Macro: 0.5002\n",
      "Epoch 10/10, Train Loss: 0.1046, Accuracy: 0.8977, F1 Micro: 0.6815, F1 Macro: 0.5282\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8926, F1 Micro: 0.683, F1 Macro: 0.4942\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.83      0.80      1134\n",
      "      Abusive       0.82      0.84      0.83       992\n",
      "HS_Individual       0.64      0.66      0.65       732\n",
      "     HS_Group       0.61      0.55      0.58       402\n",
      "  HS_Religion       0.65      0.32      0.43       157\n",
      "      HS_Race       0.84      0.39      0.53       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.65      0.72      0.69       762\n",
      "      HS_Weak       0.61      0.62      0.62       689\n",
      "  HS_Moderate       0.48      0.44      0.46       331\n",
      "    HS_Strong       0.89      0.21      0.34       114\n",
      "\n",
      "    micro avg       0.69      0.67      0.68      5556\n",
      "    macro avg       0.58      0.47      0.49      5556\n",
      " weighted avg       0.68      0.67      0.67      5556\n",
      "  samples avg       0.39      0.38      0.36      5556\n",
      "\n",
      "Training completed in 81.72168803215027 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.885, F1 Micro: 0.6516, F1 Macro: 0.4272\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 98.50237989425659 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.445, Accuracy: 0.8561, F1 Micro: 0.385, F1 Macro: 0.1176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3341, Accuracy: 0.8838, F1 Micro: 0.5863, F1 Macro: 0.2827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2742, Accuracy: 0.8948, F1 Micro: 0.6518, F1 Macro: 0.4003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2211, Accuracy: 0.8996, F1 Micro: 0.6705, F1 Macro: 0.4669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1896, Accuracy: 0.8994, F1 Micro: 0.7105, F1 Macro: 0.5321\n",
      "Epoch 6/10, Train Loss: 0.1519, Accuracy: 0.9031, F1 Micro: 0.7081, F1 Macro: 0.5189\n",
      "Epoch 7/10, Train Loss: 0.1289, Accuracy: 0.9022, F1 Micro: 0.693, F1 Macro: 0.5063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1115, Accuracy: 0.9026, F1 Micro: 0.7225, F1 Macro: 0.5561\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9017, F1 Micro: 0.704, F1 Macro: 0.5581\n",
      "Epoch 10/10, Train Loss: 0.0789, Accuracy: 0.9046, F1 Micro: 0.7223, F1 Macro: 0.5708\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9026, F1 Micro: 0.7225, F1 Macro: 0.5561\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.86      0.82      1134\n",
      "      Abusive       0.84      0.90      0.87       992\n",
      "HS_Individual       0.65      0.73      0.69       732\n",
      "     HS_Group       0.64      0.61      0.63       402\n",
      "  HS_Religion       0.73      0.49      0.59       157\n",
      "      HS_Race       0.75      0.52      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.67      0.79      0.72       762\n",
      "      HS_Weak       0.61      0.69      0.65       689\n",
      "  HS_Moderate       0.53      0.54      0.53       331\n",
      "    HS_Strong       0.94      0.40      0.56       114\n",
      "\n",
      "    micro avg       0.71      0.74      0.72      5556\n",
      "    macro avg       0.60      0.54      0.56      5556\n",
      " weighted avg       0.70      0.74      0.71      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 102.08245611190796 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.437, Accuracy: 0.8568, F1 Micro: 0.3951, F1 Macro: 0.1195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3308, Accuracy: 0.8841, F1 Micro: 0.5904, F1 Macro: 0.3336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2728, Accuracy: 0.8957, F1 Micro: 0.6567, F1 Macro: 0.3839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2233, Accuracy: 0.8981, F1 Micro: 0.6964, F1 Macro: 0.4852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1877, Accuracy: 0.899, F1 Micro: 0.7165, F1 Macro: 0.5523\n",
      "Epoch 6/10, Train Loss: 0.1548, Accuracy: 0.8991, F1 Micro: 0.7163, F1 Macro: 0.5585\n",
      "Epoch 7/10, Train Loss: 0.1293, Accuracy: 0.903, F1 Micro: 0.7093, F1 Macro: 0.5486\n",
      "Epoch 8/10, Train Loss: 0.109, Accuracy: 0.9038, F1 Micro: 0.6974, F1 Macro: 0.5254\n",
      "Epoch 9/10, Train Loss: 0.0901, Accuracy: 0.8996, F1 Micro: 0.6939, F1 Macro: 0.5496\n",
      "Epoch 10/10, Train Loss: 0.0797, Accuracy: 0.8994, F1 Micro: 0.7159, F1 Macro: 0.5734\n",
      "Model 2 - Iteration 2535: Accuracy: 0.899, F1 Micro: 0.7165, F1 Macro: 0.5523\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.89      0.82      1134\n",
      "      Abusive       0.82      0.89      0.85       992\n",
      "HS_Individual       0.64      0.72      0.68       732\n",
      "     HS_Group       0.60      0.64      0.62       402\n",
      "  HS_Religion       0.66      0.45      0.54       157\n",
      "      HS_Race       0.73      0.57      0.64       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.66      0.81      0.73       762\n",
      "      HS_Weak       0.63      0.69      0.66       689\n",
      "  HS_Moderate       0.49      0.52      0.50       331\n",
      "    HS_Strong       0.85      0.46      0.59       114\n",
      "\n",
      "    micro avg       0.69      0.74      0.72      5556\n",
      "    macro avg       0.57      0.55      0.55      5556\n",
      " weighted avg       0.68      0.74      0.71      5556\n",
      "  samples avg       0.41      0.41      0.39      5556\n",
      "\n",
      "Training completed in 100.55813145637512 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4511, Accuracy: 0.8458, F1 Micro: 0.2509, F1 Macro: 0.0851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3451, Accuracy: 0.8819, F1 Micro: 0.5824, F1 Macro: 0.2861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2794, Accuracy: 0.8925, F1 Micro: 0.6362, F1 Macro: 0.3702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2254, Accuracy: 0.8992, F1 Micro: 0.692, F1 Macro: 0.4941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1896, Accuracy: 0.8973, F1 Micro: 0.7135, F1 Macro: 0.5357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1551, Accuracy: 0.9036, F1 Micro: 0.7165, F1 Macro: 0.5471\n",
      "Epoch 7/10, Train Loss: 0.1279, Accuracy: 0.9021, F1 Micro: 0.7153, F1 Macro: 0.5423\n",
      "Epoch 8/10, Train Loss: 0.1079, Accuracy: 0.905, F1 Micro: 0.7089, F1 Macro: 0.5405\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.903, F1 Micro: 0.7122, F1 Macro: 0.5613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0758, Accuracy: 0.9058, F1 Micro: 0.7184, F1 Macro: 0.5716\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9058, F1 Micro: 0.7184, F1 Macro: 0.5716\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.80      0.81      1134\n",
      "      Abusive       0.83      0.89      0.86       992\n",
      "HS_Individual       0.70      0.64      0.67       732\n",
      "     HS_Group       0.63      0.62      0.62       402\n",
      "  HS_Religion       0.68      0.64      0.66       157\n",
      "      HS_Race       0.73      0.52      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.71      0.72       762\n",
      "      HS_Weak       0.67      0.59      0.63       689\n",
      "  HS_Moderate       0.55      0.52      0.53       331\n",
      "    HS_Strong       0.89      0.64      0.74       114\n",
      "\n",
      "    micro avg       0.74      0.70      0.72      5556\n",
      "    macro avg       0.60      0.55      0.57      5556\n",
      " weighted avg       0.72      0.70      0.71      5556\n",
      "  samples avg       0.42      0.39      0.39      5556\n",
      "\n",
      "Training completed in 103.78912496566772 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8908, F1 Micro: 0.6741, F1 Macro: 0.4714\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 88.59582901000977 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4221, Accuracy: 0.8704, F1 Micro: 0.4867, F1 Macro: 0.1964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3047, Accuracy: 0.8895, F1 Micro: 0.6699, F1 Macro: 0.4016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2423, Accuracy: 0.8997, F1 Micro: 0.6982, F1 Macro: 0.4771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2003, Accuracy: 0.9007, F1 Micro: 0.7015, F1 Macro: 0.5275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9019, F1 Micro: 0.7123, F1 Macro: 0.5641\n",
      "Epoch 6/10, Train Loss: 0.1437, Accuracy: 0.9035, F1 Micro: 0.6705, F1 Macro: 0.5236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9078, F1 Micro: 0.7207, F1 Macro: 0.5736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.097, Accuracy: 0.9106, F1 Micro: 0.7327, F1 Macro: 0.6023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9113, F1 Micro: 0.7358, F1 Macro: 0.6084\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9085, F1 Micro: 0.7268, F1 Macro: 0.5987\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9113, F1 Micro: 0.7358, F1 Macro: 0.6084\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.82      0.82      1134\n",
      "      Abusive       0.86      0.88      0.87       992\n",
      "HS_Individual       0.69      0.70      0.69       732\n",
      "     HS_Group       0.71      0.57      0.63       402\n",
      "  HS_Religion       0.67      0.59      0.63       157\n",
      "      HS_Race       0.85      0.57      0.69       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.57      0.08      0.14        51\n",
      "     HS_Other       0.74      0.75      0.74       762\n",
      "      HS_Weak       0.65      0.67      0.66       689\n",
      "  HS_Moderate       0.62      0.47      0.53       331\n",
      "    HS_Strong       0.89      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.74      0.57      0.61      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 126.6072165966034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4155, Accuracy: 0.8693, F1 Micro: 0.4784, F1 Macro: 0.1931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3016, Accuracy: 0.8895, F1 Micro: 0.6764, F1 Macro: 0.4416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.241, Accuracy: 0.898, F1 Micro: 0.7056, F1 Macro: 0.528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1959, Accuracy: 0.9034, F1 Micro: 0.7132, F1 Macro: 0.5571\n",
      "Epoch 5/10, Train Loss: 0.1666, Accuracy: 0.9062, F1 Micro: 0.6993, F1 Macro: 0.5538\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9049, F1 Micro: 0.7005, F1 Macro: 0.5248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9063, F1 Micro: 0.7196, F1 Macro: 0.573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.088, Accuracy: 0.9091, F1 Micro: 0.7305, F1 Macro: 0.59\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0778, Accuracy: 0.9125, F1 Micro: 0.7308, F1 Macro: 0.5985\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.9113, F1 Micro: 0.7306, F1 Macro: 0.5965\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9125, F1 Micro: 0.7308, F1 Macro: 0.5985\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.79      0.82      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.69      0.69      0.69       732\n",
      "     HS_Group       0.76      0.50      0.60       402\n",
      "  HS_Religion       0.70      0.57      0.62       157\n",
      "      HS_Race       0.81      0.61      0.70       120\n",
      "  HS_Physical       0.80      0.06      0.10        72\n",
      "    HS_Gender       0.67      0.04      0.07        51\n",
      "     HS_Other       0.77      0.70      0.73       762\n",
      "      HS_Weak       0.67      0.65      0.66       689\n",
      "  HS_Moderate       0.65      0.40      0.50       331\n",
      "    HS_Strong       0.87      0.76      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.69      0.73      5556\n",
      "    macro avg       0.76      0.55      0.60      5556\n",
      " weighted avg       0.77      0.69      0.72      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 124.05700492858887 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4278, Accuracy: 0.8635, F1 Micro: 0.4459, F1 Macro: 0.1731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3102, Accuracy: 0.8898, F1 Micro: 0.6696, F1 Macro: 0.4271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2454, Accuracy: 0.8983, F1 Micro: 0.6992, F1 Macro: 0.5013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9026, F1 Micro: 0.7063, F1 Macro: 0.543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.165, Accuracy: 0.9014, F1 Micro: 0.7216, F1 Macro: 0.5713\n",
      "Epoch 6/10, Train Loss: 0.1415, Accuracy: 0.9042, F1 Micro: 0.6826, F1 Macro: 0.5148\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9051, F1 Micro: 0.7166, F1 Macro: 0.5705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.912, F1 Micro: 0.7312, F1 Macro: 0.5862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9113, F1 Micro: 0.7374, F1 Macro: 0.6073\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.9112, F1 Micro: 0.7337, F1 Macro: 0.6065\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9113, F1 Micro: 0.7374, F1 Macro: 0.6073\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.85      0.90      0.87       992\n",
      "HS_Individual       0.72      0.66      0.69       732\n",
      "     HS_Group       0.65      0.64      0.64       402\n",
      "  HS_Religion       0.67      0.63      0.65       157\n",
      "      HS_Race       0.75      0.60      0.67       120\n",
      "  HS_Physical       1.00      0.04      0.08        72\n",
      "    HS_Gender       0.60      0.06      0.11        51\n",
      "     HS_Other       0.73      0.75      0.74       762\n",
      "      HS_Weak       0.70      0.61      0.65       689\n",
      "  HS_Moderate       0.55      0.55      0.55       331\n",
      "    HS_Strong       0.83      0.78      0.81       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.74      5556\n",
      "    macro avg       0.74      0.59      0.61      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 124.1810553073883 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.896, F1 Micro: 0.6893, F1 Macro: 0.5048\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 78.78880095481873 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.411, Accuracy: 0.8794, F1 Micro: 0.5781, F1 Macro: 0.2642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2937, Accuracy: 0.8921, F1 Micro: 0.681, F1 Macro: 0.4716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2324, Accuracy: 0.9033, F1 Micro: 0.6891, F1 Macro: 0.4994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.193, Accuracy: 0.9073, F1 Micro: 0.7083, F1 Macro: 0.5411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.9063, F1 Micro: 0.7344, F1 Macro: 0.591\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9091, F1 Micro: 0.73, F1 Macro: 0.5889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9078, F1 Micro: 0.7362, F1 Macro: 0.5937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9134, F1 Micro: 0.7396, F1 Macro: 0.6356\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9124, F1 Micro: 0.7328, F1 Macro: 0.6381\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.906, F1 Micro: 0.7361, F1 Macro: 0.6379\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9134, F1 Micro: 0.7396, F1 Macro: 0.6356\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.81      0.82      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.70      0.69      0.70       732\n",
      "     HS_Group       0.72      0.57      0.63       402\n",
      "  HS_Religion       0.65      0.62      0.64       157\n",
      "      HS_Race       0.85      0.63      0.73       120\n",
      "  HS_Physical       0.78      0.10      0.17        72\n",
      "    HS_Gender       0.67      0.20      0.30        51\n",
      "     HS_Other       0.76      0.71      0.74       762\n",
      "      HS_Weak       0.67      0.67      0.67       689\n",
      "  HS_Moderate       0.59      0.47      0.52       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5556\n",
      "    macro avg       0.75      0.59      0.64      5556\n",
      " weighted avg       0.77      0.71      0.73      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 141.50501585006714 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4061, Accuracy: 0.8784, F1 Micro: 0.5771, F1 Macro: 0.2654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2899, Accuracy: 0.8948, F1 Micro: 0.6875, F1 Macro: 0.4883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2332, Accuracy: 0.904, F1 Micro: 0.6961, F1 Macro: 0.5199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9076, F1 Micro: 0.7141, F1 Macro: 0.5572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.154, Accuracy: 0.9077, F1 Micro: 0.7245, F1 Macro: 0.5838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9107, F1 Micro: 0.728, F1 Macro: 0.5825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1002, Accuracy: 0.9063, F1 Micro: 0.7326, F1 Macro: 0.594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9119, F1 Micro: 0.7389, F1 Macro: 0.6215\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9116, F1 Micro: 0.7297, F1 Macro: 0.6105\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9129, F1 Micro: 0.7331, F1 Macro: 0.6288\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9119, F1 Micro: 0.7389, F1 Macro: 0.6215\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.82      1134\n",
      "      Abusive       0.86      0.89      0.88       992\n",
      "HS_Individual       0.68      0.70      0.69       732\n",
      "     HS_Group       0.71      0.60      0.65       402\n",
      "  HS_Religion       0.63      0.62      0.63       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       0.88      0.10      0.18        72\n",
      "    HS_Gender       0.50      0.06      0.11        51\n",
      "     HS_Other       0.76      0.72      0.74       762\n",
      "      HS_Weak       0.65      0.67      0.66       689\n",
      "  HS_Moderate       0.61      0.52      0.56       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.72      0.74      5556\n",
      "    macro avg       0.73      0.60      0.62      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 144.2845423221588 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4184, Accuracy: 0.8764, F1 Micro: 0.5533, F1 Macro: 0.2508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2962, Accuracy: 0.8941, F1 Micro: 0.6816, F1 Macro: 0.4918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2348, Accuracy: 0.9037, F1 Micro: 0.7008, F1 Macro: 0.5079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1933, Accuracy: 0.906, F1 Micro: 0.7145, F1 Macro: 0.5511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.9077, F1 Micro: 0.7285, F1 Macro: 0.5846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.126, Accuracy: 0.9083, F1 Micro: 0.7356, F1 Macro: 0.5862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9055, F1 Micro: 0.7361, F1 Macro: 0.604\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.9132, F1 Micro: 0.7333, F1 Macro: 0.622\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9124, F1 Micro: 0.7333, F1 Macro: 0.6152\n",
      "Epoch 10/10, Train Loss: 0.0599, Accuracy: 0.908, F1 Micro: 0.7337, F1 Macro: 0.6232\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9055, F1 Micro: 0.7361, F1 Macro: 0.604\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.90      0.83      1134\n",
      "      Abusive       0.83      0.92      0.87       992\n",
      "HS_Individual       0.67      0.68      0.68       732\n",
      "     HS_Group       0.59      0.70      0.64       402\n",
      "  HS_Religion       0.67      0.61      0.64       157\n",
      "      HS_Race       0.70      0.60      0.65       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.71      0.10      0.17        51\n",
      "     HS_Other       0.68      0.84      0.75       762\n",
      "      HS_Weak       0.65      0.65      0.65       689\n",
      "  HS_Moderate       0.52      0.63      0.57       331\n",
      "    HS_Strong       0.91      0.63      0.75       114\n",
      "\n",
      "    micro avg       0.71      0.77      0.74      5556\n",
      "    macro avg       0.73      0.61      0.60      5556\n",
      " weighted avg       0.71      0.77      0.73      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 142.08458971977234 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.8989, F1 Micro: 0.6991, F1 Macro: 0.5279\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 71.82725143432617 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3958, Accuracy: 0.8792, F1 Micro: 0.5909, F1 Macro: 0.2824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2762, Accuracy: 0.8984, F1 Micro: 0.6926, F1 Macro: 0.4636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2248, Accuracy: 0.904, F1 Micro: 0.7123, F1 Macro: 0.5323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1912, Accuracy: 0.9106, F1 Micro: 0.7262, F1 Macro: 0.5718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1496, Accuracy: 0.9111, F1 Micro: 0.7438, F1 Macro: 0.6041\n",
      "Epoch 6/10, Train Loss: 0.1195, Accuracy: 0.915, F1 Micro: 0.7347, F1 Macro: 0.6075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.9108, F1 Micro: 0.7522, F1 Macro: 0.6395\n",
      "Epoch 8/10, Train Loss: 0.0858, Accuracy: 0.9142, F1 Micro: 0.7498, F1 Macro: 0.6348\n",
      "Epoch 9/10, Train Loss: 0.0692, Accuracy: 0.9157, F1 Micro: 0.7433, F1 Macro: 0.6586\n",
      "Epoch 10/10, Train Loss: 0.0551, Accuracy: 0.9159, F1 Micro: 0.7384, F1 Macro: 0.6551\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9108, F1 Micro: 0.7522, F1 Macro: 0.6395\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.90      0.84      1134\n",
      "      Abusive       0.82      0.94      0.88       992\n",
      "HS_Individual       0.66      0.80      0.72       732\n",
      "     HS_Group       0.67      0.62      0.65       402\n",
      "  HS_Religion       0.59      0.70      0.64       157\n",
      "      HS_Race       0.65      0.67      0.66       120\n",
      "  HS_Physical       0.71      0.07      0.13        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.73      0.78      0.76       762\n",
      "      HS_Weak       0.64      0.77      0.70       689\n",
      "  HS_Moderate       0.60      0.48      0.54       331\n",
      "    HS_Strong       0.84      0.78      0.81       114\n",
      "\n",
      "    micro avg       0.72      0.79      0.75      5556\n",
      "    macro avg       0.69      0.65      0.64      5556\n",
      " weighted avg       0.72      0.79      0.74      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 155.88728833198547 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3918, Accuracy: 0.8794, F1 Micro: 0.6137, F1 Macro: 0.3151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2731, Accuracy: 0.8996, F1 Micro: 0.7031, F1 Macro: 0.5244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2253, Accuracy: 0.9062, F1 Micro: 0.7115, F1 Macro: 0.5522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.192, Accuracy: 0.911, F1 Micro: 0.7233, F1 Macro: 0.5767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9094, F1 Micro: 0.7308, F1 Macro: 0.5878\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9136, F1 Micro: 0.725, F1 Macro: 0.5885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0987, Accuracy: 0.9105, F1 Micro: 0.7419, F1 Macro: 0.6183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0813, Accuracy: 0.9123, F1 Micro: 0.7422, F1 Macro: 0.6218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0696, Accuracy: 0.9134, F1 Micro: 0.7447, F1 Macro: 0.6466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0603, Accuracy: 0.9147, F1 Micro: 0.7486, F1 Macro: 0.6707\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9147, F1 Micro: 0.7486, F1 Macro: 0.6707\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.74      0.60      0.66       732\n",
      "     HS_Group       0.62      0.73      0.67       402\n",
      "  HS_Religion       0.65      0.66      0.65       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       0.72      0.25      0.37        72\n",
      "    HS_Gender       0.78      0.27      0.41        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.58      0.64       689\n",
      "  HS_Moderate       0.54      0.66      0.59       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.74      0.65      0.67      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 161.41652607917786 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.402, Accuracy: 0.8763, F1 Micro: 0.5966, F1 Macro: 0.2853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.898, F1 Micro: 0.6946, F1 Macro: 0.4771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2263, Accuracy: 0.9047, F1 Micro: 0.7087, F1 Macro: 0.5329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9111, F1 Micro: 0.7283, F1 Macro: 0.5807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9146, F1 Micro: 0.7509, F1 Macro: 0.6021\n",
      "Epoch 6/10, Train Loss: 0.1226, Accuracy: 0.9143, F1 Micro: 0.7348, F1 Macro: 0.594\n",
      "Epoch 7/10, Train Loss: 0.0989, Accuracy: 0.9103, F1 Micro: 0.7449, F1 Macro: 0.6244\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.915, F1 Micro: 0.743, F1 Macro: 0.6162\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9141, F1 Micro: 0.7364, F1 Macro: 0.6346\n",
      "Epoch 10/10, Train Loss: 0.0588, Accuracy: 0.9145, F1 Micro: 0.7375, F1 Macro: 0.6481\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9146, F1 Micro: 0.7509, F1 Macro: 0.6021\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.71      0.72      0.71       732\n",
      "     HS_Group       0.65      0.62      0.63       402\n",
      "  HS_Religion       0.76      0.55      0.64       157\n",
      "      HS_Race       0.80      0.54      0.65       120\n",
      "  HS_Physical       1.00      0.03      0.05        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.83      0.76       762\n",
      "      HS_Weak       0.71      0.66      0.69       689\n",
      "  HS_Moderate       0.59      0.52      0.55       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.71      0.58      0.60      5556\n",
      " weighted avg       0.75      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 155.34318780899048 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9013, F1 Micro: 0.7076, F1 Macro: 0.5461\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 64.14461755752563 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3873, Accuracy: 0.8774, F1 Micro: 0.6529, F1 Macro: 0.3503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2664, Accuracy: 0.9012, F1 Micro: 0.6822, F1 Macro: 0.4567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2222, Accuracy: 0.9069, F1 Micro: 0.6872, F1 Macro: 0.522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.182, Accuracy: 0.9142, F1 Micro: 0.7352, F1 Macro: 0.5656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1476, Accuracy: 0.9098, F1 Micro: 0.7497, F1 Macro: 0.6113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1217, Accuracy: 0.9162, F1 Micro: 0.7531, F1 Macro: 0.6192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.9184, F1 Micro: 0.7613, F1 Macro: 0.6603\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9165, F1 Micro: 0.7548, F1 Macro: 0.6614\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.9172, F1 Micro: 0.7525, F1 Macro: 0.6641\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.916, F1 Micro: 0.75, F1 Macro: 0.6769\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9184, F1 Micro: 0.7613, F1 Macro: 0.6603\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.69      0.74      0.72       732\n",
      "     HS_Group       0.73      0.59      0.65       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       0.82      0.12      0.22        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.67      0.72      0.69       689\n",
      "  HS_Moderate       0.63      0.51      0.56       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.75      0.63      0.66      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 171.44702577590942 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3812, Accuracy: 0.8786, F1 Micro: 0.657, F1 Macro: 0.3805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2633, Accuracy: 0.9021, F1 Micro: 0.6879, F1 Macro: 0.4802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.9095, F1 Micro: 0.7016, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1798, Accuracy: 0.9122, F1 Micro: 0.7392, F1 Macro: 0.5742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1456, Accuracy: 0.9113, F1 Micro: 0.7471, F1 Macro: 0.6027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9174, F1 Micro: 0.7518, F1 Macro: 0.6046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9167, F1 Micro: 0.7523, F1 Macro: 0.6419\n",
      "Epoch 8/10, Train Loss: 0.0792, Accuracy: 0.9126, F1 Micro: 0.7479, F1 Macro: 0.6249\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9132, F1 Micro: 0.7483, F1 Macro: 0.6538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9161, F1 Micro: 0.7588, F1 Macro: 0.6841\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9161, F1 Micro: 0.7588, F1 Macro: 0.6841\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.68      0.74      0.71       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.63      0.68      0.66       157\n",
      "      HS_Race       0.72      0.71      0.71       120\n",
      "  HS_Physical       0.80      0.28      0.41        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.76      0.76      0.76       762\n",
      "      HS_Weak       0.66      0.71      0.68       689\n",
      "  HS_Moderate       0.61      0.55      0.58       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 173.7557225227356 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3927, Accuracy: 0.8797, F1 Micro: 0.6444, F1 Macro: 0.3382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2672, Accuracy: 0.9006, F1 Micro: 0.6838, F1 Macro: 0.4594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2196, Accuracy: 0.9076, F1 Micro: 0.6957, F1 Macro: 0.5481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9134, F1 Micro: 0.7319, F1 Macro: 0.5714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1455, Accuracy: 0.9125, F1 Micro: 0.7502, F1 Macro: 0.6053\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9146, F1 Micro: 0.7399, F1 Macro: 0.597\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.9161, F1 Micro: 0.7483, F1 Macro: 0.6434\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9162, F1 Micro: 0.7471, F1 Macro: 0.6434\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9129, F1 Micro: 0.749, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0577, Accuracy: 0.9141, F1 Micro: 0.756, F1 Macro: 0.6657\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9141, F1 Micro: 0.756, F1 Macro: 0.6657\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.84      0.93      0.88       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.68      0.61      0.65       402\n",
      "  HS_Religion       0.64      0.68      0.66       157\n",
      "      HS_Race       0.72      0.71      0.71       120\n",
      "  HS_Physical       0.69      0.15      0.25        72\n",
      "    HS_Gender       0.54      0.39      0.45        51\n",
      "     HS_Other       0.74      0.77      0.76       762\n",
      "      HS_Weak       0.66      0.74      0.69       689\n",
      "  HS_Moderate       0.56      0.53      0.55       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.74      0.77      0.76      5556\n",
      "    macro avg       0.70      0.66      0.67      5556\n",
      " weighted avg       0.74      0.77      0.75      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 171.26772713661194 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9034, F1 Micro: 0.7149, F1 Macro: 0.5638\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 59.366087675094604 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3852, Accuracy: 0.8857, F1 Micro: 0.6218, F1 Macro: 0.3159\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.9012, F1 Micro: 0.7021, F1 Macro: 0.5003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2109, Accuracy: 0.9099, F1 Micro: 0.704, F1 Macro: 0.5356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1693, Accuracy: 0.9163, F1 Micro: 0.742, F1 Macro: 0.5853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.9111, F1 Micro: 0.7547, F1 Macro: 0.6407\n",
      "Epoch 6/10, Train Loss: 0.1147, Accuracy: 0.9158, F1 Micro: 0.7513, F1 Macro: 0.6247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0899, Accuracy: 0.9179, F1 Micro: 0.7565, F1 Macro: 0.6606\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9122, F1 Micro: 0.754, F1 Macro: 0.671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9202, F1 Micro: 0.7576, F1 Macro: 0.6888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9184, F1 Micro: 0.762, F1 Macro: 0.6833\n",
      "Model 1 - Iteration 5812: Accuracy: 0.9184, F1 Micro: 0.762, F1 Macro: 0.6833\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.72      0.71      0.72       732\n",
      "     HS_Group       0.67      0.67      0.67       402\n",
      "  HS_Religion       0.65      0.73      0.68       157\n",
      "      HS_Race       0.74      0.74      0.74       120\n",
      "  HS_Physical       0.75      0.25      0.38        72\n",
      "    HS_Gender       0.57      0.31      0.41        51\n",
      "     HS_Other       0.77      0.76      0.77       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.57      0.60      0.58       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 187.3462474346161 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3798, Accuracy: 0.8853, F1 Micro: 0.632, F1 Macro: 0.3431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.263, Accuracy: 0.902, F1 Micro: 0.7046, F1 Macro: 0.5285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2099, Accuracy: 0.911, F1 Micro: 0.7143, F1 Macro: 0.5591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1735, Accuracy: 0.915, F1 Micro: 0.7329, F1 Macro: 0.5774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.9181, F1 Micro: 0.7607, F1 Macro: 0.6355\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9168, F1 Micro: 0.7481, F1 Macro: 0.6179\n",
      "Epoch 7/10, Train Loss: 0.0917, Accuracy: 0.9148, F1 Micro: 0.7575, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.9177, F1 Micro: 0.7619, F1 Macro: 0.6706\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.921, F1 Micro: 0.7578, F1 Macro: 0.6809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.051, Accuracy: 0.9206, F1 Micro: 0.7657, F1 Macro: 0.6733\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9206, F1 Micro: 0.7657, F1 Macro: 0.6733\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.70      0.61      0.66       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.78      0.70      0.74       120\n",
      "  HS_Physical       0.86      0.25      0.39        72\n",
      "    HS_Gender       0.58      0.22      0.31        51\n",
      "     HS_Other       0.77      0.78      0.78       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.59      0.54      0.57       331\n",
      "    HS_Strong       0.89      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.75      0.64      0.67      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 185.1462161540985 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3909, Accuracy: 0.884, F1 Micro: 0.6144, F1 Macro: 0.3069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2675, Accuracy: 0.9001, F1 Micro: 0.7066, F1 Macro: 0.5095\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9094, F1 Micro: 0.7013, F1 Macro: 0.5388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9164, F1 Micro: 0.7396, F1 Macro: 0.5758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9136, F1 Micro: 0.7535, F1 Macro: 0.6429\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9151, F1 Micro: 0.737, F1 Macro: 0.6027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9168, F1 Micro: 0.7604, F1 Macro: 0.669\n",
      "Epoch 8/10, Train Loss: 0.0738, Accuracy: 0.9109, F1 Micro: 0.7569, F1 Macro: 0.6678\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9142, F1 Micro: 0.7547, F1 Macro: 0.6622\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9166, F1 Micro: 0.7538, F1 Macro: 0.6682\n",
      "Model 3 - Iteration 5812: Accuracy: 0.9168, F1 Micro: 0.7604, F1 Macro: 0.669\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.88      0.89      0.88       992\n",
      "HS_Individual       0.69      0.76      0.73       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.72      0.62      0.67       157\n",
      "      HS_Race       0.74      0.62      0.67       120\n",
      "  HS_Physical       0.58      0.15      0.24        72\n",
      "    HS_Gender       0.55      0.41      0.47        51\n",
      "     HS_Other       0.74      0.79      0.76       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.59      0.53      0.56       331\n",
      "    HS_Strong       0.85      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.75      0.77      0.76      5556\n",
      "    macro avg       0.71      0.65      0.67      5556\n",
      " weighted avg       0.75      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 182.13785195350647 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9053, F1 Micro: 0.7209, F1 Macro: 0.5778\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 53.60188174247742 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3732, Accuracy: 0.8902, F1 Micro: 0.648, F1 Macro: 0.3621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.26, Accuracy: 0.9056, F1 Micro: 0.7184, F1 Macro: 0.5176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2105, Accuracy: 0.9098, F1 Micro: 0.7371, F1 Macro: 0.5762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9077, F1 Micro: 0.7505, F1 Macro: 0.6045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9134, F1 Micro: 0.7566, F1 Macro: 0.6185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1088, Accuracy: 0.9187, F1 Micro: 0.7586, F1 Macro: 0.6404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9193, F1 Micro: 0.7594, F1 Macro: 0.6675\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9162, F1 Micro: 0.7582, F1 Macro: 0.6879\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.9184, F1 Micro: 0.757, F1 Macro: 0.6781\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9178, F1 Micro: 0.7592, F1 Macro: 0.6847\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9193, F1 Micro: 0.7594, F1 Macro: 0.6675\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.82      0.83      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.71      0.73      0.72       732\n",
      "     HS_Group       0.73      0.58      0.64       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.88      0.61      0.72       120\n",
      "  HS_Physical       0.81      0.18      0.30        72\n",
      "    HS_Gender       0.67      0.27      0.39        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.64      0.48      0.55       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.77      0.62      0.67      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 198.04542517662048 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3678, Accuracy: 0.8916, F1 Micro: 0.6471, F1 Macro: 0.409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2572, Accuracy: 0.9051, F1 Micro: 0.72, F1 Macro: 0.5252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2098, Accuracy: 0.9113, F1 Micro: 0.7413, F1 Macro: 0.5863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.166, Accuracy: 0.9118, F1 Micro: 0.754, F1 Macro: 0.6026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9184, F1 Micro: 0.7591, F1 Macro: 0.6197\n",
      "Epoch 6/10, Train Loss: 0.1078, Accuracy: 0.92, F1 Micro: 0.7582, F1 Macro: 0.6348\n",
      "Epoch 7/10, Train Loss: 0.0878, Accuracy: 0.9194, F1 Micro: 0.7454, F1 Macro: 0.6293\n",
      "Epoch 8/10, Train Loss: 0.0761, Accuracy: 0.9163, F1 Micro: 0.7509, F1 Macro: 0.6448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.9203, F1 Micro: 0.7675, F1 Macro: 0.6837\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9195, F1 Micro: 0.7494, F1 Macro: 0.6735\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9203, F1 Micro: 0.7675, F1 Macro: 0.6837\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.74      0.72       732\n",
      "     HS_Group       0.70      0.62      0.66       402\n",
      "  HS_Religion       0.70      0.56      0.62       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.86      0.25      0.39        72\n",
      "    HS_Gender       0.71      0.33      0.45        51\n",
      "     HS_Other       0.76      0.80      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.61      0.52      0.56       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.76      0.65      0.68      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 195.4439878463745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.8868, F1 Micro: 0.6178, F1 Macro: 0.3268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2609, Accuracy: 0.9056, F1 Micro: 0.7077, F1 Macro: 0.4945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2082, Accuracy: 0.9125, F1 Micro: 0.7409, F1 Macro: 0.5814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9122, F1 Micro: 0.7537, F1 Macro: 0.6083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1355, Accuracy: 0.9176, F1 Micro: 0.7609, F1 Macro: 0.6271\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9199, F1 Micro: 0.757, F1 Macro: 0.6208\n",
      "Epoch 7/10, Train Loss: 0.0861, Accuracy: 0.9196, F1 Micro: 0.7603, F1 Macro: 0.6449\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.9167, F1 Micro: 0.7537, F1 Macro: 0.6639\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.919, F1 Micro: 0.7557, F1 Macro: 0.6775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0526, Accuracy: 0.9183, F1 Micro: 0.7615, F1 Macro: 0.6741\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9183, F1 Micro: 0.7615, F1 Macro: 0.6741\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.66      0.67      0.67       402\n",
      "  HS_Religion       0.74      0.55      0.63       157\n",
      "      HS_Race       0.77      0.62      0.69       120\n",
      "  HS_Physical       1.00      0.17      0.29        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.75      0.79      0.77       762\n",
      "      HS_Weak       0.71      0.66      0.69       689\n",
      "  HS_Moderate       0.58      0.61      0.59       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.76      5556\n",
      "    macro avg       0.76      0.65      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 196.10312747955322 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9069, F1 Micro: 0.7256, F1 Macro: 0.5886\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 48.28436231613159 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3704, Accuracy: 0.884, F1 Micro: 0.6651, F1 Macro: 0.3729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2483, Accuracy: 0.9072, F1 Micro: 0.7067, F1 Macro: 0.4921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2043, Accuracy: 0.9153, F1 Micro: 0.7434, F1 Macro: 0.5741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1624, Accuracy: 0.9152, F1 Micro: 0.7457, F1 Macro: 0.596\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.917, F1 Micro: 0.741, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1101, Accuracy: 0.9184, F1 Micro: 0.7584, F1 Macro: 0.6724\n",
      "Epoch 7/10, Train Loss: 0.0878, Accuracy: 0.9188, F1 Micro: 0.7528, F1 Macro: 0.6516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9191, F1 Micro: 0.7591, F1 Macro: 0.6812\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.918, F1 Micro: 0.7576, F1 Macro: 0.6839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9209, F1 Micro: 0.7652, F1 Macro: 0.6947\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9209, F1 Micro: 0.7652, F1 Macro: 0.6947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.83      0.84      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.74      0.69      0.72       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.70      0.64      0.67       157\n",
      "      HS_Race       0.76      0.64      0.70       120\n",
      "  HS_Physical       0.79      0.26      0.40        72\n",
      "    HS_Gender       0.66      0.49      0.56        51\n",
      "     HS_Other       0.77      0.77      0.77       762\n",
      "      HS_Weak       0.72      0.66      0.69       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 203.60398316383362 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3638, Accuracy: 0.8903, F1 Micro: 0.6545, F1 Macro: 0.4007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2501, Accuracy: 0.9072, F1 Micro: 0.7108, F1 Macro: 0.5201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2053, Accuracy: 0.9147, F1 Micro: 0.7416, F1 Macro: 0.5836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1625, Accuracy: 0.9157, F1 Micro: 0.7444, F1 Macro: 0.5958\n",
      "Epoch 5/10, Train Loss: 0.1289, Accuracy: 0.9172, F1 Micro: 0.7391, F1 Macro: 0.6061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9157, F1 Micro: 0.762, F1 Macro: 0.6685\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9184, F1 Micro: 0.7588, F1 Macro: 0.6621\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9179, F1 Micro: 0.7592, F1 Macro: 0.6756\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9181, F1 Micro: 0.7609, F1 Macro: 0.6905\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9182, F1 Micro: 0.759, F1 Macro: 0.6865\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9157, F1 Micro: 0.762, F1 Macro: 0.6685\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.85      0.92      0.89       992\n",
      "HS_Individual       0.68      0.78      0.73       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.66      0.66      0.66       157\n",
      "      HS_Race       0.76      0.67      0.71       120\n",
      "  HS_Physical       0.53      0.22      0.31        72\n",
      "    HS_Gender       0.55      0.35      0.43        51\n",
      "     HS_Other       0.75      0.78      0.76       762\n",
      "      HS_Weak       0.65      0.76      0.70       689\n",
      "  HS_Moderate       0.59      0.59      0.59       331\n",
      "    HS_Strong       0.91      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5556\n",
      "    macro avg       0.70      0.66      0.67      5556\n",
      " weighted avg       0.74      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 200.5873122215271 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3754, Accuracy: 0.8856, F1 Micro: 0.6596, F1 Macro: 0.3814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2505, Accuracy: 0.9063, F1 Micro: 0.7075, F1 Macro: 0.5102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9128, F1 Micro: 0.7306, F1 Macro: 0.5623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.9158, F1 Micro: 0.7411, F1 Macro: 0.594\n",
      "Epoch 5/10, Train Loss: 0.1311, Accuracy: 0.9165, F1 Micro: 0.7364, F1 Macro: 0.6149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9173, F1 Micro: 0.7629, F1 Macro: 0.6588\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9182, F1 Micro: 0.7567, F1 Macro: 0.6457\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9155, F1 Micro: 0.7639, F1 Macro: 0.6753\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9156, F1 Micro: 0.7473, F1 Macro: 0.6637\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9149, F1 Micro: 0.7607, F1 Macro: 0.68\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9155, F1 Micro: 0.7639, F1 Macro: 0.6753\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.90      0.85      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.67      0.78      0.72       732\n",
      "     HS_Group       0.68      0.65      0.66       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.69      0.73      0.71       120\n",
      "  HS_Physical       0.71      0.14      0.23        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.72      0.81      0.76       762\n",
      "      HS_Weak       0.66      0.76      0.71       689\n",
      "  HS_Moderate       0.58      0.60      0.59       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.74      0.79      0.76      5556\n",
      "    macro avg       0.71      0.68      0.68      5556\n",
      " weighted avg       0.74      0.79      0.76      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 202.7885959148407 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9079, F1 Micro: 0.7294, F1 Macro: 0.5977\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 44.78130745887756 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3644, Accuracy: 0.8915, F1 Micro: 0.6329, F1 Macro: 0.3795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9054, F1 Micro: 0.6915, F1 Macro: 0.487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1999, Accuracy: 0.9125, F1 Micro: 0.7193, F1 Macro: 0.5451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.159, Accuracy: 0.9195, F1 Micro: 0.7611, F1 Macro: 0.6336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1268, Accuracy: 0.9182, F1 Micro: 0.762, F1 Macro: 0.6564\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1027, Accuracy: 0.9213, F1 Micro: 0.7655, F1 Macro: 0.6463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0845, Accuracy: 0.9225, F1 Micro: 0.7729, F1 Macro: 0.6889\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.9189, F1 Micro: 0.7709, F1 Macro: 0.6833\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9217, F1 Micro: 0.7712, F1 Macro: 0.6932\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.923, F1 Micro: 0.7657, F1 Macro: 0.6928\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9225, F1 Micro: 0.7729, F1 Macro: 0.6889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.70      0.67      0.68       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.87      0.18      0.30        72\n",
      "    HS_Gender       0.54      0.41      0.47        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.62      0.57      0.60       331\n",
      "    HS_Strong       0.89      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.75      0.66      0.69      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 215.06561255455017 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3601, Accuracy: 0.892, F1 Micro: 0.6441, F1 Macro: 0.4242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9067, F1 Micro: 0.6969, F1 Macro: 0.4955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1997, Accuracy: 0.9143, F1 Micro: 0.7309, F1 Macro: 0.5501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.16, Accuracy: 0.9182, F1 Micro: 0.7591, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1252, Accuracy: 0.9176, F1 Micro: 0.7596, F1 Macro: 0.653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1036, Accuracy: 0.9202, F1 Micro: 0.7599, F1 Macro: 0.6534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.9192, F1 Micro: 0.7671, F1 Macro: 0.6916\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9183, F1 Micro: 0.767, F1 Macro: 0.6929\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9166, F1 Micro: 0.7627, F1 Macro: 0.6897\n",
      "Epoch 10/10, Train Loss: 0.0505, Accuracy: 0.9193, F1 Micro: 0.7597, F1 Macro: 0.6836\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9192, F1 Micro: 0.7671, F1 Macro: 0.6916\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.71      0.72      0.72       732\n",
      "     HS_Group       0.68      0.67      0.67       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.75      0.63      0.69       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.59      0.56      0.58       331\n",
      "    HS_Strong       0.89      0.87      0.88       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 215.89369463920593 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3705, Accuracy: 0.8883, F1 Micro: 0.6191, F1 Macro: 0.3649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2499, Accuracy: 0.9038, F1 Micro: 0.6743, F1 Macro: 0.4629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1992, Accuracy: 0.9138, F1 Micro: 0.7196, F1 Macro: 0.5568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1582, Accuracy: 0.9208, F1 Micro: 0.7562, F1 Macro: 0.6238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1279, Accuracy: 0.9181, F1 Micro: 0.761, F1 Macro: 0.6626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1053, Accuracy: 0.9209, F1 Micro: 0.7634, F1 Macro: 0.6456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0876, Accuracy: 0.9191, F1 Micro: 0.7685, F1 Macro: 0.6755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9205, F1 Micro: 0.7689, F1 Macro: 0.6864\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9193, F1 Micro: 0.7671, F1 Macro: 0.6836\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9202, F1 Micro: 0.7687, F1 Macro: 0.6974\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9205, F1 Micro: 0.7689, F1 Macro: 0.6864\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.66      0.69      0.67       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.65      0.43      0.52        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.58      0.55      0.57       331\n",
      "    HS_Strong       0.91      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 216.4844172000885 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.9091, F1 Micro: 0.733, F1 Macro: 0.606\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 40.83743119239807 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3631, Accuracy: 0.8906, F1 Micro: 0.6169, F1 Macro: 0.3782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.9086, F1 Micro: 0.714, F1 Macro: 0.5369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2025, Accuracy: 0.9146, F1 Micro: 0.7444, F1 Macro: 0.5814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1614, Accuracy: 0.9204, F1 Micro: 0.7566, F1 Macro: 0.6182\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9101, F1 Micro: 0.7556, F1 Macro: 0.6625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1028, Accuracy: 0.9195, F1 Micro: 0.767, F1 Macro: 0.6808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.081, Accuracy: 0.9209, F1 Micro: 0.7726, F1 Macro: 0.6903\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9178, F1 Micro: 0.7642, F1 Macro: 0.6905\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9194, F1 Micro: 0.7689, F1 Macro: 0.6989\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9202, F1 Micro: 0.7664, F1 Macro: 0.6913\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9209, F1 Micro: 0.7726, F1 Macro: 0.6903\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.70      0.63      0.67       402\n",
      "  HS_Religion       0.71      0.62      0.66       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.89      0.22      0.36        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.74      0.83      0.78       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 221.1205596923828 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3594, Accuracy: 0.8925, F1 Micro: 0.6267, F1 Macro: 0.4037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2489, Accuracy: 0.9082, F1 Micro: 0.7092, F1 Macro: 0.5248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9162, F1 Micro: 0.7388, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1599, Accuracy: 0.917, F1 Micro: 0.7523, F1 Macro: 0.6157\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.908, F1 Micro: 0.7498, F1 Macro: 0.661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9212, F1 Micro: 0.7538, F1 Macro: 0.6588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9188, F1 Micro: 0.7663, F1 Macro: 0.6825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9215, F1 Micro: 0.7675, F1 Macro: 0.6832\n",
      "Epoch 9/10, Train Loss: 0.0548, Accuracy: 0.9191, F1 Micro: 0.7621, F1 Macro: 0.6847\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9191, F1 Micro: 0.7587, F1 Macro: 0.6892\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9215, F1 Micro: 0.7675, F1 Macro: 0.6832\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.73      0.57      0.64       402\n",
      "  HS_Religion       0.68      0.63      0.66       157\n",
      "      HS_Race       0.82      0.58      0.68       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.62      0.41      0.49        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.66      0.49      0.56       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.75      0.77      5556\n",
      "    macro avg       0.78      0.64      0.68      5556\n",
      " weighted avg       0.78      0.75      0.76      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 222.3462381362915 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3696, Accuracy: 0.8892, F1 Micro: 0.6125, F1 Macro: 0.3798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2509, Accuracy: 0.9079, F1 Micro: 0.7105, F1 Macro: 0.5381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.202, Accuracy: 0.9157, F1 Micro: 0.7476, F1 Macro: 0.587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.9193, F1 Micro: 0.7553, F1 Macro: 0.6226\n",
      "Epoch 5/10, Train Loss: 0.1322, Accuracy: 0.9098, F1 Micro: 0.7521, F1 Macro: 0.6538\n",
      "Epoch 6/10, Train Loss: 0.1026, Accuracy: 0.9195, F1 Micro: 0.7505, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9211, F1 Micro: 0.7685, F1 Macro: 0.6846\n",
      "Epoch 8/10, Train Loss: 0.0686, Accuracy: 0.9209, F1 Micro: 0.7635, F1 Macro: 0.6752\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9189, F1 Micro: 0.7655, F1 Macro: 0.687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.921, F1 Micro: 0.7707, F1 Macro: 0.6889\n",
      "Model 3 - Iteration 7336: Accuracy: 0.921, F1 Micro: 0.7707, F1 Macro: 0.6889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.69      0.66      0.68       402\n",
      "  HS_Religion       0.69      0.63      0.66       157\n",
      "      HS_Race       0.73      0.68      0.71       120\n",
      "  HS_Physical       0.93      0.18      0.30        72\n",
      "    HS_Gender       0.65      0.47      0.55        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.60      0.59      0.60       331\n",
      "    HS_Strong       0.87      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 220.90136575698853 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9101, F1 Micro: 0.7361, F1 Macro: 0.6128\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 36.25493574142456 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3538, Accuracy: 0.8954, F1 Micro: 0.6682, F1 Macro: 0.4312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.9085, F1 Micro: 0.7296, F1 Macro: 0.5605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1891, Accuracy: 0.9197, F1 Micro: 0.7556, F1 Macro: 0.6116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1532, Accuracy: 0.9205, F1 Micro: 0.7676, F1 Macro: 0.6405\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9221, F1 Micro: 0.7633, F1 Macro: 0.6632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0991, Accuracy: 0.924, F1 Micro: 0.7736, F1 Macro: 0.6748\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9172, F1 Micro: 0.7654, F1 Macro: 0.6876\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9151, F1 Micro: 0.7662, F1 Macro: 0.6888\n",
      "Epoch 9/10, Train Loss: 0.0556, Accuracy: 0.9228, F1 Micro: 0.7709, F1 Macro: 0.6983\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9195, F1 Micro: 0.7701, F1 Macro: 0.7021\n",
      "Model 1 - Iteration 7656: Accuracy: 0.924, F1 Micro: 0.7736, F1 Macro: 0.6748\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.85      0.52      0.65       157\n",
      "      HS_Race       0.87      0.65      0.74       120\n",
      "  HS_Physical       0.93      0.19      0.32        72\n",
      "    HS_Gender       0.65      0.25      0.37        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.71       689\n",
      "  HS_Moderate       0.67      0.49      0.57       331\n",
      "    HS_Strong       0.92      0.72      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.80      0.62      0.67      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 229.76691508293152 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3511, Accuracy: 0.8962, F1 Micro: 0.6628, F1 Macro: 0.4372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2415, Accuracy: 0.9085, F1 Micro: 0.7311, F1 Macro: 0.5689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1903, Accuracy: 0.9181, F1 Micro: 0.7482, F1 Macro: 0.5925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9151, F1 Micro: 0.7558, F1 Macro: 0.6336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9229, F1 Micro: 0.7639, F1 Macro: 0.6439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1021, Accuracy: 0.9215, F1 Micro: 0.7679, F1 Macro: 0.6788\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9174, F1 Micro: 0.7657, F1 Macro: 0.6837\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9192, F1 Micro: 0.7672, F1 Macro: 0.6898\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9233, F1 Micro: 0.7721, F1 Macro: 0.6939\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9219, F1 Micro: 0.7708, F1 Macro: 0.6903\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9233, F1 Micro: 0.7721, F1 Macro: 0.6939\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.76      0.58      0.66       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.80      0.61      0.69       120\n",
      "  HS_Physical       0.80      0.28      0.41        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.69      0.49      0.57       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.77      0.65      0.69      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 232.3793981075287 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3578, Accuracy: 0.8953, F1 Micro: 0.668, F1 Macro: 0.4265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2418, Accuracy: 0.9086, F1 Micro: 0.729, F1 Macro: 0.5608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1895, Accuracy: 0.9187, F1 Micro: 0.7555, F1 Macro: 0.6011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.153, Accuracy: 0.919, F1 Micro: 0.7577, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.92, F1 Micro: 0.7601, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1017, Accuracy: 0.9221, F1 Micro: 0.7694, F1 Macro: 0.6847\n",
      "Epoch 7/10, Train Loss: 0.0801, Accuracy: 0.9216, F1 Micro: 0.7666, F1 Macro: 0.6824\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9208, F1 Micro: 0.7669, F1 Macro: 0.6812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9238, F1 Micro: 0.7716, F1 Macro: 0.6946\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9214, F1 Micro: 0.7712, F1 Macro: 0.6967\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9238, F1 Micro: 0.7716, F1 Macro: 0.6946\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.76      0.58      0.66       157\n",
      "      HS_Race       0.75      0.66      0.70       120\n",
      "  HS_Physical       0.83      0.28      0.42        72\n",
      "    HS_Gender       0.52      0.49      0.51        51\n",
      "     HS_Other       0.81      0.76      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.63      0.54      0.58       331\n",
      "    HS_Strong       0.88      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.76      0.66      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 232.9988236427307 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9111, F1 Micro: 0.7389, F1 Macro: 0.6185\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 33.2737922668457 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3563, Accuracy: 0.8952, F1 Micro: 0.6543, F1 Macro: 0.3785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2403, Accuracy: 0.9087, F1 Micro: 0.73, F1 Macro: 0.5438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.195, Accuracy: 0.9185, F1 Micro: 0.7494, F1 Macro: 0.6007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1542, Accuracy: 0.9204, F1 Micro: 0.7652, F1 Macro: 0.6566\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9236, F1 Micro: 0.764, F1 Macro: 0.6539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9243, F1 Micro: 0.7706, F1 Macro: 0.6824\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.923, F1 Micro: 0.7706, F1 Macro: 0.6999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.9235, F1 Micro: 0.7714, F1 Macro: 0.6947\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9237, F1 Micro: 0.7708, F1 Macro: 0.7046\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9217, F1 Micro: 0.7654, F1 Macro: 0.6821\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9235, F1 Micro: 0.7714, F1 Macro: 0.6947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.91      0.90      0.90       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.69      0.64      0.66       402\n",
      "  HS_Religion       0.73      0.55      0.63       157\n",
      "      HS_Race       0.77      0.75      0.76       120\n",
      "  HS_Physical       0.82      0.25      0.38        72\n",
      "    HS_Gender       0.70      0.41      0.52        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.73      0.66      0.69       689\n",
      "  HS_Moderate       0.62      0.55      0.58       331\n",
      "    HS_Strong       0.85      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.79      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 235.80029439926147 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.352, Accuracy: 0.8938, F1 Micro: 0.6342, F1 Macro: 0.3836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2395, Accuracy: 0.9102, F1 Micro: 0.7248, F1 Macro: 0.5485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9189, F1 Micro: 0.751, F1 Macro: 0.5971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.919, F1 Micro: 0.7651, F1 Macro: 0.6325\n",
      "Epoch 5/10, Train Loss: 0.1224, Accuracy: 0.9186, F1 Micro: 0.75, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.919, F1 Micro: 0.7654, F1 Macro: 0.6623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9225, F1 Micro: 0.7746, F1 Macro: 0.7038\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9216, F1 Micro: 0.772, F1 Macro: 0.6961\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9213, F1 Micro: 0.7601, F1 Macro: 0.6894\n",
      "Epoch 10/10, Train Loss: 0.0498, Accuracy: 0.9192, F1 Micro: 0.7686, F1 Macro: 0.6891\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9225, F1 Micro: 0.7746, F1 Macro: 0.7038\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.74      0.59      0.66       402\n",
      "  HS_Religion       0.71      0.68      0.69       157\n",
      "      HS_Race       0.78      0.73      0.76       120\n",
      "  HS_Physical       0.73      0.31      0.43        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.66      0.50      0.57       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.77      5556\n",
      "    macro avg       0.74      0.68      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 236.4337420463562 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3596, Accuracy: 0.8921, F1 Micro: 0.6369, F1 Macro: 0.3765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2395, Accuracy: 0.9108, F1 Micro: 0.7255, F1 Macro: 0.5389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.9169, F1 Micro: 0.7372, F1 Macro: 0.5976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1533, Accuracy: 0.9173, F1 Micro: 0.7607, F1 Macro: 0.6379\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.9204, F1 Micro: 0.7492, F1 Macro: 0.6354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9201, F1 Micro: 0.7623, F1 Macro: 0.6507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9208, F1 Micro: 0.769, F1 Macro: 0.6829\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.92, F1 Micro: 0.7679, F1 Macro: 0.6781\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9187, F1 Micro: 0.7576, F1 Macro: 0.6788\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9191, F1 Micro: 0.7654, F1 Macro: 0.683\n",
      "Model 3 - Iteration 7901: Accuracy: 0.9208, F1 Micro: 0.769, F1 Macro: 0.6829\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.69      0.62      0.65       402\n",
      "  HS_Religion       0.79      0.62      0.69       157\n",
      "      HS_Race       0.75      0.69      0.72       120\n",
      "  HS_Physical       0.71      0.17      0.27        72\n",
      "    HS_Gender       0.53      0.45      0.49        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.60      0.55      0.57       331\n",
      "    HS_Strong       0.85      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.74      0.66      0.68      5556\n",
      " weighted avg       0.77      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 235.9380645751953 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9119, F1 Micro: 0.7413, F1 Macro: 0.6239\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 29.43562936782837 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3535, Accuracy: 0.8933, F1 Micro: 0.6709, F1 Macro: 0.3953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2399, Accuracy: 0.9113, F1 Micro: 0.7346, F1 Macro: 0.5567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.9191, F1 Micro: 0.745, F1 Macro: 0.5995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1482, Accuracy: 0.9166, F1 Micro: 0.7656, F1 Macro: 0.6518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9219, F1 Micro: 0.7727, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.924, F1 Micro: 0.7745, F1 Macro: 0.694\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.9192, F1 Micro: 0.767, F1 Macro: 0.6816\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9228, F1 Micro: 0.7718, F1 Macro: 0.697\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9213, F1 Micro: 0.7702, F1 Macro: 0.6985\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.923, F1 Micro: 0.7722, F1 Macro: 0.7\n",
      "Model 1 - Iteration 8165: Accuracy: 0.924, F1 Micro: 0.7745, F1 Macro: 0.694\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.76      0.60      0.67       402\n",
      "  HS_Religion       0.83      0.58      0.68       157\n",
      "      HS_Race       0.86      0.59      0.70       120\n",
      "  HS_Physical       0.85      0.24      0.37        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.79      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.74      0.71       689\n",
      "  HS_Moderate       0.67      0.51      0.58       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 241.33317875862122 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3502, Accuracy: 0.895, F1 Micro: 0.6779, F1 Macro: 0.4252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2394, Accuracy: 0.9121, F1 Micro: 0.7315, F1 Macro: 0.5636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1956, Accuracy: 0.9189, F1 Micro: 0.7395, F1 Macro: 0.5909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1529, Accuracy: 0.9161, F1 Micro: 0.7622, F1 Macro: 0.6225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9178, F1 Micro: 0.7676, F1 Macro: 0.6664\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9174, F1 Micro: 0.765, F1 Macro: 0.6777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9232, F1 Micro: 0.7733, F1 Macro: 0.6848\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9228, F1 Micro: 0.7697, F1 Macro: 0.7012\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9214, F1 Micro: 0.7727, F1 Macro: 0.7018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9212, F1 Micro: 0.7739, F1 Macro: 0.7027\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9212, F1 Micro: 0.7739, F1 Macro: 0.7027\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.67      0.80      0.73       732\n",
      "     HS_Group       0.76      0.58      0.65       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.73      0.69      0.71       120\n",
      "  HS_Physical       0.69      0.33      0.45        72\n",
      "    HS_Gender       0.63      0.53      0.57        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.66      0.78      0.71       689\n",
      "  HS_Moderate       0.67      0.48      0.56       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.74      0.69      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 243.9280321598053 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3586, Accuracy: 0.8926, F1 Micro: 0.6733, F1 Macro: 0.4017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2394, Accuracy: 0.9095, F1 Micro: 0.7286, F1 Macro: 0.5523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1947, Accuracy: 0.9196, F1 Micro: 0.7449, F1 Macro: 0.6004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1511, Accuracy: 0.9197, F1 Micro: 0.7662, F1 Macro: 0.6153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9204, F1 Micro: 0.7709, F1 Macro: 0.6672\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9209, F1 Micro: 0.7657, F1 Macro: 0.6747\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9202, F1 Micro: 0.7675, F1 Macro: 0.6693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9229, F1 Micro: 0.7736, F1 Macro: 0.6986\n",
      "Epoch 9/10, Train Loss: 0.051, Accuracy: 0.9221, F1 Micro: 0.7704, F1 Macro: 0.7007\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9179, F1 Micro: 0.7689, F1 Macro: 0.6975\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9229, F1 Micro: 0.7736, F1 Macro: 0.6986\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.74      0.69      0.72       732\n",
      "     HS_Group       0.66      0.70      0.68       402\n",
      "  HS_Religion       0.78      0.66      0.71       157\n",
      "      HS_Race       0.76      0.66      0.71       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.59      0.53      0.56        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.74      0.67      0.70       689\n",
      "  HS_Moderate       0.59      0.63      0.61       331\n",
      "    HS_Strong       0.88      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 241.82968425750732 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9127, F1 Micro: 0.7435, F1 Macro: 0.6289\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 27.119659423828125 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3559, Accuracy: 0.8964, F1 Micro: 0.655, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2384, Accuracy: 0.9101, F1 Micro: 0.7157, F1 Macro: 0.518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1877, Accuracy: 0.9217, F1 Micro: 0.7517, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1518, Accuracy: 0.9239, F1 Micro: 0.7657, F1 Macro: 0.6554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.696\n",
      "Epoch 6/10, Train Loss: 0.0915, Accuracy: 0.9258, F1 Micro: 0.7741, F1 Macro: 0.6952\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9221, F1 Micro: 0.7748, F1 Macro: 0.7007\n",
      "Epoch 8/10, Train Loss: 0.0635, Accuracy: 0.9213, F1 Micro: 0.7731, F1 Macro: 0.7047\n",
      "Epoch 9/10, Train Loss: 0.0542, Accuracy: 0.9246, F1 Micro: 0.7762, F1 Macro: 0.7174\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.92, F1 Micro: 0.774, F1 Macro: 0.712\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9248, F1 Micro: 0.7793, F1 Macro: 0.696\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.74      0.65      0.69       402\n",
      "  HS_Religion       0.74      0.64      0.68       157\n",
      "      HS_Race       0.75      0.76      0.75       120\n",
      "  HS_Physical       0.82      0.19      0.31        72\n",
      "    HS_Gender       0.69      0.35      0.47        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 246.49494791030884 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.35, Accuracy: 0.8979, F1 Micro: 0.6652, F1 Macro: 0.4537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2357, Accuracy: 0.9098, F1 Micro: 0.7292, F1 Macro: 0.5217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9205, F1 Micro: 0.7517, F1 Macro: 0.6021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.9198, F1 Micro: 0.7647, F1 Macro: 0.6489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1224, Accuracy: 0.9214, F1 Micro: 0.766, F1 Macro: 0.6682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9231, F1 Micro: 0.7736, F1 Macro: 0.6873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9222, F1 Micro: 0.7742, F1 Macro: 0.698\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9211, F1 Micro: 0.7624, F1 Macro: 0.6929\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9215, F1 Micro: 0.7707, F1 Macro: 0.7039\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9213, F1 Micro: 0.7724, F1 Macro: 0.7113\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9222, F1 Micro: 0.7742, F1 Macro: 0.698\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.70      0.78      0.74       732\n",
      "     HS_Group       0.75      0.61      0.67       402\n",
      "  HS_Religion       0.71      0.65      0.68       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.88      0.29      0.44        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.67      0.50      0.57       331\n",
      "    HS_Strong       0.87      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 250.2440242767334 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3583, Accuracy: 0.8958, F1 Micro: 0.6531, F1 Macro: 0.4315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2374, Accuracy: 0.9105, F1 Micro: 0.7312, F1 Macro: 0.5446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1897, Accuracy: 0.9208, F1 Micro: 0.7486, F1 Macro: 0.5966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1525, Accuracy: 0.9227, F1 Micro: 0.7619, F1 Macro: 0.6491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1206, Accuracy: 0.924, F1 Micro: 0.7714, F1 Macro: 0.6817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9224, F1 Micro: 0.7732, F1 Macro: 0.6785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9249, F1 Micro: 0.7767, F1 Macro: 0.6882\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9217, F1 Micro: 0.7713, F1 Macro: 0.6963\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9172, F1 Micro: 0.7657, F1 Macro: 0.7034\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9214, F1 Micro: 0.7708, F1 Macro: 0.7065\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9249, F1 Micro: 0.7767, F1 Macro: 0.6882\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.72      0.73      0.73       732\n",
      "     HS_Group       0.74      0.63      0.68       402\n",
      "  HS_Religion       0.79      0.62      0.69       157\n",
      "      HS_Race       0.76      0.69      0.72       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.66      0.55      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.65      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 249.13208079338074 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9134, F1 Micro: 0.7455, F1 Macro: 0.6329\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 24.684613704681396 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.348, Accuracy: 0.8975, F1 Micro: 0.6519, F1 Macro: 0.4091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2337, Accuracy: 0.9097, F1 Micro: 0.7247, F1 Macro: 0.5259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1862, Accuracy: 0.9194, F1 Micro: 0.7548, F1 Macro: 0.6177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1504, Accuracy: 0.9191, F1 Micro: 0.7714, F1 Macro: 0.6771\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9245, F1 Micro: 0.7632, F1 Macro: 0.6792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0982, Accuracy: 0.9199, F1 Micro: 0.7736, F1 Macro: 0.6995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.924, F1 Micro: 0.7739, F1 Macro: 0.7018\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9241, F1 Micro: 0.7718, F1 Macro: 0.6993\n",
      "Epoch 9/10, Train Loss: 0.0527, Accuracy: 0.9232, F1 Micro: 0.7726, F1 Macro: 0.7025\n",
      "Epoch 10/10, Train Loss: 0.0435, Accuracy: 0.9207, F1 Micro: 0.7705, F1 Macro: 0.705\n",
      "Model 1 - Iteration 8616: Accuracy: 0.924, F1 Micro: 0.7739, F1 Macro: 0.7018\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.66      0.66      0.66       157\n",
      "      HS_Race       0.82      0.67      0.74       120\n",
      "  HS_Physical       0.71      0.33      0.45        72\n",
      "    HS_Gender       0.54      0.49      0.52        51\n",
      "     HS_Other       0.80      0.77      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.66      0.49      0.56       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 253.5654218196869 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3431, Accuracy: 0.8992, F1 Micro: 0.6686, F1 Macro: 0.4389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2326, Accuracy: 0.9098, F1 Micro: 0.733, F1 Macro: 0.5593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.186, Accuracy: 0.9187, F1 Micro: 0.7585, F1 Macro: 0.6123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9203, F1 Micro: 0.7675, F1 Macro: 0.6501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9228, F1 Micro: 0.7694, F1 Macro: 0.6766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9208, F1 Micro: 0.7762, F1 Macro: 0.6951\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9234, F1 Micro: 0.7586, F1 Macro: 0.6813\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9255, F1 Micro: 0.7755, F1 Macro: 0.7013\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9235, F1 Micro: 0.7711, F1 Macro: 0.6955\n",
      "Epoch 10/10, Train Loss: 0.0415, Accuracy: 0.9227, F1 Micro: 0.7761, F1 Macro: 0.7068\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9208, F1 Micro: 0.7762, F1 Macro: 0.6951\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.66      0.72      0.69       402\n",
      "  HS_Religion       0.69      0.70      0.69       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.84      0.22      0.35        72\n",
      "    HS_Gender       0.59      0.39      0.47        51\n",
      "     HS_Other       0.74      0.83      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.58      0.65      0.61       331\n",
      "    HS_Strong       0.86      0.85      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.73      0.69      0.70      5556\n",
      " weighted avg       0.76      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 253.73325967788696 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3542, Accuracy: 0.8965, F1 Micro: 0.6555, F1 Macro: 0.4356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2338, Accuracy: 0.9104, F1 Micro: 0.7219, F1 Macro: 0.5493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1875, Accuracy: 0.9192, F1 Micro: 0.7577, F1 Macro: 0.6203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1523, Accuracy: 0.9199, F1 Micro: 0.7711, F1 Macro: 0.6627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9236, F1 Micro: 0.7738, F1 Macro: 0.6694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.9233, F1 Micro: 0.7782, F1 Macro: 0.6907\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.9227, F1 Micro: 0.7608, F1 Macro: 0.6765\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9211, F1 Micro: 0.7635, F1 Macro: 0.6705\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9225, F1 Micro: 0.7766, F1 Macro: 0.6992\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9234, F1 Micro: 0.7734, F1 Macro: 0.701\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9233, F1 Micro: 0.7782, F1 Macro: 0.6907\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.65      0.74      0.69       402\n",
      "  HS_Religion       0.77      0.60      0.67       157\n",
      "      HS_Race       0.77      0.66      0.71       120\n",
      "  HS_Physical       0.77      0.14      0.24        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.74      0.67      0.71       689\n",
      "  HS_Moderate       0.58      0.68      0.63       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.68      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 252.77654027938843 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9139, F1 Micro: 0.7473, F1 Macro: 0.6366\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 21.727357625961304 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3478, Accuracy: 0.8962, F1 Micro: 0.6843, F1 Macro: 0.4309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2329, Accuracy: 0.9152, F1 Micro: 0.7285, F1 Macro: 0.5512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9208, F1 Micro: 0.7664, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9243, F1 Micro: 0.7738, F1 Macro: 0.6683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9237, F1 Micro: 0.7747, F1 Macro: 0.6787\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9255, F1 Micro: 0.7671, F1 Macro: 0.6882\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9164, F1 Micro: 0.7676, F1 Macro: 0.6968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0628, Accuracy: 0.9261, F1 Micro: 0.7797, F1 Macro: 0.7124\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9239, F1 Micro: 0.7774, F1 Macro: 0.7066\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9257, F1 Micro: 0.7759, F1 Macro: 0.7091\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9261, F1 Micro: 0.7797, F1 Macro: 0.7124\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.71      0.66      0.68       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.78      0.70      0.74       120\n",
      "  HS_Physical       0.72      0.32      0.44        72\n",
      "    HS_Gender       0.60      0.53      0.56        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.74      0.68      0.71       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.85      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 259.1157901287079 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3419, Accuracy: 0.8973, F1 Micro: 0.6866, F1 Macro: 0.4356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2322, Accuracy: 0.9158, F1 Micro: 0.7359, F1 Macro: 0.5857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9198, F1 Micro: 0.7627, F1 Macro: 0.6068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9227, F1 Micro: 0.7633, F1 Macro: 0.6304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.12, Accuracy: 0.9238, F1 Micro: 0.7747, F1 Macro: 0.6743\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9235, F1 Micro: 0.7648, F1 Macro: 0.6829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0827, Accuracy: 0.9233, F1 Micro: 0.7769, F1 Macro: 0.7012\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9247, F1 Micro: 0.7753, F1 Macro: 0.7111\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9223, F1 Micro: 0.7669, F1 Macro: 0.6899\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9208, F1 Micro: 0.7715, F1 Macro: 0.7053\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9233, F1 Micro: 0.7769, F1 Macro: 0.7012\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.67      0.70      0.68       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.83      0.68      0.75       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.62      0.45      0.52        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.59      0.62      0.61       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 258.77682757377625 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3515, Accuracy: 0.8955, F1 Micro: 0.6876, F1 Macro: 0.4433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.234, Accuracy: 0.9157, F1 Micro: 0.7276, F1 Macro: 0.559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9187, F1 Micro: 0.7582, F1 Macro: 0.6145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1484, Accuracy: 0.9223, F1 Micro: 0.7631, F1 Macro: 0.6493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.923, F1 Micro: 0.7768, F1 Macro: 0.6633\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9219, F1 Micro: 0.7722, F1 Macro: 0.6709\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9192, F1 Micro: 0.771, F1 Macro: 0.6809\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.92, F1 Micro: 0.7714, F1 Macro: 0.695\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9236, F1 Micro: 0.7666, F1 Macro: 0.6929\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.922, F1 Micro: 0.7708, F1 Macro: 0.6987\n",
      "Model 3 - Iteration 8816: Accuracy: 0.923, F1 Micro: 0.7768, F1 Macro: 0.6633\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.73      0.74      0.74       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.73      0.65      0.69       157\n",
      "      HS_Race       0.71      0.71      0.71       120\n",
      "  HS_Physical       0.86      0.08      0.15        72\n",
      "    HS_Gender       0.60      0.18      0.27        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.62      0.60      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.65      0.66      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 256.8881540298462 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9145, F1 Micro: 0.749, F1 Macro: 0.6397\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.479255199432373 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3465, Accuracy: 0.8998, F1 Micro: 0.6742, F1 Macro: 0.4407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.23, Accuracy: 0.9088, F1 Micro: 0.7403, F1 Macro: 0.5641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1806, Accuracy: 0.9204, F1 Micro: 0.7632, F1 Macro: 0.6114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1391, Accuracy: 0.9207, F1 Micro: 0.7743, F1 Macro: 0.6738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1131, Accuracy: 0.9245, F1 Micro: 0.7808, F1 Macro: 0.7052\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9194, F1 Micro: 0.7713, F1 Macro: 0.6947\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.92, F1 Micro: 0.77, F1 Macro: 0.6872\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9256, F1 Micro: 0.7796, F1 Macro: 0.7109\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9182, F1 Micro: 0.7712, F1 Macro: 0.7025\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9225, F1 Micro: 0.7709, F1 Macro: 0.7003\n",
      "Model 1 - Iteration 9016: Accuracy: 0.9245, F1 Micro: 0.7808, F1 Macro: 0.7052\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.66      0.72      0.69       402\n",
      "  HS_Religion       0.83      0.59      0.69       157\n",
      "      HS_Race       0.84      0.62      0.72       120\n",
      "  HS_Physical       0.83      0.28      0.42        72\n",
      "    HS_Gender       0.71      0.39      0.51        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.74      0.69      0.71       689\n",
      "  HS_Moderate       0.57      0.66      0.62       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 261.3132972717285 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3404, Accuracy: 0.8991, F1 Micro: 0.6723, F1 Macro: 0.4374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2301, Accuracy: 0.91, F1 Micro: 0.7477, F1 Macro: 0.5863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1803, Accuracy: 0.9212, F1 Micro: 0.7584, F1 Macro: 0.6035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1422, Accuracy: 0.9209, F1 Micro: 0.7729, F1 Macro: 0.6419\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9213, F1 Micro: 0.7727, F1 Macro: 0.6689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.9245, F1 Micro: 0.778, F1 Macro: 0.7019\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9246, F1 Micro: 0.7727, F1 Macro: 0.6998\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9235, F1 Micro: 0.7716, F1 Macro: 0.7015\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9221, F1 Micro: 0.7696, F1 Macro: 0.7099\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9232, F1 Micro: 0.7769, F1 Macro: 0.7075\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9245, F1 Micro: 0.778, F1 Macro: 0.7019\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.71      0.70      0.70       402\n",
      "  HS_Religion       0.78      0.59      0.67       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       0.90      0.26      0.41        72\n",
      "    HS_Gender       0.58      0.41      0.48        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.69      0.71       689\n",
      "  HS_Moderate       0.64      0.62      0.63       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 260.6805431842804 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3502, Accuracy: 0.8989, F1 Micro: 0.6788, F1 Macro: 0.459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2318, Accuracy: 0.9093, F1 Micro: 0.7445, F1 Macro: 0.5751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1824, Accuracy: 0.9201, F1 Micro: 0.7632, F1 Macro: 0.6171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1432, Accuracy: 0.9227, F1 Micro: 0.7712, F1 Macro: 0.6379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1164, Accuracy: 0.9225, F1 Micro: 0.7734, F1 Macro: 0.6837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9252, F1 Micro: 0.7805, F1 Macro: 0.6908\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.924, F1 Micro: 0.7735, F1 Macro: 0.6904\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9229, F1 Micro: 0.7735, F1 Macro: 0.702\n",
      "Epoch 9/10, Train Loss: 0.0546, Accuracy: 0.9221, F1 Micro: 0.7685, F1 Macro: 0.7075\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.926, F1 Micro: 0.7741, F1 Macro: 0.7163\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9252, F1 Micro: 0.7805, F1 Macro: 0.6908\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.75      0.74       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.74      0.61      0.67       157\n",
      "      HS_Race       0.79      0.64      0.71       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.88      0.80      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 262.2594783306122 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.915, F1 Micro: 0.7506, F1 Macro: 0.6429\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 17.174411058425903 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.344, Accuracy: 0.9003, F1 Micro: 0.6908, F1 Macro: 0.4762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2324, Accuracy: 0.9143, F1 Micro: 0.7183, F1 Macro: 0.5458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1849, Accuracy: 0.9184, F1 Micro: 0.7661, F1 Macro: 0.6319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1503, Accuracy: 0.9245, F1 Micro: 0.7743, F1 Macro: 0.6446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1151, Accuracy: 0.9257, F1 Micro: 0.7803, F1 Macro: 0.6899\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9242, F1 Micro: 0.7741, F1 Macro: 0.6838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.9258, F1 Micro: 0.7835, F1 Macro: 0.71\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.9232, F1 Micro: 0.7764, F1 Macro: 0.7077\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9248, F1 Micro: 0.7722, F1 Macro: 0.7058\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.924, F1 Micro: 0.7787, F1 Macro: 0.7145\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9258, F1 Micro: 0.7835, F1 Macro: 0.71\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.73      0.63      0.68       157\n",
      "      HS_Race       0.78      0.67      0.72       120\n",
      "  HS_Physical       0.81      0.29      0.43        72\n",
      "    HS_Gender       0.65      0.51      0.57        51\n",
      "     HS_Other       0.79      0.83      0.81       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.67      0.52      0.59       331\n",
      "    HS_Strong       0.81      0.88      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 267.1887996196747 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3392, Accuracy: 0.8987, F1 Micro: 0.6954, F1 Macro: 0.493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2331, Accuracy: 0.9133, F1 Micro: 0.7184, F1 Macro: 0.5664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9153, F1 Micro: 0.7603, F1 Macro: 0.6128\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1516, Accuracy: 0.9209, F1 Micro: 0.7692, F1 Macro: 0.6233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9229, F1 Micro: 0.7737, F1 Macro: 0.6798\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9238, F1 Micro: 0.771, F1 Macro: 0.6917\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9245, F1 Micro: 0.7735, F1 Macro: 0.6988\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9252, F1 Micro: 0.7753, F1 Macro: 0.7095\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9222, F1 Micro: 0.7747, F1 Macro: 0.6977\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9238, F1 Micro: 0.7727, F1 Macro: 0.7061\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9252, F1 Micro: 0.7753, F1 Macro: 0.7095\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.79      0.65      0.71       732\n",
      "     HS_Group       0.67      0.72      0.69       402\n",
      "  HS_Religion       0.73      0.62      0.67       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.77      0.33      0.47        72\n",
      "    HS_Gender       0.68      0.45      0.54        51\n",
      "     HS_Other       0.81      0.76      0.79       762\n",
      "      HS_Weak       0.77      0.63      0.69       689\n",
      "  HS_Moderate       0.59      0.65      0.62       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 268.12517952919006 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3461, Accuracy: 0.8995, F1 Micro: 0.6951, F1 Macro: 0.4873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2335, Accuracy: 0.9133, F1 Micro: 0.7102, F1 Macro: 0.5573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1846, Accuracy: 0.9178, F1 Micro: 0.7674, F1 Macro: 0.6219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1525, Accuracy: 0.9203, F1 Micro: 0.7731, F1 Macro: 0.6454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1188, Accuracy: 0.9226, F1 Micro: 0.7767, F1 Macro: 0.6762\n",
      "Epoch 6/10, Train Loss: 0.0915, Accuracy: 0.9233, F1 Micro: 0.7732, F1 Macro: 0.6835\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9247, F1 Micro: 0.7729, F1 Macro: 0.6839\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.921, F1 Micro: 0.7727, F1 Macro: 0.6928\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9199, F1 Micro: 0.7686, F1 Macro: 0.696\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9213, F1 Micro: 0.7717, F1 Macro: 0.7046\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9226, F1 Micro: 0.7767, F1 Macro: 0.6762\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.89      0.86      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.78      0.58      0.67       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.55      0.24      0.33        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.75      0.66      0.68      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 265.99297285079956 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.9155, F1 Micro: 0.752, F1 Macro: 0.6457\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 14.264867305755615 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3454, Accuracy: 0.8989, F1 Micro: 0.6776, F1 Macro: 0.4386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2362, Accuracy: 0.9058, F1 Micro: 0.7344, F1 Macro: 0.5608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.9246, F1 Micro: 0.7723, F1 Macro: 0.6238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1439, Accuracy: 0.9226, F1 Micro: 0.7772, F1 Macro: 0.6747\n",
      "Epoch 5/10, Train Loss: 0.1146, Accuracy: 0.9231, F1 Micro: 0.7764, F1 Macro: 0.7021\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9195, F1 Micro: 0.7727, F1 Macro: 0.7055\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9233, F1 Micro: 0.777, F1 Macro: 0.7099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9255, F1 Micro: 0.7778, F1 Macro: 0.7065\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.925, F1 Micro: 0.7762, F1 Macro: 0.7061\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9243, F1 Micro: 0.7772, F1 Macro: 0.7192\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9255, F1 Micro: 0.7778, F1 Macro: 0.7065\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.90      0.92      0.91       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.74      0.60      0.66       402\n",
      "  HS_Religion       0.78      0.60      0.68       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       0.77      0.32      0.45        72\n",
      "    HS_Gender       0.65      0.47      0.55        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.68      0.52      0.59       331\n",
      "    HS_Strong       0.87      0.81      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.67      0.71      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 267.98825573921204 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3406, Accuracy: 0.9013, F1 Micro: 0.6872, F1 Macro: 0.4804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2351, Accuracy: 0.905, F1 Micro: 0.733, F1 Macro: 0.5666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.184, Accuracy: 0.9236, F1 Micro: 0.7678, F1 Macro: 0.6192\n",
      "Epoch 4/10, Train Loss: 0.1448, Accuracy: 0.9123, F1 Micro: 0.7634, F1 Macro: 0.6687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9253, F1 Micro: 0.7755, F1 Macro: 0.6868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9253, F1 Micro: 0.7769, F1 Macro: 0.7019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9243, F1 Micro: 0.7808, F1 Macro: 0.7012\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9266, F1 Micro: 0.7764, F1 Macro: 0.7095\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9178, F1 Micro: 0.7699, F1 Macro: 0.6993\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.923, F1 Micro: 0.7763, F1 Macro: 0.7119\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9243, F1 Micro: 0.7808, F1 Macro: 0.7012\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.73      0.63      0.67       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.76      0.72      0.74       120\n",
      "  HS_Physical       0.80      0.33      0.47        72\n",
      "    HS_Gender       0.60      0.35      0.44        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.64      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 269.30292558670044 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3482, Accuracy: 0.8982, F1 Micro: 0.67, F1 Macro: 0.4463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2365, Accuracy: 0.9071, F1 Micro: 0.7332, F1 Macro: 0.5573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.9223, F1 Micro: 0.7689, F1 Macro: 0.6192\n",
      "Epoch 4/10, Train Loss: 0.1455, Accuracy: 0.9177, F1 Micro: 0.7648, F1 Macro: 0.6534\n",
      "Epoch 5/10, Train Loss: 0.1183, Accuracy: 0.9203, F1 Micro: 0.7643, F1 Macro: 0.6724\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9227, F1 Micro: 0.764, F1 Macro: 0.6754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0745, Accuracy: 0.9248, F1 Micro: 0.7759, F1 Macro: 0.6967\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.921, F1 Micro: 0.7731, F1 Macro: 0.6964\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.921, F1 Micro: 0.7722, F1 Macro: 0.7021\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9217, F1 Micro: 0.7723, F1 Macro: 0.7012\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9248, F1 Micro: 0.7759, F1 Macro: 0.6967\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.77      0.73      0.75       120\n",
      "  HS_Physical       0.81      0.24      0.37        72\n",
      "    HS_Gender       0.55      0.47      0.51        51\n",
      "     HS_Other       0.80      0.76      0.78       762\n",
      "      HS_Weak       0.71      0.70      0.71       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.88      0.80      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.76      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 266.60662627220154 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9159, F1 Micro: 0.7533, F1 Macro: 0.6483\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 15.221803188323975 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3418, Accuracy: 0.8997, F1 Micro: 0.6723, F1 Macro: 0.4264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.228, Accuracy: 0.9139, F1 Micro: 0.7378, F1 Macro: 0.5809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9233, F1 Micro: 0.764, F1 Macro: 0.6235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9234, F1 Micro: 0.7699, F1 Macro: 0.6284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.9236, F1 Micro: 0.779, F1 Macro: 0.7046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0903, Accuracy: 0.926, F1 Micro: 0.7824, F1 Macro: 0.715\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9237, F1 Micro: 0.7713, F1 Macro: 0.7016\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9258, F1 Micro: 0.7707, F1 Macro: 0.6982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9252, F1 Micro: 0.7838, F1 Macro: 0.72\n",
      "Epoch 10/10, Train Loss: 0.0443, Accuracy: 0.9164, F1 Micro: 0.7689, F1 Macro: 0.7084\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9252, F1 Micro: 0.7838, F1 Macro: 0.72\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.76      0.66      0.70       402\n",
      "  HS_Religion       0.70      0.63      0.66       157\n",
      "      HS_Race       0.78      0.71      0.74       120\n",
      "  HS_Physical       0.67      0.39      0.49        72\n",
      "    HS_Gender       0.62      0.51      0.56        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.70      0.74      0.72       689\n",
      "  HS_Moderate       0.68      0.58      0.63       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.75      0.70      0.72      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 275.19540762901306 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3389, Accuracy: 0.9006, F1 Micro: 0.6719, F1 Macro: 0.4538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2277, Accuracy: 0.9115, F1 Micro: 0.7416, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.9223, F1 Micro: 0.7663, F1 Macro: 0.6181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1441, Accuracy: 0.9216, F1 Micro: 0.7697, F1 Macro: 0.6481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1131, Accuracy: 0.9263, F1 Micro: 0.7787, F1 Macro: 0.6984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0916, Accuracy: 0.925, F1 Micro: 0.7796, F1 Macro: 0.7083\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9239, F1 Micro: 0.7752, F1 Macro: 0.7069\n",
      "Epoch 8/10, Train Loss: 0.0607, Accuracy: 0.9239, F1 Micro: 0.7687, F1 Macro: 0.6973\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.921, F1 Micro: 0.778, F1 Macro: 0.7112\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9228, F1 Micro: 0.7718, F1 Macro: 0.6988\n",
      "Model 2 - Iteration 9418: Accuracy: 0.925, F1 Micro: 0.7796, F1 Macro: 0.7083\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.75      0.61      0.67       157\n",
      "      HS_Race       0.78      0.75      0.77       120\n",
      "  HS_Physical       0.83      0.28      0.42        72\n",
      "    HS_Gender       0.61      0.49      0.54        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.69      0.75      0.72       689\n",
      "  HS_Moderate       0.65      0.53      0.58       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 273.92455673217773 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3469, Accuracy: 0.8993, F1 Micro: 0.666, F1 Macro: 0.4186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2288, Accuracy: 0.9106, F1 Micro: 0.7338, F1 Macro: 0.5868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1857, Accuracy: 0.9222, F1 Micro: 0.7627, F1 Macro: 0.6234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1441, Accuracy: 0.9219, F1 Micro: 0.7696, F1 Macro: 0.6419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1142, Accuracy: 0.9241, F1 Micro: 0.7759, F1 Macro: 0.6938\n",
      "Epoch 6/10, Train Loss: 0.0928, Accuracy: 0.9224, F1 Micro: 0.7742, F1 Macro: 0.6791\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9222, F1 Micro: 0.7694, F1 Macro: 0.6929\n",
      "Epoch 8/10, Train Loss: 0.0613, Accuracy: 0.9225, F1 Micro: 0.7548, F1 Macro: 0.6829\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.923, F1 Micro: 0.7727, F1 Macro: 0.6978\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9206, F1 Micro: 0.7711, F1 Macro: 0.6989\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9241, F1 Micro: 0.7759, F1 Macro: 0.6938\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.77      0.72      0.75       120\n",
      "  HS_Physical       0.73      0.15      0.25        72\n",
      "    HS_Gender       0.62      0.47      0.53        51\n",
      "     HS_Other       0.77      0.78      0.78       762\n",
      "      HS_Weak       0.74      0.69      0.71       689\n",
      "  HS_Moderate       0.62      0.61      0.61       331\n",
      "    HS_Strong       0.83      0.88      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 272.25604486465454 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9163, F1 Micro: 0.7545, F1 Macro: 0.651\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.401798963546753 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3406, Accuracy: 0.9019, F1 Micro: 0.6874, F1 Macro: 0.4447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2331, Accuracy: 0.9117, F1 Micro: 0.7044, F1 Macro: 0.5387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1811, Accuracy: 0.9223, F1 Micro: 0.7618, F1 Macro: 0.6363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.922, F1 Micro: 0.7755, F1 Macro: 0.6622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1163, Accuracy: 0.9272, F1 Micro: 0.7805, F1 Macro: 0.6805\n",
      "Epoch 6/10, Train Loss: 0.0913, Accuracy: 0.9198, F1 Micro: 0.7719, F1 Macro: 0.6989\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9208, F1 Micro: 0.7755, F1 Macro: 0.7053\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9224, F1 Micro: 0.7683, F1 Macro: 0.693\n",
      "Epoch 9/10, Train Loss: 0.0522, Accuracy: 0.9227, F1 Micro: 0.769, F1 Macro: 0.6993\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9236, F1 Micro: 0.7716, F1 Macro: 0.7127\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9272, F1 Micro: 0.7805, F1 Macro: 0.6805\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.79      0.67      0.72       732\n",
      "     HS_Group       0.72      0.70      0.71       402\n",
      "  HS_Religion       0.79      0.51      0.62       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       1.00      0.11      0.20        72\n",
      "    HS_Gender       0.80      0.31      0.45        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.78      0.65      0.71       689\n",
      "  HS_Moderate       0.63      0.62      0.62       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.81      0.64      0.68      5556\n",
      " weighted avg       0.81      0.75      0.77      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 277.6841218471527 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3347, Accuracy: 0.9028, F1 Micro: 0.6944, F1 Macro: 0.4871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2312, Accuracy: 0.9123, F1 Micro: 0.7062, F1 Macro: 0.5502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1822, Accuracy: 0.9215, F1 Micro: 0.7553, F1 Macro: 0.6116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1454, Accuracy: 0.9201, F1 Micro: 0.7696, F1 Macro: 0.642\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9218, F1 Micro: 0.7693, F1 Macro: 0.6773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0893, Accuracy: 0.9234, F1 Micro: 0.7768, F1 Macro: 0.6984\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.924, F1 Micro: 0.7756, F1 Macro: 0.696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9226, F1 Micro: 0.7776, F1 Macro: 0.7001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.9262, F1 Micro: 0.7846, F1 Macro: 0.7143\n",
      "Epoch 10/10, Train Loss: 0.0422, Accuracy: 0.9227, F1 Micro: 0.775, F1 Macro: 0.7113\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9262, F1 Micro: 0.7846, F1 Macro: 0.7143\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.87      0.86      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.73      0.63      0.68       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.74      0.65      0.69       120\n",
      "  HS_Physical       0.71      0.35      0.47        72\n",
      "    HS_Gender       0.58      0.51      0.54        51\n",
      "     HS_Other       0.79      0.82      0.80       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.66      0.54      0.60       331\n",
      "    HS_Strong       0.88      0.89      0.88       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.75      0.69      0.71      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 280.78307485580444 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3435, Accuracy: 0.9026, F1 Micro: 0.6899, F1 Macro: 0.4783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2338, Accuracy: 0.9147, F1 Micro: 0.7267, F1 Macro: 0.5657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9209, F1 Micro: 0.7572, F1 Macro: 0.6133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1452, Accuracy: 0.9223, F1 Micro: 0.7746, F1 Macro: 0.661\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.922, F1 Micro: 0.7664, F1 Macro: 0.6743\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9214, F1 Micro: 0.7719, F1 Macro: 0.6788\n",
      "Epoch 7/10, Train Loss: 0.0751, Accuracy: 0.9197, F1 Micro: 0.7687, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.922, F1 Micro: 0.7762, F1 Macro: 0.6912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9259, F1 Micro: 0.7768, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9237, F1 Micro: 0.777, F1 Macro: 0.7154\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9237, F1 Micro: 0.777, F1 Macro: 0.7154\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.71      0.67      0.69       402\n",
      "  HS_Religion       0.72      0.59      0.65       157\n",
      "      HS_Race       0.74      0.75      0.75       120\n",
      "  HS_Physical       0.76      0.40      0.53        72\n",
      "    HS_Gender       0.55      0.57      0.56        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.63      0.62      0.62       331\n",
      "    HS_Strong       0.85      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.74      0.70      0.72      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 280.560462474823 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9167, F1 Micro: 0.7556, F1 Macro: 0.6533\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 10.380124568939209 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3423, Accuracy: 0.899, F1 Micro: 0.6541, F1 Macro: 0.3972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2297, Accuracy: 0.9166, F1 Micro: 0.7482, F1 Macro: 0.5859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1818, Accuracy: 0.9235, F1 Micro: 0.7682, F1 Macro: 0.6613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.144, Accuracy: 0.9223, F1 Micro: 0.7753, F1 Macro: 0.6711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1124, Accuracy: 0.9239, F1 Micro: 0.776, F1 Macro: 0.6889\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.9175, F1 Micro: 0.7719, F1 Macro: 0.6982\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.9237, F1 Micro: 0.7743, F1 Macro: 0.7061\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9234, F1 Micro: 0.7752, F1 Macro: 0.7099\n",
      "Epoch 9/10, Train Loss: 0.051, Accuracy: 0.9218, F1 Micro: 0.7756, F1 Macro: 0.7086\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9244, F1 Micro: 0.7756, F1 Macro: 0.7121\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9239, F1 Micro: 0.776, F1 Macro: 0.6889\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.71      0.80      0.75       732\n",
      "     HS_Group       0.79      0.51      0.62       402\n",
      "  HS_Religion       0.82      0.55      0.66       157\n",
      "      HS_Race       0.87      0.61      0.72       120\n",
      "  HS_Physical       0.77      0.28      0.41        72\n",
      "    HS_Gender       0.57      0.47      0.52        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.67      0.79      0.73       689\n",
      "  HS_Moderate       0.72      0.47      0.57       331\n",
      "    HS_Strong       0.94      0.64      0.76       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.64      0.69      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 282.611857175827 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3362, Accuracy: 0.8977, F1 Micro: 0.6468, F1 Macro: 0.4113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2294, Accuracy: 0.9135, F1 Micro: 0.7477, F1 Macro: 0.595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1818, Accuracy: 0.9218, F1 Micro: 0.7682, F1 Macro: 0.6275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1417, Accuracy: 0.9215, F1 Micro: 0.77, F1 Macro: 0.654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9238, F1 Micro: 0.7719, F1 Macro: 0.6886\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9178, F1 Micro: 0.7679, F1 Macro: 0.687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.924, F1 Micro: 0.7806, F1 Macro: 0.7127\n",
      "Epoch 8/10, Train Loss: 0.0579, Accuracy: 0.922, F1 Micro: 0.7747, F1 Macro: 0.695\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9216, F1 Micro: 0.7726, F1 Macro: 0.6995\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9222, F1 Micro: 0.7672, F1 Macro: 0.7017\n",
      "Model 2 - Iteration 9818: Accuracy: 0.924, F1 Micro: 0.7806, F1 Macro: 0.7127\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.69      0.68      0.69       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.73      0.72      0.73       120\n",
      "  HS_Physical       0.83      0.33      0.48        72\n",
      "    HS_Gender       0.61      0.45      0.52        51\n",
      "     HS_Other       0.77      0.82      0.80       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.60      0.60      0.60       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.70      0.71      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 283.2229657173157 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3445, Accuracy: 0.8987, F1 Micro: 0.6572, F1 Macro: 0.4161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2298, Accuracy: 0.9137, F1 Micro: 0.7511, F1 Macro: 0.5956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1827, Accuracy: 0.9212, F1 Micro: 0.7638, F1 Macro: 0.6402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1426, Accuracy: 0.9191, F1 Micro: 0.77, F1 Macro: 0.6687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.919, F1 Micro: 0.7711, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9182, F1 Micro: 0.7712, F1 Macro: 0.6941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9235, F1 Micro: 0.7749, F1 Macro: 0.702\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9197, F1 Micro: 0.7694, F1 Macro: 0.692\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9229, F1 Micro: 0.7743, F1 Macro: 0.708\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.9193, F1 Micro: 0.7696, F1 Macro: 0.6989\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9235, F1 Micro: 0.7749, F1 Macro: 0.702\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.66      0.69       402\n",
      "  HS_Religion       0.71      0.68      0.69       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.61      0.32      0.42        72\n",
      "    HS_Gender       0.52      0.47      0.49        51\n",
      "     HS_Other       0.80      0.77      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.59      0.61       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.73      0.68      0.70      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 284.3424415588379 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.917, F1 Micro: 0.7565, F1 Macro: 0.6553\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 8.722702264785767 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3344, Accuracy: 0.8987, F1 Micro: 0.6605, F1 Macro: 0.3947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.225, Accuracy: 0.9162, F1 Micro: 0.7499, F1 Macro: 0.5787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1773, Accuracy: 0.9216, F1 Micro: 0.7723, F1 Macro: 0.6382\n",
      "Epoch 4/10, Train Loss: 0.1425, Accuracy: 0.9221, F1 Micro: 0.7702, F1 Macro: 0.6437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1146, Accuracy: 0.9265, F1 Micro: 0.7783, F1 Macro: 0.7003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0875, Accuracy: 0.9277, F1 Micro: 0.7854, F1 Macro: 0.7123\n",
      "Epoch 7/10, Train Loss: 0.0699, Accuracy: 0.9247, F1 Micro: 0.7632, F1 Macro: 0.6911\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9265, F1 Micro: 0.7834, F1 Macro: 0.7133\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9242, F1 Micro: 0.7788, F1 Macro: 0.7146\n",
      "Epoch 10/10, Train Loss: 0.0423, Accuracy: 0.9233, F1 Micro: 0.7804, F1 Macro: 0.7073\n",
      "Model 1 - Iteration 10018: Accuracy: 0.9277, F1 Micro: 0.7854, F1 Macro: 0.7123\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.76      0.75       732\n",
      "     HS_Group       0.79      0.61      0.69       402\n",
      "  HS_Religion       0.83      0.61      0.70       157\n",
      "      HS_Race       0.87      0.62      0.72       120\n",
      "  HS_Physical       0.78      0.29      0.42        72\n",
      "    HS_Gender       0.60      0.47      0.53        51\n",
      "     HS_Other       0.78      0.81      0.80       762\n",
      "      HS_Weak       0.71      0.74      0.73       689\n",
      "  HS_Moderate       0.70      0.53      0.61       331\n",
      "    HS_Strong       0.88      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.77      0.79      5556\n",
      "    macro avg       0.79      0.67      0.71      5556\n",
      " weighted avg       0.80      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 286.3518497943878 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.331, Accuracy: 0.8988, F1 Micro: 0.6744, F1 Macro: 0.3882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2259, Accuracy: 0.9146, F1 Micro: 0.7536, F1 Macro: 0.6036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9208, F1 Micro: 0.7665, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1424, Accuracy: 0.9204, F1 Micro: 0.7691, F1 Macro: 0.6527\n",
      "Epoch 5/10, Train Loss: 0.1132, Accuracy: 0.9258, F1 Micro: 0.769, F1 Macro: 0.6821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.086, Accuracy: 0.9257, F1 Micro: 0.7744, F1 Macro: 0.6964\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.926, F1 Micro: 0.7718, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9252, F1 Micro: 0.7781, F1 Macro: 0.7077\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9227, F1 Micro: 0.7689, F1 Macro: 0.7035\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9216, F1 Micro: 0.773, F1 Macro: 0.7077\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9252, F1 Micro: 0.7781, F1 Macro: 0.7077\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.73      0.64      0.68       402\n",
      "  HS_Religion       0.78      0.57      0.66       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.78      0.29      0.42        72\n",
      "    HS_Gender       0.62      0.49      0.55        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.91      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.67      0.71      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 287.860230922699 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.339, Accuracy: 0.8992, F1 Micro: 0.674, F1 Macro: 0.429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.225, Accuracy: 0.9163, F1 Micro: 0.7491, F1 Macro: 0.5891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1792, Accuracy: 0.9222, F1 Micro: 0.7708, F1 Macro: 0.6294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1422, Accuracy: 0.9222, F1 Micro: 0.7717, F1 Macro: 0.6409\n",
      "Epoch 5/10, Train Loss: 0.1128, Accuracy: 0.924, F1 Micro: 0.7685, F1 Macro: 0.6829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.9234, F1 Micro: 0.7749, F1 Macro: 0.6945\n",
      "Epoch 7/10, Train Loss: 0.0715, Accuracy: 0.9229, F1 Micro: 0.768, F1 Macro: 0.7041\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9238, F1 Micro: 0.7762, F1 Macro: 0.7076\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9245, F1 Micro: 0.7702, F1 Macro: 0.6981\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9243, F1 Micro: 0.7748, F1 Macro: 0.7039\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9238, F1 Micro: 0.7762, F1 Macro: 0.7076\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.72      0.62      0.67       402\n",
      "  HS_Religion       0.77      0.54      0.63       157\n",
      "      HS_Race       0.79      0.73      0.76       120\n",
      "  HS_Physical       0.81      0.31      0.44        72\n",
      "    HS_Gender       0.62      0.55      0.58        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.68      0.71      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 288.8696310520172 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9174, F1 Micro: 0.7575, F1 Macro: 0.6574\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 6.298850774765015 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3314, Accuracy: 0.9019, F1 Micro: 0.6919, F1 Macro: 0.4715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2202, Accuracy: 0.9164, F1 Micro: 0.7329, F1 Macro: 0.573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1773, Accuracy: 0.9213, F1 Micro: 0.7707, F1 Macro: 0.6401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1419, Accuracy: 0.9242, F1 Micro: 0.7838, F1 Macro: 0.6886\n",
      "Epoch 5/10, Train Loss: 0.1097, Accuracy: 0.9262, F1 Micro: 0.7822, F1 Macro: 0.7008\n",
      "Epoch 6/10, Train Loss: 0.0877, Accuracy: 0.9267, F1 Micro: 0.7794, F1 Macro: 0.7086\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.9229, F1 Micro: 0.7751, F1 Macro: 0.7092\n",
      "Epoch 8/10, Train Loss: 0.0584, Accuracy: 0.9258, F1 Micro: 0.7794, F1 Macro: 0.7076\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.9246, F1 Micro: 0.7732, F1 Macro: 0.6989\n",
      "Epoch 10/10, Train Loss: 0.0379, Accuracy: 0.9259, F1 Micro: 0.7797, F1 Macro: 0.7121\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9242, F1 Micro: 0.7838, F1 Macro: 0.6886\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.90      0.86      1134\n",
      "      Abusive       0.86      0.93      0.89       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.82      0.60      0.69       157\n",
      "      HS_Race       0.72      0.72      0.72       120\n",
      "  HS_Physical       0.75      0.17      0.27        72\n",
      "    HS_Gender       0.55      0.33      0.41        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.71      0.75      0.73       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.80      0.78      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.77      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 291.1478180885315 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3278, Accuracy: 0.9023, F1 Micro: 0.697, F1 Macro: 0.4892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2221, Accuracy: 0.9166, F1 Micro: 0.7331, F1 Macro: 0.5769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1765, Accuracy: 0.9195, F1 Micro: 0.7647, F1 Macro: 0.6146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1411, Accuracy: 0.9215, F1 Micro: 0.7712, F1 Macro: 0.6678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.9239, F1 Micro: 0.7789, F1 Macro: 0.6913\n",
      "Epoch 6/10, Train Loss: 0.087, Accuracy: 0.9255, F1 Micro: 0.7747, F1 Macro: 0.6899\n",
      "Epoch 7/10, Train Loss: 0.0708, Accuracy: 0.9219, F1 Micro: 0.7699, F1 Macro: 0.7048\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9193, F1 Micro: 0.7668, F1 Macro: 0.698\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.9203, F1 Micro: 0.7696, F1 Macro: 0.702\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9172, F1 Micro: 0.7685, F1 Macro: 0.6998\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9239, F1 Micro: 0.7789, F1 Macro: 0.6913\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.70      0.67      0.68       402\n",
      "  HS_Religion       0.76      0.62      0.68       157\n",
      "      HS_Race       0.72      0.68      0.70       120\n",
      "  HS_Physical       0.88      0.19      0.32        72\n",
      "    HS_Gender       0.68      0.33      0.45        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.85      0.88      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.67      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 292.4588575363159 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3358, Accuracy: 0.9005, F1 Micro: 0.6943, F1 Macro: 0.4833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2212, Accuracy: 0.9162, F1 Micro: 0.7308, F1 Macro: 0.572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1763, Accuracy: 0.9222, F1 Micro: 0.7672, F1 Macro: 0.6298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1411, Accuracy: 0.9191, F1 Micro: 0.7685, F1 Macro: 0.6687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9245, F1 Micro: 0.7789, F1 Macro: 0.6844\n",
      "Epoch 6/10, Train Loss: 0.0858, Accuracy: 0.9222, F1 Micro: 0.7715, F1 Macro: 0.6799\n",
      "Epoch 7/10, Train Loss: 0.0748, Accuracy: 0.9232, F1 Micro: 0.7695, F1 Macro: 0.6914\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9242, F1 Micro: 0.7751, F1 Macro: 0.6984\n",
      "Epoch 9/10, Train Loss: 0.0491, Accuracy: 0.9211, F1 Micro: 0.769, F1 Macro: 0.6892\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.9229, F1 Micro: 0.7717, F1 Macro: 0.6961\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9245, F1 Micro: 0.7789, F1 Macro: 0.6844\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.91      0.88      0.89       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.70      0.69      0.70       402\n",
      "  HS_Religion       0.74      0.63      0.68       157\n",
      "      HS_Race       0.75      0.71      0.73       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.56      0.37      0.45        51\n",
      "     HS_Other       0.76      0.81      0.78       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.83      0.88      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.68      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 291.66370248794556 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9176, F1 Micro: 0.7583, F1 Macro: 0.6586\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.917448043823242 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3302, Accuracy: 0.8956, F1 Micro: 0.6237, F1 Macro: 0.3517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2153, Accuracy: 0.916, F1 Micro: 0.7432, F1 Macro: 0.5825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.174, Accuracy: 0.9198, F1 Micro: 0.7707, F1 Macro: 0.6442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9248, F1 Micro: 0.7745, F1 Macro: 0.6811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1087, Accuracy: 0.9261, F1 Micro: 0.7785, F1 Macro: 0.7048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0856, Accuracy: 0.9263, F1 Micro: 0.78, F1 Macro: 0.6837\n",
      "Epoch 7/10, Train Loss: 0.0703, Accuracy: 0.9204, F1 Micro: 0.7724, F1 Macro: 0.7012\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.9233, F1 Micro: 0.7729, F1 Macro: 0.7061\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9172, F1 Micro: 0.7668, F1 Macro: 0.698\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9201, F1 Micro: 0.7708, F1 Macro: 0.7056\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9263, F1 Micro: 0.78, F1 Macro: 0.6837\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.73      0.66      0.69       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.85      0.59      0.70       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.65      0.22      0.32        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.75      0.69      0.72       689\n",
      "  HS_Moderate       0.66      0.56      0.61       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.80      0.64      0.68      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 299.0376160144806 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3254, Accuracy: 0.8981, F1 Micro: 0.6441, F1 Macro: 0.3895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2172, Accuracy: 0.9148, F1 Micro: 0.7328, F1 Macro: 0.5915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1751, Accuracy: 0.9219, F1 Micro: 0.767, F1 Macro: 0.62\n",
      "Epoch 4/10, Train Loss: 0.1411, Accuracy: 0.9231, F1 Micro: 0.766, F1 Macro: 0.6652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1104, Accuracy: 0.9236, F1 Micro: 0.7695, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.9171, F1 Micro: 0.7699, F1 Macro: 0.6832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.923, F1 Micro: 0.7783, F1 Macro: 0.7117\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.9235, F1 Micro: 0.7772, F1 Macro: 0.7116\n",
      "Epoch 9/10, Train Loss: 0.0475, Accuracy: 0.9209, F1 Micro: 0.7692, F1 Macro: 0.7025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.922, F1 Micro: 0.7787, F1 Macro: 0.7121\n",
      "Model 2 - Iteration 10418: Accuracy: 0.922, F1 Micro: 0.7787, F1 Macro: 0.7121\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.86      1134\n",
      "      Abusive       0.87      0.94      0.90       992\n",
      "HS_Individual       0.68      0.80      0.73       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.75      0.65      0.70       120\n",
      "  HS_Physical       0.70      0.39      0.50        72\n",
      "    HS_Gender       0.56      0.57      0.56        51\n",
      "     HS_Other       0.75      0.83      0.78       762\n",
      "      HS_Weak       0.66      0.77      0.71       689\n",
      "  HS_Moderate       0.68      0.54      0.60       331\n",
      "    HS_Strong       0.89      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.44      5556\n",
      "\n",
      "Training completed in 299.8184931278229 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3345, Accuracy: 0.8972, F1 Micro: 0.6383, F1 Macro: 0.393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2165, Accuracy: 0.9142, F1 Micro: 0.7355, F1 Macro: 0.583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1726, Accuracy: 0.9162, F1 Micro: 0.7636, F1 Macro: 0.6232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.9233, F1 Micro: 0.7694, F1 Macro: 0.6647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1105, Accuracy: 0.9218, F1 Micro: 0.7732, F1 Macro: 0.6968\n",
      "Epoch 6/10, Train Loss: 0.0864, Accuracy: 0.9179, F1 Micro: 0.7693, F1 Macro: 0.672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0715, Accuracy: 0.9253, F1 Micro: 0.7777, F1 Macro: 0.691\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9231, F1 Micro: 0.7741, F1 Macro: 0.7076\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9233, F1 Micro: 0.7716, F1 Macro: 0.6979\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.9204, F1 Micro: 0.7733, F1 Macro: 0.7035\n",
      "Model 3 - Iteration 10418: Accuracy: 0.9253, F1 Micro: 0.7777, F1 Macro: 0.691\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.74      0.73      0.74       732\n",
      "     HS_Group       0.73      0.62      0.67       402\n",
      "  HS_Religion       0.76      0.57      0.65       157\n",
      "      HS_Race       0.73      0.75      0.74       120\n",
      "  HS_Physical       1.00      0.15      0.27        72\n",
      "    HS_Gender       0.59      0.47      0.52        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.72      0.71      0.72       689\n",
      "  HS_Moderate       0.65      0.56      0.60       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.45      0.43      0.42      5556\n",
      "\n",
      "Training completed in 298.4096984863281 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9179, F1 Micro: 0.7591, F1 Macro: 0.66\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.0755035877227783 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3265, Accuracy: 0.9015, F1 Micro: 0.693, F1 Macro: 0.4555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2187, Accuracy: 0.918, F1 Micro: 0.7438, F1 Macro: 0.5803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1698, Accuracy: 0.9224, F1 Micro: 0.7568, F1 Macro: 0.6332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1349, Accuracy: 0.9207, F1 Micro: 0.7675, F1 Macro: 0.6681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.108, Accuracy: 0.9271, F1 Micro: 0.7778, F1 Macro: 0.6927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0871, Accuracy: 0.9252, F1 Micro: 0.7854, F1 Macro: 0.7011\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9263, F1 Micro: 0.7815, F1 Macro: 0.7156\n",
      "Epoch 8/10, Train Loss: 0.0543, Accuracy: 0.9228, F1 Micro: 0.7766, F1 Macro: 0.7042\n",
      "Epoch 9/10, Train Loss: 0.0484, Accuracy: 0.9244, F1 Micro: 0.7772, F1 Macro: 0.7083\n",
      "Epoch 10/10, Train Loss: 0.0384, Accuracy: 0.9243, F1 Micro: 0.7771, F1 Macro: 0.7089\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9252, F1 Micro: 0.7854, F1 Macro: 0.7011\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.73      0.76      0.74       732\n",
      "     HS_Group       0.71      0.69      0.70       402\n",
      "  HS_Religion       0.69      0.70      0.70       157\n",
      "      HS_Race       0.77      0.68      0.72       120\n",
      "  HS_Physical       1.00      0.24      0.38        72\n",
      "    HS_Gender       0.61      0.33      0.43        51\n",
      "     HS_Other       0.76      0.83      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.90      0.79      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.79      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.46      0.45      0.44      5556\n",
      "\n",
      "Training completed in 301.0681436061859 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3234, Accuracy: 0.9004, F1 Micro: 0.6946, F1 Macro: 0.4774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2183, Accuracy: 0.9163, F1 Micro: 0.7459, F1 Macro: 0.5923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1711, Accuracy: 0.9207, F1 Micro: 0.7617, F1 Macro: 0.6095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1375, Accuracy: 0.9226, F1 Micro: 0.7699, F1 Macro: 0.6668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1086, Accuracy: 0.9263, F1 Micro: 0.7742, F1 Macro: 0.686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0868, Accuracy: 0.9254, F1 Micro: 0.7762, F1 Macro: 0.6886\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9237, F1 Micro: 0.7738, F1 Macro: 0.7093\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9194, F1 Micro: 0.7694, F1 Macro: 0.6893\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9228, F1 Micro: 0.7679, F1 Macro: 0.7022\n",
      "Epoch 10/10, Train Loss: 0.0383, Accuracy: 0.9246, F1 Micro: 0.7691, F1 Macro: 0.7054\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9254, F1 Micro: 0.7762, F1 Macro: 0.6886\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.77      0.64      0.70       402\n",
      "  HS_Religion       0.77      0.58      0.66       157\n",
      "      HS_Race       0.75      0.67      0.70       120\n",
      "  HS_Physical       0.95      0.25      0.40        72\n",
      "    HS_Gender       0.70      0.27      0.39        51\n",
      "     HS_Other       0.78      0.77      0.78       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.70      0.55      0.62       331\n",
      "    HS_Strong       0.91      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.75      0.78      5556\n",
      "    macro avg       0.79      0.64      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 301.6297483444214 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3309, Accuracy: 0.9013, F1 Micro: 0.6902, F1 Macro: 0.4581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.221, Accuracy: 0.9169, F1 Micro: 0.7495, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1711, Accuracy: 0.9201, F1 Micro: 0.7501, F1 Macro: 0.6179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.9219, F1 Micro: 0.7705, F1 Macro: 0.6707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1098, Accuracy: 0.9267, F1 Micro: 0.7744, F1 Macro: 0.6842\n",
      "Epoch 6/10, Train Loss: 0.0888, Accuracy: 0.923, F1 Micro: 0.769, F1 Macro: 0.6794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0696, Accuracy: 0.9236, F1 Micro: 0.7759, F1 Macro: 0.7116\n",
      "Epoch 8/10, Train Loss: 0.0543, Accuracy: 0.9198, F1 Micro: 0.7668, F1 Macro: 0.6964\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9215, F1 Micro: 0.7709, F1 Macro: 0.699\n",
      "Epoch 10/10, Train Loss: 0.04, Accuracy: 0.9229, F1 Micro: 0.7716, F1 Macro: 0.7047\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9236, F1 Micro: 0.7759, F1 Macro: 0.7116\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.85      1134\n",
      "      Abusive       0.89      0.92      0.90       992\n",
      "HS_Individual       0.74      0.70      0.72       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.73      0.61      0.66       157\n",
      "      HS_Race       0.74      0.76      0.75       120\n",
      "  HS_Physical       0.82      0.32      0.46        72\n",
      "    HS_Gender       0.59      0.53      0.56        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.59      0.62      0.61       331\n",
      "    HS_Strong       0.90      0.83      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.69      0.71      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 302.75548243522644 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9181, F1 Micro: 0.7598, F1 Macro: 0.6614\n",
      "Total sampling time: 1041.05 seconds\n",
      "Total runtime: 20063.985627651215 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72acc0",
   "metadata": {
    "papermill": {
     "duration": 0.174051,
     "end_time": "2025-03-15T13:24:25.554643",
     "exception": false,
     "start_time": "2025-03-15T13:24:25.380592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c2e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5538, Accuracy: 0.8312, F1 Micro: 0.076, F1 Macro: 0.0284\n",
      "Epoch 2/10, Train Loss: 0.4149, Accuracy: 0.8299, F1 Micro: 0.0348, F1 Macro: 0.0147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3904, Accuracy: 0.8403, F1 Micro: 0.1715, F1 Macro: 0.0617\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3714, Accuracy: 0.8588, F1 Micro: 0.3943, F1 Macro: 0.1258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3395, Accuracy: 0.8751, F1 Micro: 0.5537, F1 Macro: 0.2498\n",
      "Epoch 6/10, Train Loss: 0.2805, Accuracy: 0.8743, F1 Micro: 0.5144, F1 Macro: 0.2417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2621, Accuracy: 0.8781, F1 Micro: 0.5935, F1 Macro: 0.3158\n",
      "Epoch 8/10, Train Loss: 0.2328, Accuracy: 0.878, F1 Micro: 0.5855, F1 Macro: 0.3282\n",
      "Epoch 9/10, Train Loss: 0.1927, Accuracy: 0.8766, F1 Micro: 0.5851, F1 Macro: 0.3423\n",
      "Epoch 10/10, Train Loss: 0.1855, Accuracy: 0.8779, F1 Micro: 0.5753, F1 Macro: 0.3353\n",
      "Model 1 - Iteration 658: Accuracy: 0.8781, F1 Micro: 0.5935, F1 Macro: 0.3158\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.79      0.78      1134\n",
      "      Abusive       0.85      0.73      0.79       992\n",
      "HS_Individual       0.66      0.44      0.52       732\n",
      "     HS_Group       0.49      0.31      0.38       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.60      0.65      0.63       762\n",
      "      HS_Weak       0.63      0.38      0.47       689\n",
      "  HS_Moderate       0.36      0.15      0.21       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.70      0.52      0.59      5556\n",
      "    macro avg       0.36      0.29      0.32      5556\n",
      " weighted avg       0.62      0.52      0.55      5556\n",
      "  samples avg       0.37      0.30      0.30      5556\n",
      "\n",
      "Training completed in 54.81217813491821 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5532, Accuracy: 0.8307, F1 Micro: 0.0557, F1 Macro: 0.022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4153, Accuracy: 0.8354, F1 Micro: 0.1246, F1 Macro: 0.0426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3907, Accuracy: 0.8379, F1 Micro: 0.1605, F1 Macro: 0.0512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3735, Accuracy: 0.8544, F1 Micro: 0.3403, F1 Macro: 0.1177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3425, Accuracy: 0.8734, F1 Micro: 0.5257, F1 Macro: 0.2423\n",
      "Epoch 6/10, Train Loss: 0.2883, Accuracy: 0.872, F1 Micro: 0.4793, F1 Macro: 0.2231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.267, Accuracy: 0.8783, F1 Micro: 0.582, F1 Macro: 0.286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2375, Accuracy: 0.8796, F1 Micro: 0.5986, F1 Macro: 0.3176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1991, Accuracy: 0.8792, F1 Micro: 0.6063, F1 Macro: 0.3306\n",
      "Epoch 10/10, Train Loss: 0.1879, Accuracy: 0.8815, F1 Micro: 0.5947, F1 Macro: 0.334\n",
      "Model 2 - Iteration 658: Accuracy: 0.8792, F1 Micro: 0.6063, F1 Macro: 0.3306\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.77      0.78      0.77      1134\n",
      "      Abusive       0.84      0.73      0.79       992\n",
      "HS_Individual       0.63      0.52      0.57       732\n",
      "     HS_Group       0.55      0.31      0.39       402\n",
      "  HS_Religion       0.00      0.00      0.00       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.64      0.65      0.65       762\n",
      "      HS_Weak       0.60      0.46      0.52       689\n",
      "  HS_Moderate       0.36      0.22      0.28       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.54      0.61      5556\n",
      "    macro avg       0.37      0.31      0.33      5556\n",
      " weighted avg       0.62      0.54      0.57      5556\n",
      "  samples avg       0.38      0.31      0.32      5556\n",
      "\n",
      "Training completed in 60.13046717643738 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.8364, F1 Micro: 0.1418, F1 Macro: 0.0476\n",
      "Epoch 2/10, Train Loss: 0.4137, Accuracy: 0.8326, F1 Micro: 0.0674, F1 Macro: 0.0265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3895, Accuracy: 0.8433, F1 Micro: 0.2118, F1 Macro: 0.0733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3691, Accuracy: 0.8583, F1 Micro: 0.3778, F1 Macro: 0.1299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3367, Accuracy: 0.8761, F1 Micro: 0.5576, F1 Macro: 0.256\n",
      "Epoch 6/10, Train Loss: 0.2823, Accuracy: 0.8729, F1 Micro: 0.49, F1 Macro: 0.2327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2572, Accuracy: 0.8798, F1 Micro: 0.5782, F1 Macro: 0.3101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2312, Accuracy: 0.8798, F1 Micro: 0.5983, F1 Macro: 0.3538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1925, Accuracy: 0.8797, F1 Micro: 0.6079, F1 Macro: 0.3704\n",
      "Epoch 10/10, Train Loss: 0.1801, Accuracy: 0.8804, F1 Micro: 0.5923, F1 Macro: 0.354\n",
      "Model 3 - Iteration 658: Accuracy: 0.8797, F1 Micro: 0.6079, F1 Macro: 0.3704\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.75      0.77      1134\n",
      "      Abusive       0.80      0.80      0.80       992\n",
      "HS_Individual       0.69      0.40      0.51       732\n",
      "     HS_Group       0.51      0.50      0.50       402\n",
      "  HS_Religion       0.59      0.25      0.36       157\n",
      "      HS_Race       0.00      0.00      0.00       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.54      0.60       762\n",
      "      HS_Weak       0.63      0.42      0.50       689\n",
      "  HS_Moderate       0.39      0.41      0.40       331\n",
      "    HS_Strong       0.00      0.00      0.00       114\n",
      "\n",
      "    micro avg       0.69      0.54      0.61      5556\n",
      "    macro avg       0.42      0.34      0.37      5556\n",
      " weighted avg       0.65      0.54      0.58      5556\n",
      "  samples avg       0.37      0.31      0.31      5556\n",
      "\n",
      "Training completed in 58.22942090034485 s\n",
      "Averaged - Iteration 658: Accuracy: 0.879, F1 Micro: 0.6026, F1 Macro: 0.3389\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 988\n",
      "Sampling duration: 126.56432747840881 seconds\n",
      "New train size: 1646\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4674, Accuracy: 0.8336, F1 Micro: 0.0902, F1 Macro: 0.034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3672, Accuracy: 0.8709, F1 Micro: 0.504, F1 Macro: 0.2073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3116, Accuracy: 0.8847, F1 Micro: 0.5903, F1 Macro: 0.2945\n",
      "Epoch 4/10, Train Loss: 0.2622, Accuracy: 0.8795, F1 Micro: 0.5087, F1 Macro: 0.2659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2259, Accuracy: 0.8904, F1 Micro: 0.67, F1 Macro: 0.4796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1962, Accuracy: 0.8969, F1 Micro: 0.6736, F1 Macro: 0.4417\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1693, Accuracy: 0.899, F1 Micro: 0.6836, F1 Macro: 0.4812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1496, Accuracy: 0.8939, F1 Micro: 0.7049, F1 Macro: 0.5244\n",
      "Epoch 9/10, Train Loss: 0.1244, Accuracy: 0.8978, F1 Micro: 0.6962, F1 Macro: 0.525\n",
      "Epoch 10/10, Train Loss: 0.106, Accuracy: 0.9015, F1 Micro: 0.6923, F1 Macro: 0.5226\n",
      "Model 1 - Iteration 1646: Accuracy: 0.8939, F1 Micro: 0.7049, F1 Macro: 0.5244\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.75      0.88      0.81      1134\n",
      "      Abusive       0.83      0.87      0.85       992\n",
      "HS_Individual       0.62      0.76      0.68       732\n",
      "     HS_Group       0.61      0.58      0.59       402\n",
      "  HS_Religion       0.60      0.32      0.41       157\n",
      "      HS_Race       0.73      0.51      0.60       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.63      0.81      0.71       762\n",
      "      HS_Weak       0.59      0.73      0.65       689\n",
      "  HS_Moderate       0.48      0.48      0.48       331\n",
      "    HS_Strong       0.91      0.34      0.50       114\n",
      "\n",
      "    micro avg       0.68      0.74      0.70      5556\n",
      "    macro avg       0.56      0.52      0.52      5556\n",
      " weighted avg       0.67      0.74      0.69      5556\n",
      "  samples avg       0.40      0.41      0.39      5556\n",
      "\n",
      "Training completed in 81.99246644973755 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4659, Accuracy: 0.8352, F1 Micro: 0.1143, F1 Macro: 0.0409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3673, Accuracy: 0.8708, F1 Micro: 0.4944, F1 Macro: 0.2274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3166, Accuracy: 0.8811, F1 Micro: 0.5661, F1 Macro: 0.2692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2703, Accuracy: 0.8855, F1 Micro: 0.5742, F1 Macro: 0.3031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2318, Accuracy: 0.8934, F1 Micro: 0.6565, F1 Macro: 0.436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2015, Accuracy: 0.8966, F1 Micro: 0.6727, F1 Macro: 0.4365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1719, Accuracy: 0.8976, F1 Micro: 0.6748, F1 Macro: 0.4221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1511, Accuracy: 0.8997, F1 Micro: 0.6944, F1 Macro: 0.4925\n",
      "Epoch 9/10, Train Loss: 0.1187, Accuracy: 0.8974, F1 Micro: 0.6937, F1 Macro: 0.5102\n",
      "Epoch 10/10, Train Loss: 0.1046, Accuracy: 0.8977, F1 Micro: 0.6912, F1 Macro: 0.5168\n",
      "Model 2 - Iteration 1646: Accuracy: 0.8997, F1 Micro: 0.6944, F1 Macro: 0.4925\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.81      0.82      1134\n",
      "      Abusive       0.86      0.82      0.84       992\n",
      "HS_Individual       0.68      0.62      0.65       732\n",
      "     HS_Group       0.63      0.60      0.61       402\n",
      "  HS_Religion       0.74      0.22      0.33       157\n",
      "      HS_Race       0.84      0.41      0.55       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.74      0.71       762\n",
      "      HS_Weak       0.65      0.60      0.62       689\n",
      "  HS_Moderate       0.50      0.49      0.50       331\n",
      "    HS_Strong       1.00      0.16      0.27       114\n",
      "\n",
      "    micro avg       0.73      0.66      0.69      5556\n",
      "    macro avg       0.62      0.46      0.49      5556\n",
      " weighted avg       0.72      0.66      0.68      5556\n",
      "  samples avg       0.39      0.37      0.36      5556\n",
      "\n",
      "Training completed in 84.54692077636719 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4608, Accuracy: 0.8381, F1 Micro: 0.1498, F1 Macro: 0.0521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3686, Accuracy: 0.8674, F1 Micro: 0.465, F1 Macro: 0.1847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3134, Accuracy: 0.8852, F1 Micro: 0.5988, F1 Macro: 0.2939\n",
      "Epoch 4/10, Train Loss: 0.2627, Accuracy: 0.8855, F1 Micro: 0.5693, F1 Macro: 0.3145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.226, Accuracy: 0.8927, F1 Micro: 0.6509, F1 Macro: 0.4524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1973, Accuracy: 0.8972, F1 Micro: 0.6723, F1 Macro: 0.4366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1659, Accuracy: 0.8962, F1 Micro: 0.6891, F1 Macro: 0.4778\n",
      "Epoch 8/10, Train Loss: 0.1469, Accuracy: 0.8949, F1 Micro: 0.6889, F1 Macro: 0.4548\n",
      "Epoch 9/10, Train Loss: 0.1199, Accuracy: 0.8946, F1 Micro: 0.6765, F1 Macro: 0.4752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1057, Accuracy: 0.8943, F1 Micro: 0.6906, F1 Macro: 0.5071\n",
      "Model 3 - Iteration 1646: Accuracy: 0.8943, F1 Micro: 0.6906, F1 Macro: 0.5071\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.78      0.83      0.81      1134\n",
      "      Abusive       0.81      0.90      0.85       992\n",
      "HS_Individual       0.68      0.59      0.63       732\n",
      "     HS_Group       0.55      0.64      0.59       402\n",
      "  HS_Religion       0.62      0.46      0.53       157\n",
      "      HS_Race       0.71      0.48      0.57       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.71      0.73      0.72       762\n",
      "      HS_Weak       0.62      0.57      0.60       689\n",
      "  HS_Moderate       0.44      0.56      0.49       331\n",
      "    HS_Strong       0.95      0.18      0.30       114\n",
      "\n",
      "    micro avg       0.70      0.68      0.69      5556\n",
      "    macro avg       0.57      0.49      0.51      5556\n",
      " weighted avg       0.69      0.68      0.68      5556\n",
      "  samples avg       0.41      0.39      0.38      5556\n",
      "\n",
      "Training completed in 82.36775946617126 s\n",
      "Averaged - Iteration 1646: Accuracy: 0.8875, F1 Micro: 0.6496, F1 Macro: 0.4234\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 889\n",
      "Sampling duration: 112.00675964355469 seconds\n",
      "New train size: 2535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4395, Accuracy: 0.8525, F1 Micro: 0.4088, F1 Macro: 0.1181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3291, Accuracy: 0.8804, F1 Micro: 0.6292, F1 Macro: 0.334\n",
      "Epoch 3/10, Train Loss: 0.2744, Accuracy: 0.8907, F1 Micro: 0.6217, F1 Macro: 0.3977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2258, Accuracy: 0.8988, F1 Micro: 0.6683, F1 Macro: 0.429\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1926, Accuracy: 0.9015, F1 Micro: 0.7027, F1 Macro: 0.489\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1505, Accuracy: 0.9037, F1 Micro: 0.7151, F1 Macro: 0.5324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1298, Accuracy: 0.9045, F1 Micro: 0.7175, F1 Macro: 0.535\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.9, F1 Micro: 0.7204, F1 Macro: 0.5556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0926, Accuracy: 0.9049, F1 Micro: 0.7208, F1 Macro: 0.5629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0765, Accuracy: 0.9054, F1 Micro: 0.7219, F1 Macro: 0.5632\n",
      "Model 1 - Iteration 2535: Accuracy: 0.9054, F1 Micro: 0.7219, F1 Macro: 0.5632\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.83      0.82      1134\n",
      "      Abusive       0.83      0.88      0.86       992\n",
      "HS_Individual       0.65      0.73      0.69       732\n",
      "     HS_Group       0.71      0.50      0.59       402\n",
      "  HS_Religion       0.68      0.40      0.50       157\n",
      "      HS_Race       0.79      0.52      0.63       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.04      0.08        51\n",
      "     HS_Other       0.69      0.77      0.73       762\n",
      "      HS_Weak       0.63      0.71      0.67       689\n",
      "  HS_Moderate       0.59      0.38      0.46       331\n",
      "    HS_Strong       0.91      0.62      0.74       114\n",
      "\n",
      "    micro avg       0.73      0.71      0.72      5556\n",
      "    macro avg       0.69      0.53      0.56      5556\n",
      " weighted avg       0.72      0.71      0.71      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 109.27983164787292 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4411, Accuracy: 0.8546, F1 Micro: 0.379, F1 Macro: 0.118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3353, Accuracy: 0.8808, F1 Micro: 0.6293, F1 Macro: 0.301\n",
      "Epoch 3/10, Train Loss: 0.2814, Accuracy: 0.8929, F1 Micro: 0.6264, F1 Macro: 0.3793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2291, Accuracy: 0.9, F1 Micro: 0.6711, F1 Macro: 0.4367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1933, Accuracy: 0.9032, F1 Micro: 0.7017, F1 Macro: 0.4871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1514, Accuracy: 0.9051, F1 Micro: 0.7159, F1 Macro: 0.5269\n",
      "Epoch 7/10, Train Loss: 0.1285, Accuracy: 0.9063, F1 Micro: 0.7065, F1 Macro: 0.5393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1062, Accuracy: 0.9058, F1 Micro: 0.7212, F1 Macro: 0.5378\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.9073, F1 Micro: 0.719, F1 Macro: 0.5549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0774, Accuracy: 0.9069, F1 Micro: 0.7212, F1 Macro: 0.5623\n",
      "Model 2 - Iteration 2535: Accuracy: 0.9069, F1 Micro: 0.7212, F1 Macro: 0.5623\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.82      0.82      1134\n",
      "      Abusive       0.86      0.87      0.87       992\n",
      "HS_Individual       0.70      0.65      0.67       732\n",
      "     HS_Group       0.63      0.62      0.63       402\n",
      "  HS_Religion       0.72      0.40      0.51       157\n",
      "      HS_Race       0.85      0.55      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.02      0.04        51\n",
      "     HS_Other       0.71      0.78      0.74       762\n",
      "      HS_Weak       0.68      0.61      0.64       689\n",
      "  HS_Moderate       0.51      0.52      0.52       331\n",
      "    HS_Strong       0.93      0.49      0.64       114\n",
      "\n",
      "    micro avg       0.74      0.70      0.72      5556\n",
      "    macro avg       0.70      0.53      0.56      5556\n",
      " weighted avg       0.74      0.70      0.71      5556\n",
      "  samples avg       0.41      0.40      0.39      5556\n",
      "\n",
      "Training completed in 104.98453044891357 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4337, Accuracy: 0.8531, F1 Micro: 0.3589, F1 Macro: 0.1119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3306, Accuracy: 0.8825, F1 Micro: 0.6361, F1 Macro: 0.3367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2744, Accuracy: 0.8941, F1 Micro: 0.652, F1 Macro: 0.4014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.226, Accuracy: 0.8997, F1 Micro: 0.6795, F1 Macro: 0.4393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1915, Accuracy: 0.9019, F1 Micro: 0.7044, F1 Macro: 0.4846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1521, Accuracy: 0.8952, F1 Micro: 0.7105, F1 Macro: 0.506\n",
      "Epoch 7/10, Train Loss: 0.1314, Accuracy: 0.9064, F1 Micro: 0.7103, F1 Macro: 0.5169\n",
      "Epoch 8/10, Train Loss: 0.1073, Accuracy: 0.904, F1 Micro: 0.7041, F1 Macro: 0.5192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0913, Accuracy: 0.9019, F1 Micro: 0.7162, F1 Macro: 0.5524\n",
      "Epoch 10/10, Train Loss: 0.0797, Accuracy: 0.9075, F1 Micro: 0.7139, F1 Macro: 0.5636\n",
      "Model 3 - Iteration 2535: Accuracy: 0.9019, F1 Micro: 0.7162, F1 Macro: 0.5524\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.84      0.82      1134\n",
      "      Abusive       0.83      0.90      0.86       992\n",
      "HS_Individual       0.70      0.63      0.66       732\n",
      "     HS_Group       0.57      0.70      0.63       402\n",
      "  HS_Religion       0.61      0.61      0.61       157\n",
      "      HS_Race       0.71      0.47      0.56       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.73      0.77      0.75       762\n",
      "      HS_Weak       0.67      0.60      0.63       689\n",
      "  HS_Moderate       0.46      0.62      0.53       331\n",
      "    HS_Strong       0.98      0.41      0.58       114\n",
      "\n",
      "    micro avg       0.71      0.72      0.72      5556\n",
      "    macro avg       0.59      0.55      0.55      5556\n",
      " weighted avg       0.71      0.72      0.71      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 102.67372059822083 s\n",
      "Averaged - Iteration 2535: Accuracy: 0.8932, F1 Micro: 0.673, F1 Macro: 0.4687\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 800\n",
      "Sampling duration: 100.22887849807739 seconds\n",
      "New train size: 3335\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4238, Accuracy: 0.869, F1 Micro: 0.4848, F1 Macro: 0.2052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3043, Accuracy: 0.8812, F1 Micro: 0.5304, F1 Macro: 0.2557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2473, Accuracy: 0.8989, F1 Micro: 0.6576, F1 Macro: 0.4354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.207, Accuracy: 0.9071, F1 Micro: 0.7187, F1 Macro: 0.5465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9095, F1 Micro: 0.7272, F1 Macro: 0.5433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1404, Accuracy: 0.9113, F1 Micro: 0.7326, F1 Macro: 0.565\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9114, F1 Micro: 0.7249, F1 Macro: 0.5777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0944, Accuracy: 0.9107, F1 Micro: 0.739, F1 Macro: 0.592\n",
      "Epoch 9/10, Train Loss: 0.075, Accuracy: 0.9086, F1 Micro: 0.7306, F1 Macro: 0.5923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9135, F1 Micro: 0.7396, F1 Macro: 0.6163\n",
      "Model 1 - Iteration 3335: Accuracy: 0.9135, F1 Micro: 0.7396, F1 Macro: 0.6163\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.83      0.83      1134\n",
      "      Abusive       0.89      0.85      0.87       992\n",
      "HS_Individual       0.70      0.69      0.69       732\n",
      "     HS_Group       0.68      0.59      0.63       402\n",
      "  HS_Religion       0.75      0.51      0.61       157\n",
      "      HS_Race       0.79      0.57      0.66       120\n",
      "  HS_Physical       0.55      0.08      0.14        72\n",
      "    HS_Gender       0.75      0.12      0.20        51\n",
      "     HS_Other       0.75      0.75      0.75       762\n",
      "      HS_Weak       0.68      0.66      0.67       689\n",
      "  HS_Moderate       0.60      0.49      0.54       331\n",
      "    HS_Strong       0.88      0.72      0.79       114\n",
      "\n",
      "    micro avg       0.77      0.71      0.74      5556\n",
      "    macro avg       0.74      0.57      0.62      5556\n",
      " weighted avg       0.76      0.71      0.73      5556\n",
      "  samples avg       0.42      0.40      0.39      5556\n",
      "\n",
      "Training completed in 127.45210814476013 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4255, Accuracy: 0.8668, F1 Micro: 0.4447, F1 Macro: 0.2002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3101, Accuracy: 0.8831, F1 Micro: 0.537, F1 Macro: 0.2637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2474, Accuracy: 0.9007, F1 Micro: 0.6839, F1 Macro: 0.4543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2075, Accuracy: 0.9043, F1 Micro: 0.7168, F1 Macro: 0.5348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1745, Accuracy: 0.9115, F1 Micro: 0.7303, F1 Macro: 0.5608\n",
      "Epoch 6/10, Train Loss: 0.1347, Accuracy: 0.9106, F1 Micro: 0.7268, F1 Macro: 0.5646\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9086, F1 Micro: 0.7057, F1 Macro: 0.5674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9066, F1 Micro: 0.734, F1 Macro: 0.5971\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9062, F1 Micro: 0.7293, F1 Macro: 0.5985\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9132, F1 Micro: 0.7333, F1 Macro: 0.6092\n",
      "Model 2 - Iteration 3335: Accuracy: 0.9066, F1 Micro: 0.734, F1 Macro: 0.5971\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.79      0.88      0.83      1134\n",
      "      Abusive       0.86      0.88      0.87       992\n",
      "HS_Individual       0.70      0.66      0.68       732\n",
      "     HS_Group       0.57      0.72      0.64       402\n",
      "  HS_Religion       0.64      0.57      0.61       157\n",
      "      HS_Race       0.68      0.65      0.67       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       1.00      0.08      0.15        51\n",
      "     HS_Other       0.71      0.81      0.75       762\n",
      "      HS_Weak       0.67      0.62      0.65       689\n",
      "  HS_Moderate       0.49      0.64      0.56       331\n",
      "    HS_Strong       0.88      0.68      0.77       114\n",
      "\n",
      "    micro avg       0.72      0.75      0.73      5556\n",
      "    macro avg       0.67      0.60      0.60      5556\n",
      " weighted avg       0.72      0.75      0.73      5556\n",
      "  samples avg       0.42      0.42      0.41      5556\n",
      "\n",
      "Training completed in 123.62266969680786 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4196, Accuracy: 0.8697, F1 Micro: 0.4584, F1 Macro: 0.1998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3036, Accuracy: 0.8805, F1 Micro: 0.5234, F1 Macro: 0.2774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2438, Accuracy: 0.9004, F1 Micro: 0.6686, F1 Macro: 0.4391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2063, Accuracy: 0.9062, F1 Micro: 0.7099, F1 Macro: 0.5076\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9069, F1 Micro: 0.7074, F1 Macro: 0.5387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.91, F1 Micro: 0.7258, F1 Macro: 0.5549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9111, F1 Micro: 0.7272, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9042, F1 Micro: 0.7336, F1 Macro: 0.5951\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9096, F1 Micro: 0.7285, F1 Macro: 0.6063\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.912, F1 Micro: 0.7244, F1 Macro: 0.5897\n",
      "Model 3 - Iteration 3335: Accuracy: 0.9042, F1 Micro: 0.7336, F1 Macro: 0.5951\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.76      0.90      0.83      1134\n",
      "      Abusive       0.85      0.88      0.86       992\n",
      "HS_Individual       0.63      0.74      0.68       732\n",
      "     HS_Group       0.62      0.66      0.64       402\n",
      "  HS_Religion       0.63      0.62      0.62       157\n",
      "      HS_Race       0.67      0.60      0.63       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.75      0.06      0.11        51\n",
      "     HS_Other       0.70      0.81      0.75       762\n",
      "      HS_Weak       0.62      0.73      0.67       689\n",
      "  HS_Moderate       0.53      0.57      0.55       331\n",
      "    HS_Strong       0.85      0.65      0.74       114\n",
      "\n",
      "    micro avg       0.70      0.77      0.73      5556\n",
      "    macro avg       0.68      0.60      0.60      5556\n",
      " weighted avg       0.70      0.77      0.73      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 124.89079022407532 s\n",
      "Averaged - Iteration 3335: Accuracy: 0.897, F1 Micro: 0.6887, F1 Macro: 0.5023\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 720\n",
      "Sampling duration: 91.4861855506897 seconds\n",
      "New train size: 4055\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4071, Accuracy: 0.877, F1 Micro: 0.537, F1 Macro: 0.2465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2924, Accuracy: 0.8953, F1 Micro: 0.6513, F1 Macro: 0.4551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.233, Accuracy: 0.9025, F1 Micro: 0.7049, F1 Macro: 0.4947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1904, Accuracy: 0.9088, F1 Micro: 0.7117, F1 Macro: 0.5141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1584, Accuracy: 0.9097, F1 Micro: 0.7373, F1 Macro: 0.5718\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9082, F1 Micro: 0.7305, F1 Macro: 0.5824\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9089, F1 Micro: 0.7319, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0833, Accuracy: 0.911, F1 Micro: 0.7408, F1 Macro: 0.6098\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9117, F1 Micro: 0.7336, F1 Macro: 0.602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9132, F1 Micro: 0.741, F1 Macro: 0.6064\n",
      "Model 1 - Iteration 4055: Accuracy: 0.9132, F1 Micro: 0.741, F1 Macro: 0.6064\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1134\n",
      "      Abusive       0.88      0.87      0.88       992\n",
      "HS_Individual       0.70      0.68      0.69       732\n",
      "     HS_Group       0.69      0.61      0.64       402\n",
      "  HS_Religion       0.73      0.47      0.57       157\n",
      "      HS_Race       0.85      0.59      0.70       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.86      0.12      0.21        51\n",
      "     HS_Other       0.72      0.79      0.75       762\n",
      "      HS_Weak       0.68      0.66      0.67       689\n",
      "  HS_Moderate       0.59      0.50      0.54       331\n",
      "    HS_Strong       0.87      0.73      0.79       114\n",
      "\n",
      "    micro avg       0.76      0.72      0.74      5556\n",
      "    macro avg       0.70      0.57      0.61      5556\n",
      " weighted avg       0.75      0.72      0.73      5556\n",
      "  samples avg       0.43      0.41      0.40      5556\n",
      "\n",
      "Training completed in 142.70732402801514 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4063, Accuracy: 0.8794, F1 Micro: 0.584, F1 Macro: 0.2718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2986, Accuracy: 0.8953, F1 Micro: 0.6785, F1 Macro: 0.4422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2366, Accuracy: 0.904, F1 Micro: 0.6932, F1 Macro: 0.5044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1918, Accuracy: 0.9101, F1 Micro: 0.7163, F1 Macro: 0.5358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.162, Accuracy: 0.9109, F1 Micro: 0.7454, F1 Macro: 0.5835\n",
      "Epoch 6/10, Train Loss: 0.1226, Accuracy: 0.9103, F1 Micro: 0.732, F1 Macro: 0.5688\n",
      "Epoch 7/10, Train Loss: 0.0977, Accuracy: 0.9109, F1 Micro: 0.7373, F1 Macro: 0.5894\n",
      "Epoch 8/10, Train Loss: 0.0827, Accuracy: 0.9132, F1 Micro: 0.7441, F1 Macro: 0.6016\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9125, F1 Micro: 0.748, F1 Macro: 0.615\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9151, F1 Micro: 0.7464, F1 Macro: 0.6177\n",
      "Model 2 - Iteration 4055: Accuracy: 0.9125, F1 Micro: 0.748, F1 Macro: 0.615\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.87      0.84      1134\n",
      "      Abusive       0.88      0.88      0.88       992\n",
      "HS_Individual       0.67      0.76      0.71       732\n",
      "     HS_Group       0.68      0.59      0.63       402\n",
      "  HS_Religion       0.74      0.57      0.65       157\n",
      "      HS_Race       0.71      0.67      0.69       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.44      0.14      0.21        51\n",
      "     HS_Other       0.72      0.81      0.76       762\n",
      "      HS_Weak       0.65      0.73      0.69       689\n",
      "  HS_Moderate       0.56      0.51      0.53       331\n",
      "    HS_Strong       0.89      0.63      0.74       114\n",
      "\n",
      "    micro avg       0.74      0.75      0.75      5556\n",
      "    macro avg       0.69      0.60      0.62      5556\n",
      " weighted avg       0.74      0.75      0.74      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 141.1144995689392 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4026, Accuracy: 0.8774, F1 Micro: 0.5293, F1 Macro: 0.2407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2923, Accuracy: 0.8959, F1 Micro: 0.6721, F1 Macro: 0.4507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9016, F1 Micro: 0.6856, F1 Macro: 0.4922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1923, Accuracy: 0.9076, F1 Micro: 0.6925, F1 Macro: 0.5039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9111, F1 Micro: 0.7425, F1 Macro: 0.5704\n",
      "Epoch 6/10, Train Loss: 0.1215, Accuracy: 0.9117, F1 Micro: 0.7379, F1 Macro: 0.577\n",
      "Epoch 7/10, Train Loss: 0.1013, Accuracy: 0.9146, F1 Micro: 0.7423, F1 Macro: 0.5942\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9142, F1 Micro: 0.7423, F1 Macro: 0.6011\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9125, F1 Micro: 0.7415, F1 Macro: 0.6145\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9067, F1 Micro: 0.7353, F1 Macro: 0.6149\n",
      "Model 3 - Iteration 4055: Accuracy: 0.9111, F1 Micro: 0.7425, F1 Macro: 0.5704\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.84      1134\n",
      "      Abusive       0.85      0.90      0.87       992\n",
      "HS_Individual       0.67      0.77      0.72       732\n",
      "     HS_Group       0.72      0.59      0.65       402\n",
      "  HS_Religion       0.72      0.45      0.55       157\n",
      "      HS_Race       0.84      0.48      0.61       120\n",
      "  HS_Physical       0.00      0.00      0.00        72\n",
      "    HS_Gender       0.00      0.00      0.00        51\n",
      "     HS_Other       0.69      0.80      0.74       762\n",
      "      HS_Weak       0.65      0.75      0.70       689\n",
      "  HS_Moderate       0.60      0.46      0.52       331\n",
      "    HS_Strong       0.92      0.50      0.65       114\n",
      "\n",
      "    micro avg       0.74      0.74      0.74      5556\n",
      "    macro avg       0.62      0.55      0.57      5556\n",
      " weighted avg       0.73      0.74      0.73      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 139.2394983768463 s\n",
      "Averaged - Iteration 4055: Accuracy: 0.9, F1 Micro: 0.6997, F1 Macro: 0.5213\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 648\n",
      "Sampling duration: 82.55195784568787 seconds\n",
      "New train size: 4703\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3902, Accuracy: 0.8816, F1 Micro: 0.6158, F1 Macro: 0.3205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2768, Accuracy: 0.8967, F1 Micro: 0.6969, F1 Macro: 0.4647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2253, Accuracy: 0.9068, F1 Micro: 0.7213, F1 Macro: 0.5541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1794, Accuracy: 0.9125, F1 Micro: 0.7361, F1 Macro: 0.5784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.9119, F1 Micro: 0.7464, F1 Macro: 0.6027\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9139, F1 Micro: 0.7408, F1 Macro: 0.6113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9148, F1 Micro: 0.7492, F1 Macro: 0.6088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9143, F1 Micro: 0.7506, F1 Macro: 0.6201\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.915, F1 Micro: 0.74, F1 Macro: 0.6296\n",
      "Epoch 10/10, Train Loss: 0.0607, Accuracy: 0.9157, F1 Micro: 0.7391, F1 Macro: 0.6385\n",
      "Model 1 - Iteration 4703: Accuracy: 0.9143, F1 Micro: 0.7506, F1 Macro: 0.6201\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.88      0.86      0.87       992\n",
      "HS_Individual       0.70      0.73      0.71       732\n",
      "     HS_Group       0.67      0.64      0.66       402\n",
      "  HS_Religion       0.72      0.53      0.61       157\n",
      "      HS_Race       0.75      0.62      0.68       120\n",
      "  HS_Physical       0.67      0.06      0.10        72\n",
      "    HS_Gender       0.60      0.12      0.20        51\n",
      "     HS_Other       0.72      0.82      0.76       762\n",
      "      HS_Weak       0.68      0.71      0.70       689\n",
      "  HS_Moderate       0.57      0.57      0.57       331\n",
      "    HS_Strong       0.89      0.63      0.74       114\n",
      "\n",
      "    micro avg       0.75      0.75      0.75      5556\n",
      "    macro avg       0.72      0.60      0.62      5556\n",
      " weighted avg       0.75      0.75      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 158.81455159187317 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.393, Accuracy: 0.8801, F1 Micro: 0.6109, F1 Macro: 0.3025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2807, Accuracy: 0.8937, F1 Micro: 0.6918, F1 Macro: 0.4668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2286, Accuracy: 0.9067, F1 Micro: 0.7097, F1 Macro: 0.5059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1859, Accuracy: 0.9078, F1 Micro: 0.7356, F1 Macro: 0.5798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.9129, F1 Micro: 0.7446, F1 Macro: 0.5888\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9155, F1 Micro: 0.7432, F1 Macro: 0.603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0969, Accuracy: 0.9148, F1 Micro: 0.7463, F1 Macro: 0.5998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.9147, F1 Micro: 0.7502, F1 Macro: 0.6124\n",
      "Epoch 9/10, Train Loss: 0.0686, Accuracy: 0.9137, F1 Micro: 0.7349, F1 Macro: 0.5973\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.9153, F1 Micro: 0.7441, F1 Macro: 0.6376\n",
      "Model 2 - Iteration 4703: Accuracy: 0.9147, F1 Micro: 0.7502, F1 Macro: 0.6124\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.85      0.84      1134\n",
      "      Abusive       0.90      0.85      0.87       992\n",
      "HS_Individual       0.72      0.71      0.71       732\n",
      "     HS_Group       0.66      0.67      0.67       402\n",
      "  HS_Religion       0.72      0.58      0.64       157\n",
      "      HS_Race       0.74      0.62      0.68       120\n",
      "  HS_Physical       0.60      0.04      0.08        72\n",
      "    HS_Gender       0.50      0.06      0.11        51\n",
      "     HS_Other       0.72      0.81      0.76       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.54      0.59      0.56       331\n",
      "    HS_Strong       0.90      0.61      0.73       114\n",
      "\n",
      "    micro avg       0.76      0.74      0.75      5556\n",
      "    macro avg       0.71      0.59      0.61      5556\n",
      " weighted avg       0.76      0.74      0.74      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 158.58403158187866 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3896, Accuracy: 0.8822, F1 Micro: 0.6125, F1 Macro: 0.3133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2787, Accuracy: 0.8908, F1 Micro: 0.6926, F1 Macro: 0.4727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2249, Accuracy: 0.9076, F1 Micro: 0.7208, F1 Macro: 0.5426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1818, Accuracy: 0.9076, F1 Micro: 0.7287, F1 Macro: 0.5619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9091, F1 Micro: 0.7432, F1 Macro: 0.5889\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9125, F1 Micro: 0.7394, F1 Macro: 0.6086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0975, Accuracy: 0.9126, F1 Micro: 0.746, F1 Macro: 0.6107\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9161, F1 Micro: 0.7434, F1 Macro: 0.6113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.069, Accuracy: 0.9148, F1 Micro: 0.7462, F1 Macro: 0.6232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9169, F1 Micro: 0.7489, F1 Macro: 0.6457\n",
      "Model 3 - Iteration 4703: Accuracy: 0.9169, F1 Micro: 0.7489, F1 Macro: 0.6457\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.81      0.83      1134\n",
      "      Abusive       0.89      0.86      0.87       992\n",
      "HS_Individual       0.72      0.69      0.71       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.72      0.52      0.61       157\n",
      "      HS_Race       0.82      0.56      0.66       120\n",
      "  HS_Physical       0.75      0.17      0.27        72\n",
      "    HS_Gender       0.85      0.22      0.34        51\n",
      "     HS_Other       0.76      0.75      0.75       762\n",
      "      HS_Weak       0.70      0.69      0.69       689\n",
      "  HS_Moderate       0.63      0.54      0.58       331\n",
      "    HS_Strong       0.92      0.67      0.77       114\n",
      "\n",
      "    micro avg       0.78      0.72      0.75      5556\n",
      "    macro avg       0.78      0.59      0.65      5556\n",
      " weighted avg       0.78      0.72      0.74      5556\n",
      "  samples avg       0.43      0.40      0.40      5556\n",
      "\n",
      "Training completed in 160.26869249343872 s\n",
      "Averaged - Iteration 4703: Accuracy: 0.9026, F1 Micro: 0.7081, F1 Macro: 0.5387\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 584\n",
      "Sampling duration: 74.22923731803894 seconds\n",
      "New train size: 5287\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.38, Accuracy: 0.8836, F1 Micro: 0.6268, F1 Macro: 0.3306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2619, Accuracy: 0.8932, F1 Micro: 0.7081, F1 Macro: 0.517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2178, Accuracy: 0.9104, F1 Micro: 0.729, F1 Macro: 0.5524\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9108, F1 Micro: 0.7408, F1 Macro: 0.58\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9085, F1 Micro: 0.7483, F1 Macro: 0.6132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9156, F1 Micro: 0.7552, F1 Macro: 0.6199\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9173, F1 Micro: 0.7504, F1 Macro: 0.6409\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9156, F1 Micro: 0.7374, F1 Macro: 0.6275\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9164, F1 Micro: 0.7465, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9156, F1 Micro: 0.7608, F1 Macro: 0.678\n",
      "Model 1 - Iteration 5287: Accuracy: 0.9156, F1 Micro: 0.7608, F1 Macro: 0.678\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.84      1134\n",
      "      Abusive       0.85      0.92      0.88       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.64      0.71      0.67       402\n",
      "  HS_Religion       0.68      0.61      0.64       157\n",
      "      HS_Race       0.74      0.68      0.71       120\n",
      "  HS_Physical       0.74      0.19      0.31        72\n",
      "    HS_Gender       0.60      0.41      0.49        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.69      0.68      0.69       689\n",
      "  HS_Moderate       0.56      0.66      0.60       331\n",
      "    HS_Strong       0.87      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5556\n",
      "    macro avg       0.72      0.67      0.68      5556\n",
      " weighted avg       0.74      0.78      0.76      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 173.31209135055542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3822, Accuracy: 0.8842, F1 Micro: 0.6027, F1 Macro: 0.3\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.267, Accuracy: 0.8909, F1 Micro: 0.7011, F1 Macro: 0.4873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2218, Accuracy: 0.9111, F1 Micro: 0.7306, F1 Macro: 0.5486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9112, F1 Micro: 0.7394, F1 Macro: 0.5677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1455, Accuracy: 0.9103, F1 Micro: 0.7483, F1 Macro: 0.584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1112, Accuracy: 0.9181, F1 Micro: 0.7566, F1 Macro: 0.6183\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9178, F1 Micro: 0.7537, F1 Macro: 0.6177\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.9173, F1 Micro: 0.7553, F1 Macro: 0.6477\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9178, F1 Micro: 0.7518, F1 Macro: 0.6241\n",
      "Epoch 10/10, Train Loss: 0.0534, Accuracy: 0.9146, F1 Micro: 0.7515, F1 Macro: 0.6602\n",
      "Model 2 - Iteration 5287: Accuracy: 0.9181, F1 Micro: 0.7566, F1 Macro: 0.6183\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.85      0.84      1134\n",
      "      Abusive       0.87      0.87      0.87       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.75      0.60      0.67       402\n",
      "  HS_Religion       0.73      0.57      0.64       157\n",
      "      HS_Race       0.84      0.61      0.71       120\n",
      "  HS_Physical       0.75      0.04      0.08        72\n",
      "    HS_Gender       1.00      0.06      0.11        51\n",
      "     HS_Other       0.75      0.77      0.76       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.64      0.51      0.57       331\n",
      "    HS_Strong       0.90      0.63      0.74       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5556\n",
      "    macro avg       0.79      0.58      0.62      5556\n",
      " weighted avg       0.78      0.74      0.75      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 171.52275776863098 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3793, Accuracy: 0.8833, F1 Micro: 0.5831, F1 Macro: 0.2902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2619, Accuracy: 0.8975, F1 Micro: 0.7073, F1 Macro: 0.4899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2174, Accuracy: 0.9108, F1 Micro: 0.7255, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1737, Accuracy: 0.9115, F1 Micro: 0.7414, F1 Macro: 0.5648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1459, Accuracy: 0.9071, F1 Micro: 0.7464, F1 Macro: 0.619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9122, F1 Micro: 0.7476, F1 Macro: 0.6135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.9173, F1 Micro: 0.7554, F1 Macro: 0.6298\n",
      "Epoch 8/10, Train Loss: 0.0744, Accuracy: 0.9176, F1 Micro: 0.7506, F1 Macro: 0.6302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9183, F1 Micro: 0.7556, F1 Macro: 0.6537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9182, F1 Micro: 0.7576, F1 Macro: 0.6664\n",
      "Model 3 - Iteration 5287: Accuracy: 0.9182, F1 Micro: 0.7576, F1 Macro: 0.6664\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.92      0.86      0.89       992\n",
      "HS_Individual       0.73      0.68      0.70       732\n",
      "     HS_Group       0.66      0.68      0.67       402\n",
      "  HS_Religion       0.70      0.59      0.64       157\n",
      "      HS_Race       0.74      0.62      0.68       120\n",
      "  HS_Physical       0.68      0.21      0.32        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.76      0.77      0.76       762\n",
      "      HS_Weak       0.71      0.66      0.68       689\n",
      "  HS_Moderate       0.58      0.61      0.59       331\n",
      "    HS_Strong       0.86      0.79      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.74      0.76      5556\n",
      "    macro avg       0.73      0.63      0.67      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.42      0.41      0.40      5556\n",
      "\n",
      "Training completed in 176.65186548233032 s\n",
      "Averaged - Iteration 5287: Accuracy: 0.9047, F1 Micro: 0.7152, F1 Macro: 0.5552\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 525\n",
      "Sampling duration: 67.23436760902405 seconds\n",
      "New train size: 5812\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3797, Accuracy: 0.885, F1 Micro: 0.6237, F1 Macro: 0.317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2619, Accuracy: 0.9045, F1 Micro: 0.6916, F1 Macro: 0.5131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2123, Accuracy: 0.9136, F1 Micro: 0.7203, F1 Macro: 0.5486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.9151, F1 Micro: 0.7447, F1 Macro: 0.5979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1382, Accuracy: 0.9099, F1 Micro: 0.752, F1 Macro: 0.6087\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9155, F1 Micro: 0.7489, F1 Macro: 0.6008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0903, Accuracy: 0.9167, F1 Micro: 0.7574, F1 Macro: 0.6388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.919, F1 Micro: 0.7598, F1 Macro: 0.6627\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.918, F1 Micro: 0.7581, F1 Macro: 0.6661\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9184, F1 Micro: 0.7583, F1 Macro: 0.674\n",
      "Model 1 - Iteration 5812: Accuracy: 0.919, F1 Micro: 0.7598, F1 Macro: 0.6627\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.84      1134\n",
      "      Abusive       0.88      0.89      0.89       992\n",
      "HS_Individual       0.71      0.71      0.71       732\n",
      "     HS_Group       0.72      0.61      0.66       402\n",
      "  HS_Religion       0.78      0.49      0.60       157\n",
      "      HS_Race       0.87      0.63      0.73       120\n",
      "  HS_Physical       0.60      0.17      0.26        72\n",
      "    HS_Gender       0.76      0.25      0.38        51\n",
      "     HS_Other       0.74      0.80      0.77       762\n",
      "      HS_Weak       0.69      0.69      0.69       689\n",
      "  HS_Moderate       0.62      0.54      0.58       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.76      0.62      0.66      5556\n",
      " weighted avg       0.77      0.74      0.75      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 186.21517968177795 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3804, Accuracy: 0.8861, F1 Micro: 0.603, F1 Macro: 0.2899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.265, Accuracy: 0.9052, F1 Micro: 0.7074, F1 Macro: 0.5321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2151, Accuracy: 0.913, F1 Micro: 0.7221, F1 Macro: 0.5466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9156, F1 Micro: 0.7486, F1 Macro: 0.5939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.912, F1 Micro: 0.7498, F1 Macro: 0.5985\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9143, F1 Micro: 0.7585, F1 Macro: 0.618\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9183, F1 Micro: 0.7532, F1 Macro: 0.6197\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9181, F1 Micro: 0.7584, F1 Macro: 0.629\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9156, F1 Micro: 0.7584, F1 Macro: 0.6613\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9165, F1 Micro: 0.7505, F1 Macro: 0.656\n",
      "Model 2 - Iteration 5812: Accuracy: 0.9143, F1 Micro: 0.7585, F1 Macro: 0.618\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.80      0.89      0.84      1134\n",
      "      Abusive       0.87      0.90      0.89       992\n",
      "HS_Individual       0.66      0.80      0.73       732\n",
      "     HS_Group       0.72      0.60      0.66       402\n",
      "  HS_Religion       0.79      0.45      0.57       157\n",
      "      HS_Race       0.80      0.65      0.72       120\n",
      "  HS_Physical       0.50      0.03      0.05        72\n",
      "    HS_Gender       0.67      0.08      0.14        51\n",
      "     HS_Other       0.68      0.86      0.76       762\n",
      "      HS_Weak       0.63      0.79      0.70       689\n",
      "  HS_Moderate       0.64      0.51      0.57       331\n",
      "    HS_Strong       0.88      0.72      0.79       114\n",
      "\n",
      "    micro avg       0.74      0.78      0.76      5556\n",
      "    macro avg       0.72      0.61      0.62      5556\n",
      " weighted avg       0.74      0.78      0.75      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 184.6648178100586 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3782, Accuracy: 0.8863, F1 Micro: 0.6184, F1 Macro: 0.3182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2648, Accuracy: 0.9047, F1 Micro: 0.7064, F1 Macro: 0.5324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2133, Accuracy: 0.9131, F1 Micro: 0.7248, F1 Macro: 0.5349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9148, F1 Micro: 0.7463, F1 Macro: 0.5876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9107, F1 Micro: 0.7509, F1 Macro: 0.6023\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9159, F1 Micro: 0.7603, F1 Macro: 0.6139\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9128, F1 Micro: 0.7491, F1 Macro: 0.6366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.918, F1 Micro: 0.763, F1 Macro: 0.6643\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.9208, F1 Micro: 0.7624, F1 Macro: 0.6749\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9129, F1 Micro: 0.7544, F1 Macro: 0.6642\n",
      "Model 3 - Iteration 5812: Accuracy: 0.918, F1 Micro: 0.763, F1 Macro: 0.6643\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.70      0.72      0.71       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.73      0.46      0.57       157\n",
      "      HS_Race       0.83      0.61      0.70       120\n",
      "  HS_Physical       0.59      0.18      0.28        72\n",
      "    HS_Gender       0.65      0.29      0.41        51\n",
      "     HS_Other       0.73      0.82      0.77       762\n",
      "      HS_Weak       0.68      0.69      0.69       689\n",
      "  HS_Moderate       0.60      0.62      0.61       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.76      0.77      0.76      5556\n",
      "    macro avg       0.73      0.64      0.66      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 186.3616189956665 s\n",
      "Averaged - Iteration 5812: Accuracy: 0.9062, F1 Micro: 0.7209, F1 Macro: 0.5669\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 473\n",
      "Sampling duration: 60.61956024169922 seconds\n",
      "New train size: 6285\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.375, Accuracy: 0.8871, F1 Micro: 0.6577, F1 Macro: 0.3596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.9083, F1 Micro: 0.7139, F1 Macro: 0.541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.91, F1 Micro: 0.734, F1 Macro: 0.5791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1665, Accuracy: 0.9165, F1 Micro: 0.757, F1 Macro: 0.6129\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9134, F1 Micro: 0.7524, F1 Macro: 0.6142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1046, Accuracy: 0.916, F1 Micro: 0.7573, F1 Macro: 0.6406\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.9186, F1 Micro: 0.7486, F1 Macro: 0.6584\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9189, F1 Micro: 0.7561, F1 Macro: 0.6661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9194, F1 Micro: 0.7608, F1 Macro: 0.6697\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9156, F1 Micro: 0.7589, F1 Macro: 0.6714\n",
      "Model 1 - Iteration 6285: Accuracy: 0.9194, F1 Micro: 0.7608, F1 Macro: 0.6697\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.83      0.84      1134\n",
      "      Abusive       0.88      0.90      0.89       992\n",
      "HS_Individual       0.72      0.68      0.70       732\n",
      "     HS_Group       0.69      0.66      0.67       402\n",
      "  HS_Religion       0.76      0.60      0.67       157\n",
      "      HS_Race       0.80      0.64      0.71       120\n",
      "  HS_Physical       0.86      0.17      0.28        72\n",
      "    HS_Gender       0.60      0.29      0.39        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.71      0.66      0.69       689\n",
      "  HS_Moderate       0.61      0.58      0.60       331\n",
      "    HS_Strong       0.90      0.75      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.74      0.76      5556\n",
      "    macro avg       0.76      0.63      0.67      5556\n",
      " weighted avg       0.78      0.74      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 197.99880695343018 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3766, Accuracy: 0.8882, F1 Micro: 0.6572, F1 Macro: 0.3613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2545, Accuracy: 0.9073, F1 Micro: 0.7081, F1 Macro: 0.5363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2098, Accuracy: 0.9092, F1 Micro: 0.7378, F1 Macro: 0.5865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9151, F1 Micro: 0.756, F1 Macro: 0.605\n",
      "Epoch 5/10, Train Loss: 0.136, Accuracy: 0.9129, F1 Micro: 0.7543, F1 Macro: 0.607\n",
      "Epoch 6/10, Train Loss: 0.1065, Accuracy: 0.9165, F1 Micro: 0.7545, F1 Macro: 0.6367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0828, Accuracy: 0.9175, F1 Micro: 0.7599, F1 Macro: 0.6709\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9174, F1 Micro: 0.7494, F1 Macro: 0.6407\n",
      "Epoch 9/10, Train Loss: 0.0571, Accuracy: 0.9204, F1 Micro: 0.754, F1 Macro: 0.6624\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9156, F1 Micro: 0.7575, F1 Macro: 0.6803\n",
      "Model 2 - Iteration 6285: Accuracy: 0.9175, F1 Micro: 0.7599, F1 Macro: 0.6709\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.70      0.72      0.71       732\n",
      "     HS_Group       0.70      0.63      0.66       402\n",
      "  HS_Religion       0.72      0.67      0.69       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.52      0.18      0.27        72\n",
      "    HS_Gender       0.46      0.35      0.40        51\n",
      "     HS_Other       0.78      0.77      0.77       762\n",
      "      HS_Weak       0.68      0.70      0.69       689\n",
      "  HS_Moderate       0.60      0.54      0.57       331\n",
      "    HS_Strong       0.89      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.76      0.76      5556\n",
      "    macro avg       0.71      0.65      0.67      5556\n",
      " weighted avg       0.76      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 197.93867230415344 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3726, Accuracy: 0.8892, F1 Micro: 0.6649, F1 Macro: 0.3816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2534, Accuracy: 0.9087, F1 Micro: 0.7153, F1 Macro: 0.5383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2106, Accuracy: 0.9126, F1 Micro: 0.7326, F1 Macro: 0.572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1664, Accuracy: 0.9159, F1 Micro: 0.7537, F1 Macro: 0.5883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.9174, F1 Micro: 0.7568, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1063, Accuracy: 0.9183, F1 Micro: 0.7582, F1 Macro: 0.634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.9196, F1 Micro: 0.7636, F1 Macro: 0.6672\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.9191, F1 Micro: 0.7623, F1 Macro: 0.6663\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9209, F1 Micro: 0.7614, F1 Macro: 0.6716\n",
      "Epoch 10/10, Train Loss: 0.0546, Accuracy: 0.9195, F1 Micro: 0.7589, F1 Macro: 0.6753\n",
      "Model 3 - Iteration 6285: Accuracy: 0.9196, F1 Micro: 0.7636, F1 Macro: 0.6672\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.85      0.91      0.88       992\n",
      "HS_Individual       0.73      0.69      0.71       732\n",
      "     HS_Group       0.69      0.67      0.68       402\n",
      "  HS_Religion       0.70      0.66      0.68       157\n",
      "      HS_Race       0.75      0.72      0.74       120\n",
      "  HS_Physical       0.65      0.18      0.28        72\n",
      "    HS_Gender       0.46      0.24      0.31        51\n",
      "     HS_Other       0.79      0.76      0.78       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.59      0.58      0.58       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.72      0.64      0.67      5556\n",
      " weighted avg       0.77      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 198.69630527496338 s\n",
      "Averaged - Iteration 6285: Accuracy: 0.9076, F1 Micro: 0.7254, F1 Macro: 0.5782\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 6584\n",
      "Acquired samples: 299\n",
      "Sampling duration: 55.39461088180542 seconds\n",
      "New train size: 6584\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3668, Accuracy: 0.8911, F1 Micro: 0.6317, F1 Macro: 0.3348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2502, Accuracy: 0.9057, F1 Micro: 0.6996, F1 Macro: 0.5178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9135, F1 Micro: 0.7389, F1 Macro: 0.5814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9164, F1 Micro: 0.7409, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9162, F1 Micro: 0.7571, F1 Macro: 0.625\n",
      "Epoch 6/10, Train Loss: 0.1123, Accuracy: 0.9172, F1 Micro: 0.7475, F1 Macro: 0.6161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9198, F1 Micro: 0.7677, F1 Macro: 0.673\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9199, F1 Micro: 0.765, F1 Macro: 0.6661\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.92, F1 Micro: 0.7616, F1 Macro: 0.6776\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9169, F1 Micro: 0.7567, F1 Macro: 0.6817\n",
      "Model 1 - Iteration 6584: Accuracy: 0.9198, F1 Micro: 0.7677, F1 Macro: 0.673\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.85      0.93      0.89       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.72      0.64      0.68       157\n",
      "      HS_Race       0.83      0.63      0.72       120\n",
      "  HS_Physical       0.60      0.17      0.26        72\n",
      "    HS_Gender       0.57      0.31      0.41        51\n",
      "     HS_Other       0.76      0.79      0.77       762\n",
      "      HS_Weak       0.70      0.71      0.71       689\n",
      "  HS_Moderate       0.60      0.57      0.58       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.65      0.67      5556\n",
      " weighted avg       0.76      0.77      0.76      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 204.432861328125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.37, Accuracy: 0.8923, F1 Micro: 0.6473, F1 Macro: 0.3468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2548, Accuracy: 0.9051, F1 Micro: 0.6874, F1 Macro: 0.4862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9143, F1 Micro: 0.7363, F1 Macro: 0.5819\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9151, F1 Micro: 0.7483, F1 Macro: 0.5858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9144, F1 Micro: 0.7545, F1 Macro: 0.6286\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9168, F1 Micro: 0.7513, F1 Macro: 0.6204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.9182, F1 Micro: 0.7607, F1 Macro: 0.6675\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9172, F1 Micro: 0.7556, F1 Macro: 0.6674\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.916, F1 Micro: 0.7562, F1 Macro: 0.6554\n",
      "Epoch 10/10, Train Loss: 0.0485, Accuracy: 0.919, F1 Micro: 0.7597, F1 Macro: 0.6884\n",
      "Model 2 - Iteration 6584: Accuracy: 0.9182, F1 Micro: 0.7607, F1 Macro: 0.6675\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.86      0.84      1134\n",
      "      Abusive       0.86      0.91      0.88       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.74      0.55      0.63       157\n",
      "      HS_Race       0.88      0.62      0.73       120\n",
      "  HS_Physical       0.78      0.19      0.31        72\n",
      "    HS_Gender       0.57      0.25      0.35        51\n",
      "     HS_Other       0.74      0.78      0.76       762\n",
      "      HS_Weak       0.69      0.70      0.69       689\n",
      "  HS_Moderate       0.61      0.57      0.59       331\n",
      "    HS_Strong       0.90      0.77      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.75      0.76      5556\n",
      "    macro avg       0.75      0.63      0.67      5556\n",
      " weighted avg       0.76      0.75      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 204.41936898231506 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3677, Accuracy: 0.8903, F1 Micro: 0.6194, F1 Macro: 0.3402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2513, Accuracy: 0.9072, F1 Micro: 0.708, F1 Macro: 0.5167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9132, F1 Micro: 0.7315, F1 Macro: 0.5653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1646, Accuracy: 0.916, F1 Micro: 0.7504, F1 Macro: 0.5862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.9124, F1 Micro: 0.7532, F1 Macro: 0.6292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1102, Accuracy: 0.9197, F1 Micro: 0.7607, F1 Macro: 0.6332\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9172, F1 Micro: 0.7584, F1 Macro: 0.6736\n",
      "Epoch 8/10, Train Loss: 0.0743, Accuracy: 0.921, F1 Micro: 0.7604, F1 Macro: 0.6666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9199, F1 Micro: 0.7659, F1 Macro: 0.672\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9198, F1 Micro: 0.763, F1 Macro: 0.6893\n",
      "Model 3 - Iteration 6584: Accuracy: 0.9199, F1 Micro: 0.7659, F1 Macro: 0.672\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.84      1134\n",
      "      Abusive       0.89      0.89      0.89       992\n",
      "HS_Individual       0.72      0.73      0.72       732\n",
      "     HS_Group       0.69      0.63      0.66       402\n",
      "  HS_Religion       0.77      0.62      0.69       157\n",
      "      HS_Race       0.84      0.61      0.71       120\n",
      "  HS_Physical       0.73      0.22      0.34        72\n",
      "    HS_Gender       0.67      0.24      0.35        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.70      0.71      0.70       689\n",
      "  HS_Moderate       0.61      0.58      0.59       331\n",
      "    HS_Strong       0.90      0.71      0.79       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.76      0.63      0.67      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.43      0.43      0.41      5556\n",
      "\n",
      "Training completed in 205.59235978126526 s\n",
      "Averaged - Iteration 6584: Accuracy: 0.9088, F1 Micro: 0.7293, F1 Macro: 0.5875\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 396\n",
      "Sampling duration: 51.06953692436218 seconds\n",
      "New train size: 6980\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3628, Accuracy: 0.8915, F1 Micro: 0.6808, F1 Macro: 0.4505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2468, Accuracy: 0.9042, F1 Micro: 0.7175, F1 Macro: 0.5443\n",
      "Epoch 3/10, Train Loss: 0.1978, Accuracy: 0.9127, F1 Micro: 0.7044, F1 Macro: 0.554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1612, Accuracy: 0.9179, F1 Micro: 0.7607, F1 Macro: 0.6157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9182, F1 Micro: 0.7615, F1 Macro: 0.6539\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9187, F1 Micro: 0.7615, F1 Macro: 0.6466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.921, F1 Micro: 0.7629, F1 Macro: 0.6553\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.918, F1 Micro: 0.741, F1 Macro: 0.6404\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9203, F1 Micro: 0.7608, F1 Macro: 0.6776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9204, F1 Micro: 0.7675, F1 Macro: 0.6875\n",
      "Model 1 - Iteration 6980: Accuracy: 0.9204, F1 Micro: 0.7675, F1 Macro: 0.6875\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.85      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.73      0.70      0.72       732\n",
      "     HS_Group       0.68      0.67      0.68       402\n",
      "  HS_Religion       0.70      0.61      0.65       157\n",
      "      HS_Race       0.80      0.66      0.72       120\n",
      "  HS_Physical       1.00      0.22      0.36        72\n",
      "    HS_Gender       0.62      0.39      0.48        51\n",
      "     HS_Other       0.75      0.80      0.78       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.58      0.60      0.59       331\n",
      "    HS_Strong       0.90      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.77      0.76      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.77      0.76      0.76      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 214.74747467041016 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.365, Accuracy: 0.8941, F1 Micro: 0.6669, F1 Macro: 0.4171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2504, Accuracy: 0.9059, F1 Micro: 0.7148, F1 Macro: 0.5334\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9133, F1 Micro: 0.7082, F1 Macro: 0.5586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1616, Accuracy: 0.9171, F1 Micro: 0.7567, F1 Macro: 0.6114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1251, Accuracy: 0.9195, F1 Micro: 0.7603, F1 Macro: 0.6321\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.9159, F1 Micro: 0.7599, F1 Macro: 0.6461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9174, F1 Micro: 0.766, F1 Macro: 0.6568\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.92, F1 Micro: 0.763, F1 Macro: 0.6627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9228, F1 Micro: 0.7667, F1 Macro: 0.6797\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9197, F1 Micro: 0.7654, F1 Macro: 0.6857\n",
      "Model 2 - Iteration 6980: Accuracy: 0.9228, F1 Micro: 0.7667, F1 Macro: 0.6797\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.82      0.84      1134\n",
      "      Abusive       0.88      0.91      0.90       992\n",
      "HS_Individual       0.76      0.67      0.71       732\n",
      "     HS_Group       0.70      0.65      0.67       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.74      0.74      0.74       120\n",
      "  HS_Physical       1.00      0.12      0.22        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.79      0.75      0.77       762\n",
      "      HS_Weak       0.75      0.65      0.69       689\n",
      "  HS_Moderate       0.63      0.59      0.61       331\n",
      "    HS_Strong       0.90      0.76      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.74      0.77      5556\n",
      "    macro avg       0.79      0.64      0.68      5556\n",
      " weighted avg       0.80      0.74      0.76      5556\n",
      "  samples avg       0.44      0.42      0.41      5556\n",
      "\n",
      "Training completed in 215.40196061134338 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3616, Accuracy: 0.8932, F1 Micro: 0.6787, F1 Macro: 0.4304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2482, Accuracy: 0.903, F1 Micro: 0.7162, F1 Macro: 0.534\n",
      "Epoch 3/10, Train Loss: 0.1975, Accuracy: 0.9129, F1 Micro: 0.7066, F1 Macro: 0.5528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1629, Accuracy: 0.9163, F1 Micro: 0.7593, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9195, F1 Micro: 0.7652, F1 Macro: 0.6575\n",
      "Epoch 6/10, Train Loss: 0.1017, Accuracy: 0.9205, F1 Micro: 0.7574, F1 Macro: 0.6542\n",
      "Epoch 7/10, Train Loss: 0.0833, Accuracy: 0.9219, F1 Micro: 0.7563, F1 Macro: 0.6422\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9184, F1 Micro: 0.757, F1 Macro: 0.6642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9218, F1 Micro: 0.771, F1 Macro: 0.6881\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.9224, F1 Micro: 0.7706, F1 Macro: 0.6907\n",
      "Model 3 - Iteration 6980: Accuracy: 0.9218, F1 Micro: 0.771, F1 Macro: 0.6881\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.86      0.91      0.89       992\n",
      "HS_Individual       0.74      0.71      0.72       732\n",
      "     HS_Group       0.71      0.68      0.69       402\n",
      "  HS_Religion       0.69      0.60      0.64       157\n",
      "      HS_Race       0.73      0.76      0.75       120\n",
      "  HS_Physical       1.00      0.19      0.33        72\n",
      "    HS_Gender       0.70      0.37      0.49        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.69      0.70       689\n",
      "  HS_Moderate       0.61      0.59      0.60       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 214.62888264656067 s\n",
      "Averaged - Iteration 6980: Accuracy: 0.91, F1 Micro: 0.7329, F1 Macro: 0.5964\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 356\n",
      "Sampling duration: 46.83472967147827 seconds\n",
      "New train size: 7336\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3556, Accuracy: 0.8922, F1 Micro: 0.6621, F1 Macro: 0.4101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9057, F1 Micro: 0.7278, F1 Macro: 0.5699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.198, Accuracy: 0.9152, F1 Micro: 0.7533, F1 Macro: 0.6054\n",
      "Epoch 4/10, Train Loss: 0.1538, Accuracy: 0.9187, F1 Micro: 0.7461, F1 Macro: 0.6045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.9218, F1 Micro: 0.7694, F1 Macro: 0.6652\n",
      "Epoch 6/10, Train Loss: 0.0995, Accuracy: 0.9175, F1 Micro: 0.7575, F1 Macro: 0.647\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.92, F1 Micro: 0.7645, F1 Macro: 0.6626\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9165, F1 Micro: 0.7631, F1 Macro: 0.6777\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9191, F1 Micro: 0.7663, F1 Macro: 0.684\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9213, F1 Micro: 0.7637, F1 Macro: 0.6913\n",
      "Model 1 - Iteration 7336: Accuracy: 0.9218, F1 Micro: 0.7694, F1 Macro: 0.6652\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.89      0.88      0.88       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.78      0.58      0.67       157\n",
      "      HS_Race       0.77      0.73      0.75       120\n",
      "  HS_Physical       0.70      0.10      0.17        72\n",
      "    HS_Gender       0.64      0.27      0.38        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.71      0.72      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.60       331\n",
      "    HS_Strong       0.90      0.68      0.78       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.76      0.63      0.67      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 220.0472548007965 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3594, Accuracy: 0.8916, F1 Micro: 0.6554, F1 Macro: 0.3513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2504, Accuracy: 0.9084, F1 Micro: 0.7186, F1 Macro: 0.536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9173, F1 Micro: 0.7476, F1 Macro: 0.5808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1546, Accuracy: 0.9192, F1 Micro: 0.7561, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9226, F1 Micro: 0.7689, F1 Macro: 0.6651\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9196, F1 Micro: 0.7593, F1 Macro: 0.6385\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9202, F1 Micro: 0.7672, F1 Macro: 0.6723\n",
      "Epoch 8/10, Train Loss: 0.0672, Accuracy: 0.9192, F1 Micro: 0.7655, F1 Macro: 0.6665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0558, Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.6963\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9186, F1 Micro: 0.7623, F1 Macro: 0.6885\n",
      "Model 2 - Iteration 7336: Accuracy: 0.9218, F1 Micro: 0.7713, F1 Macro: 0.6963\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.84      0.84      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.71      0.72       732\n",
      "     HS_Group       0.71      0.70      0.71       402\n",
      "  HS_Religion       0.70      0.67      0.68       157\n",
      "      HS_Race       0.77      0.67      0.71       120\n",
      "  HS_Physical       0.87      0.28      0.42        72\n",
      "    HS_Gender       0.54      0.43      0.48        51\n",
      "     HS_Other       0.76      0.78      0.77       762\n",
      "      HS_Weak       0.70      0.69      0.70       689\n",
      "  HS_Moderate       0.63      0.64      0.63       331\n",
      "    HS_Strong       0.91      0.70      0.79       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.75      0.67      0.70      5556\n",
      " weighted avg       0.78      0.76      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 222.17806458473206 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3548, Accuracy: 0.8935, F1 Micro: 0.6674, F1 Macro: 0.3996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2483, Accuracy: 0.9057, F1 Micro: 0.722, F1 Macro: 0.5461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1991, Accuracy: 0.9178, F1 Micro: 0.7524, F1 Macro: 0.5801\n",
      "Epoch 4/10, Train Loss: 0.155, Accuracy: 0.9189, F1 Micro: 0.7489, F1 Macro: 0.6111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1273, Accuracy: 0.9172, F1 Micro: 0.7616, F1 Macro: 0.6546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1021, Accuracy: 0.9196, F1 Micro: 0.7644, F1 Macro: 0.6549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0829, Accuracy: 0.9209, F1 Micro: 0.7682, F1 Macro: 0.6679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.9204, F1 Micro: 0.7692, F1 Macro: 0.6829\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9174, F1 Micro: 0.7656, F1 Macro: 0.6874\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9195, F1 Micro: 0.7661, F1 Macro: 0.6961\n",
      "Model 3 - Iteration 7336: Accuracy: 0.9204, F1 Micro: 0.7692, F1 Macro: 0.6829\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.72      0.72       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.70      0.67      0.68       157\n",
      "      HS_Race       0.70      0.75      0.73       120\n",
      "  HS_Physical       0.71      0.21      0.32        72\n",
      "    HS_Gender       0.67      0.31      0.43        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.70      0.70      0.70       689\n",
      "  HS_Moderate       0.56      0.61      0.58       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.67      0.68      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 225.21108102798462 s\n",
      "Averaged - Iteration 7336: Accuracy: 0.9109, F1 Micro: 0.736, F1 Macro: 0.6035\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 320\n",
      "Sampling duration: 41.679415464401245 seconds\n",
      "New train size: 7656\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3564, Accuracy: 0.8919, F1 Micro: 0.6205, F1 Macro: 0.4081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2401, Accuracy: 0.9124, F1 Micro: 0.7232, F1 Macro: 0.5445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1992, Accuracy: 0.9169, F1 Micro: 0.7429, F1 Macro: 0.588\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1564, Accuracy: 0.9195, F1 Micro: 0.7595, F1 Macro: 0.6216\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1196, Accuracy: 0.9191, F1 Micro: 0.7666, F1 Macro: 0.6504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.9209, F1 Micro: 0.768, F1 Macro: 0.6557\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9208, F1 Micro: 0.7665, F1 Macro: 0.6891\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9202, F1 Micro: 0.7677, F1 Macro: 0.6884\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9182, F1 Micro: 0.758, F1 Macro: 0.6922\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9207, F1 Micro: 0.764, F1 Macro: 0.6944\n",
      "Model 1 - Iteration 7656: Accuracy: 0.9209, F1 Micro: 0.768, F1 Macro: 0.6557\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.88      0.89       992\n",
      "HS_Individual       0.76      0.67      0.71       732\n",
      "     HS_Group       0.64      0.75      0.69       402\n",
      "  HS_Religion       0.75      0.64      0.69       157\n",
      "      HS_Race       0.74      0.63      0.68       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.90      0.18      0.30        51\n",
      "     HS_Other       0.75      0.83      0.78       762\n",
      "      HS_Weak       0.75      0.64      0.69       689\n",
      "  HS_Moderate       0.56      0.69      0.62       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.76      0.77      5556\n",
      "    macro avg       0.78      0.63      0.66      5556\n",
      " weighted avg       0.78      0.76      0.76      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 232.9085338115692 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3589, Accuracy: 0.8925, F1 Micro: 0.6213, F1 Macro: 0.4153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2439, Accuracy: 0.9098, F1 Micro: 0.7114, F1 Macro: 0.5274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.9175, F1 Micro: 0.752, F1 Macro: 0.5887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1546, Accuracy: 0.9201, F1 Micro: 0.7611, F1 Macro: 0.6113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9121, F1 Micro: 0.7611, F1 Macro: 0.6462\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9213, F1 Micro: 0.76, F1 Macro: 0.6545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0789, Accuracy: 0.9203, F1 Micro: 0.7718, F1 Macro: 0.6863\n",
      "Epoch 8/10, Train Loss: 0.0646, Accuracy: 0.9191, F1 Micro: 0.7691, F1 Macro: 0.6905\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9217, F1 Micro: 0.7638, F1 Macro: 0.69\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9197, F1 Micro: 0.7657, F1 Macro: 0.6982\n",
      "Model 2 - Iteration 7656: Accuracy: 0.9203, F1 Micro: 0.7718, F1 Macro: 0.6863\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.87      0.85      1134\n",
      "      Abusive       0.87      0.92      0.89       992\n",
      "HS_Individual       0.71      0.74      0.72       732\n",
      "     HS_Group       0.70      0.70      0.70       402\n",
      "  HS_Religion       0.72      0.60      0.66       157\n",
      "      HS_Race       0.74      0.72      0.73       120\n",
      "  HS_Physical       0.62      0.21      0.31        72\n",
      "    HS_Gender       0.51      0.37      0.43        51\n",
      "     HS_Other       0.74      0.81      0.77       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.76      0.78      0.77      5556\n",
      "    macro avg       0.72      0.68      0.69      5556\n",
      " weighted avg       0.76      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 232.04434156417847 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3555, Accuracy: 0.8915, F1 Micro: 0.6205, F1 Macro: 0.4098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.9099, F1 Micro: 0.7055, F1 Macro: 0.5244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9182, F1 Micro: 0.7472, F1 Macro: 0.579\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9193, F1 Micro: 0.7456, F1 Macro: 0.6073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9172, F1 Micro: 0.766, F1 Macro: 0.6538\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9199, F1 Micro: 0.7616, F1 Macro: 0.6593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0819, Accuracy: 0.921, F1 Micro: 0.7668, F1 Macro: 0.6875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9203, F1 Micro: 0.7697, F1 Macro: 0.6859\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9235, F1 Micro: 0.7661, F1 Macro: 0.6919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9215, F1 Micro: 0.7741, F1 Macro: 0.7048\n",
      "Model 3 - Iteration 7656: Accuracy: 0.9215, F1 Micro: 0.7741, F1 Macro: 0.7048\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.87      0.91      0.89       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.71      0.66      0.69       157\n",
      "      HS_Race       0.75      0.68      0.72       120\n",
      "  HS_Physical       0.69      0.31      0.42        72\n",
      "    HS_Gender       0.64      0.49      0.56        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.62      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.77      5556\n",
      "    macro avg       0.74      0.69      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 233.4295527935028 s\n",
      "Averaged - Iteration 7656: Accuracy: 0.9117, F1 Micro: 0.7387, F1 Macro: 0.6095\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 7901\n",
      "Acquired samples: 245\n",
      "Sampling duration: 37.02462315559387 seconds\n",
      "New train size: 7901\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3588, Accuracy: 0.8952, F1 Micro: 0.6611, F1 Macro: 0.4007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2442, Accuracy: 0.9085, F1 Micro: 0.7338, F1 Macro: 0.5696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1975, Accuracy: 0.9186, F1 Micro: 0.7456, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.9187, F1 Micro: 0.7687, F1 Macro: 0.6327\n",
      "Epoch 5/10, Train Loss: 0.1217, Accuracy: 0.9213, F1 Micro: 0.7683, F1 Macro: 0.6479\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9204, F1 Micro: 0.7612, F1 Macro: 0.6509\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9175, F1 Micro: 0.7665, F1 Macro: 0.6835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9241, F1 Micro: 0.774, F1 Macro: 0.6928\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9233, F1 Micro: 0.7729, F1 Macro: 0.7007\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9234, F1 Micro: 0.766, F1 Macro: 0.7012\n",
      "Model 1 - Iteration 7901: Accuracy: 0.9241, F1 Micro: 0.774, F1 Macro: 0.6928\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.84      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.75      0.69      0.72       732\n",
      "     HS_Group       0.71      0.66      0.69       402\n",
      "  HS_Religion       0.79      0.57      0.66       157\n",
      "      HS_Race       0.78      0.76      0.77       120\n",
      "  HS_Physical       1.00      0.18      0.31        72\n",
      "    HS_Gender       0.68      0.41      0.51        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.62      0.61      0.62       331\n",
      "    HS_Strong       0.88      0.75      0.81       114\n",
      "\n",
      "    micro avg       0.79      0.75      0.77      5556\n",
      "    macro avg       0.79      0.65      0.69      5556\n",
      " weighted avg       0.80      0.75      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 238.80813217163086 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3617, Accuracy: 0.8961, F1 Micro: 0.6673, F1 Macro: 0.4094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9117, F1 Micro: 0.7344, F1 Macro: 0.5649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1995, Accuracy: 0.9176, F1 Micro: 0.7499, F1 Macro: 0.5961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1577, Accuracy: 0.9157, F1 Micro: 0.7591, F1 Macro: 0.6247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9229, F1 Micro: 0.7707, F1 Macro: 0.6327\n",
      "Epoch 6/10, Train Loss: 0.1018, Accuracy: 0.9226, F1 Micro: 0.7672, F1 Macro: 0.6434\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9188, F1 Micro: 0.7669, F1 Macro: 0.673\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9217, F1 Micro: 0.764, F1 Macro: 0.6697\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.921, F1 Micro: 0.7691, F1 Macro: 0.6932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9213, F1 Micro: 0.7723, F1 Macro: 0.6956\n",
      "Model 2 - Iteration 7901: Accuracy: 0.9213, F1 Micro: 0.7723, F1 Macro: 0.6956\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.86      0.92      0.89       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.65      0.71      0.68       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.68      0.75      0.71       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.55      0.41      0.47        51\n",
      "     HS_Other       0.77      0.79      0.78       762\n",
      "      HS_Weak       0.74      0.67      0.70       689\n",
      "  HS_Moderate       0.59      0.64      0.62       331\n",
      "    HS_Strong       0.82      0.88      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.77      0.77      5556\n",
      "    macro avg       0.73      0.69      0.70      5556\n",
      " weighted avg       0.77      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 243.01913285255432 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3577, Accuracy: 0.8957, F1 Micro: 0.6682, F1 Macro: 0.4049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2428, Accuracy: 0.9098, F1 Micro: 0.7305, F1 Macro: 0.5505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1974, Accuracy: 0.9181, F1 Micro: 0.7516, F1 Macro: 0.5932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1563, Accuracy: 0.914, F1 Micro: 0.756, F1 Macro: 0.6164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9229, F1 Micro: 0.7694, F1 Macro: 0.6388\n",
      "Epoch 6/10, Train Loss: 0.1008, Accuracy: 0.9216, F1 Micro: 0.7687, F1 Macro: 0.6589\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9114, F1 Micro: 0.7554, F1 Macro: 0.6751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0708, Accuracy: 0.9241, F1 Micro: 0.7705, F1 Macro: 0.6889\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9194, F1 Micro: 0.7701, F1 Macro: 0.7024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.926, F1 Micro: 0.78, F1 Macro: 0.703\n",
      "Model 3 - Iteration 7901: Accuracy: 0.926, F1 Micro: 0.78, F1 Macro: 0.703\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.89      0.90      0.90       992\n",
      "HS_Individual       0.76      0.70      0.73       732\n",
      "     HS_Group       0.70      0.72      0.71       402\n",
      "  HS_Religion       0.77      0.61      0.68       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       0.77      0.24      0.36        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.75      0.66      0.70       689\n",
      "  HS_Moderate       0.65      0.63      0.64       331\n",
      "    HS_Strong       0.80      0.86      0.83       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.76      0.67      0.70      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 244.4662857055664 s\n",
      "Averaged - Iteration 7901: Accuracy: 0.9126, F1 Micro: 0.7413, F1 Macro: 0.6158\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 264\n",
      "Sampling duration: 33.7598819732666 seconds\n",
      "New train size: 8165\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3461, Accuracy: 0.8972, F1 Micro: 0.6741, F1 Macro: 0.461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2383, Accuracy: 0.9129, F1 Micro: 0.7293, F1 Macro: 0.5402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1881, Accuracy: 0.9184, F1 Micro: 0.7583, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.9195, F1 Micro: 0.7667, F1 Macro: 0.6243\n",
      "Epoch 5/10, Train Loss: 0.1216, Accuracy: 0.9218, F1 Micro: 0.7651, F1 Macro: 0.6651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0966, Accuracy: 0.9173, F1 Micro: 0.7682, F1 Macro: 0.6683\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.9191, F1 Micro: 0.7675, F1 Macro: 0.6836\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9215, F1 Micro: 0.766, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9211, F1 Micro: 0.7698, F1 Macro: 0.6953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9192, F1 Micro: 0.7707, F1 Macro: 0.7053\n",
      "Model 1 - Iteration 8165: Accuracy: 0.9192, F1 Micro: 0.7707, F1 Macro: 0.7053\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.88      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.68      0.76      0.72       732\n",
      "     HS_Group       0.68      0.64      0.66       402\n",
      "  HS_Religion       0.72      0.69      0.70       157\n",
      "      HS_Race       0.79      0.69      0.74       120\n",
      "  HS_Physical       0.55      0.32      0.40        72\n",
      "    HS_Gender       0.64      0.49      0.56        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.66      0.74      0.70       689\n",
      "  HS_Moderate       0.60      0.58      0.59       331\n",
      "    HS_Strong       0.88      0.85      0.87       114\n",
      "\n",
      "    micro avg       0.75      0.79      0.77      5556\n",
      "    macro avg       0.72      0.70      0.71      5556\n",
      " weighted avg       0.75      0.79      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 249.21705079078674 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3491, Accuracy: 0.8966, F1 Micro: 0.6786, F1 Macro: 0.4575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2431, Accuracy: 0.9142, F1 Micro: 0.7325, F1 Macro: 0.5537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1908, Accuracy: 0.9181, F1 Micro: 0.7536, F1 Macro: 0.6009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1586, Accuracy: 0.9213, F1 Micro: 0.7719, F1 Macro: 0.6465\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9216, F1 Micro: 0.7668, F1 Macro: 0.6533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9209, F1 Micro: 0.7728, F1 Macro: 0.6784\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9181, F1 Micro: 0.7657, F1 Macro: 0.6875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9218, F1 Micro: 0.7754, F1 Macro: 0.686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9228, F1 Micro: 0.776, F1 Macro: 0.7063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.9271, F1 Micro: 0.7796, F1 Macro: 0.7058\n",
      "Model 2 - Iteration 8165: Accuracy: 0.9271, F1 Micro: 0.7796, F1 Macro: 0.7058\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.82      0.85      1134\n",
      "      Abusive       0.91      0.89      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.76      0.63      0.69       402\n",
      "  HS_Religion       0.80      0.55      0.65       157\n",
      "      HS_Race       0.86      0.61      0.71       120\n",
      "  HS_Physical       0.77      0.32      0.45        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.69      0.56      0.62       331\n",
      "    HS_Strong       0.92      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.79      0.65      0.71      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 253.03144574165344 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3465, Accuracy: 0.897, F1 Micro: 0.677, F1 Macro: 0.4584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9134, F1 Micro: 0.7312, F1 Macro: 0.5398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.9175, F1 Micro: 0.7494, F1 Macro: 0.5854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.159, Accuracy: 0.9206, F1 Micro: 0.7681, F1 Macro: 0.6256\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9199, F1 Micro: 0.7614, F1 Macro: 0.6536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9209, F1 Micro: 0.7685, F1 Macro: 0.6745\n",
      "Epoch 7/10, Train Loss: 0.0775, Accuracy: 0.9207, F1 Micro: 0.7684, F1 Macro: 0.694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.924, F1 Micro: 0.7755, F1 Macro: 0.7007\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9243, F1 Micro: 0.7745, F1 Macro: 0.7143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9221, F1 Micro: 0.7807, F1 Macro: 0.709\n",
      "Model 3 - Iteration 8165: Accuracy: 0.9221, F1 Micro: 0.7807, F1 Macro: 0.709\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.69      0.78      0.74       732\n",
      "     HS_Group       0.71      0.68      0.69       402\n",
      "  HS_Religion       0.70      0.68      0.69       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.68      0.32      0.43        72\n",
      "    HS_Gender       0.56      0.49      0.52        51\n",
      "     HS_Other       0.74      0.84      0.79       762\n",
      "      HS_Weak       0.68      0.77      0.72       689\n",
      "  HS_Moderate       0.63      0.61      0.62       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.81      0.78      5556\n",
      "    macro avg       0.73      0.70      0.71      5556\n",
      " weighted avg       0.76      0.81      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 245.93545246124268 s\n",
      "Averaged - Iteration 8165: Accuracy: 0.9132, F1 Micro: 0.7437, F1 Macro: 0.6218\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 237\n",
      "Sampling duration: 30.618091106414795 seconds\n",
      "New train size: 8402\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.356, Accuracy: 0.8956, F1 Micro: 0.631, F1 Macro: 0.4175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2386, Accuracy: 0.9116, F1 Micro: 0.7322, F1 Macro: 0.5756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1871, Accuracy: 0.919, F1 Micro: 0.7599, F1 Macro: 0.6102\n",
      "Epoch 4/10, Train Loss: 0.147, Accuracy: 0.9205, F1 Micro: 0.7591, F1 Macro: 0.625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.923, F1 Micro: 0.7711, F1 Macro: 0.6722\n",
      "Epoch 6/10, Train Loss: 0.0984, Accuracy: 0.9227, F1 Micro: 0.7654, F1 Macro: 0.6901\n",
      "Epoch 7/10, Train Loss: 0.0801, Accuracy: 0.9232, F1 Micro: 0.7708, F1 Macro: 0.6829\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9204, F1 Micro: 0.7705, F1 Macro: 0.7025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9232, F1 Micro: 0.7724, F1 Macro: 0.7056\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.917, F1 Micro: 0.767, F1 Macro: 0.7035\n",
      "Model 1 - Iteration 8402: Accuracy: 0.9232, F1 Micro: 0.7724, F1 Macro: 0.7056\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.73      0.60      0.66       402\n",
      "  HS_Religion       0.73      0.60      0.66       157\n",
      "      HS_Race       0.88      0.58      0.70       120\n",
      "  HS_Physical       0.87      0.36      0.51        72\n",
      "    HS_Gender       0.66      0.49      0.56        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.69      0.71      0.70       689\n",
      "  HS_Moderate       0.64      0.52      0.58       331\n",
      "    HS_Strong       0.91      0.78      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.77      5556\n",
      "    macro avg       0.78      0.66      0.71      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 248.8104748725891 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3587, Accuracy: 0.895, F1 Micro: 0.6231, F1 Macro: 0.378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2418, Accuracy: 0.9123, F1 Micro: 0.7395, F1 Macro: 0.5784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1911, Accuracy: 0.9182, F1 Micro: 0.7598, F1 Macro: 0.6119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9189, F1 Micro: 0.76, F1 Macro: 0.6237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1252, Accuracy: 0.9242, F1 Micro: 0.7732, F1 Macro: 0.6761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9249, F1 Micro: 0.7767, F1 Macro: 0.6935\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9221, F1 Micro: 0.7728, F1 Macro: 0.6805\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9212, F1 Micro: 0.7749, F1 Macro: 0.7031\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9225, F1 Micro: 0.7754, F1 Macro: 0.7073\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9226, F1 Micro: 0.7765, F1 Macro: 0.7045\n",
      "Model 2 - Iteration 8402: Accuracy: 0.9249, F1 Micro: 0.7767, F1 Macro: 0.6935\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.84      0.85      1134\n",
      "      Abusive       0.88      0.91      0.89       992\n",
      "HS_Individual       0.79      0.67      0.73       732\n",
      "     HS_Group       0.67      0.72      0.70       402\n",
      "  HS_Religion       0.75      0.60      0.67       157\n",
      "      HS_Race       0.74      0.80      0.77       120\n",
      "  HS_Physical       0.65      0.21      0.32        72\n",
      "    HS_Gender       0.50      0.35      0.41        51\n",
      "     HS_Other       0.79      0.77      0.78       762\n",
      "      HS_Weak       0.78      0.65      0.71       689\n",
      "  HS_Moderate       0.60      0.66      0.63       331\n",
      "    HS_Strong       0.90      0.83      0.87       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.74      0.67      0.69      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 253.5916132926941 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.356, Accuracy: 0.8939, F1 Micro: 0.6146, F1 Macro: 0.3837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2393, Accuracy: 0.9123, F1 Micro: 0.732, F1 Macro: 0.5548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1904, Accuracy: 0.9187, F1 Micro: 0.7576, F1 Macro: 0.606\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9208, F1 Micro: 0.768, F1 Macro: 0.6377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1242, Accuracy: 0.9242, F1 Micro: 0.778, F1 Macro: 0.6836\n",
      "Epoch 6/10, Train Loss: 0.093, Accuracy: 0.9216, F1 Micro: 0.7594, F1 Macro: 0.6914\n",
      "Epoch 7/10, Train Loss: 0.0789, Accuracy: 0.9221, F1 Micro: 0.7743, F1 Macro: 0.6798\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.9215, F1 Micro: 0.7686, F1 Macro: 0.7118\n",
      "Epoch 9/10, Train Loss: 0.0543, Accuracy: 0.9225, F1 Micro: 0.7741, F1 Macro: 0.7033\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9235, F1 Micro: 0.7698, F1 Macro: 0.7116\n",
      "Model 3 - Iteration 8402: Accuracy: 0.9242, F1 Micro: 0.778, F1 Macro: 0.6836\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.89      0.88      0.89       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.74      0.64      0.69       157\n",
      "      HS_Race       0.80      0.73      0.77       120\n",
      "  HS_Physical       0.83      0.14      0.24        72\n",
      "    HS_Gender       0.61      0.27      0.38        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.64      0.54      0.59       331\n",
      "    HS_Strong       0.87      0.87      0.87       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.66      0.68      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 250.34417247772217 s\n",
      "Averaged - Iteration 8402: Accuracy: 0.9139, F1 Micro: 0.7457, F1 Macro: 0.6264\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 214\n",
      "Sampling duration: 27.350574731826782 seconds\n",
      "New train size: 8616\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3473, Accuracy: 0.8975, F1 Micro: 0.6781, F1 Macro: 0.4578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2399, Accuracy: 0.9121, F1 Micro: 0.7273, F1 Macro: 0.5475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1897, Accuracy: 0.9187, F1 Micro: 0.7585, F1 Macro: 0.5987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1533, Accuracy: 0.9219, F1 Micro: 0.7651, F1 Macro: 0.6305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1168, Accuracy: 0.9226, F1 Micro: 0.7684, F1 Macro: 0.6874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9217, F1 Micro: 0.7695, F1 Macro: 0.678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9208, F1 Micro: 0.7724, F1 Macro: 0.6959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.9251, F1 Micro: 0.779, F1 Macro: 0.7032\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9216, F1 Micro: 0.7663, F1 Macro: 0.6998\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9236, F1 Micro: 0.773, F1 Macro: 0.7116\n",
      "Model 1 - Iteration 8616: Accuracy: 0.9251, F1 Micro: 0.779, F1 Macro: 0.7032\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.91      0.90      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.74      0.63      0.68       157\n",
      "      HS_Race       0.80      0.69      0.74       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.67      0.43      0.52        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.65      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 259.7467770576477 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3512, Accuracy: 0.8979, F1 Micro: 0.6708, F1 Macro: 0.4273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2421, Accuracy: 0.9126, F1 Micro: 0.7296, F1 Macro: 0.5464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1904, Accuracy: 0.9191, F1 Micro: 0.7601, F1 Macro: 0.5825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1559, Accuracy: 0.9219, F1 Micro: 0.7687, F1 Macro: 0.6326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9189, F1 Micro: 0.7716, F1 Macro: 0.6871\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9217, F1 Micro: 0.7725, F1 Macro: 0.6796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0769, Accuracy: 0.9252, F1 Micro: 0.7761, F1 Macro: 0.7037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9238, F1 Micro: 0.7783, F1 Macro: 0.701\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9213, F1 Micro: 0.7723, F1 Macro: 0.7111\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9223, F1 Micro: 0.7782, F1 Macro: 0.7069\n",
      "Model 2 - Iteration 8616: Accuracy: 0.9238, F1 Micro: 0.7783, F1 Macro: 0.701\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.73      0.66      0.70       402\n",
      "  HS_Religion       0.73      0.68      0.71       157\n",
      "      HS_Race       0.75      0.68      0.71       120\n",
      "  HS_Physical       1.00      0.25      0.40        72\n",
      "    HS_Gender       0.61      0.37      0.46        51\n",
      "     HS_Other       0.76      0.79      0.78       762\n",
      "      HS_Weak       0.70      0.73      0.72       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.85      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.68      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 259.3660635948181 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3478, Accuracy: 0.899, F1 Micro: 0.6719, F1 Macro: 0.4354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2405, Accuracy: 0.9124, F1 Micro: 0.7368, F1 Macro: 0.5637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1901, Accuracy: 0.9178, F1 Micro: 0.7602, F1 Macro: 0.5875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1541, Accuracy: 0.9224, F1 Micro: 0.7716, F1 Macro: 0.6377\n",
      "Epoch 5/10, Train Loss: 0.1203, Accuracy: 0.9187, F1 Micro: 0.7686, F1 Macro: 0.683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9223, F1 Micro: 0.7736, F1 Macro: 0.6819\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9228, F1 Micro: 0.7711, F1 Macro: 0.6916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9239, F1 Micro: 0.7776, F1 Macro: 0.7033\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.923, F1 Micro: 0.7663, F1 Macro: 0.7066\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9238, F1 Micro: 0.7692, F1 Macro: 0.7027\n",
      "Model 3 - Iteration 8616: Accuracy: 0.9239, F1 Micro: 0.7776, F1 Macro: 0.7033\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.91      0.88      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.71      0.68      0.69       402\n",
      "  HS_Religion       0.70      0.61      0.65       157\n",
      "      HS_Race       0.75      0.66      0.70       120\n",
      "  HS_Physical       0.81      0.29      0.43        72\n",
      "    HS_Gender       0.59      0.45      0.51        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.61      0.62       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.75      0.68      0.70      5556\n",
      " weighted avg       0.78      0.77      0.78      5556\n",
      "  samples avg       0.43      0.43      0.42      5556\n",
      "\n",
      "Training completed in 255.483948469162 s\n",
      "Averaged - Iteration 8616: Accuracy: 0.9145, F1 Micro: 0.7476, F1 Macro: 0.6308\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 24.386058807373047 seconds\n",
      "New train size: 8816\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3497, Accuracy: 0.8977, F1 Micro: 0.686, F1 Macro: 0.484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2345, Accuracy: 0.9107, F1 Micro: 0.7418, F1 Macro: 0.5732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1898, Accuracy: 0.9187, F1 Micro: 0.7623, F1 Macro: 0.6038\n",
      "Epoch 4/10, Train Loss: 0.1482, Accuracy: 0.9215, F1 Micro: 0.7458, F1 Macro: 0.616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9253, F1 Micro: 0.7714, F1 Macro: 0.6566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9193, F1 Micro: 0.7722, F1 Macro: 0.6898\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9232, F1 Micro: 0.7722, F1 Macro: 0.6964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.9229, F1 Micro: 0.7772, F1 Macro: 0.7007\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9178, F1 Micro: 0.771, F1 Macro: 0.7147\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9239, F1 Micro: 0.7748, F1 Macro: 0.711\n",
      "Model 1 - Iteration 8816: Accuracy: 0.9229, F1 Micro: 0.7772, F1 Macro: 0.7007\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.72      0.72      0.72       732\n",
      "     HS_Group       0.68      0.69      0.68       402\n",
      "  HS_Religion       0.66      0.69      0.67       157\n",
      "      HS_Race       0.76      0.71      0.73       120\n",
      "  HS_Physical       0.84      0.22      0.35        72\n",
      "    HS_Gender       0.62      0.47      0.53        51\n",
      "     HS_Other       0.79      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.70      0.70       689\n",
      "  HS_Moderate       0.60      0.61      0.60       331\n",
      "    HS_Strong       0.85      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.77      0.78      0.78      5556\n",
      "    macro avg       0.74      0.69      0.70      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 260.8804233074188 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3524, Accuracy: 0.8968, F1 Micro: 0.6751, F1 Macro: 0.4528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2365, Accuracy: 0.9103, F1 Micro: 0.7401, F1 Macro: 0.5735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1895, Accuracy: 0.9202, F1 Micro: 0.7619, F1 Macro: 0.6\n",
      "Epoch 4/10, Train Loss: 0.1499, Accuracy: 0.9225, F1 Micro: 0.7527, F1 Macro: 0.6237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1223, Accuracy: 0.9247, F1 Micro: 0.767, F1 Macro: 0.6563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9188, F1 Micro: 0.7706, F1 Macro: 0.6726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9228, F1 Micro: 0.7755, F1 Macro: 0.6942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9207, F1 Micro: 0.7774, F1 Macro: 0.7047\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9195, F1 Micro: 0.7745, F1 Macro: 0.7013\n",
      "Epoch 10/10, Train Loss: 0.0437, Accuracy: 0.9199, F1 Micro: 0.7678, F1 Macro: 0.6968\n",
      "Model 2 - Iteration 8816: Accuracy: 0.9207, F1 Micro: 0.7774, F1 Macro: 0.7047\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.89      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.66      0.73      0.69       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.73      0.73      0.73       120\n",
      "  HS_Physical       0.71      0.28      0.40        72\n",
      "    HS_Gender       0.55      0.47      0.51        51\n",
      "     HS_Other       0.75      0.83      0.79       762\n",
      "      HS_Weak       0.68      0.73      0.70       689\n",
      "  HS_Moderate       0.59      0.66      0.62       331\n",
      "    HS_Strong       0.84      0.84      0.84       114\n",
      "\n",
      "    micro avg       0.75      0.80      0.78      5556\n",
      "    macro avg       0.72      0.71      0.70      5556\n",
      " weighted avg       0.75      0.80      0.78      5556\n",
      "  samples avg       0.44      0.45      0.43      5556\n",
      "\n",
      "Training completed in 262.657235622406 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3481, Accuracy: 0.898, F1 Micro: 0.6866, F1 Macro: 0.4698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2358, Accuracy: 0.908, F1 Micro: 0.7413, F1 Macro: 0.5755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1899, Accuracy: 0.9179, F1 Micro: 0.7626, F1 Macro: 0.5912\n",
      "Epoch 4/10, Train Loss: 0.1484, Accuracy: 0.9232, F1 Micro: 0.7569, F1 Macro: 0.6104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9234, F1 Micro: 0.766, F1 Macro: 0.6542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9151, F1 Micro: 0.7672, F1 Macro: 0.6847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9226, F1 Micro: 0.7718, F1 Macro: 0.6959\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9206, F1 Micro: 0.7716, F1 Macro: 0.7061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9217, F1 Micro: 0.7768, F1 Macro: 0.7133\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9237, F1 Micro: 0.7766, F1 Macro: 0.7128\n",
      "Model 3 - Iteration 8816: Accuracy: 0.9217, F1 Micro: 0.7768, F1 Macro: 0.7133\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.88      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.70      0.76      0.73       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.67      0.70      0.68       157\n",
      "      HS_Race       0.73      0.79      0.76       120\n",
      "  HS_Physical       0.73      0.33      0.46        72\n",
      "    HS_Gender       0.56      0.55      0.55        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.68      0.74      0.71       689\n",
      "  HS_Moderate       0.63      0.57      0.60       331\n",
      "    HS_Strong       0.86      0.84      0.85       114\n",
      "\n",
      "    micro avg       0.76      0.79      0.78      5556\n",
      "    macro avg       0.73      0.71      0.71      5556\n",
      " weighted avg       0.76      0.79      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 262.67119312286377 s\n",
      "Averaged - Iteration 8816: Accuracy: 0.9149, F1 Micro: 0.7492, F1 Macro: 0.635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 22.07442831993103 seconds\n",
      "New train size: 9016\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3401, Accuracy: 0.9001, F1 Micro: 0.6633, F1 Macro: 0.4229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.235, Accuracy: 0.9124, F1 Micro: 0.7414, F1 Macro: 0.5513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1824, Accuracy: 0.9197, F1 Micro: 0.7496, F1 Macro: 0.5825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1503, Accuracy: 0.9212, F1 Micro: 0.7635, F1 Macro: 0.6382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.925, F1 Micro: 0.7783, F1 Macro: 0.6852\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9198, F1 Micro: 0.7741, F1 Macro: 0.686\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9232, F1 Micro: 0.7733, F1 Macro: 0.6877\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9179, F1 Micro: 0.7687, F1 Macro: 0.7014\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9225, F1 Micro: 0.7739, F1 Macro: 0.707\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9204, F1 Micro: 0.7744, F1 Macro: 0.7093\n",
      "Model 1 - Iteration 9016: Accuracy: 0.925, F1 Micro: 0.7783, F1 Macro: 0.6852\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.72      0.75      0.74       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.79      0.54      0.64       157\n",
      "      HS_Race       0.87      0.69      0.77       120\n",
      "  HS_Physical       0.85      0.15      0.26        72\n",
      "    HS_Gender       0.64      0.35      0.46        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.73      0.72       689\n",
      "  HS_Moderate       0.67      0.52      0.58       331\n",
      "    HS_Strong       0.88      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.64      0.69      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.41      5556\n",
      "\n",
      "Training completed in 263.6434099674225 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3427, Accuracy: 0.899, F1 Micro: 0.6532, F1 Macro: 0.4285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2363, Accuracy: 0.9138, F1 Micro: 0.7325, F1 Macro: 0.5249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1864, Accuracy: 0.9223, F1 Micro: 0.7585, F1 Macro: 0.6024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1492, Accuracy: 0.9252, F1 Micro: 0.7711, F1 Macro: 0.6414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9258, F1 Micro: 0.7731, F1 Macro: 0.6701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9235, F1 Micro: 0.7797, F1 Macro: 0.675\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9243, F1 Micro: 0.7706, F1 Macro: 0.6873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9262, F1 Micro: 0.78, F1 Macro: 0.7026\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9264, F1 Micro: 0.7788, F1 Macro: 0.7052\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9233, F1 Micro: 0.7764, F1 Macro: 0.709\n",
      "Model 2 - Iteration 9016: Accuracy: 0.9262, F1 Micro: 0.78, F1 Macro: 0.7026\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.90      0.92      0.91       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.72      0.65      0.69       402\n",
      "  HS_Religion       0.70      0.65      0.67       157\n",
      "      HS_Race       0.78      0.69      0.73       120\n",
      "  HS_Physical       1.00      0.25      0.40        72\n",
      "    HS_Gender       0.54      0.43      0.48        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.66      0.58      0.62       331\n",
      "    HS_Strong       0.87      0.84      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 267.2909288406372 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3408, Accuracy: 0.8991, F1 Micro: 0.6503, F1 Macro: 0.3949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2365, Accuracy: 0.9123, F1 Micro: 0.7306, F1 Macro: 0.5165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1881, Accuracy: 0.9217, F1 Micro: 0.759, F1 Macro: 0.5956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1511, Accuracy: 0.9233, F1 Micro: 0.7717, F1 Macro: 0.6444\n",
      "Epoch 5/10, Train Loss: 0.1186, Accuracy: 0.9257, F1 Micro: 0.7681, F1 Macro: 0.673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9216, F1 Micro: 0.7787, F1 Macro: 0.6945\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.9255, F1 Micro: 0.7744, F1 Macro: 0.6932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.063, Accuracy: 0.9278, F1 Micro: 0.7802, F1 Macro: 0.7059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0536, Accuracy: 0.9263, F1 Micro: 0.7846, F1 Macro: 0.7201\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9228, F1 Micro: 0.7737, F1 Macro: 0.7136\n",
      "Model 3 - Iteration 9016: Accuracy: 0.9263, F1 Micro: 0.7846, F1 Macro: 0.7201\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.89      0.93      0.91       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.70      0.67      0.69       402\n",
      "  HS_Religion       0.73      0.61      0.67       157\n",
      "      HS_Race       0.82      0.65      0.73       120\n",
      "  HS_Physical       0.89      0.35      0.50        72\n",
      "    HS_Gender       0.64      0.55      0.59        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.73      0.71      0.72       689\n",
      "  HS_Moderate       0.61      0.60      0.60       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.77      0.69      0.72      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 267.67654180526733 s\n",
      "Averaged - Iteration 9016: Accuracy: 0.9155, F1 Micro: 0.7509, F1 Macro: 0.6386\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 200\n",
      "Sampling duration: 19.475049257278442 seconds\n",
      "New train size: 9216\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3421, Accuracy: 0.8965, F1 Micro: 0.67, F1 Macro: 0.3905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2299, Accuracy: 0.9143, F1 Micro: 0.7425, F1 Macro: 0.5792\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1806, Accuracy: 0.921, F1 Micro: 0.7633, F1 Macro: 0.6227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.922, F1 Micro: 0.7634, F1 Macro: 0.6229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1165, Accuracy: 0.9242, F1 Micro: 0.7663, F1 Macro: 0.6618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0958, Accuracy: 0.9186, F1 Micro: 0.7696, F1 Macro: 0.6899\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0771, Accuracy: 0.9213, F1 Micro: 0.7729, F1 Macro: 0.6996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9261, F1 Micro: 0.7796, F1 Macro: 0.702\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.9243, F1 Micro: 0.7705, F1 Macro: 0.711\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.925, F1 Micro: 0.7753, F1 Macro: 0.7116\n",
      "Model 1 - Iteration 9216: Accuracy: 0.9261, F1 Micro: 0.7796, F1 Macro: 0.702\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.84      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.70      0.72       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.79      0.60      0.68       157\n",
      "      HS_Race       0.79      0.65      0.71       120\n",
      "  HS_Physical       0.79      0.26      0.40        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.80      0.80      0.80       762\n",
      "      HS_Weak       0.72      0.68      0.70       689\n",
      "  HS_Moderate       0.65      0.61      0.63       331\n",
      "    HS_Strong       0.92      0.74      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.78      0.66      0.70      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 274.5535125732422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3443, Accuracy: 0.8987, F1 Micro: 0.6741, F1 Macro: 0.4174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2314, Accuracy: 0.9147, F1 Micro: 0.7405, F1 Macro: 0.5771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1838, Accuracy: 0.9221, F1 Micro: 0.7691, F1 Macro: 0.6217\n",
      "Epoch 4/10, Train Loss: 0.1524, Accuracy: 0.9235, F1 Micro: 0.7661, F1 Macro: 0.6397\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9217, F1 Micro: 0.7598, F1 Macro: 0.6469\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9219, F1 Micro: 0.7635, F1 Macro: 0.6706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9233, F1 Micro: 0.7743, F1 Macro: 0.6948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9227, F1 Micro: 0.7756, F1 Macro: 0.6947\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9218, F1 Micro: 0.7727, F1 Macro: 0.7063\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9235, F1 Micro: 0.7736, F1 Macro: 0.7031\n",
      "Model 2 - Iteration 9216: Accuracy: 0.9227, F1 Micro: 0.7756, F1 Macro: 0.6947\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.75      0.73       732\n",
      "     HS_Group       0.71      0.63      0.67       402\n",
      "  HS_Religion       0.75      0.60      0.66       157\n",
      "      HS_Race       0.73      0.67      0.70       120\n",
      "  HS_Physical       0.86      0.26      0.40        72\n",
      "    HS_Gender       0.58      0.43      0.49        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.70      0.72      0.71       689\n",
      "  HS_Moderate       0.63      0.56      0.59       331\n",
      "    HS_Strong       0.89      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.67      0.69      5556\n",
      " weighted avg       0.77      0.78      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 268.67079067230225 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3403, Accuracy: 0.8983, F1 Micro: 0.6784, F1 Macro: 0.4126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2298, Accuracy: 0.9132, F1 Micro: 0.7401, F1 Macro: 0.5679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1815, Accuracy: 0.9219, F1 Micro: 0.7643, F1 Macro: 0.6199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9206, F1 Micro: 0.7679, F1 Macro: 0.6458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.9251, F1 Micro: 0.7762, F1 Macro: 0.6764\n",
      "Epoch 6/10, Train Loss: 0.0944, Accuracy: 0.924, F1 Micro: 0.7642, F1 Macro: 0.6736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9255, F1 Micro: 0.7764, F1 Macro: 0.7047\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9235, F1 Micro: 0.7731, F1 Macro: 0.6866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9256, F1 Micro: 0.7798, F1 Macro: 0.7127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9249, F1 Micro: 0.7817, F1 Macro: 0.7176\n",
      "Model 3 - Iteration 9216: Accuracy: 0.9249, F1 Micro: 0.7817, F1 Macro: 0.7176\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.73      0.74      0.73       732\n",
      "     HS_Group       0.70      0.68      0.69       402\n",
      "  HS_Religion       0.75      0.59      0.66       157\n",
      "      HS_Race       0.76      0.63      0.69       120\n",
      "  HS_Physical       0.83      0.40      0.54        72\n",
      "    HS_Gender       0.58      0.57      0.57        51\n",
      "     HS_Other       0.77      0.82      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.62      0.61      0.62       331\n",
      "    HS_Strong       0.93      0.76      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.69      0.72      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 273.85939025878906 s\n",
      "Averaged - Iteration 9216: Accuracy: 0.916, F1 Micro: 0.7523, F1 Macro: 0.6419\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 9218\n",
      "Acquired samples: 2\n",
      "Sampling duration: 16.868199586868286 seconds\n",
      "New train size: 9218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.342, Accuracy: 0.9007, F1 Micro: 0.676, F1 Macro: 0.445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2303, Accuracy: 0.9139, F1 Micro: 0.7403, F1 Macro: 0.552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.185, Accuracy: 0.923, F1 Micro: 0.7657, F1 Macro: 0.6096\n",
      "Epoch 4/10, Train Loss: 0.1471, Accuracy: 0.922, F1 Micro: 0.7631, F1 Macro: 0.6325\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1177, Accuracy: 0.9206, F1 Micro: 0.7753, F1 Macro: 0.685\n",
      "Epoch 6/10, Train Loss: 0.0981, Accuracy: 0.9215, F1 Micro: 0.7672, F1 Macro: 0.6739\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9213, F1 Micro: 0.7713, F1 Macro: 0.696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9219, F1 Micro: 0.777, F1 Macro: 0.704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9248, F1 Micro: 0.7785, F1 Macro: 0.7065\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9207, F1 Micro: 0.7736, F1 Macro: 0.7071\n",
      "Model 1 - Iteration 9218: Accuracy: 0.9248, F1 Micro: 0.7785, F1 Macro: 0.7065\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.73      0.61      0.66       402\n",
      "  HS_Religion       0.73      0.64      0.68       157\n",
      "      HS_Race       0.72      0.67      0.69       120\n",
      "  HS_Physical       0.85      0.32      0.46        72\n",
      "    HS_Gender       0.66      0.45      0.53        51\n",
      "     HS_Other       0.78      0.81      0.79       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.65      0.52      0.58       331\n",
      "    HS_Strong       0.87      0.85      0.86       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.71      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 272.4951810836792 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3435, Accuracy: 0.9005, F1 Micro: 0.6775, F1 Macro: 0.418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2334, Accuracy: 0.9146, F1 Micro: 0.7434, F1 Macro: 0.563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1866, Accuracy: 0.9214, F1 Micro: 0.7623, F1 Macro: 0.6011\n",
      "Epoch 4/10, Train Loss: 0.1468, Accuracy: 0.9238, F1 Micro: 0.7608, F1 Macro: 0.6312\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1183, Accuracy: 0.9255, F1 Micro: 0.7808, F1 Macro: 0.6722\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9248, F1 Micro: 0.7734, F1 Macro: 0.6729\n",
      "Epoch 7/10, Train Loss: 0.0752, Accuracy: 0.9209, F1 Micro: 0.774, F1 Macro: 0.7001\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9209, F1 Micro: 0.7741, F1 Macro: 0.6986\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9226, F1 Micro: 0.7731, F1 Macro: 0.7022\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.925, F1 Micro: 0.7775, F1 Macro: 0.7144\n",
      "Model 2 - Iteration 9218: Accuracy: 0.9255, F1 Micro: 0.7808, F1 Macro: 0.6722\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.89      0.90      0.89       992\n",
      "HS_Individual       0.74      0.75      0.74       732\n",
      "     HS_Group       0.74      0.64      0.69       402\n",
      "  HS_Religion       0.78      0.58      0.66       157\n",
      "      HS_Race       0.70      0.74      0.72       120\n",
      "  HS_Physical       0.89      0.11      0.20        72\n",
      "    HS_Gender       0.69      0.22      0.33        51\n",
      "     HS_Other       0.77      0.81      0.79       762\n",
      "      HS_Weak       0.73      0.73      0.73       689\n",
      "  HS_Moderate       0.65      0.57      0.61       331\n",
      "    HS_Strong       0.93      0.79      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.64      0.67      5556\n",
      " weighted avg       0.79      0.77      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 270.70209765434265 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3425, Accuracy: 0.9009, F1 Micro: 0.682, F1 Macro: 0.4386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2322, Accuracy: 0.9138, F1 Micro: 0.7385, F1 Macro: 0.544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.9202, F1 Micro: 0.7671, F1 Macro: 0.6049\n",
      "Epoch 4/10, Train Loss: 0.1478, Accuracy: 0.9192, F1 Micro: 0.7391, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9234, F1 Micro: 0.773, F1 Macro: 0.6748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9253, F1 Micro: 0.7736, F1 Macro: 0.6849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9274, F1 Micro: 0.7798, F1 Macro: 0.7055\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.9248, F1 Micro: 0.7755, F1 Macro: 0.7021\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9261, F1 Micro: 0.7762, F1 Macro: 0.7129\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9207, F1 Micro: 0.7671, F1 Macro: 0.7098\n",
      "Model 3 - Iteration 9218: Accuracy: 0.9274, F1 Micro: 0.7798, F1 Macro: 0.7055\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.83      0.85      1134\n",
      "      Abusive       0.91      0.88      0.90       992\n",
      "HS_Individual       0.75      0.72      0.73       732\n",
      "     HS_Group       0.75      0.63      0.68       402\n",
      "  HS_Religion       0.82      0.61      0.70       157\n",
      "      HS_Race       0.82      0.63      0.71       120\n",
      "  HS_Physical       0.83      0.26      0.40        72\n",
      "    HS_Gender       0.62      0.47      0.53        51\n",
      "     HS_Other       0.81      0.77      0.79       762\n",
      "      HS_Weak       0.74      0.70      0.72       689\n",
      "  HS_Moderate       0.67      0.55      0.60       331\n",
      "    HS_Strong       0.87      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.82      0.75      0.78      5556\n",
      "    macro avg       0.79      0.66      0.71      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.43      0.42      0.41      5556\n",
      "\n",
      "Training completed in 270.3162968158722 s\n",
      "Averaged - Iteration 9218: Accuracy: 0.9164, F1 Micro: 0.7536, F1 Macro: 0.6444\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 16.894900798797607 seconds\n",
      "New train size: 9418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3403, Accuracy: 0.9004, F1 Micro: 0.6787, F1 Macro: 0.4123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2321, Accuracy: 0.9143, F1 Micro: 0.7457, F1 Macro: 0.5912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1829, Accuracy: 0.9179, F1 Micro: 0.7614, F1 Macro: 0.6084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1509, Accuracy: 0.925, F1 Micro: 0.7754, F1 Macro: 0.6617\n",
      "Epoch 5/10, Train Loss: 0.1197, Accuracy: 0.9219, F1 Micro: 0.7738, F1 Macro: 0.6809\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9244, F1 Micro: 0.7747, F1 Macro: 0.6838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9263, F1 Micro: 0.7811, F1 Macro: 0.6959\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9239, F1 Micro: 0.775, F1 Macro: 0.7025\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9243, F1 Micro: 0.7746, F1 Macro: 0.7129\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9196, F1 Micro: 0.7732, F1 Macro: 0.7064\n",
      "Model 1 - Iteration 9418: Accuracy: 0.9263, F1 Micro: 0.7811, F1 Macro: 0.6959\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.85      0.86      1134\n",
      "      Abusive       0.90      0.91      0.90       992\n",
      "HS_Individual       0.74      0.75      0.74       732\n",
      "     HS_Group       0.76      0.63      0.69       402\n",
      "  HS_Religion       0.86      0.53      0.65       157\n",
      "      HS_Race       0.85      0.60      0.70       120\n",
      "  HS_Physical       0.82      0.25      0.38        72\n",
      "    HS_Gender       0.66      0.41      0.51        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.72      0.73      0.73       689\n",
      "  HS_Moderate       0.65      0.54      0.59       331\n",
      "    HS_Strong       0.92      0.73      0.81       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.64      0.70      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 276.0244414806366 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3444, Accuracy: 0.895, F1 Micro: 0.6792, F1 Macro: 0.3773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2334, Accuracy: 0.9144, F1 Micro: 0.7464, F1 Macro: 0.5945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9209, F1 Micro: 0.7646, F1 Macro: 0.6155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.925, F1 Micro: 0.7738, F1 Macro: 0.6517\n",
      "Epoch 5/10, Train Loss: 0.1193, Accuracy: 0.9234, F1 Micro: 0.7733, F1 Macro: 0.6791\n",
      "Epoch 6/10, Train Loss: 0.0909, Accuracy: 0.9233, F1 Micro: 0.7716, F1 Macro: 0.6788\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9227, F1 Micro: 0.7696, F1 Macro: 0.6878\n",
      "Epoch 8/10, Train Loss: 0.061, Accuracy: 0.9247, F1 Micro: 0.7735, F1 Macro: 0.6942\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9211, F1 Micro: 0.7707, F1 Macro: 0.7075\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9219, F1 Micro: 0.7699, F1 Macro: 0.6995\n",
      "Model 2 - Iteration 9418: Accuracy: 0.925, F1 Micro: 0.7738, F1 Macro: 0.6517\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.92      0.84      0.88       992\n",
      "HS_Individual       0.77      0.70      0.73       732\n",
      "     HS_Group       0.73      0.68      0.71       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.76      0.77      0.76       120\n",
      "  HS_Physical       0.83      0.07      0.13        72\n",
      "    HS_Gender       0.56      0.10      0.17        51\n",
      "     HS_Other       0.77      0.80      0.78       762\n",
      "      HS_Weak       0.76      0.66      0.70       689\n",
      "  HS_Moderate       0.62      0.59      0.61       331\n",
      "    HS_Strong       0.86      0.78      0.82       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.77      5556\n",
      "    macro avg       0.77      0.62      0.65      5556\n",
      " weighted avg       0.80      0.74      0.77      5556\n",
      "  samples avg       0.43      0.41      0.41      5556\n",
      "\n",
      "Training completed in 272.823673248291 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3414, Accuracy: 0.8987, F1 Micro: 0.6863, F1 Macro: 0.4087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2327, Accuracy: 0.9136, F1 Micro: 0.741, F1 Macro: 0.5805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.183, Accuracy: 0.9161, F1 Micro: 0.7599, F1 Macro: 0.6033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.148, Accuracy: 0.926, F1 Micro: 0.7765, F1 Macro: 0.6605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9255, F1 Micro: 0.781, F1 Macro: 0.6965\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.9221, F1 Micro: 0.7693, F1 Macro: 0.6765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9253, F1 Micro: 0.7821, F1 Macro: 0.704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9264, F1 Micro: 0.7835, F1 Macro: 0.7171\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9231, F1 Micro: 0.7782, F1 Macro: 0.7182\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9249, F1 Micro: 0.7754, F1 Macro: 0.7135\n",
      "Model 3 - Iteration 9418: Accuracy: 0.9264, F1 Micro: 0.7835, F1 Macro: 0.7171\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.70      0.66      0.68       402\n",
      "  HS_Religion       0.79      0.61      0.69       157\n",
      "      HS_Race       0.78      0.71      0.74       120\n",
      "  HS_Physical       0.88      0.32      0.47        72\n",
      "    HS_Gender       0.60      0.49      0.54        51\n",
      "     HS_Other       0.78      0.81      0.80       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.64      0.58      0.61       331\n",
      "    HS_Strong       0.88      0.88      0.88       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.77      0.69      0.72      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.45      0.43      0.43      5556\n",
      "\n",
      "Training completed in 277.3964674472809 s\n",
      "Averaged - Iteration 9418: Accuracy: 0.9169, F1 Micro: 0.7548, F1 Macro: 0.6464\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 14.269962549209595 seconds\n",
      "New train size: 9618\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.34, Accuracy: 0.9012, F1 Micro: 0.68, F1 Macro: 0.4247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2261, Accuracy: 0.9156, F1 Micro: 0.7361, F1 Macro: 0.5578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1848, Accuracy: 0.9204, F1 Micro: 0.7633, F1 Macro: 0.6258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1461, Accuracy: 0.9238, F1 Micro: 0.7699, F1 Macro: 0.6332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1134, Accuracy: 0.9253, F1 Micro: 0.7772, F1 Macro: 0.7001\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9186, F1 Micro: 0.7701, F1 Macro: 0.6923\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9238, F1 Micro: 0.7745, F1 Macro: 0.6962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9258, F1 Micro: 0.7837, F1 Macro: 0.7132\n",
      "Epoch 9/10, Train Loss: 0.0525, Accuracy: 0.9248, F1 Micro: 0.7806, F1 Macro: 0.7212\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9257, F1 Micro: 0.7799, F1 Macro: 0.7179\n",
      "Model 1 - Iteration 9618: Accuracy: 0.9258, F1 Micro: 0.7837, F1 Macro: 0.7132\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.87      0.86      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.74      0.73      0.73       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.77      0.59      0.67       157\n",
      "      HS_Race       0.84      0.61      0.71       120\n",
      "  HS_Physical       0.80      0.33      0.47        72\n",
      "    HS_Gender       0.59      0.53      0.56        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.64      0.62      0.63       331\n",
      "    HS_Strong       0.87      0.79      0.83       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.77      0.68      0.71      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.43      5556\n",
      "\n",
      "Training completed in 282.8146662712097 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3427, Accuracy: 0.9018, F1 Micro: 0.6738, F1 Macro: 0.4185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9151, F1 Micro: 0.7424, F1 Macro: 0.5786\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1869, Accuracy: 0.9215, F1 Micro: 0.7644, F1 Macro: 0.6228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1463, Accuracy: 0.9239, F1 Micro: 0.7678, F1 Macro: 0.6405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1137, Accuracy: 0.9222, F1 Micro: 0.7762, F1 Macro: 0.6927\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9235, F1 Micro: 0.7756, F1 Macro: 0.6931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9264, F1 Micro: 0.7776, F1 Macro: 0.7119\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.9267, F1 Micro: 0.775, F1 Macro: 0.7036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0541, Accuracy: 0.9251, F1 Micro: 0.7818, F1 Macro: 0.7259\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9262, F1 Micro: 0.7745, F1 Macro: 0.7129\n",
      "Model 2 - Iteration 9618: Accuracy: 0.9251, F1 Micro: 0.7818, F1 Macro: 0.7259\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.86      0.85      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.72      0.74      0.73       732\n",
      "     HS_Group       0.72      0.68      0.70       402\n",
      "  HS_Religion       0.71      0.68      0.70       157\n",
      "      HS_Race       0.75      0.77      0.76       120\n",
      "  HS_Physical       0.74      0.39      0.51        72\n",
      "    HS_Gender       0.56      0.63      0.59        51\n",
      "     HS_Other       0.78      0.79      0.78       762\n",
      "      HS_Weak       0.72      0.71      0.71       689\n",
      "  HS_Moderate       0.66      0.60      0.63       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.75      0.72      0.73      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 284.3700466156006 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3404, Accuracy: 0.9, F1 Micro: 0.6739, F1 Macro: 0.4208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.228, Accuracy: 0.916, F1 Micro: 0.7371, F1 Macro: 0.5586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1853, Accuracy: 0.9195, F1 Micro: 0.7568, F1 Macro: 0.6127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.146, Accuracy: 0.9244, F1 Micro: 0.7637, F1 Macro: 0.6318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1146, Accuracy: 0.9255, F1 Micro: 0.779, F1 Macro: 0.6959\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.921, F1 Micro: 0.773, F1 Macro: 0.694\n",
      "Epoch 7/10, Train Loss: 0.0756, Accuracy: 0.9239, F1 Micro: 0.773, F1 Macro: 0.71\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9248, F1 Micro: 0.7729, F1 Macro: 0.7049\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9173, F1 Micro: 0.7684, F1 Macro: 0.7081\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9246, F1 Micro: 0.7778, F1 Macro: 0.7168\n",
      "Model 3 - Iteration 9618: Accuracy: 0.9255, F1 Micro: 0.779, F1 Macro: 0.6959\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.75      0.73      0.74       732\n",
      "     HS_Group       0.72      0.65      0.68       402\n",
      "  HS_Religion       0.72      0.69      0.71       157\n",
      "      HS_Race       0.83      0.63      0.72       120\n",
      "  HS_Physical       0.61      0.19      0.29        72\n",
      "    HS_Gender       0.58      0.49      0.53        51\n",
      "     HS_Other       0.82      0.74      0.78       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.62      0.57      0.60       331\n",
      "    HS_Strong       0.88      0.77      0.82       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.75      0.66      0.70      5556\n",
      " weighted avg       0.79      0.76      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 281.20821809768677 s\n",
      "Averaged - Iteration 9618: Accuracy: 0.9172, F1 Micro: 0.756, F1 Macro: 0.6492\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 12.044142723083496 seconds\n",
      "New train size: 9818\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3325, Accuracy: 0.9023, F1 Micro: 0.6964, F1 Macro: 0.5042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2292, Accuracy: 0.9155, F1 Micro: 0.7527, F1 Macro: 0.5958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1826, Accuracy: 0.9224, F1 Micro: 0.7635, F1 Macro: 0.6061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1461, Accuracy: 0.9207, F1 Micro: 0.7661, F1 Macro: 0.6583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9237, F1 Micro: 0.7742, F1 Macro: 0.6751\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.92, F1 Micro: 0.7733, F1 Macro: 0.6903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0725, Accuracy: 0.9255, F1 Micro: 0.7746, F1 Macro: 0.7044\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.92, F1 Micro: 0.7726, F1 Macro: 0.6951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9265, F1 Micro: 0.7768, F1 Macro: 0.708\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9249, F1 Micro: 0.7759, F1 Macro: 0.7129\n",
      "Model 1 - Iteration 9818: Accuracy: 0.9265, F1 Micro: 0.7768, F1 Macro: 0.708\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.89      0.81      0.85      1134\n",
      "      Abusive       0.88      0.93      0.90       992\n",
      "HS_Individual       0.75      0.71      0.73       732\n",
      "     HS_Group       0.76      0.59      0.66       402\n",
      "  HS_Religion       0.78      0.61      0.68       157\n",
      "      HS_Race       0.82      0.62      0.71       120\n",
      "  HS_Physical       0.77      0.32      0.45        72\n",
      "    HS_Gender       0.62      0.51      0.56        51\n",
      "     HS_Other       0.82      0.76      0.79       762\n",
      "      HS_Weak       0.73      0.68      0.70       689\n",
      "  HS_Moderate       0.69      0.50      0.58       331\n",
      "    HS_Strong       0.86      0.89      0.88       114\n",
      "\n",
      "    micro avg       0.81      0.74      0.78      5556\n",
      "    macro avg       0.78      0.66      0.71      5556\n",
      " weighted avg       0.81      0.74      0.77      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 289.09205293655396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3337, Accuracy: 0.9, F1 Micro: 0.6881, F1 Macro: 0.491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.232, Accuracy: 0.9164, F1 Micro: 0.7492, F1 Macro: 0.5879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1829, Accuracy: 0.9219, F1 Micro: 0.7519, F1 Macro: 0.5895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1453, Accuracy: 0.9164, F1 Micro: 0.7625, F1 Macro: 0.6608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9254, F1 Micro: 0.7782, F1 Macro: 0.6786\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9245, F1 Micro: 0.7763, F1 Macro: 0.6867\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.923, F1 Micro: 0.768, F1 Macro: 0.7024\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9212, F1 Micro: 0.7723, F1 Macro: 0.7031\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9241, F1 Micro: 0.7742, F1 Macro: 0.6983\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9216, F1 Micro: 0.7724, F1 Macro: 0.6996\n",
      "Model 2 - Iteration 9818: Accuracy: 0.9254, F1 Micro: 0.7782, F1 Macro: 0.6786\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.93      0.86      0.89       992\n",
      "HS_Individual       0.73      0.76      0.75       732\n",
      "     HS_Group       0.75      0.64      0.69       402\n",
      "  HS_Religion       0.78      0.62      0.70       157\n",
      "      HS_Race       0.78      0.68      0.73       120\n",
      "  HS_Physical       0.80      0.17      0.28        72\n",
      "    HS_Gender       0.61      0.22      0.32        51\n",
      "     HS_Other       0.78      0.78      0.78       762\n",
      "      HS_Weak       0.71      0.74      0.72       689\n",
      "  HS_Moderate       0.65      0.55      0.59       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.77      0.64      0.68      5556\n",
      " weighted avg       0.80      0.76      0.77      5556\n",
      "  samples avg       0.43      0.42      0.42      5556\n",
      "\n",
      "Training completed in 284.86336636543274 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3319, Accuracy: 0.9005, F1 Micro: 0.6903, F1 Macro: 0.4939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2311, Accuracy: 0.914, F1 Micro: 0.7455, F1 Macro: 0.5777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9233, F1 Micro: 0.768, F1 Macro: 0.6178\n",
      "Epoch 4/10, Train Loss: 0.1436, Accuracy: 0.9149, F1 Micro: 0.755, F1 Macro: 0.6492\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1118, Accuracy: 0.9245, F1 Micro: 0.7745, F1 Macro: 0.6834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0907, Accuracy: 0.9257, F1 Micro: 0.7805, F1 Macro: 0.7049\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9225, F1 Micro: 0.7734, F1 Macro: 0.7084\n",
      "Epoch 8/10, Train Loss: 0.0595, Accuracy: 0.9224, F1 Micro: 0.7755, F1 Macro: 0.7057\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9219, F1 Micro: 0.7755, F1 Macro: 0.6991\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9233, F1 Micro: 0.7713, F1 Macro: 0.7058\n",
      "Model 3 - Iteration 9818: Accuracy: 0.9257, F1 Micro: 0.7805, F1 Macro: 0.7049\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.86      1134\n",
      "      Abusive       0.90      0.90      0.90       992\n",
      "HS_Individual       0.74      0.72      0.73       732\n",
      "     HS_Group       0.72      0.67      0.69       402\n",
      "  HS_Religion       0.74      0.64      0.68       157\n",
      "      HS_Race       0.80      0.69      0.74       120\n",
      "  HS_Physical       0.89      0.24      0.37        72\n",
      "    HS_Gender       0.65      0.39      0.49        51\n",
      "     HS_Other       0.78      0.80      0.79       762\n",
      "      HS_Weak       0.72      0.69      0.70       689\n",
      "  HS_Moderate       0.66      0.59      0.62       331\n",
      "    HS_Strong       0.88      0.88      0.88       114\n",
      "\n",
      "    micro avg       0.79      0.77      0.78      5556\n",
      "    macro avg       0.78      0.67      0.70      5556\n",
      " weighted avg       0.79      0.77      0.78      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 289.0218811035156 s\n",
      "Averaged - Iteration 9818: Accuracy: 0.9176, F1 Micro: 0.7569, F1 Macro: 0.6512\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 9.606392860412598 seconds\n",
      "New train size: 10018\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3347, Accuracy: 0.8993, F1 Micro: 0.668, F1 Macro: 0.4764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2282, Accuracy: 0.9153, F1 Micro: 0.7478, F1 Macro: 0.5908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1753, Accuracy: 0.9143, F1 Micro: 0.7526, F1 Macro: 0.6112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1439, Accuracy: 0.9231, F1 Micro: 0.7708, F1 Macro: 0.6685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1141, Accuracy: 0.9236, F1 Micro: 0.7714, F1 Macro: 0.692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.923, F1 Micro: 0.777, F1 Macro: 0.7015\n",
      "Epoch 7/10, Train Loss: 0.0739, Accuracy: 0.9184, F1 Micro: 0.767, F1 Macro: 0.695\n",
      "Epoch 8/10, Train Loss: 0.0623, Accuracy: 0.923, F1 Micro: 0.7752, F1 Macro: 0.7006\n",
      "Epoch 9/10, Train Loss: 0.0498, Accuracy: 0.9232, F1 Micro: 0.7681, F1 Macro: 0.7079\n",
      "Epoch 10/10, Train Loss: 0.0441, Accuracy: 0.9218, F1 Micro: 0.7723, F1 Macro: 0.7093\n",
      "Model 1 - Iteration 10018: Accuracy: 0.923, F1 Micro: 0.777, F1 Macro: 0.7015\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.87      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.75      0.73       732\n",
      "     HS_Group       0.73      0.65      0.69       402\n",
      "  HS_Religion       0.71      0.64      0.67       157\n",
      "      HS_Race       0.77      0.71      0.74       120\n",
      "  HS_Physical       0.94      0.24      0.38        72\n",
      "    HS_Gender       0.55      0.47      0.51        51\n",
      "     HS_Other       0.77      0.80      0.79       762\n",
      "      HS_Weak       0.69      0.72      0.71       689\n",
      "  HS_Moderate       0.65      0.58      0.61       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.76      0.68      0.70      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.44      0.42      5556\n",
      "\n",
      "Training completed in 291.39523005485535 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3385, Accuracy: 0.9004, F1 Micro: 0.6741, F1 Macro: 0.4662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2297, Accuracy: 0.9168, F1 Micro: 0.7487, F1 Macro: 0.5845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1788, Accuracy: 0.9183, F1 Micro: 0.7615, F1 Macro: 0.6173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9217, F1 Micro: 0.7728, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1151, Accuracy: 0.9227, F1 Micro: 0.7766, F1 Macro: 0.6959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9264, F1 Micro: 0.7797, F1 Macro: 0.7053\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9235, F1 Micro: 0.7709, F1 Macro: 0.7033\n",
      "Epoch 8/10, Train Loss: 0.0597, Accuracy: 0.9218, F1 Micro: 0.7705, F1 Macro: 0.6991\n",
      "Epoch 9/10, Train Loss: 0.0486, Accuracy: 0.9217, F1 Micro: 0.7698, F1 Macro: 0.7062\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9225, F1 Micro: 0.7731, F1 Macro: 0.7016\n",
      "Model 2 - Iteration 10018: Accuracy: 0.9264, F1 Micro: 0.7797, F1 Macro: 0.7053\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.87      0.85      0.86      1134\n",
      "      Abusive       0.90      0.89      0.89       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.76      0.63      0.69       402\n",
      "  HS_Religion       0.81      0.62      0.71       157\n",
      "      HS_Race       0.80      0.68      0.74       120\n",
      "  HS_Physical       0.95      0.28      0.43        72\n",
      "    HS_Gender       0.53      0.41      0.46        51\n",
      "     HS_Other       0.80      0.78      0.79       762\n",
      "      HS_Weak       0.72      0.70      0.71       689\n",
      "  HS_Moderate       0.69      0.54      0.61       331\n",
      "    HS_Strong       0.90      0.82      0.86       114\n",
      "\n",
      "    micro avg       0.80      0.76      0.78      5556\n",
      "    macro avg       0.79      0.66      0.71      5556\n",
      " weighted avg       0.80      0.76      0.78      5556\n",
      "  samples avg       0.44      0.42      0.42      5556\n",
      "\n",
      "Training completed in 295.30785155296326 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3378, Accuracy: 0.9007, F1 Micro: 0.677, F1 Macro: 0.4846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2275, Accuracy: 0.9159, F1 Micro: 0.7474, F1 Macro: 0.5802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1778, Accuracy: 0.9187, F1 Micro: 0.7669, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1421, Accuracy: 0.9234, F1 Micro: 0.7741, F1 Macro: 0.6881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1162, Accuracy: 0.9232, F1 Micro: 0.7742, F1 Macro: 0.6943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0876, Accuracy: 0.9254, F1 Micro: 0.7812, F1 Macro: 0.7098\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9219, F1 Micro: 0.7757, F1 Macro: 0.7083\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9223, F1 Micro: 0.7681, F1 Macro: 0.6991\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9241, F1 Micro: 0.7814, F1 Macro: 0.7226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9244, F1 Micro: 0.7821, F1 Macro: 0.7142\n",
      "Model 3 - Iteration 10018: Accuracy: 0.9244, F1 Micro: 0.7821, F1 Macro: 0.7142\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.88      0.86      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.73      0.75      0.74       732\n",
      "     HS_Group       0.69      0.68      0.69       402\n",
      "  HS_Religion       0.69      0.68      0.68       157\n",
      "      HS_Race       0.75      0.70      0.72       120\n",
      "  HS_Physical       0.72      0.36      0.48        72\n",
      "    HS_Gender       0.57      0.49      0.53        51\n",
      "     HS_Other       0.76      0.81      0.79       762\n",
      "      HS_Weak       0.72      0.72      0.72       689\n",
      "  HS_Moderate       0.63      0.60      0.61       331\n",
      "    HS_Strong       0.83      0.87      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.79      0.78      5556\n",
      "    macro avg       0.74      0.70      0.71      5556\n",
      " weighted avg       0.78      0.79      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 294.90434432029724 s\n",
      "Averaged - Iteration 10018: Accuracy: 0.9179, F1 Micro: 0.7578, F1 Macro: 0.6535\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 7.412939786911011 seconds\n",
      "New train size: 10218\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3309, Accuracy: 0.8985, F1 Micro: 0.6426, F1 Macro: 0.4113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2248, Accuracy: 0.9175, F1 Micro: 0.7505, F1 Macro: 0.5904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9223, F1 Micro: 0.7654, F1 Macro: 0.6042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1401, Accuracy: 0.9243, F1 Micro: 0.7768, F1 Macro: 0.6642\n",
      "Epoch 5/10, Train Loss: 0.1107, Accuracy: 0.9259, F1 Micro: 0.7767, F1 Macro: 0.6964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9245, F1 Micro: 0.7811, F1 Macro: 0.6992\n",
      "Epoch 7/10, Train Loss: 0.0737, Accuracy: 0.9226, F1 Micro: 0.7754, F1 Macro: 0.712\n",
      "Epoch 8/10, Train Loss: 0.0637, Accuracy: 0.9245, F1 Micro: 0.7712, F1 Macro: 0.7029\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.9222, F1 Micro: 0.7792, F1 Macro: 0.7091\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.9228, F1 Micro: 0.7791, F1 Macro: 0.7195\n",
      "Model 1 - Iteration 10218: Accuracy: 0.9245, F1 Micro: 0.7811, F1 Macro: 0.6992\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.84      0.87      0.86      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.72      0.76      0.74       732\n",
      "     HS_Group       0.72      0.62      0.66       402\n",
      "  HS_Religion       0.77      0.59      0.66       157\n",
      "      HS_Race       0.78      0.63      0.70       120\n",
      "  HS_Physical       0.94      0.22      0.36        72\n",
      "    HS_Gender       0.67      0.47      0.55        51\n",
      "     HS_Other       0.76      0.84      0.80       762\n",
      "      HS_Weak       0.71      0.72      0.72       689\n",
      "  HS_Moderate       0.64      0.53      0.58       331\n",
      "    HS_Strong       0.82      0.90      0.86       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.67      0.70      5556\n",
      " weighted avg       0.78      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 294.7812616825104 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3332, Accuracy: 0.9004, F1 Micro: 0.6611, F1 Macro: 0.4331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.227, Accuracy: 0.9134, F1 Micro: 0.75, F1 Macro: 0.5965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9216, F1 Micro: 0.7676, F1 Macro: 0.6149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1413, Accuracy: 0.9236, F1 Micro: 0.7741, F1 Macro: 0.6601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1111, Accuracy: 0.9234, F1 Micro: 0.7751, F1 Macro: 0.685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0889, Accuracy: 0.924, F1 Micro: 0.7758, F1 Macro: 0.6892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.9239, F1 Micro: 0.7769, F1 Macro: 0.7052\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9228, F1 Micro: 0.7702, F1 Macro: 0.6992\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9191, F1 Micro: 0.7749, F1 Macro: 0.7127\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9224, F1 Micro: 0.7748, F1 Macro: 0.711\n",
      "Model 2 - Iteration 10218: Accuracy: 0.9239, F1 Micro: 0.7769, F1 Macro: 0.7052\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.85      0.85      1134\n",
      "      Abusive       0.87      0.94      0.90       992\n",
      "HS_Individual       0.73      0.73      0.73       732\n",
      "     HS_Group       0.71      0.65      0.68       402\n",
      "  HS_Religion       0.80      0.52      0.63       157\n",
      "      HS_Race       0.79      0.70      0.74       120\n",
      "  HS_Physical       0.74      0.32      0.45        72\n",
      "    HS_Gender       0.58      0.49      0.53        51\n",
      "     HS_Other       0.78      0.79      0.79       762\n",
      "      HS_Weak       0.71      0.71      0.71       689\n",
      "  HS_Moderate       0.62      0.58      0.60       331\n",
      "    HS_Strong       0.90      0.81      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.77      0.78      5556\n",
      "    macro avg       0.76      0.67      0.71      5556\n",
      " weighted avg       0.78      0.77      0.77      5556\n",
      "  samples avg       0.45      0.44      0.42      5556\n",
      "\n",
      "Training completed in 298.934378862381 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3299, Accuracy: 0.8979, F1 Micro: 0.6469, F1 Macro: 0.4247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.228, Accuracy: 0.916, F1 Micro: 0.747, F1 Macro: 0.5851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.179, Accuracy: 0.9231, F1 Micro: 0.7616, F1 Macro: 0.6081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1413, Accuracy: 0.9216, F1 Micro: 0.7771, F1 Macro: 0.665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.11, Accuracy: 0.9248, F1 Micro: 0.7803, F1 Macro: 0.6963\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9162, F1 Micro: 0.7687, F1 Macro: 0.6883\n",
      "Epoch 7/10, Train Loss: 0.0723, Accuracy: 0.9207, F1 Micro: 0.7755, F1 Macro: 0.7004\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9253, F1 Micro: 0.772, F1 Macro: 0.695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0479, Accuracy: 0.925, F1 Micro: 0.7826, F1 Macro: 0.7134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9263, F1 Micro: 0.7844, F1 Macro: 0.7269\n",
      "Model 3 - Iteration 10218: Accuracy: 0.9263, F1 Micro: 0.7844, F1 Macro: 0.7269\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.86      0.86      0.86      1134\n",
      "      Abusive       0.89      0.92      0.91       992\n",
      "HS_Individual       0.74      0.75      0.74       732\n",
      "     HS_Group       0.71      0.64      0.67       402\n",
      "  HS_Religion       0.67      0.66      0.66       157\n",
      "      HS_Race       0.82      0.70      0.76       120\n",
      "  HS_Physical       0.69      0.47      0.56        72\n",
      "    HS_Gender       0.58      0.61      0.60        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.72      0.74      0.73       689\n",
      "  HS_Moderate       0.64      0.56      0.60       331\n",
      "    HS_Strong       0.89      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.79      0.78      0.78      5556\n",
      "    macro avg       0.75      0.71      0.73      5556\n",
      " weighted avg       0.79      0.78      0.78      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 298.7270197868347 s\n",
      "Averaged - Iteration 10218: Accuracy: 0.9181, F1 Micro: 0.7587, F1 Macro: 0.6557\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 200\n",
      "Sampling duration: 4.963056325912476 seconds\n",
      "New train size: 10418\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3278, Accuracy: 0.8953, F1 Micro: 0.702, F1 Macro: 0.5101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2178, Accuracy: 0.9151, F1 Micro: 0.7403, F1 Macro: 0.5859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1699, Accuracy: 0.9204, F1 Micro: 0.7688, F1 Macro: 0.6259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1413, Accuracy: 0.9232, F1 Micro: 0.7762, F1 Macro: 0.6784\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.9235, F1 Micro: 0.7756, F1 Macro: 0.686\n",
      "Epoch 6/10, Train Loss: 0.0877, Accuracy: 0.9169, F1 Micro: 0.7706, F1 Macro: 0.6789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9217, F1 Micro: 0.7777, F1 Macro: 0.7103\n",
      "Epoch 8/10, Train Loss: 0.0572, Accuracy: 0.9239, F1 Micro: 0.7775, F1 Macro: 0.7093\n",
      "Epoch 9/10, Train Loss: 0.0476, Accuracy: 0.9222, F1 Micro: 0.7752, F1 Macro: 0.7039\n",
      "Epoch 10/10, Train Loss: 0.0419, Accuracy: 0.9233, F1 Micro: 0.7715, F1 Macro: 0.6992\n",
      "Model 1 - Iteration 10418: Accuracy: 0.9217, F1 Micro: 0.7777, F1 Macro: 0.7103\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.82      0.89      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.70      0.77      0.74       732\n",
      "     HS_Group       0.70      0.65      0.68       402\n",
      "  HS_Religion       0.65      0.71      0.68       157\n",
      "      HS_Race       0.74      0.65      0.69       120\n",
      "  HS_Physical       0.73      0.33      0.46        72\n",
      "    HS_Gender       0.60      0.55      0.57        51\n",
      "     HS_Other       0.75      0.82      0.78       762\n",
      "      HS_Weak       0.69      0.73      0.71       689\n",
      "  HS_Moderate       0.63      0.58      0.60       331\n",
      "    HS_Strong       0.87      0.86      0.86       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.73      0.71      0.71      5556\n",
      " weighted avg       0.76      0.80      0.78      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 299.59557914733887 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3287, Accuracy: 0.8957, F1 Micro: 0.6967, F1 Macro: 0.5021\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2196, Accuracy: 0.9165, F1 Micro: 0.7421, F1 Macro: 0.5885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1714, Accuracy: 0.9203, F1 Micro: 0.7691, F1 Macro: 0.6256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1425, Accuracy: 0.9246, F1 Micro: 0.7796, F1 Macro: 0.6721\n",
      "Epoch 5/10, Train Loss: 0.1088, Accuracy: 0.9242, F1 Micro: 0.7789, F1 Macro: 0.6834\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9113, F1 Micro: 0.7628, F1 Macro: 0.6627\n",
      "Epoch 7/10, Train Loss: 0.0745, Accuracy: 0.9216, F1 Micro: 0.7742, F1 Macro: 0.7058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0574, Accuracy: 0.9272, F1 Micro: 0.7809, F1 Macro: 0.7082\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9228, F1 Micro: 0.7767, F1 Macro: 0.7171\n",
      "Epoch 10/10, Train Loss: 0.041, Accuracy: 0.9247, F1 Micro: 0.7787, F1 Macro: 0.7175\n",
      "Model 2 - Iteration 10418: Accuracy: 0.9272, F1 Micro: 0.7809, F1 Macro: 0.7082\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.88      0.82      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.77      0.68      0.72       732\n",
      "     HS_Group       0.73      0.70      0.71       402\n",
      "  HS_Religion       0.81      0.64      0.71       157\n",
      "      HS_Race       0.81      0.68      0.74       120\n",
      "  HS_Physical       0.77      0.28      0.41        72\n",
      "    HS_Gender       0.61      0.39      0.48        51\n",
      "     HS_Other       0.79      0.78      0.79       762\n",
      "      HS_Weak       0.75      0.66      0.70       689\n",
      "  HS_Moderate       0.67      0.63      0.65       331\n",
      "    HS_Strong       0.86      0.82      0.84       114\n",
      "\n",
      "    micro avg       0.81      0.75      0.78      5556\n",
      "    macro avg       0.78      0.67      0.71      5556\n",
      " weighted avg       0.81      0.75      0.78      5556\n",
      "  samples avg       0.45      0.42      0.42      5556\n",
      "\n",
      "Training completed in 299.2861080169678 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3279, Accuracy: 0.8951, F1 Micro: 0.7079, F1 Macro: 0.5182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2174, Accuracy: 0.9154, F1 Micro: 0.7443, F1 Macro: 0.585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1705, Accuracy: 0.9208, F1 Micro: 0.7686, F1 Macro: 0.6365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1411, Accuracy: 0.9242, F1 Micro: 0.7766, F1 Macro: 0.6779\n",
      "Epoch 5/10, Train Loss: 0.1067, Accuracy: 0.9251, F1 Micro: 0.7723, F1 Macro: 0.6794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.921, F1 Micro: 0.7775, F1 Macro: 0.6978\n",
      "Epoch 7/10, Train Loss: 0.0694, Accuracy: 0.9239, F1 Micro: 0.7767, F1 Macro: 0.7071\n",
      "Epoch 8/10, Train Loss: 0.056, Accuracy: 0.9261, F1 Micro: 0.7759, F1 Macro: 0.7105\n",
      "Epoch 9/10, Train Loss: 0.0456, Accuracy: 0.9252, F1 Micro: 0.7753, F1 Macro: 0.7119\n",
      "Epoch 10/10, Train Loss: 0.04, Accuracy: 0.9243, F1 Micro: 0.7739, F1 Macro: 0.7132\n",
      "Model 3 - Iteration 10418: Accuracy: 0.921, F1 Micro: 0.7775, F1 Macro: 0.6978\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.81      0.90      0.85      1134\n",
      "      Abusive       0.88      0.92      0.90       992\n",
      "HS_Individual       0.70      0.77      0.73       732\n",
      "     HS_Group       0.67      0.68      0.68       402\n",
      "  HS_Religion       0.71      0.68      0.69       157\n",
      "      HS_Race       0.77      0.70      0.73       120\n",
      "  HS_Physical       1.00      0.21      0.34        72\n",
      "    HS_Gender       0.63      0.43      0.51        51\n",
      "     HS_Other       0.74      0.85      0.79       762\n",
      "      HS_Weak       0.68      0.75      0.71       689\n",
      "  HS_Moderate       0.59      0.61      0.60       331\n",
      "    HS_Strong       0.88      0.78      0.83       114\n",
      "\n",
      "    micro avg       0.76      0.80      0.78      5556\n",
      "    macro avg       0.76      0.69      0.70      5556\n",
      " weighted avg       0.76      0.80      0.77      5556\n",
      "  samples avg       0.45      0.45      0.43      5556\n",
      "\n",
      "Training completed in 301.18900322914124 s\n",
      "Averaged - Iteration 10418: Accuracy: 0.9183, F1 Micro: 0.7594, F1 Macro: 0.6575\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 10535\n",
      "Acquired samples: 117\n",
      "Sampling duration: 2.2233481407165527 seconds\n",
      "New train size: 10535\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3247, Accuracy: 0.905, F1 Micro: 0.6901, F1 Macro: 0.4657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2187, Accuracy: 0.9173, F1 Micro: 0.7339, F1 Macro: 0.5772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.165, Accuracy: 0.9233, F1 Micro: 0.7705, F1 Macro: 0.6222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1376, Accuracy: 0.9237, F1 Micro: 0.7757, F1 Macro: 0.6708\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9257, F1 Micro: 0.7726, F1 Macro: 0.6802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0877, Accuracy: 0.9242, F1 Micro: 0.7764, F1 Macro: 0.6809\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9182, F1 Micro: 0.768, F1 Macro: 0.6873\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9224, F1 Micro: 0.775, F1 Macro: 0.7075\n",
      "Epoch 9/10, Train Loss: 0.0484, Accuracy: 0.9222, F1 Micro: 0.7741, F1 Macro: 0.7043\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.9227, F1 Micro: 0.7691, F1 Macro: 0.7019\n",
      "Model 1 - Iteration 10535: Accuracy: 0.9242, F1 Micro: 0.7764, F1 Macro: 0.6809\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.90      0.89      0.90       992\n",
      "HS_Individual       0.74      0.71      0.73       732\n",
      "     HS_Group       0.71      0.68      0.70       402\n",
      "  HS_Religion       0.78      0.58      0.66       157\n",
      "      HS_Race       0.76      0.68      0.72       120\n",
      "  HS_Physical       0.92      0.15      0.26        72\n",
      "    HS_Gender       0.78      0.27      0.41        51\n",
      "     HS_Other       0.75      0.82      0.79       762\n",
      "      HS_Weak       0.73      0.69      0.71       689\n",
      "  HS_Moderate       0.64      0.59      0.61       331\n",
      "    HS_Strong       0.86      0.83      0.84       114\n",
      "\n",
      "    micro avg       0.79      0.76      0.78      5556\n",
      "    macro avg       0.78      0.65      0.68      5556\n",
      " weighted avg       0.79      0.76      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 302.32883858680725 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3285, Accuracy: 0.9026, F1 Micro: 0.6779, F1 Macro: 0.4348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2204, Accuracy: 0.917, F1 Micro: 0.7354, F1 Macro: 0.578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1659, Accuracy: 0.9233, F1 Micro: 0.7714, F1 Macro: 0.6263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1391, Accuracy: 0.9233, F1 Micro: 0.774, F1 Macro: 0.6683\n",
      "Epoch 5/10, Train Loss: 0.1126, Accuracy: 0.9271, F1 Micro: 0.7734, F1 Macro: 0.6767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0848, Accuracy: 0.9218, F1 Micro: 0.777, F1 Macro: 0.6828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9242, F1 Micro: 0.7794, F1 Macro: 0.6943\n",
      "Epoch 8/10, Train Loss: 0.0566, Accuracy: 0.9217, F1 Micro: 0.7748, F1 Macro: 0.7048\n",
      "Epoch 9/10, Train Loss: 0.0464, Accuracy: 0.9216, F1 Micro: 0.773, F1 Macro: 0.707\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.9233, F1 Micro: 0.7783, F1 Macro: 0.7133\n",
      "Model 2 - Iteration 10535: Accuracy: 0.9242, F1 Micro: 0.7794, F1 Macro: 0.6943\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.85      0.86      0.85      1134\n",
      "      Abusive       0.89      0.91      0.90       992\n",
      "HS_Individual       0.71      0.77      0.74       732\n",
      "     HS_Group       0.76      0.64      0.69       402\n",
      "  HS_Religion       0.76      0.55      0.64       157\n",
      "      HS_Race       0.76      0.65      0.70       120\n",
      "  HS_Physical       0.89      0.24      0.37        72\n",
      "    HS_Gender       0.63      0.37      0.47        51\n",
      "     HS_Other       0.74      0.82      0.78       762\n",
      "      HS_Weak       0.70      0.75      0.72       689\n",
      "  HS_Moderate       0.68      0.55      0.61       331\n",
      "    HS_Strong       0.87      0.83      0.85       114\n",
      "\n",
      "    micro avg       0.78      0.78      0.78      5556\n",
      "    macro avg       0.77      0.66      0.69      5556\n",
      " weighted avg       0.78      0.78      0.77      5556\n",
      "  samples avg       0.44      0.43      0.42      5556\n",
      "\n",
      "Training completed in 305.3300316333771 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.3252, Accuracy: 0.9008, F1 Micro: 0.6688, F1 Macro: 0.4232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2194, Accuracy: 0.9166, F1 Micro: 0.7315, F1 Macro: 0.5591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1673, Accuracy: 0.9227, F1 Micro: 0.7628, F1 Macro: 0.6117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1393, Accuracy: 0.9233, F1 Micro: 0.7731, F1 Macro: 0.6825\n",
      "Epoch 5/10, Train Loss: 0.1108, Accuracy: 0.9234, F1 Micro: 0.756, F1 Macro: 0.6599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.084, Accuracy: 0.9222, F1 Micro: 0.7779, F1 Macro: 0.6915\n",
      "Epoch 7/10, Train Loss: 0.0688, Accuracy: 0.9249, F1 Micro: 0.7759, F1 Macro: 0.6878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0568, Accuracy: 0.9224, F1 Micro: 0.778, F1 Macro: 0.7075\n",
      "Epoch 9/10, Train Loss: 0.0459, Accuracy: 0.9204, F1 Micro: 0.7651, F1 Macro: 0.7035\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9183, F1 Micro: 0.7689, F1 Macro: 0.6964\n",
      "Model 3 - Iteration 10535: Accuracy: 0.9224, F1 Micro: 0.778, F1 Macro: 0.7075\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           HS       0.83      0.88      0.85      1134\n",
      "      Abusive       0.87      0.93      0.90       992\n",
      "HS_Individual       0.69      0.80      0.74       732\n",
      "     HS_Group       0.75      0.58      0.66       402\n",
      "  HS_Religion       0.74      0.61      0.66       157\n",
      "      HS_Race       0.78      0.73      0.76       120\n",
      "  HS_Physical       0.79      0.36      0.50        72\n",
      "    HS_Gender       0.54      0.49      0.52        51\n",
      "     HS_Other       0.76      0.82      0.79       762\n",
      "      HS_Weak       0.67      0.78      0.72       689\n",
      "  HS_Moderate       0.67      0.48      0.56       331\n",
      "    HS_Strong       0.87      0.82      0.85       114\n",
      "\n",
      "    micro avg       0.77      0.79      0.78      5556\n",
      "    macro avg       0.75      0.69      0.71      5556\n",
      " weighted avg       0.77      0.79      0.77      5556\n",
      "  samples avg       0.45      0.44      0.43      5556\n",
      "\n",
      "Training completed in 304.77865767478943 s\n",
      "Averaged - Iteration 10535: Accuracy: 0.9185, F1 Micro: 0.7601, F1 Macro: 0.6588\n",
      "Total sampling time: 1188.87 seconds\n",
      "Total runtime: 20416.44749212265 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6530808,
     "sourceId": 10555762,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20133.802931,
   "end_time": "2025-03-15T13:24:29.112135",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-15T07:48:55.309204",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00ffc955b93942fbb70e1eb81ab6ce6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f38b41ace03a4f1cb319956082a915d5",
       "placeholder": "​",
       "style": "IPY_MODEL_b83f16be07ec4ad8933438ae023c44ac",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00/2.00 [00:00&lt;00:00, 154B/s]"
      }
     },
     "0d6281ebaf9a4880bae9e2d327193c92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "121566d73fba4b58952626d2afab7c1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a3c68e93fee4ab1880af840c1cb28a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_121566d73fba4b58952626d2afab7c1d",
       "placeholder": "​",
       "style": "IPY_MODEL_b93325d53ce64bd1afda9428559960c8",
       "tabbable": null,
       "tooltip": null,
       "value": " 112/112 [00:00&lt;00:00, 11.0kB/s]"
      }
     },
     "3414c2cc34814d1a9c0ce3ecdb3b32af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34fbce2199ac458885e0f1b7dd41d5a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d9db41d2047e47f0a016b02157b8f503",
       "placeholder": "​",
       "style": "IPY_MODEL_67548d027726424cb230d1972df291ed",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "371fdf0f94564fb0b90c9da9c36c8e53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ecfe9c2a8f1e4ce583be855b3d82fa57",
        "IPY_MODEL_f05d009193944c8d999f4d8d64103dd5",
        "IPY_MODEL_6c8ff2ec77c44eef8bd5d50a0e63ae50"
       ],
       "layout": "IPY_MODEL_ba46b9a86f924e42b01d74dc51c3b8df",
       "tabbable": null,
       "tooltip": null
      }
     },
     "444dadf2763c4e9ca05f159222530581": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b116cca38cf946f4ac7aed1c874233dd",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a90e3096b7b64651a6d9f26b57727147",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "4493f896390145698e6bc7f149464cd3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "469d790ca1934406a9f2b8ecb78f6db3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_34fbce2199ac458885e0f1b7dd41d5a6",
        "IPY_MODEL_81fe1a03235a47b68a0df7227aa1fa28",
        "IPY_MODEL_00ffc955b93942fbb70e1eb81ab6ce6f"
       ],
       "layout": "IPY_MODEL_9bdda2f755ac41e0874e285a73e46845",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4b214010b1ea4992a82ff3acd93f79f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_922d48f10f4942bea48230f62c4128f3",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8049bc65e36d4f3c82f0139131e91aea",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "5a22313bebfb4b339c792d6700b825b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "67548d027726424cb230d1972df291ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "67c8ffe12e364c0b9798d423c480b97f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9e948dfa0f3549489f8e04c392882aa0",
       "placeholder": "​",
       "style": "IPY_MODEL_ea7e98045f6145c884397ae8c7c31478",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "6a8b7e1fd47d4675919ad7a9d23de78d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c8ff2ec77c44eef8bd5d50a0e63ae50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4493f896390145698e6bc7f149464cd3",
       "placeholder": "​",
       "style": "IPY_MODEL_6a8b7e1fd47d4675919ad7a9d23de78d",
       "tabbable": null,
       "tooltip": null,
       "value": " 229k/229k [00:00&lt;00:00, 5.53MB/s]"
      }
     },
     "6ddfce64c9284519ae7961d771bcbf52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72abd892b976401f8af9d3948a78c042": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ccbba87496043688263c94ce48221b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8049bc65e36d4f3c82f0139131e91aea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "81fe1a03235a47b68a0df7227aa1fa28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dc64382c78664cfd8aec41dc1c9dd1fa",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0d6281ebaf9a4880bae9e2d327193c92",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "91d43eafd68842b496ebc4a75382e35d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "922d48f10f4942bea48230f62c4128f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9bdda2f755ac41e0874e285a73e46845": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d2a30ecbde646f7b14c6816494c799c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e948dfa0f3549489f8e04c392882aa0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7b4c9591c3641dfb8254878ce3b9c08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c6eb0aa8847248fa87d3fc2bdf6a78fa",
        "IPY_MODEL_444dadf2763c4e9ca05f159222530581",
        "IPY_MODEL_2a3c68e93fee4ab1880af840c1cb28a7"
       ],
       "layout": "IPY_MODEL_7ccbba87496043688263c94ce48221b1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a90e3096b7b64651a6d9f26b57727147": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ae656f2dfc754f99866cc6bec2b93208": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "afc9ec8d7351447ca052a12a92588f3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6ddfce64c9284519ae7961d771bcbf52",
       "placeholder": "​",
       "style": "IPY_MODEL_91d43eafd68842b496ebc4a75382e35d",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.53k/1.53k [00:00&lt;00:00, 146kB/s]"
      }
     },
     "b116cca38cf946f4ac7aed1c874233dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b83f16be07ec4ad8933438ae023c44ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b93325d53ce64bd1afda9428559960c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ba46b9a86f924e42b01d74dc51c3b8df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6eb0aa8847248fa87d3fc2bdf6a78fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e7f1df4c87824ee1959373986c5ed3fa",
       "placeholder": "​",
       "style": "IPY_MODEL_5a22313bebfb4b339c792d6700b825b1",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "d9db41d2047e47f0a016b02157b8f503": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc64382c78664cfd8aec41dc1c9dd1fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de53d9bb3ad84b8d8156781279bb6c35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_67c8ffe12e364c0b9798d423c480b97f",
        "IPY_MODEL_4b214010b1ea4992a82ff3acd93f79f6",
        "IPY_MODEL_afc9ec8d7351447ca052a12a92588f3f"
       ],
       "layout": "IPY_MODEL_72abd892b976401f8af9d3948a78c042",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e7f1df4c87824ee1959373986c5ed3fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea7e98045f6145c884397ae8c7c31478": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ecfe9c2a8f1e4ce583be855b3d82fa57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3414c2cc34814d1a9c0ce3ecdb3b32af",
       "placeholder": "​",
       "style": "IPY_MODEL_ef3da7a1d71e4771a24bcea18d8fff54",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: 100%"
      }
     },
     "ef3da7a1d71e4771a24bcea18d8fff54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f05d009193944c8d999f4d8d64103dd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9d2a30ecbde646f7b14c6816494c799c",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ae656f2dfc754f99866cc6bec2b93208",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "f38b41ace03a4f1cb319956082a915d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
